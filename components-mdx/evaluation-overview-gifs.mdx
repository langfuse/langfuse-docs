<Tabs items={["Analytics", "User Feedback", "LLM-as-a-Judge", "Prompt Experiment", "Annotation Queue", "Custom Evals"]}>
<Tab>

Plot evaluation results in the Langfuse Dashboard.

<Video
  src="https://static.langfuse.com/docs-videos/scores-dashboard.mp4"
  aspectRatio={16 / 9}
  gifStyle
/>

</Tab>
<Tab>

Collect feedback from your users. Can be captured in the frontend via our Browser SDK, server-side via the SDKs or API. Video includes example application.

<Video
  src="https://static.langfuse.com/docs-videos/scores-user-feedback.mp4"
  aspectRatio={16 / 9}
  gifStyle
/>

</Tab>
<Tab>

Run fully managed LLM-as-a-judge evaluations on production or development traces. Can be applied to any step within your application for step-wise evaluations.

<Video
  src="https://static.langfuse.com/docs-videos/scores-llm-as-a-judge.mp4%20MOVED%20TO%20R2.mp4"
  aspectRatio={16 / 9}
  gifStyle
/>

</Tab>
<Tab>

Evaluate prompts and models on datasets directly in the user interface. No custom code is needed.

<Video
  src="https://static.langfuse.com/docs-videos/prompt-experiments.mp4"
  aspectRatio={16 / 9}
  gifStyle
/>

</Tab>

<Tab>

Baseline your evaluation workflow with human annotations via Annotation Queues.

<Video
  src="https://static.langfuse.com/docs-videos/scores-annotation-queue.mp4"
  aspectRatio={16 / 9}
  gifStyle
/>

</Tab>
<Tab>

Add custom evaluation results, supports numeric, boolean and categorical values.

```bash
POST /api/public/scores
```

Add scores via Python or JS SDK.

```python filename="Example (Python)"
langfuse.score(
  trace_id="123",
  name="my_custom_evaluator",
  value=0.5,
)
```

</Tab>
</Tabs>

## Add Additional Attributes

Langfuse allows you to pass additional attributes to your spans. These can include `user_id`, `tags`, `session_id`, and custom `metadata`. Enriching traces with these details is important for analysis, debugging, and monitoring of your application's behavior across different users or sessions.

The following code demonstrates how to start a custom span with `langfuse.start_as_current_span` and then update the trace associated with this span using `span.update_trace()`. 

**→ Learn more about [Updating Trace and Span Attributes](https://langfuse.com/docs/sdk/python/sdk-v3#updating-observations).**


```python
from langfuse import get_client
 
langfuse = get_client()

with langfuse.start_as_current_span(
    name="your-trace",
    ) as span:
    
    # Run your application here
    result = your_application(input)
    print(result)

    # Pass additional attributes to the span
    span.update_trace(
        input=input,
        output=result,
        user_id="user_123",
        session_id="session_abc",
        tags=["dev", "your-tag"],
        metadata={"email": "user@langfuse.com"},
        version="1.0.0"
        )

# Flush events in short-lived applications
langfuse.flush()
```

## Score Traces and Spans

[Scores](https://langfuse.com/docs/scores/overview) are used to evaluate single observations or entire traces. They enables you to implement custom quality checks at runtime or facilitate human-in-the-loop evaluation processes.

In the example below, we demonstrate how to score a specific span for `relevance` (a numeric score) and the overall trace for `feedback` (a categorical score). This helps in systematically assessing and improving your application.

**→ Learn more about [Custom Scores in Langfuse](https://langfuse.com/docs/scores/custom).**


```python
from langfuse import get_client
 
langfuse = get_client()

with langfuse.start_as_current_span(
    name="your-trace",
    ) as span:
    
    # Run your application here
    result = your_application(input)
    print(result)
    
    # Score this specific span
    span.score(name="relevance", value=0.9, data_type="NUMERIC")

    # Score the overall trace
    span.score_trace(name="feedback", value="positive", data_type="CATEGORICAL")

# Flush events in short-lived applications
langfuse.flush()

```

## Dataset Experiments

Offline evaluation using datasets is a critical part of the LLM development lifecycle. Langfuse supports this through Dataset Experiments. The typical workflow involves:

1.  **Benchmark Dataset**: Defining a dataset with input prompts and their corresponding expected outputs.
2.  **Application Run**: Running your LLM application against each item in the dataset.
3.  **Evaluation**: Comparing the generated outputs against the expected results or using other scoring mechanisms (e.g., model-based evaluation) to assess performance.

The following example demonstrates how to use a pre-existing dataset containing development tasks to run an experiment with your application.

**→ Learn more about [Langfuse Dataset Experiments](https://langfuse.com/docs/datasets/overview).**

```python
from langfuse import get_client
 
langfuse = get_client()
 
# Fetch an existing dataset
dataset = langfuse.get_dataset(name="your-dataset")

```

Next, we iterate through each item in the dataset, run our CrewAI application (`your_application`) with the item's input, and log the results as a run associated with that dataset item in Langfuse. This allows for structured evaluation and comparison of different application versions or prompt configurations.

The `item.run()` context manager is used to create a new trace for each dataset item processed in the experiment. Optionally you can score the dataset runs.


```python
for item in dataset.items:
    # Use the item.run() context manager for automatic trace linking
    with item.run(
        run_name="<run_name>",
        run_description="My first run",
        run_metadata={"model": "llama3"},
    ) as root_span:
        # All operations within this block are part of the trace for this dataset item
        
        # Call your application logic - this can use any combination of decorators, 
        # context managers, or manual observations
        with langfuse.start_as_current_generation(
            name="llm-call",
            model="gpt-4o",
            input=item.input
        ) as generation:
            # Your LLM application logic here
            output = my_llm_application.run(item.input)
            generation.update(output=output)
 
        # Optionally, score the result against the expected output
        root_span.score_trace(
            name="<example_eval>",
            value=my_eval_fn(item.input, output, item.expected_output),
            comment="This is a comment",  # optional, useful to add reasoning
        )
 
# Flush the langfuse client to ensure all data is sent to the server at the end of the experiment run
langfuse.flush()
```

## More Langfuse Features

Langfuse offers more features to enhance your LLM development and observability workflow:

- [LLM-as-a-Judge Evaluators](https://langfuse.com/docs/scores/model-based-evals)
- [Custom Dashboards](https://langfuse.com/docs/analytics/custom-dashboards)
- [LLM Playground](https://langfuse.com/docs/playground)
- [Prompt Management](https://langfuse.com/docs/prompts/get-started)
- [Prompt Experiments](https://langfuse.com/docs/datasets/prompt-experiments)
- [More Integrations](https://langfuse.com/docs/integrations/overview)
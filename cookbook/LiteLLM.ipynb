{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b26fc84-994d-4e93-9e57-6a427561ac86",
   "metadata": {},
   "source": [
    "# LiteLLM Cookbook\n",
    "\n",
    "The LiteLLM Proxy simplifies your work by standardizing API interactions across multiple providers such as OpenAI, Azure, Anthropic, and others. It removes the complexity of direct API calls by centralizing interactions with over 50 large language model APIs through a single endpoint, allowing you to focus on innovation rather than integration details.\n",
    "\n",
    "Let's dive into how you can set up and start using LiteLLM with Langfuse:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee92c44-cc7c-4bdb-9153-0ddc8c7c07b0",
   "metadata": {},
   "source": [
    "### Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74e39484-8901-4c98-95ec-911ea9f170b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langfuse in /Users/richardkruemmel/Code/langfuse-docs/.venv/lib/python3.12/site-packages (2.26.1)\n",
      "Requirement already satisfied: openai in /Users/richardkruemmel/Code/langfuse-docs/.venv/lib/python3.12/site-packages (1.17.1)\n",
      "Requirement already satisfied: litellm[proxy] in /Users/richardkruemmel/Code/langfuse-docs/.venv/lib/python3.12/site-packages (1.35.5)\n",
      "Requirement already satisfied: PyJWT<3.0.0,>=2.8.0 in /Users/richardkruemmel/Code/langfuse-docs/.venv/lib/python3.12/site-packages (from litellm[proxy]) (2.8.0)\n",
      "Requirement already satisfied: aiohttp in /Users/richardkruemmel/Code/langfuse-docs/.venv/lib/python3.12/site-packages (from litellm[proxy]) (3.9.4)\n",
      "Requirement already satisfied: apscheduler<4.0.0,>=3.10.4 in /Users/richardkruemmel/Code/langfuse-docs/.venv/lib/python3.12/site-packages (from litellm[proxy]) (3.10.4)\n",
      "Requirement already satisfied: backoff in /Users/richardkruemmel/Code/langfuse-docs/.venv/lib/python3.12/site-packages (from litellm[proxy]) (2.2.1)\n",
      "Requirement already satisfied: click in /Users/richardkruemmel/Code/langfuse-docs/.venv/lib/python3.12/site-packages (from litellm[proxy]) (8.1.7)\n",
      "Requirement already satisfied: cryptography<43.0.0,>=42.0.5 in /Users/richardkruemmel/Code/langfuse-docs/.venv/lib/python3.12/site-packages (from litellm[proxy]) (42.0.5)\n",
      "Requirement already satisfied: fastapi<0.110.0,>=0.109.1 in /Users/richardkruemmel/Code/langfuse-docs/.venv/lib/python3.12/site-packages (from litellm[proxy]) (0.109.2)\n",
      "Requirement already satisfied: fastapi-sso<0.11.0,>=0.10.0 in /Users/richardkruemmel/Code/langfuse-docs/.venv/lib/python3.12/site-packages (from litellm[proxy]) (0.10.0)\n",
      "Requirement already satisfied: gunicorn<22.0.0,>=21.2.0 in /Users/richardkruemmel/Code/langfuse-docs/.venv/lib/python3.12/site-packages (from litellm[proxy]) (21.2.0)\n",
      "Requirement already satisfied: importlib-metadata>=6.8.0 in /Users/richardkruemmel/Code/langfuse-docs/.venv/lib/python3.12/site-packages (from litellm[proxy]) (7.1.0)\n",
      "Requirement already satisfied: jinja2<4.0.0,>=3.1.2 in /Users/richardkruemmel/Code/langfuse-docs/.venv/lib/python3.12/site-packages (from litellm[proxy]) (3.1.3)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.7 in /Users/richardkruemmel/Code/langfuse-docs/.venv/lib/python3.12/site-packages (from litellm[proxy]) (3.10.0)\n",
      "Requirement already satisfied: python-dotenv>=0.2.0 in /Users/richardkruemmel/Code/langfuse-docs/.venv/lib/python3.12/site-packages (from litellm[proxy]) (1.0.1)\n",
      "Requirement already satisfied: python-multipart<0.0.10,>=0.0.9 in /Users/richardkruemmel/Code/langfuse-docs/.venv/lib/python3.12/site-packages (from litellm[proxy]) (0.0.9)\n",
      "Requirement already satisfied: pyyaml<7.0.0,>=6.0.1 in /Users/richardkruemmel/Code/langfuse-docs/.venv/lib/python3.12/site-packages (from litellm[proxy]) (6.0.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.31.0 in /Users/richardkruemmel/Code/langfuse-docs/.venv/lib/python3.12/site-packages (from litellm[proxy]) (2.31.0)\n",
      "Requirement already satisfied: rq in /Users/richardkruemmel/Code/langfuse-docs/.venv/lib/python3.12/site-packages (from litellm[proxy]) (1.16.1)\n",
      "Requirement already satisfied: tiktoken>=0.4.0 in /Users/richardkruemmel/Code/langfuse-docs/.venv/lib/python3.12/site-packages (from litellm[proxy]) (0.6.0)\n",
      "Requirement already satisfied: tokenizers in /Users/richardkruemmel/Code/langfuse-docs/.venv/lib/python3.12/site-packages (from litellm[proxy]) (0.15.2)\n",
      "Requirement already satisfied: uvicorn<0.23.0,>=0.22.0 in /Users/richardkruemmel/Code/langfuse-docs/.venv/lib/python3.12/site-packages (from litellm[proxy]) (0.22.0)\n",
      "Requirement already satisfied: httpx<1.0,>=0.15.4 in /Users/richardkruemmel/Code/langfuse-docs/.venv/lib/python3.12/site-packages (from langfuse) (0.27.0)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in /Users/richardkruemmel/Code/langfuse-docs/.venv/lib/python3.12/site-packages (from langfuse) (23.2)\n",
      "Requirement already satisfied: pydantic<3.0,>=1.10.7 in /Users/richardkruemmel/Code/langfuse-docs/.venv/lib/python3.12/site-packages (from langfuse) (2.7.0)\n",
      "Requirement already satisfied: wrapt<2.0,>=1.14 in /Users/richardkruemmel/Code/langfuse-docs/.venv/lib/python3.12/site-packages (from langfuse) (1.16.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/richardkruemmel/Code/langfuse-docs/.venv/lib/python3.12/site-packages (from openai) (4.3.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/richardkruemmel/Code/langfuse-docs/.venv/lib/python3.12/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: sniffio in /Users/richardkruemmel/Code/langfuse-docs/.venv/lib/python3.12/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /Users/richardkruemmel/Code/langfuse-docs/.venv/lib/python3.12/site-packages (from openai) (4.66.2)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in /Users/richardkruemmel/Code/langfuse-docs/.venv/lib/python3.12/site-packages (from openai) (4.11.0)\n",
      "Requirement already satisfied: idna>=2.8 in /Users/richardkruemmel/Code/langfuse-docs/.venv/lib/python3.12/site-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
      "Requirement already satisfied: six>=1.4.0 in /Users/richardkruemmel/Code/langfuse-docs/.venv/lib/python3.12/site-packages (from apscheduler<4.0.0,>=3.10.4->litellm[proxy]) (1.16.0)\n",
      "Requirement already satisfied: pytz in /Users/richardkruemmel/Code/langfuse-docs/.venv/lib/python3.12/site-packages (from apscheduler<4.0.0,>=3.10.4->litellm[proxy]) (2024.1)\n",
      "Requirement already satisfied: tzlocal!=3.*,>=2.0 in /Users/richardkruemmel/Code/langfuse-docs/.venv/lib/python3.12/site-packages (from apscheduler<4.0.0,>=3.10.4->litellm[proxy]) (5.2)\n",
      "Requirement already satisfied: cffi>=1.12 in /Users/richardkruemmel/Code/langfuse-docs/.venv/lib/python3.12/site-packages (from cryptography<43.0.0,>=42.0.5->litellm[proxy]) (1.16.0)\n",
      "Requirement already satisfied: starlette<0.37.0,>=0.36.3 in /Users/richardkruemmel/Code/langfuse-docs/.venv/lib/python3.12/site-packages (from fastapi<0.110.0,>=0.109.1->litellm[proxy]) (0.36.3)\n",
      "Requirement already satisfied: oauthlib>=3.1.0 in /Users/richardkruemmel/Code/langfuse-docs/.venv/lib/python3.12/site-packages (from fastapi-sso<0.11.0,>=0.10.0->litellm[proxy]) (3.2.2)\n",
      "Requirement already satisfied: certifi in /Users/richardkruemmel/Code/langfuse-docs/.venv/lib/python3.12/site-packages (from httpx<1.0,>=0.15.4->langfuse) (2024.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/richardkruemmel/Code/langfuse-docs/.venv/lib/python3.12/site-packages (from httpx<1.0,>=0.15.4->langfuse) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/richardkruemmel/Code/langfuse-docs/.venv/lib/python3.12/site-packages (from httpcore==1.*->httpx<1.0,>=0.15.4->langfuse) (0.14.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/richardkruemmel/Code/langfuse-docs/.venv/lib/python3.12/site-packages (from importlib-metadata>=6.8.0->litellm[proxy]) (3.18.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/richardkruemmel/Code/langfuse-docs/.venv/lib/python3.12/site-packages (from jinja2<4.0.0,>=3.1.2->litellm[proxy]) (2.1.5)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/richardkruemmel/Code/langfuse-docs/.venv/lib/python3.12/site-packages (from pydantic<3.0,>=1.10.7->langfuse) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.1 in /Users/richardkruemmel/Code/langfuse-docs/.venv/lib/python3.12/site-packages (from pydantic<3.0,>=1.10.7->langfuse) (2.18.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/richardkruemmel/Code/langfuse-docs/.venv/lib/python3.12/site-packages (from requests<3.0.0,>=2.31.0->litellm[proxy]) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/richardkruemmel/Code/langfuse-docs/.venv/lib/python3.12/site-packages (from requests<3.0.0,>=2.31.0->litellm[proxy]) (2.2.1)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /Users/richardkruemmel/Code/langfuse-docs/.venv/lib/python3.12/site-packages (from tiktoken>=0.4.0->litellm[proxy]) (2023.12.25)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/richardkruemmel/Code/langfuse-docs/.venv/lib/python3.12/site-packages (from aiohttp->litellm[proxy]) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/richardkruemmel/Code/langfuse-docs/.venv/lib/python3.12/site-packages (from aiohttp->litellm[proxy]) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/richardkruemmel/Code/langfuse-docs/.venv/lib/python3.12/site-packages (from aiohttp->litellm[proxy]) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/richardkruemmel/Code/langfuse-docs/.venv/lib/python3.12/site-packages (from aiohttp->litellm[proxy]) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/richardkruemmel/Code/langfuse-docs/.venv/lib/python3.12/site-packages (from aiohttp->litellm[proxy]) (1.9.4)\n",
      "Requirement already satisfied: redis>=3.5 in /Users/richardkruemmel/Code/langfuse-docs/.venv/lib/python3.12/site-packages (from rq->litellm[proxy]) (5.0.3)\n",
      "Requirement already satisfied: huggingface_hub<1.0,>=0.16.4 in /Users/richardkruemmel/Code/langfuse-docs/.venv/lib/python3.12/site-packages (from tokenizers->litellm[proxy]) (0.22.2)\n",
      "Requirement already satisfied: pycparser in /Users/richardkruemmel/Code/langfuse-docs/.venv/lib/python3.12/site-packages (from cffi>=1.12->cryptography<43.0.0,>=42.0.5->litellm[proxy]) (2.22)\n",
      "Requirement already satisfied: filelock in /Users/richardkruemmel/Code/langfuse-docs/.venv/lib/python3.12/site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers->litellm[proxy]) (3.13.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/richardkruemmel/Code/langfuse-docs/.venv/lib/python3.12/site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers->litellm[proxy]) (2024.3.1)\n",
      "Requirement already satisfied: email-validator>=2.0.0 in /Users/richardkruemmel/Code/langfuse-docs/.venv/lib/python3.12/site-packages (from pydantic[email]>=1.8.0->fastapi-sso<0.11.0,>=0.10.0->litellm[proxy]) (2.1.1)\n",
      "Requirement already satisfied: dnspython>=2.0.0 in /Users/richardkruemmel/Code/langfuse-docs/.venv/lib/python3.12/site-packages (from email-validator>=2.0.0->pydantic[email]>=1.8.0->fastapi-sso<0.11.0,>=0.10.0->litellm[proxy]) (2.6.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install \"litellm[proxy]\" langfuse openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc213d5-116e-4a99-95b0-423bbb6bfb75",
   "metadata": {},
   "source": [
    "### Setup environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c69298dc-6c55-46e4-801f-dd74f47a8675",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from langfuse.openai import auth_check\n",
    "\n",
    "os.environ[\"LANGFUSE_PUBLIC_KEY\"] = \"\"\n",
    "os.environ[\"LANGFUSE_SECRET_KEY\"] = \"\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
    "\n",
    "# Test connection to Langfuse\n",
    "auth_check()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d92f6fdd-48f1-4f40-bf58-17be90b8e3b5",
   "metadata": {},
   "source": [
    "### Setup Lite LLM Proxy\n",
    "\n",
    "1. Create a config file called litellm_config.yaml that looks like this:\n",
    "        ```\n",
    "        model_list:\n",
    "          - model_name: gpt-3.5-turbo\n",
    "            litellm_params:\n",
    "              model: gpt-3.5-turbo\n",
    "              api_key: \n",
    "          - model_name: ollama/llama2\n",
    "            litellm_params:\n",
    "              model: ollama/llama2\n",
    "          - model_name: ollama/mistral\n",
    "            litellm_params:\n",
    "              model: ollama/mistral\n",
    "        ```\n",
    "2. Add your OpenAI API Key\n",
    "3. Ensure that you have pulled the llama2 (7b) and mistral (7b) ollama models.\n",
    "4. Run in the CLI the following command: `litellm --config litellm_config.yaml`\n",
    "\n",
    "The Lite LLM Proxy should be now running on http://0.0.0.0:4000\n",
    "\n",
    "To verify the connection you can run `litellm --test`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "042146c5-679b-4259-a55c-56aaf51bdfd7",
   "metadata": {},
   "source": [
    "### Example using Langfuse Wrapped OpenAI SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aeb4761a-7387-4375-aab0-da02c32f1415",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "from langfuse.openai import openai\n",
    "\n",
    "# Set PROXY_URL to the url of your lite_llm_proxy (by default: http://0.0.0.0:4000)\n",
    "PROXY_URL=\"http://0.0.0.0:4000\"\n",
    "\n",
    "system_prompt = \"You are a very accurate calculator. You output only the result of the calculation.\"\n",
    "\n",
    "client = openai.OpenAI(\n",
    "    base_url=PROXY_URL,\n",
    ")\n",
    "gpt_completion = client.chat.completions.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "      {\"role\": \"system\", \"content\": system_prompt},\n",
    "      {\"role\": \"user\", \"content\": \"1 + 1 = \"}],\n",
    ")\n",
    "print(gpt_completion.choices[0].message.content)\n",
    "\n",
    "llama_completion = client.chat.completions.create(\n",
    "  model=\"ollama/llama2\",\n",
    "  messages=[\n",
    "      {\"role\": \"system\", \"content\": system_prompt},\n",
    "      {\"role\": \"user\", \"content\": \"3 + 3 = \"}],\n",
    ")\n",
    "print(llama_completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4144dcf-3478-4e3f-bed2-a394e8ccc253",
   "metadata": {},
   "source": [
    "### Example using @observe() decorator\n",
    "\n",
    "The @observe() decorator integrates tracing directly into your Python applications, automatically capturing and logging execution details such as inputs, outputs, timings, and more. The decorator simplifies achieving in-depth observability in your applications with minimal code.\n",
    "\n",
    "For more details on how to utilize this decorator and customize your tracing, refer to our [documentation](https://langfuse.com/docs/sdk/python/decorators).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1aeaa015-6b6b-4ac8-8e07-55e1783a398e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rap 1: I grab the mic and ignite the crowd with my lyrical might, turning up the heat with every rhyme that I recite.\n",
      "Rap 2: \n",
      "\"Ground zero, where the party's at its prime,\n",
      "Lights, cameras, action, we're living large in time,\n",
      "Rhymes so tight, they'll make your speakers sing,\n",
      "Get ready to flow with me, let the magic blend.\"\n",
      "Rap 3:  \"Microphone master, spinning words so slick,\n",
      "From the concrete jungle to the desert tick,\n",
      "Lyrical acrobatics, I'm taking flight,\n",
      "Got that fire in my heart, igniting the night.\"\n"
     ]
    }
   ],
   "source": [
    "from langfuse.decorators import observe\n",
    "from langfuse.openai import openai\n",
    "\n",
    "@observe()\n",
    "def rap_battle():\n",
    "    client = openai.OpenAI(\n",
    "        base_url=PROXY_URL,\n",
    "    )\n",
    "\n",
    "    system_prompt = \"You are a rap artist. Drop a fresh line.\"\n",
    "\n",
    "    # First model starts the rap\n",
    "    gpt_completion = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": \"Kick it off, here's the mic...\"}\n",
    "        ],\n",
    "    )\n",
    "    first_rap = gpt_completion.choices[0].message.content\n",
    "    print(\"Rap 1:\", first_rap)\n",
    "\n",
    "    # Second model responds\n",
    "    llama_completion = client.chat.completions.create(\n",
    "        model=\"ollama/llama2\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": first_rap}\n",
    "        ],\n",
    "    )\n",
    "    second_rap = llama_completion.choices[0].message.content\n",
    "    print(\"Rap 2:\", second_rap)\n",
    "\n",
    "    # Third model continues\n",
    "    mistral_completion = client.chat.completions.create(\n",
    "        model=\"ollama/mistral\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": second_rap}\n",
    "        ],\n",
    "    )\n",
    "    third_rap = mistral_completion.choices[0].message.content\n",
    "    print(\"Rap 3:\", third_rap)\n",
    "\n",
    "# Call the function\n",
    "rap_battle()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

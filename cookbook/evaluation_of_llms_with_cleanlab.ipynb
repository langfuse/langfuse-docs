{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Langfuse Integration with Cleanlab\"\n",
    "description: \"Automatically evaluate LLMs in real time with Cleanlab's trustworthy Language Model (TLM)\"\n",
    "---\n",
    "\n",
    "# Automated Evaluations with Cleanlab\n",
    "\n",
    "Cleanlabâ€™s [Trustworthy Language Model](https://cleanlab.ai/tlm/) (TLM) enables Langfuse users to quickly identify low quality and hallucinated responses from any LLM trace.\n",
    "\n",
    "## What is TLM?\n",
    "\n",
    "TLM is an automated evaluation tool that add reliability and explainability to every LLM output. TLM automatically finds the poor quality and incorrect LLM responses lurking within your production logs and traces. This helps you perform better Evals, with significantly less manual review and annotation work to find these bad responses yourself. TLM also enables smart-routing for LLM-automated responses and decision-making using trustworthiness scores for every LLM output.\n",
    "\n",
    "**TLM provides users with:**\n",
    "- Trustworthiness scores and explanation for every LLM response\n",
    "- Higher accuracy: rigorous [benchmarks](https://cleanlab.ai/blog/trustworthy-language-model/) show TLM consistently produces more accurate results than other LLMs like GPT 4/4o and Claude.\n",
    "- Scalable API: designed to handle large datasets, TLM is suitable for most enterprise applications, including data extraction, tagging/labeling, Q&A (RAG), and more.\n",
    "\n",
    "## Getting Started\n",
    "\n",
    "This guide will walk you through the process of evaluating LLM responses captured in Langfuse with Cleanlab's Trustworthy Language Models (TLM)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install dependencies & Set environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q langfuse openai cleanlab-tlm --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from getpass import getpass\n",
    "import dotenv\n",
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### API Keys\n",
    "\n",
    "This guide requires a Cleanlab TLM API key. If you don't have one, you can sign up for a free trial [here](https://tlm.cleanlab.ai/).\n",
    "\n",
    "This guide requires four API keys:\n",
    "- [Langfuse Public Key](https://us.cloud.langfuse.com/)\n",
    "- [Langfuse Secret Key](https://us.cloud.langfuse.com/)\n",
    "- [OpenAI API Key](https://platform.openai.com/api-keys)\n",
    "- [Cleanlab TLM API Key](https://tlm.cleanlab.ai/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get keys for your project from the project settings page: https://cloud.langfuse.com\n",
    "\n",
    "os.environ[\"LANGFUSE_PUBLIC_KEY\"] = \"pk-lf-...\" \n",
    "os.environ[\"LANGFUSE_SECRET_KEY\"] = \"sk-lf-...\"\n",
    "os.environ[\"LANGFUSE_HOST\"] = \"https://cloud.langfuse.com\" # ðŸ‡ªðŸ‡º EU region\n",
    "# os.environ[\"LANGFUSE_HOST\"] = \"https://us.cloud.langfuse.com\" # ðŸ‡ºðŸ‡¸ US region\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"<openai_api_key>\"\n",
    "\n",
    "os.environ[\"CLEANLAB_TLM_API_KEY\"] = \"<cleanlab_tlm_api_key>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare trace dataset and load into Langfuse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the sake of demonstration purposes, we'll briefly generate some traces and track them in Langfuse. Typically, you would have already captured traces in Langfuse and would skip to \"Download trace dataset from Langfuse\"\n",
    "\n",
    "NOTE: TLM requires the entire input to the LLM to be provided. This includes any system prompts, context, or other information that was originally provided to the LLM to generate the response. Notice below that we include the system prompt in the trace metadata since by default the trace does not include the system prompt within the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langfuse import observe, get_client\n",
    "langfuse = get_client()\n",
    "from openai import OpenAI\n",
    "\n",
    "openai = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1: What is the 3rd month of the year in alphabetical order?\n",
      "Answer 1:\n",
      "March\n",
      "\n",
      "Question 2: What is the capital of France?\n",
      "Answer 2:\n",
      "The capital of France is Paris.\n",
      "\n",
      "Question 3: How many seconds are in 100 years?\n",
      "Answer 3:\n",
      "There are 31,536,000 seconds in a year (60 seconds x 60 minutes x 24 hours x 365 days). Therefore, in 100 years, there would be 3,153,600,000 seconds.\n",
      "\n",
      "Question 4: Alice, Bob, and Charlie went to a cafÃ©. Alice paid twice as much as Bob, and Bob paid three times as much as Charlie. If the total bill was $72, how much did each person pay?\n",
      "Answer 4:\n",
      "Let's call the amount Charlie paid x. \n",
      "Alice paid twice as much as Bob, so she paid 2*(3x) = 6x.\n",
      "Bob paid three times as much as Charlie, so he paid 3x.\n",
      "\n",
      "We know the total bill was $72:\n",
      "x + 6x + 3x = 72\n",
      "10x = 72\n",
      "x = 7.2 \n",
      "\n",
      "Therefore, Charlie paid $7.20, Bob paid $21.60, and Alice paid $43.20.\n",
      "\n",
      "Question 5: When was the Declaration of Independence signed?\n",
      "Answer 5:\n",
      "The Declaration of Independence was signed on July 4, 1776.\n",
      "\n",
      "Generated 5 answers and tracked them in Langfuse.\n"
     ]
    }
   ],
   "source": [
    "# Let's use some tricky trivia questions to generate some traces\n",
    "trivia_questions = [    \n",
    "    \"What is the 3rd month of the year in alphabetical order?\",\n",
    "    \"What is the capital of France?\",\n",
    "    \"How many seconds are in 100 years?\",\n",
    "    \"Alice, Bob, and Charlie went to a cafÃ©. Alice paid twice as much as Bob, and Bob paid three times as much as Charlie. If the total bill was $72, how much did each person pay?\",\n",
    "    \"When was the Declaration of Independence signed?\"\n",
    "]\n",
    "\n",
    "@observe()\n",
    "def generate_answers(trivia_question):\n",
    "    system_prompt = \"You are a trivia master.\"\n",
    "\n",
    "    # Update the trace with the question    \n",
    "    langfuse.update_current_trace(\n",
    "        name=f\"Answering question: '{trivia_question}'\",\n",
    "        tags=[\"TLM_eval_pipeline\"],\n",
    "        metadata={\"system_prompt\": system_prompt}\n",
    "    )\n",
    "\n",
    "    response = openai.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": trivia_question},\n",
    "        ],\n",
    "    )\n",
    "    \n",
    "    answer = response.choices[0].message.content\n",
    "    return answer\n",
    "\n",
    "\n",
    "# Generate answers\n",
    "answers = []\n",
    "for i in range(len(trivia_questions)):\n",
    "    answer = generate_answers(trivia_questions[i])\n",
    "    answers.append(answer)  \n",
    "    print(f\"Question {i+1}: {trivia_questions[i]}\")\n",
    "    print(f\"Answer {i+1}:\\n{answer}\\n\")\n",
    "\n",
    "print(f\"Generated {len(answers)} answers and tracked them in Langfuse.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember, the goal of this tutorial is to show you how to build an external evaluation pipeline. These pipelines will run in your CI/CD environment, or be run in a different orchestrated container service. No matter the environment you choose, three key steps always apply:\n",
    "\n",
    "\n",
    "1.   **Fetch Your Traces**: Get your application traces to your evaluation environment\n",
    "2.   **Run Your Evaluations**: Apply any evaluation logic you prefer\n",
    "3.   **Save Your Results**: Attach your evaluations back to the Langfuse trace used for calculating them.\n",
    "\n",
    "For the rest of the notebook, we'll have one goal:\n",
    "\n",
    "---\n",
    "\n",
    "ðŸŽ¯ Goal: ***Evaluate all traces run in the past 24 hours***\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download trace dataset from Langfuse\n",
    "\n",
    "Fetching traces from Langfuse is straightforward. Just set up the Langfuse client and use one of its functions to fetch the data. We'll fetch the traces and evaluate them. After that, we'll add our scores back into Langfuse.\n",
    "\n",
    "The `fetch_traces()` function has arguments to filter the traces by tags, timestamps, and beyond. You can find more about other methods to [query traces](https://langfuse.com/docs/query-traces) in our docs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langfuse import Langfuse\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "langfuse = Langfuse()\n",
    "now = datetime.now()\n",
    "one_day_ago = now - timedelta(hours=24)\n",
    "\n",
    "traces = langfuse.fetch_traces(\n",
    "    tags=\"TLM_eval_pipeline\",\n",
    "    from_timestamp=one_day_ago,\n",
    "    to_timestamp=now,\n",
    ").data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate evaluations with TLM\n",
    "\n",
    "Langfuse can handle numerical, boolean and categorical (`string`) scores.  Wrapping your custom evaluation logic in a function is often a good practice.\n",
    "\n",
    "Instead of running TLM individually on each trace, we'll provide all of the prompt, response pairs in a list to TLM in a single call. This is more efficient and allows us to get scores and explanations for all of the traces at once. Then, using the `trace.id`, we can attach the scores and explanations back to the correct trace in Langfuse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cleanlab_tlm import TLM\n",
    "\n",
    "tlm = TLM(options={\"log\": [\"explanation\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This helper method will extract the prompt and response from each trace and return three lists: trace ID's, prompts, and responses.\n",
    "def get_prompt_response_pairs(traces):\n",
    "    prompts = []\n",
    "    responses = []\n",
    "    for trace in traces:\n",
    "        prompts.append(trace.metadata[\"system_prompt\"] + \"\\n\" + trace.input[\"args\"][0])\n",
    "        responses.append(trace.output)\n",
    "    return prompts, responses\n",
    "\n",
    "trace_ids = [trace.id for trace in traces]\n",
    "prompts, responses = get_prompt_response_pairs(traces)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's use TLM to generate a `trustworthiness score` and `explanation` for each trace.\n",
    "\n",
    "**IMPORTANT:** It is essential to always include any system prompts, context, or other information that was originally provided to the LLM to generate the response. You should construct the prompt input to `get_trustworthiness_score()` in a way that is as similar as possible to the original prompt. This is why we included the system prompt in the trace metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Querying TLM... 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ|\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trace_id</th>\n",
       "      <th>prompt</th>\n",
       "      <th>response</th>\n",
       "      <th>trust_score</th>\n",
       "      <th>explanation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2f0d41b2-9b89-4ba6-8b3f-7dadac8a8fae</td>\n",
       "      <td>You are a trivia master.\\nWhen was the Declara...</td>\n",
       "      <td>The Declaration of Independence was signed on ...</td>\n",
       "      <td>0.389889</td>\n",
       "      <td>The proposed response states that the Declarat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>f8e91744-3fcb-4ef5-b6c6-7cbcf0773144</td>\n",
       "      <td>You are a trivia master.\\nAlice, Bob, and Char...</td>\n",
       "      <td>Let's denote the amount Charlie paid as C. \\n\\...</td>\n",
       "      <td>0.669774</td>\n",
       "      <td>This response is untrustworthy due to lack of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>f9b42125-4e5e-4533-bfbb-36c30490bd1d</td>\n",
       "      <td>You are a trivia master.\\nHow many seconds are...</td>\n",
       "      <td>There are 3,153,600,000 seconds in 100 years.</td>\n",
       "      <td>0.499818</td>\n",
       "      <td>To calculate the number of seconds in 100 year...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>71b131b9-e706-41c7-9bfd-b77719783f29</td>\n",
       "      <td>You are a trivia master.\\nWhat is the capital ...</td>\n",
       "      <td>The capital of France is Paris.</td>\n",
       "      <td>0.987433</td>\n",
       "      <td>Did not find a reason to doubt trustworthiness.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>da0ee9fa-01cf-42ce-9e3e-e8d127ca105b</td>\n",
       "      <td>You are a trivia master.\\nWhat is the 3rd mont...</td>\n",
       "      <td>March.</td>\n",
       "      <td>0.114874</td>\n",
       "      <td>To determine the 3rd month of the year in alph...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               trace_id  \\\n",
       "0  2f0d41b2-9b89-4ba6-8b3f-7dadac8a8fae   \n",
       "1  f8e91744-3fcb-4ef5-b6c6-7cbcf0773144   \n",
       "2  f9b42125-4e5e-4533-bfbb-36c30490bd1d   \n",
       "3  71b131b9-e706-41c7-9bfd-b77719783f29   \n",
       "4  da0ee9fa-01cf-42ce-9e3e-e8d127ca105b   \n",
       "\n",
       "                                              prompt  \\\n",
       "0  You are a trivia master.\\nWhen was the Declara...   \n",
       "1  You are a trivia master.\\nAlice, Bob, and Char...   \n",
       "2  You are a trivia master.\\nHow many seconds are...   \n",
       "3  You are a trivia master.\\nWhat is the capital ...   \n",
       "4  You are a trivia master.\\nWhat is the 3rd mont...   \n",
       "\n",
       "                                            response  trust_score  \\\n",
       "0  The Declaration of Independence was signed on ...     0.389889   \n",
       "1  Let's denote the amount Charlie paid as C. \\n\\...     0.669774   \n",
       "2      There are 3,153,600,000 seconds in 100 years.     0.499818   \n",
       "3                    The capital of France is Paris.     0.987433   \n",
       "4                                             March.     0.114874   \n",
       "\n",
       "                                         explanation  \n",
       "0  The proposed response states that the Declarat...  \n",
       "1  This response is untrustworthy due to lack of ...  \n",
       "2  To calculate the number of seconds in 100 year...  \n",
       "3    Did not find a reason to doubt trustworthiness.  \n",
       "4  To determine the 3rd month of the year in alph...  "
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate each of the prompt, response pairs using TLM\n",
    "evaluations = tlm.get_trustworthiness_score(prompts, responses)\n",
    "\n",
    "# Extract the trustworthiness scores and explanations from the evaluations\n",
    "trust_scores = [entry[\"trustworthiness_score\"] for entry in evaluations]\n",
    "explanations = [entry[\"log\"][\"explanation\"] for entry in evaluations]\n",
    "\n",
    "# Create a DataFrame with the evaluation results\n",
    "trace_evaluations = pd.DataFrame({\n",
    "    'trace_id': trace_ids,\n",
    "    'prompt': prompts,\n",
    "    'response': responses, \n",
    "    'trust_score': trust_scores,\n",
    "    'explanation': explanations\n",
    "})\n",
    "trace_evaluations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome! Now we have a DataFrame mapping trace IDs to their scores and explanations. We've also included the prompt and response for each trace for demonstration purposes to find the **least trustworthy trace!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trace_id</th>\n",
       "      <th>prompt</th>\n",
       "      <th>response</th>\n",
       "      <th>trust_score</th>\n",
       "      <th>explanation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>da0ee9fa-01cf-42ce-9e3e-e8d127ca105b</td>\n",
       "      <td>You are a trivia master.\\nWhat is the 3rd mont...</td>\n",
       "      <td>March.</td>\n",
       "      <td>0.114874</td>\n",
       "      <td>To determine the 3rd month of the year in alph...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2f0d41b2-9b89-4ba6-8b3f-7dadac8a8fae</td>\n",
       "      <td>You are a trivia master.\\nWhen was the Declara...</td>\n",
       "      <td>The Declaration of Independence was signed on ...</td>\n",
       "      <td>0.389889</td>\n",
       "      <td>The proposed response states that the Declarat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>f9b42125-4e5e-4533-bfbb-36c30490bd1d</td>\n",
       "      <td>You are a trivia master.\\nHow many seconds are...</td>\n",
       "      <td>There are 3,153,600,000 seconds in 100 years.</td>\n",
       "      <td>0.499818</td>\n",
       "      <td>To calculate the number of seconds in 100 year...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>f8e91744-3fcb-4ef5-b6c6-7cbcf0773144</td>\n",
       "      <td>You are a trivia master.\\nAlice, Bob, and Char...</td>\n",
       "      <td>Let's denote the amount Charlie paid as C. \\n\\...</td>\n",
       "      <td>0.669774</td>\n",
       "      <td>This response is untrustworthy due to lack of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>71b131b9-e706-41c7-9bfd-b77719783f29</td>\n",
       "      <td>You are a trivia master.\\nWhat is the capital ...</td>\n",
       "      <td>The capital of France is Paris.</td>\n",
       "      <td>0.987433</td>\n",
       "      <td>Did not find a reason to doubt trustworthiness.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               trace_id  \\\n",
       "4  da0ee9fa-01cf-42ce-9e3e-e8d127ca105b   \n",
       "0  2f0d41b2-9b89-4ba6-8b3f-7dadac8a8fae   \n",
       "2  f9b42125-4e5e-4533-bfbb-36c30490bd1d   \n",
       "1  f8e91744-3fcb-4ef5-b6c6-7cbcf0773144   \n",
       "3  71b131b9-e706-41c7-9bfd-b77719783f29   \n",
       "\n",
       "                                              prompt  \\\n",
       "4  You are a trivia master.\\nWhat is the 3rd mont...   \n",
       "0  You are a trivia master.\\nWhen was the Declara...   \n",
       "2  You are a trivia master.\\nHow many seconds are...   \n",
       "1  You are a trivia master.\\nAlice, Bob, and Char...   \n",
       "3  You are a trivia master.\\nWhat is the capital ...   \n",
       "\n",
       "                                            response  trust_score  \\\n",
       "4                                             March.     0.114874   \n",
       "0  The Declaration of Independence was signed on ...     0.389889   \n",
       "2      There are 3,153,600,000 seconds in 100 years.     0.499818   \n",
       "1  Let's denote the amount Charlie paid as C. \\n\\...     0.669774   \n",
       "3                    The capital of France is Paris.     0.987433   \n",
       "\n",
       "                                         explanation  \n",
       "4  To determine the 3rd month of the year in alph...  \n",
       "0  The proposed response states that the Declarat...  \n",
       "2  To calculate the number of seconds in 100 year...  \n",
       "1  This response is untrustworthy due to lack of ...  \n",
       "3    Did not find a reason to doubt trustworthiness.  "
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_df = trace_evaluations.sort_values(by=\"trust_score\", ascending=True).head()\n",
    "sorted_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:  You are a trivia master.\n",
      "What is the 3rd month of the year in alphabetical order? \n",
      "\n",
      "OpenAI Response:  March. \n",
      "\n",
      "TLM Trust Score:  0.11487442493072615 \n",
      "\n",
      "TLM Explanation:  To determine the 3rd month of the year in alphabetical order, we first list the months: January, February, March, April, May, June, July, August, September, October, November, December. When we arrange these months alphabetically, we get: April, August, December, February, January, July, June, March, May, November, October, September. In this alphabetical list, March is the 8th month, not the 3rd. The 3rd month in alphabetical order is actually December. Therefore, the proposed response is incorrect. \n",
      "This response is untrustworthy due to lack of consistency in possible responses from the model. Here's one inconsistent alternate response that the model considered (which may not be accurate either): \n",
      "December.\n"
     ]
    }
   ],
   "source": [
    "# Let's look at the least trustworthy trace.\n",
    "print(\"Prompt: \", sorted_df.iloc[0][\"prompt\"], \"\\n\")\n",
    "print(\"OpenAI Response: \", sorted_df.iloc[0][\"response\"], \"\\n\")\n",
    "print(\"TLM Trust Score: \", sorted_df.iloc[0][\"trust_score\"], \"\\n\")\n",
    "print(\"TLM Explanation: \", sorted_df.iloc[0][\"explanation\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Awesome! TLM was able to identify multiple traces that contained incorrect answers from OpenAI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's upload the `trust_score` and `explanation` columns to Langfuse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload evaluations to Langfuse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, row in trace_evaluations.iterrows():\n",
    "    trace_id = row[\"trace_id\"]\n",
    "    trust_score = row[\"trust_score\"]\n",
    "    explanation = row[\"explanation\"]\n",
    "    \n",
    "    # Add the trustworthiness score to the trace with the explanation as a comment\n",
    "    langfuse.score(\n",
    "            trace_id=trace_id,\n",
    "            name=\"trust_score\",\n",
    "            value=trust_score,\n",
    "            comment=explanation\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should now see the TLM trustworthiness score and explanation in the Langfuse UI!\n",
    "\n",
    "![Image of Langfuse platform showing Cleanlab's TLM trust score](https://langfuse.com/images/cookbook/integration-cleanlab/tlm_trust_scores.png)\n",
    "\n",
    "If you click on a trace, you can also see the trust score and provided explanation.\n",
    "\n",
    "![Image of Langfuse platform showing Cleanlab's TLM trust score and explanation](https://langfuse.com/images/cookbook/integration-cleanlab/tlm_trust_scores_explanation.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phoenix",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
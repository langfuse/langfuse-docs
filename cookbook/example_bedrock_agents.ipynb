{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "description: This cookbook demonstrate how to trace Amazon Bedrock Agents with Langfuse.\n",
    "category: Integrations\n",
    "---\n",
    "\n",
    "# Trace Bedrock Agents with Langfuse\n",
    "\n",
    "> **What are Amazon Bedrock Agents?**\n",
    "> [Amazon Bedrock Agents](https://aws.amazon.com/bedrock/agents/) are managed services that simplify the creation and deployment of AI-powered conversational agents capable of executing tasks and retrieving information by integrating foundation models with external data sources and APIs.\n",
    "\n",
    "> **What is Langfuse?**\n",
    "> [Langfuse](https://langfuse.com/) is an open-source platform for LLM engineering. It provides tracing and monitoring capabilities for AI agents, helping developers debug, analyze, and optimize their products. Langfuse integrates with various tools and frameworks via native integrations, OpenTelemetry, and SDKs.\n",
    "\n",
    "\n",
    "This cookbook implements an OpenTelemetry-based tracing and monitoring system for [Amazon Bedrock Agents](https://aws.amazon.com/bedrock/agents/) through [Langfuse](https://langfuse.com/) integration. \n",
    "\n",
    "It creates hierarchical trace structures to track agent performance metrics including token usage, latency measurements, and execution durations across preprocessing, orchestration, and postprocessing phases. It processes both streaming and non-streaming responses, generating spans with operation attributes such as timing data, error states, and response content. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Started\n",
    "AWS account with appropriate IAM permissions for Amazon Bedrock Agents and Model Access as well as appropriate permission to deploy containers if using the Langfuse self-hosted option."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python Dependencies\n",
    "\n",
    "To run this notebook, you'll need to install some libraries in your environment:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r ./config/requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AWS Credentials\n",
    "Before using Amazon Bedrock, ensure that your AWS credentials are configured correctly. You can set them up using the AWS CLI or by setting environment variables. For this notebook assumes that the credentials are already configured. Also, if you are using a different region, please point to the right one below.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Langfuse API keys\n",
    "\n",
    "Get your Langfuse API keys by signing of for [Langfuse Cloud](https://cloud.langfuse.com/) or [self-hosting Langfuse](https://langfuse.com/self-hosting). To self-host Langfuse on AWS, you can use the [quick-start CloudFormation template](https://console.aws.amazon.com/cloudformation/home?#/stacks/create/review?templateURL=https://aws-blogs-artifacts-public.s3.us-east-1.amazonaws.com/artifacts/ML-18524/langfuse-bootstrap.yml&stackName=LangfuseBootstrap).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once your Langfuse environment is set up and you've signed in to the UI, you'll need to set up an **API key pair** for your particular Organization and Project (create a new project if you don't have one already).\n",
    "\n",
    "For more information, see the [FAQ: Where are my Langfuse API keys](https://langfuse.com/faq/all/where-are-langfuse-api-keys) and Langfuse's [getting started documentation](https://langfuse.com/docs/get-started)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Amazon Bedrock Agents and Langfuse Configuration\n",
    "\n",
    "1. Add the following information in `./config/bedrock-agents-config.json`\n",
    "2. Fill in your OpenTelemetry endpoint credentials, agent details, and other settings\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"langfuse\": {\n",
    "        \"project_name\": \"Your Project\",\n",
    "        \"environment\": \"development\",\n",
    "        \"langfuse_public_key\": \"your-public-key\",\n",
    "        \"langfuse_secret_key\": \"your-secret-key\",\n",
    "        \"langfuse_api_url\": \"your-otel-endpoint\"\n",
    "    },\n",
    "    \"agent\": {\n",
    "        \"agentId\": \"your-agent-id\", \n",
    "        \"agentAliasId\": \"your-agent-alias-id\"\n",
    "    },\n",
    "    \"user\": {\n",
    "        \"userId\": \"user123\",\n",
    "        \"agent_model_id\": \"claude-3-5-sonnet-20241022-v2:0\"\n",
    "    }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import base64\n",
    "import time\n",
    "import uuid\n",
    "import json\n",
    "\n",
    "start = time.time()\n",
    "with open('./config/bedrock-agents-config.json', 'r') as config_file:\n",
    "    config = json.load(config_file)\n",
    "    \n",
    "os.environ[\"OTEL_SERVICE_NAME\"] = 'Langfuse'\n",
    "os.environ[\"DEPLOYMENT_ENVIRONMENT\"] = config[\"langfuse\"][\"environment\"]\n",
    "\n",
    "project_name = config[\"langfuse\"][\"project_name\"]\n",
    "environment = config[\"langfuse\"][\"environment\"]\n",
    "langfuse_public_key = config[\"langfuse\"][\"langfuse_public_key\"]\n",
    "langfuse_secret_key = config[\"langfuse\"][\"langfuse_secret_key\"]\n",
    "langfuse_api_url = config[\"langfuse\"][\"langfuse_api_url\"]\n",
    "\n",
    "# User information\n",
    "userId = config[\"user\"][\"userId\"]  #This will be used in the Langfuse UI to filter traces\n",
    "\n",
    "# Tags for filtering in Langfuse\n",
    "tags = [\"bedrock-agent\", \"example\", \"development\"]\n",
    "\n",
    "# Generate a custom trace ID\n",
    "trace_id = str(uuid.uuid4())\n",
    "\n",
    "# Create auth header for Langfuse\n",
    "auth_token = base64.b64encode(\n",
    "    f\"{langfuse_public_key}:{langfuse_secret_key}\".encode()\n",
    ").decode()\n",
    "\n",
    "# Set OpenTelemetry environment variables for Langfuse\n",
    "os.environ[\"OTEL_EXPORTER_OTLP_ENDPOINT\"] = f\"{langfuse_api_url}/api/public/otel/v1/traces\"\n",
    "os.environ[\"OTEL_EXPORTER_OTLP_HEADERS\"] = f\"Authorization=Basic {auth_token}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Amazon Bedrock Agents\n",
    "\n",
    "We assume you've already created an [Amazon Bedrock Agents](https://docs.aws.amazon.com/bedrock/latest/userguide/agents.html). If you don't have one already you can follow the **[instructions here]()** to set up an example agent.\n",
    "\n",
    "Configure your agent's **ID** and (optionally) alias ID in the cell below. You can find these by looking up your agent in the [\"Agents\" page on the AWS Console for Amazon Bedrock](https://console.aws.amazon.com/bedrock/home?#/agents) or CLI.\n",
    "\n",
    "The Agent ID should be ten characters, uppercase, and alphanumeric. If you haven't created an Alias for your agent yet, you can use `TSTALIASID` to reference the latest saved development version.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Amazon Bedrock Agents Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agent configuration\n",
    "agent_model_id = config[\"user\"][\"agent_model_id\"] \n",
    "agentId = config[\"agent\"][\"agentId\"]\n",
    "agentAliasId = config[\"agent\"][\"agentAliasId\"]\n",
    "sessionId = f\"session-{int(time.time())}\"\n",
    "region = \"us-east-1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before moving on lets validate invoke agent is working correctly. The response is not important. We are just testing the API call. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "# Create the client to invoke Agents in Amazon Bedrock\n",
    "br_agents_runtime = boto3.client(\"bedrock-agent-runtime\", region_name=region)\n",
    "\n",
    "print(f\"Trying to invoke alias {agentAliasId} of agent {agentId}...\")\n",
    "agent_resp = br_agents_runtime.invoke_agent(\n",
    "    agentAliasId=agentAliasId,\n",
    "    agentId=agentId,\n",
    "    inputText=\"Hello!\",\n",
    "    sessionId=\"dummy-session\",\n",
    ")\n",
    "if \"completion\" in agent_resp:\n",
    "    print(\"âœ… Got response\")\n",
    "else:\n",
    "    raise ValueError(f\"No 'completion' in agent response:\\n{agent_resp}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up agent tracing\n",
    "\n",
    "With all the pre-requisites in place, we're ready to recording traces from your Bedrock Agent into Langfuse.\n",
    "\n",
    "First, let's load the libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.timer_lib import timer\n",
    "from core import instrument_agent_invocation, flush_telemetry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets define a wrapper function. Here we create a wrapper function that is used to Invoke the Amazon Bedrock Agent with instrumentation for Langfuse on the Amazon Bedrock Agents runtime API.\n",
    "\n",
    "1. Instrumentation for monitoring\n",
    "2. Configurable streaming support\n",
    "3. Trace enabling for debugging\n",
    "4. Flexible parameter handling through kwargs\n",
    "5. Proper logging of configuration states\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@instrument_agent_invocation\n",
    "def invoke_bedrock_agent(\n",
    "    inputText: str, agentId: str, agentAliasId: str, sessionId: str, **kwargs\n",
    "):\n",
    "    \"\"\"Invoke a Bedrock Agent with instrumentation for Langfuse.\"\"\"\n",
    "    # Create Bedrock client\n",
    "    bedrock_rt_client = boto3.client(\"bedrock-agent-runtime\")\n",
    "    use_streaming = kwargs.get(\"streaming\", False)\n",
    "    invoke_params = {\n",
    "        \"inputText\": inputText,\n",
    "        \"agentId\": agentId,\n",
    "        \"agentAliasId\": agentAliasId,\n",
    "        \"sessionId\": sessionId,\n",
    "        \"enableTrace\": True,  # Required for instrumentation\n",
    "    }\n",
    "\n",
    "    # Add streaming configurations if needed\n",
    "    if use_streaming:\n",
    "        invoke_params[\"streamingConfigurations\"] = {\n",
    "            \"applyGuardrailInterval\": 10,\n",
    "            \"streamFinalResponse\": use_streaming,\n",
    "        }\n",
    "    response = bedrock_rt_client.invoke_agent(**invoke_params)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create a wrapper function to handle the responses.\n",
    "\n",
    "1. Instrumentation for monitoring\n",
    "2. Configurable streaming support\n",
    "3. Trace enabling for debugging\n",
    "4. Flexible parameter handling through kwargs\n",
    "5. Proper logging of configuration states\n",
    "\n",
    "It's particularly useful for:\n",
    "\n",
    "1. Real-time processing of large responses\n",
    "2. Interactive applications requiring immediate feedback\n",
    "3. Debugging and monitoring streaming responses\n",
    "4. Ensuring proper text encoding/decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_streaming_response(stream):\n",
    "    \"\"\"Process a streaming response from Bedrock Agent.\"\"\"\n",
    "    full_response = \"\"\n",
    "    try:\n",
    "        for event in stream:\n",
    "            # Convert event to dictionary if it's a botocore Event object\n",
    "            event_dict = (\n",
    "                event.to_response_dict()\n",
    "                if hasattr(event, \"to_response_dict\")\n",
    "                else event\n",
    "            )\n",
    "            if \"chunk\" in event_dict:\n",
    "                chunk_data = event_dict[\"chunk\"]\n",
    "                if \"bytes\" in chunk_data:\n",
    "                    output_bytes = chunk_data[\"bytes\"]\n",
    "                    # Convert bytes to string if needed\n",
    "                    if isinstance(output_bytes, bytes):\n",
    "                        output_text = output_bytes.decode(\"utf-8\")\n",
    "                    else:\n",
    "                        output_text = str(output_bytes)\n",
    "                    full_response += output_text\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError processing stream: {e}\")\n",
    "    return full_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next code block will require some editing before running. Here we will set parameters used by Langfuse to track traces.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your prompt and streaming mode\n",
    "question = \"Whenâ€™s my next payment due, and for how much?\" # your prompt to the agent\n",
    "streaming = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Invoke Agent Function\n",
    "There we pass all the parameters Invoking the agent along with the observability integration with Langfuse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single invocation that works for both streaming and non-streaming\n",
    "response = invoke_bedrock_agent(\n",
    "    inputText=question,\n",
    "    agentId=agentId,\n",
    "    agentAliasId=agentAliasId,\n",
    "    sessionId=sessionId,\n",
    "    show_traces=True,\n",
    "    SAVE_TRACE_LOGS=True,\n",
    "    userId=userId,\n",
    "    tags=tags,\n",
    "    trace_id=trace_id,\n",
    "    project_name=project_name,\n",
    "    environment=environment,\n",
    "    langfuse_public_key=langfuse_public_key,\n",
    "    langfuse_secret_key=langfuse_secret_key,\n",
    "    langfuse_api_url=langfuse_api_url,\n",
    "    streaming=streaming,\n",
    "    model_id=agent_model_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Response Handling\n",
    "Here we accept the different types of responses from the Agent or API and print the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle the response appropriately based on streaming mode\n",
    "if isinstance(response, dict) and \"error\" in response:\n",
    "    print(f\"\\nError: {response['error']}\")\n",
    "elif streaming and isinstance(response, dict) and \"completion\" in response:\n",
    "    print(\"\\nðŸ¤– Agent response (streaming):\")\n",
    "    if \"extracted_completion\" in response:\n",
    "        print(response[\"extracted_completion\"])\n",
    "    else:\n",
    "        process_streaming_response(response[\"completion\"])\n",
    "else:\n",
    "    # Non-streaming response\n",
    "    print(\"\\nðŸ¤– Agent response:\")\n",
    "    if isinstance(response, dict) and \"extracted_completion\" in response:\n",
    "        print(response[\"extracted_completion\"])\n",
    "    elif (\n",
    "        isinstance(response, dict) \n",
    "        and \"completion\" in response\n",
    "        and hasattr(response[\"completion\"], \"__iter__\")\n",
    "    ):\n",
    "        print(\"Processing completion:\")\n",
    "        full_response = process_streaming_response(response[\"completion\"])\n",
    "        print(f\"\\nFull response: {full_response}\")\n",
    "    else:\n",
    "        print(\"Raw response:\")\n",
    "        print(f\"{response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Step\n",
    "\n",
    "Once you instrumented your agent and successfully ingested traces to Langfuse, you can evaluate and improve your agent with Langfuse. [Here is a guide](https://huggingface.co/learn/agents-course/bonus-unit2/what-is-agent-observability-and-evaluation) authored by the Langfuse team that shows this process end to end. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "description: This cookbook demonstrate how to trace Amazon Bedrock Agents with Langfuse.\n",
    "category: Integrations\n",
    "---\n",
    "\n",
    "# Trace Bedrock Agents with Langfuse\n",
    "\n",
    "> **What are Amazon Bedrock Agents?**\n",
    "> [Amazon Bedrock Agents](https://aws.amazon.com/bedrock/agents/) are managed services that simplify the creation and deployment of AI-powered conversational agents capable of executing tasks and retrieving information by integrating foundation models with external data sources and APIs.\n",
    "\n",
    "> **What is Langfuse?**\n",
    "> [Langfuse](https://langfuse.com/) is an open-source platform for LLM engineering. It provides tracing and monitoring capabilities for AI agents, helping developers debug, analyze, and optimize their products. Langfuse integrates with various tools and frameworks via native integrations, OpenTelemetry, and SDKs.\n",
    "\n",
    "\n",
    "This cookbook implements an OpenTelemetry-based tracing and monitoring system for [Amazon Bedrock Agents](https://aws.amazon.com/bedrock/agents/) through [Langfuse](https://langfuse.com/) integration. \n",
    "\n",
    "It creates hierarchical trace structures to track agent performance metrics including token usage, latency measurements, and execution durations across preprocessing, orchestration, and postprocessing phases. It processes both streaming and non-streaming responses, generating spans with operation attributes such as timing data, error states, and response content. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Started\n",
    "AWS account with appropriate IAM permissions for Amazon Bedrock Agents and Model Access as well as appropriate permission to deploy containers if using the Langfuse self-hosted option."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python Dependencies\n",
    "\n",
    "To run this notebook, you'll need to install some libraries in your environment:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AWS Credentials\n",
    "Before using Amazon Bedrock, ensure that your AWS credentials are configured correctly. You can set them up using the AWS CLI or by setting environment variables. For this notebook assumes that the credentials are already configured.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "# Create the client to invoke Agents in Amazon Bedrock:\n",
    "br_agents_runtime = boto3.client(\"bedrock-agent-runtime\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Amazon Bedrock Agent\n",
    "\n",
    "\n",
    "We assume you've already created an [Amazon Bedrock Agent](https://docs.aws.amazon.com/bedrock/latest/userguide/agents.html). If you don't have one already you can follow the **[instructions here]()** to set up an example agent.\n",
    "\n",
    "Configure your agent's **ID** and (optionally) alias ID in the cell below. You can find these by looking up your agent in the [\"Agents\" page on the AWS Console for Amazon Bedrock](https://console.aws.amazon.com/bedrock/home?#/agents) or CLI.\n",
    "\n",
    "The Agent ID should be ten characters, uppercase, and alphanumeric. If you haven't created an Alias for your agent yet, you can use `TSTALIASID` to reference the latest saved development version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_id = \"\"  # <- Configure your Bedrock Agent ID\n",
    "agent_alias_id = \"TSTALIASID\"  # <- Optionally set a different Alias ID if you have one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before moving on lets validate invoke agent is working correctly. The response is not important we are simply testing the API call. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Trying to invoke alias {agent_alias_id} of agent {agent_id}...\")\n",
    "agent_resp = br_agents_runtime.invoke_agent(\n",
    "    agentAliasId=agent_alias_id,\n",
    "    agentId=agent_id,\n",
    "    inputText=\"Hello!\",\n",
    "    sessionId=\"dummy-session\",\n",
    ")\n",
    "if \"completion\" in agent_resp:\n",
    "    print(\"âœ… Got response\")\n",
    "else:\n",
    "    raise ValueError(f\"No 'completion' in agent response:\\n{agent_resp}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Langfuse API keys\n",
    "\n",
    "Get your Langfuse API keys by signing of for [Langfuse Cloud](https://cloud.langfuse.com/) or [self-hosting Langfuse](https://langfuse.com/self-hosting). To self-host Langfuse on AWS, you can use the [quick-start CloudFormation template](https://console.aws.amazon.com/cloudformation/home?#/stacks/create/review?templateURL=https://aws-blogs-artifacts-public.s3.us-east-1.amazonaws.com/artifacts/ML-18524/langfuse-bootstrap.yml&stackName=LangfuseBootstrap).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once your Langfuse environment is set up and you've signed in to the UI, you'll need to set up an **API key pair** for your particular Organization and Project (create a new project if you don't have one already).\n",
    "\n",
    "For more information, see the [FAQ: Where are my Langfuse API keys](https://langfuse.com/faq/all/where-are-langfuse-api-keys) and Langfuse's [getting started documentation](https://langfuse.com/docs/get-started)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "langfuse_api_url = \"https://us.cloud.langfuse.com/\"  # <- Replace as described above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "langfuse_public_key = \"xxx\"  # <- Configure your own key here\n",
    "langfuse_secret_key = \"xxx\"  # <- Configure your own key here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up agent tracing\n",
    "\n",
    "With all the pre-requisites in place, we're ready to recording traces from your Bedrock Agent into Langfuse.\n",
    "\n",
    "First, let's load the libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import boto3\n",
    "import uuid\n",
    "import json\n",
    "from core.timer_lib import timer\n",
    "from core import instrument_agent_invocation, flush_telemetry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets define a wrapper function. Here we create a wrapper function that is used to Invoke the Amazon Bedrock Agent with instrumentation for Langfuse on the Amazon Bedrock Agents runtime API.\n",
    "\n",
    "1. Instrumentation for monitoring\n",
    "2. Configurable streaming support\n",
    "3. Trace enabling for debugging\n",
    "4. Flexible parameter handling through kwargs\n",
    "5. Proper logging of configuration states\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@instrument_agent_invocation\n",
    "def invoke_bedrock_agent(\n",
    "    inputText: str, agentId: str, agentAliasId: str, sessionId: str, **kwargs\n",
    "):\n",
    "    \"\"\"Invoke a Bedrock Agent with instrumentation for Langfuse.\"\"\"\n",
    "    # Create Bedrock client\n",
    "    bedrock_rt_client = boto3.client(\"bedrock-agent-runtime\")\n",
    "    use_streaming = kwargs.get(\"streaming\", False)\n",
    "    invoke_params = {\n",
    "        \"inputText\": inputText,\n",
    "        \"agentId\": agentId,\n",
    "        \"agentAliasId\": agentAliasId,\n",
    "        \"sessionId\": sessionId,\n",
    "        \"enableTrace\": True,  # Required for instrumentation\n",
    "    }\n",
    "\n",
    "    # Add streaming configurations if needed\n",
    "    if use_streaming:\n",
    "        invoke_params[\"streamingConfigurations\"] = {\n",
    "            \"applyGuardrailInterval\": 10,\n",
    "            \"streamFinalResponse\": True,\n",
    "        }\n",
    "    response = bedrock_rt_client.invoke_agent(**invoke_params)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create a wrapper function to handle the responses.\n",
    "\n",
    "1. Instrumentation for monitoring\n",
    "2. Configurable streaming support\n",
    "3. Trace enabling for debugging\n",
    "4. Flexible parameter handling through kwargs\n",
    "5. Proper logging of configuration states\n",
    "\n",
    "It's particularly useful for:\n",
    "\n",
    "1. Real-time processing of large responses\n",
    "2. Interactive applications requiring immediate feedback\n",
    "3. Debugging and monitoring streaming responses\n",
    "4. Ensuring proper text encoding/decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_streaming_response(stream):\n",
    "    \"\"\"Process a streaming response from Bedrock Agent.\"\"\"\n",
    "    full_response = \"\"\n",
    "    try:\n",
    "        for event in stream:\n",
    "            # Convert event to dictionary if it's a botocore Event object\n",
    "            event_dict = (\n",
    "                event.to_response_dict()\n",
    "                if hasattr(event, \"to_response_dict\")\n",
    "                else event\n",
    "            )\n",
    "            if \"chunk\" in event_dict:\n",
    "                chunk_data = event_dict[\"chunk\"]\n",
    "                if \"bytes\" in chunk_data:\n",
    "                    output_bytes = chunk_data[\"bytes\"]\n",
    "                    # Convert bytes to string if needed\n",
    "                    if isinstance(output_bytes, bytes):\n",
    "                        output_text = output_bytes.decode(\"utf-8\")\n",
    "                    else:\n",
    "                        output_text = str(output_bytes)\n",
    "                    full_response += output_text\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError processing stream: {e}\")\n",
    "    return full_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Langfuse Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import base64\n",
    "start = time.time()\n",
    "with open('config.json', 'r') as config_file:\n",
    "    config = json.load(config_file)\n",
    "    \n",
    " # For Langfuse specifically but you can add any other observability provider:\n",
    "os.environ[\"OTEL_SERVICE_NAME\"] = 'Langfuse'\n",
    "os.environ[\"DEPLOYMENT_ENVIRONMENT\"] = config[\"langfuse\"][\"environment\"]\n",
    "project_name = config[\"langfuse\"][\"project_name\"]\n",
    "environment = config[\"langfuse\"][\"environment\"]\n",
    "langfuse_public_key = config[\"langfuse\"][\"langfuse_public_key\"]\n",
    "langfuse_secret_key = config[\"langfuse\"][\"langfuse_secret_key\"]\n",
    "langfuse_api_url = config[\"langfuse\"][\"langfuse_api_url\"]\n",
    "\n",
    "# Create auth header\n",
    "auth_token = base64.b64encode(\n",
    "    f\"{langfuse_public_key}:{langfuse_secret_key}\".encode()\n",
    ").decode()\n",
    "\n",
    "# Set OpenTelemetry environment variables for Langfuse\n",
    "os.environ[\"OTEL_EXPORTER_OTLP_ENDPOINT\"] = f\"{langfuse_api_url}/api/public/otel/v1/traces\"\n",
    "os.environ[\"OTEL_EXPORTER_OTLP_HEADERS\"] = f\"Authorization=Basic {auth_token}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next code block will require some editing before running. Here we will set parameters used by Langfuse to track traces.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Langfuse configuration\n",
    "project_name = \"xxx\" #Enter your Langfuse Project name that you created \n",
    "environment = \"default\"  #Enter the env name\n",
    "\n",
    "# User information\n",
    "user_id = \"xxx\" #This will be used in the Langfuse UI to filter traces\n",
    "\n",
    "# Foundation Model used by the agent (used to estimate costs)\n",
    "agent_model_id = \"xxx\"  #eg \"claude-3-5-sonnet-20241022-v2:0\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agent configuration\n",
    "agentId = config[\"agent\"][\"agentId\"]\n",
    "agentAliasId = config[\"agent\"][\"agentAliasId\"]\n",
    "sessionId = f\"session-{int(time.time())}\"\n",
    "\n",
    "# User information\n",
    "userId = config[\"user\"][\"userId\"]  \n",
    "agent_model_id = config[\"user\"][\"agent_model_id\"]\n",
    "\n",
    "# Tags for filtering in Langfuse\n",
    "tags = [\"bedrock-agent\", \"example\", \"development\"]\n",
    "\n",
    "# Generate a custom trace ID\n",
    "trace_id = str(uuid.uuid4())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your prompt and streaming mode\n",
    "question = \"xxx\" # your prompt to the agent\n",
    "streaming = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Invoke Agent Function\n",
    "There we pass all the parameters Invoking the agent along with the observability integration with Langfuse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single invocation that works for both streaming and non-streaming\n",
    "response = invoke_bedrock_agent(\n",
    "    inputText=question,\n",
    "    agentId=agentId,\n",
    "    agentAliasId=agentAliasId,\n",
    "    sessionId=sessionId,\n",
    "    show_traces=True,\n",
    "    SAVE_TRACE_LOGS=True,\n",
    "    userId=userId,\n",
    "    tags=tags,\n",
    "    trace_id=trace_id,\n",
    "    project_name=project_name,\n",
    "    environment=environment,\n",
    "    langfuse_public_key=langfuse_public_key,\n",
    "    langfuse_secret_key=langfuse_secret_key,\n",
    "    langfuse_api_url=langfuse_api_url,\n",
    "    streaming=streaming,\n",
    "    model_id=agent_model_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Response Handling\n",
    "Here we accept the different types of responses from the Agent or API and print the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle the response appropriately based on streaming mode\n",
    "if isinstance(response, dict) and \"error\" in response:\n",
    "    print(f\"\\nError: {response['error']}\")\n",
    "elif streaming and isinstance(response, dict) and \"completion\" in response:\n",
    "    print(\"\\nðŸ¤– Agent response (streaming):\")\n",
    "    if \"extracted_completion\" in response:\n",
    "        print(response[\"extracted_completion\"])\n",
    "    else:\n",
    "        process_streaming_response(response[\"completion\"])\n",
    "else:\n",
    "    # Non-streaming response\n",
    "    print(\"\\nðŸ¤– Agent response:\")\n",
    "    if isinstance(response, dict) and \"extracted_completion\" in response:\n",
    "        print(response[\"extracted_completion\"])\n",
    "    elif (\n",
    "        isinstance(response, dict) \n",
    "        and \"completion\" in response\n",
    "        and hasattr(response[\"completion\"], \"__iter__\")\n",
    "    ):\n",
    "        print(\"Processing completion:\")\n",
    "        full_response = process_streaming_response(response[\"completion\"])\n",
    "        print(f\"\\nFull response: {full_response}\")\n",
    "    else:\n",
    "        print(\"Raw response:\")\n",
    "        print(f\"{response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Step\n",
    "\n",
    "Once you instrumented your agent and successfully ingested traces to Langfuse, you can evaluate and improve your agent with Langfuse. [Here is a guide](https://huggingface.co/learn/agents-course/bonus-unit2/what-is-agent-observability-and-evaluation) authored by the Langfuse team that shows this process end to end. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langfuse_obserability",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

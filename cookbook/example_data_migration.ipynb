{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TN7a0oeGwd3W"
      },
      "source": [
        "---\n",
        "title: Migrating Data from One Langfuse Project to Another\n",
        "description: Script to migrate data (prompts, traces, datasets) from a source Langfuse project to a destination Langfuse project\n",
        "---\n",
        "\n",
        "# Migrating Data from One Langfuse Project to Another\n",
        "\n",
        "This guide provides Python code  to migrate data (prompts, traces, datasets) from a source Langfuse project to a destination Langfuse project using the Langfuse Public API.\n",
        "\n",
        "This is useful for scenarios such as:\n",
        "*   Moving data from Langfuse Cloud to a self-hosted instance (or vice-versa).\n",
        "*   Migrating between different Langfuse Cloud regions or compliance environments (e.g., non-HIPAA to HIPAA).\n",
        "*   Syncing production data to a development/staging environment for testing.\n",
        "*   Creating custom scripts to synchronize specific data types (like prompts) across instances.\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "*   **Langfuse Projects:** Access to both the source and destination Langfuse projects.\n",
        "*   **Rate Limits:** Be mindful of API rate limits on your Langfuse instances (Cloud or self-hosted). Large migrations might require implementing back-off logic or running the script in batches.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7nOhilSWPOf1",
        "outputId": "6a27f9d3-6217-46f1-e921-3b9a8d6fd61b"
      },
      "outputs": [],
      "source": [
        "!pip install langfuse -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "r-L6j2kY054P"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# SOURCE LANGFUSE PROJECT\n",
        "os.environ[\"LANGFUSE_SOURCE_SECRET_KEY\"] = \"sk-lf-...\"\n",
        "os.environ[\"LANGFUSE_SOURCE_PUBLIC_KEY\"] = \"pk-lf-...\"\n",
        "os.environ[\"LANGFUSE_SOURCE_HOST\"] = \"https://cloud.langfuse.com\"\n",
        "\n",
        "# DESTINATION LANGFUSE PROJECT\n",
        "os.environ[\"LANGFUSE_DEST_SECRET_KEY\"] = \"sk-lf-...\"\n",
        "os.environ[\"LANGFUSE_DEST_PUBLIC_KEY\"] = \"pk-lf-...\"\n",
        "os.environ[\"LANGFUSE_DEST_HOST\"] = \"https://cloud.langfuse.com\"\n",
        "\n",
        "# Optional Time Filters (ISO 8601 Format, e.g., 2023-10-26T10:00:00Z)\n",
        "os.environ[\"LANGFUSE_MIGRATE_FROM_TIMESTAMP\"] = \"2025-03-01T10:00:00Z\"\n",
        "os.environ[\"LANGFUSE_MIGRATE_TO_TIMESTAMP\"] = \"2025-03-30T10:00:00Z\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43IcC-D1uUA_"
      },
      "source": [
        "## Section I – Migrating Prompts\n",
        "\n",
        "This section covers migrating prompts, including their version history."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VPuAN6qk02sK",
        "outputId": "9096749f-e4af-4332-ea6a-d2251f7eb0f6"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "from langfuse import Langfuse\n",
        "# Import the specific union member types that include the 'type' literal\n",
        "from langfuse.api.resources.prompts.types.create_prompt_request import (\n",
        "    CreatePromptRequest_Chat,\n",
        "    CreatePromptRequest_Text\n",
        ")\n",
        "# Import Prompt_Chat and Prompt_Text for isinstance checks on the source prompt\n",
        "from langfuse.api.resources.prompts.types import Prompt_Chat, Prompt_Text\n",
        "\n",
        "def migrate_prompts(source_config, dest_config):\n",
        "    \"\"\"\n",
        "    Migrates all prompts (including all versions) from a source Langfuse project\n",
        "    to a destination Langfuse project.\n",
        "    The destination Langfuse instance will automatically assign new version numbers.\n",
        "    The original source version is recorded in the commit_message.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        langfuse_source = Langfuse(\n",
        "            public_key=source_config[\"public_key\"],\n",
        "            secret_key=source_config[\"secret_key\"],\n",
        "            host=source_config.get(\"host\", \"https://cloud.langfuse.com\")\n",
        "        )\n",
        "        print(f\"Source client initialized for host: {langfuse_source.base_url}\")\n",
        "        langfuse_source.auth_check()\n",
        "        print(\"Source credentials verified.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error initializing or verifying source Langfuse client: {e}\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    try:\n",
        "        langfuse_destination = Langfuse(\n",
        "            public_key=dest_config[\"public_key\"],\n",
        "            secret_key=dest_config[\"secret_key\"],\n",
        "            host=dest_config.get(\"host\", \"https://cloud.langfuse.com\")\n",
        "        )\n",
        "        print(f\"Destination client initialized for host: {langfuse_destination.base_url}\")\n",
        "        langfuse_destination.auth_check()\n",
        "        print(\"Destination credentials verified.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error initializing or verifying destination Langfuse client: {e}\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    print(\"\\nFetching prompts from source project...\")\n",
        "    all_source_prompts_details = []\n",
        "    page = 1\n",
        "    limit = 100\n",
        "\n",
        "    while True:\n",
        "        try:\n",
        "            overview = langfuse_source.api.prompts.list(page=page, limit=limit)\n",
        "\n",
        "            if not overview.data and page == 1:\n",
        "                print(\"No prompts found in the source project.\")\n",
        "                return\n",
        "            if not overview.data:\n",
        "                break\n",
        "\n",
        "            print(f\"  Fetched page {overview.meta.page}/{overview.meta.total_pages} of prompt metadata...\")\n",
        "\n",
        "            for meta in overview.data:\n",
        "                prompt_name = meta.name\n",
        "                for v_num in meta.versions:\n",
        "                    try:\n",
        "                        source_prompt_version_detail = langfuse_source.api.prompts.get(prompt_name, version=v_num)\n",
        "                        all_source_prompts_details.append(source_prompt_version_detail)\n",
        "                    except Exception as e:\n",
        "                        print(f\"      Error fetching details for {prompt_name} v{v_num} from source: {e}\")\n",
        "\n",
        "            if overview.meta.page >= overview.meta.total_pages:\n",
        "                break\n",
        "            page += 1\n",
        "        except Exception as e:\n",
        "            print(f\"  Error fetching prompt list (page {page}) from source: {e}\")\n",
        "            break\n",
        "\n",
        "    if not all_source_prompts_details:\n",
        "        print(\"No prompt versions could be fetched from the source project.\")\n",
        "        return\n",
        "\n",
        "    print(f\"\\nFetched a total of {len(all_source_prompts_details)} prompt versions from source.\")\n",
        "    print(\"Starting migration to destination project...\\n\")\n",
        "\n",
        "    migrated_count = 0\n",
        "    failed_count = 0\n",
        "\n",
        "    for source_prompt_detail in all_source_prompts_details:\n",
        "        prompt_name_val = source_prompt_detail.name\n",
        "        prompt_version_val = source_prompt_detail.version\n",
        "        prompt_labels_val = source_prompt_detail.labels\n",
        "        prompt_content_val = source_prompt_detail.prompt\n",
        "        prompt_config_val = source_prompt_detail.config\n",
        "        prompt_tags_val = source_prompt_detail.tags\n",
        "\n",
        "        # source_prompt_detail.type is \"chat\" or \"text\" (lowercase)\n",
        "        print(f\"  Attempting to migrate source prompt: '{prompt_name_val}', original version: {prompt_version_val}, type: {source_prompt_detail.type}\")\n",
        "\n",
        "        commit_msg = (\n",
        "            f\"Migrated from source. Original prompt name: '{prompt_name_val}', \"\n",
        "            f\"original source version: {prompt_version_val}.\"\n",
        "        )\n",
        "\n",
        "        create_request = None\n",
        "        try:\n",
        "            # Use isinstance to check the actual type of the fetched prompt object\n",
        "            if isinstance(source_prompt_detail, Prompt_Chat):\n",
        "                create_request = CreatePromptRequest_Chat( # Use the type from the Union definition\n",
        "                    name=prompt_name_val,\n",
        "                    prompt=prompt_content_val,\n",
        "                    labels=prompt_labels_val if prompt_labels_val is not None else [],\n",
        "                    config=prompt_config_val,\n",
        "                    tags=prompt_tags_val if prompt_tags_val is not None else [],\n",
        "                    commit_message=commit_msg,\n",
        "                    # 'type=\"chat\"' is a default literal in CreatePromptRequest_Chat\n",
        "                )\n",
        "            elif isinstance(source_prompt_detail, Prompt_Text):\n",
        "                create_request = CreatePromptRequest_Text( # Use the type from the Union definition\n",
        "                    name=prompt_name_val,\n",
        "                    prompt=prompt_content_val,\n",
        "                    labels=prompt_labels_val if prompt_labels_val is not None else [],\n",
        "                    config=prompt_config_val,\n",
        "                    tags=prompt_tags_val if prompt_tags_val is not None else [],\n",
        "                    commit_message=commit_msg,\n",
        "                    # 'type=\"text\"' is a default literal in CreatePromptRequest_Text\n",
        "                )\n",
        "            else:\n",
        "                print(f\"    Unsupported prompt object type '{type(source_prompt_detail).__name__}' for {prompt_name_val} (source v{prompt_version_val}). Skipping.\")\n",
        "                failed_count += 1\n",
        "                continue\n",
        "\n",
        "            # The 'request' object passed to create() is now an instance of CreatePromptRequest_Chat or _Text\n",
        "            # which Pydantic will serialize including its 'type' literal.\n",
        "            created_dest_prompt = langfuse_destination.api.prompts.create(request=create_request)\n",
        "\n",
        "            print(f\"    Successfully created prompt '{created_dest_prompt.name}' in destination.\")\n",
        "            print(f\"      Source version was: {prompt_version_val}, Type: {source_prompt_detail.type}\")\n",
        "            # created_dest_prompt is also a Prompt_Chat or Prompt_Text object\n",
        "            print(f\"      Destination assigned version: {created_dest_prompt.version}, Type: {created_dest_prompt.type}\")\n",
        "            print(f\"      Labels applied: {created_dest_prompt.labels}\")\n",
        "            migrated_count += 1\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"    Error migrating prompt {prompt_name_val} (source v{prompt_version_val}): {e}\")\n",
        "            failed_count += 1\n",
        "\n",
        "    print(f\"\\nMigration complete. Successfully migrated: {migrated_count}, Failed: {failed_count}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"Langfuse Prompts Migration Script\")\n",
        "    print(\"---------------------------------\")\n",
        "    print(\"This script migrates all prompt versions. The destination project will\")\n",
        "    print(\"assign new version numbers automatically. Original source version numbers\")\n",
        "    print(\"are stored in the commit_message of the created prompts in the destination.\")\n",
        "    print(\"---------------------------------\\n\")\n",
        "\n",
        "    source_pk = os.getenv(\"LANGFUSE_SOURCE_PUBLIC_KEY\")\n",
        "    source_sk = os.getenv(\"LANGFUSE_SOURCE_SECRET_KEY\")\n",
        "    source_host = os.getenv(\"LANGFUSE_SOURCE_HOST\", \"https://cloud.langfuse.com\")\n",
        "\n",
        "    dest_pk = os.getenv(\"LANGFUSE_DEST_PUBLIC_KEY\")\n",
        "    dest_sk = os.getenv(\"LANGFUSE_DEST_SECRET_KEY\")\n",
        "    dest_host = os.getenv(\"LANGFUSE_DEST_HOST\", \"https://cloud.langfuse.com\")\n",
        "\n",
        "    if not source_pk or not source_sk:\n",
        "        print(\"Error: LANGFUSE_SOURCE_PUBLIC_KEY and LANGFUSE_SOURCE_SECRET_KEY environment variables are required for the source project.\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    if not dest_pk or not dest_sk:\n",
        "        print(\"Error: LANGFUSE_DEST_PUBLIC_KEY and LANGFUSE_DEST_SECRET_KEY environment variables are required for the destination project.\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    source_credentials = {\n",
        "        \"public_key\": source_pk,\n",
        "        \"secret_key\": source_sk,\n",
        "        \"host\": source_host,\n",
        "    }\n",
        "    destination_credentials = {\n",
        "        \"public_key\": dest_pk,\n",
        "        \"secret_key\": dest_sk,\n",
        "        \"host\": dest_host,\n",
        "    }\n",
        "\n",
        "    print(f\"Source Host: {source_host}\")\n",
        "    print(f\"Destination Host: {dest_host}\")\n",
        "\n",
        "    confirmation = input(\"\\nProceed with migration? (yes/no): \").lower()\n",
        "    if confirmation == 'yes':\n",
        "        migrate_prompts(source_credentials, destination_credentials)\n",
        "    else:\n",
        "        print(\"Migration cancelled by user.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7nOyl-NyuWa-"
      },
      "source": [
        "## Section II – Migrating Traces, Observations & Scores\n",
        "\n",
        "This section handles the core tracing data: traces, their nested observations (spans, generations, events), and associated scores."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5BzaizOIuXGI",
        "outputId": "f093d3fd-d70e-453f-bd9f-302942b42acc"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import uuid\n",
        "import datetime as dt\n",
        "import time\n",
        "from langfuse import Langfuse\n",
        "# Corrected location for MapValue:\n",
        "from langfuse.api.resources.commons.types import MapValue\n",
        "# Ingestion types:\n",
        "from langfuse.api.resources.ingestion.types import (\n",
        "    TraceBody,\n",
        "    CreateSpanBody,\n",
        "    CreateGenerationBody,\n",
        "    CreateEventBody,\n",
        "    ScoreBody,\n",
        "    IngestionEvent_TraceCreate,\n",
        "    IngestionEvent_SpanCreate,\n",
        "    IngestionEvent_GenerationCreate,\n",
        "    IngestionEvent_EventCreate,\n",
        "    IngestionEvent_ScoreCreate,\n",
        "    IngestionUsage,\n",
        ")\n",
        "# Other common types:\n",
        "from langfuse.api.resources.commons.types import ObservationLevel, ScoreSource, Usage\n",
        "from langfuse.api.resources.commons.types.score import Score_Numeric, Score_Categorical, Score_Boolean\n",
        "\n",
        "# --- Helper Function for Robust Datetime Formatting ---\n",
        "def safe_isoformat(dt_obj):\n",
        "    \"\"\"Safely formats datetime object to ISO 8601 string, handling None.\"\"\"\n",
        "    if dt_obj is None:\n",
        "        return None\n",
        "    if not isinstance(dt_obj, dt.datetime):\n",
        "        if isinstance(dt_obj, str): # Allow pre-formatted strings\n",
        "             try:\n",
        "                 dt.datetime.fromisoformat(dt_obj.replace('Z', '+00:00'))\n",
        "                 return dt_obj\n",
        "             except ValueError:\n",
        "                 print(f\"Warning: String '{dt_obj}' is not a valid ISO datetime. Returning None.\")\n",
        "                 return None\n",
        "        print(f\"Warning: Expected datetime object or ISO string, got {type(dt_obj)}. Returning None.\")\n",
        "        return None\n",
        "    try:\n",
        "        if dt_obj.tzinfo is None:\n",
        "            dt_obj = dt_obj.replace(tzinfo=dt.timezone.utc)\n",
        "        iso_str = dt_obj.isoformat(timespec='milliseconds')\n",
        "        if iso_str.endswith('+00:00'):\n",
        "            iso_str = iso_str[:-6] + 'Z'\n",
        "        return iso_str\n",
        "    except Exception as e:\n",
        "        print(f\"Warning: Could not format datetime {dt_obj}: {e}. Returning None.\")\n",
        "        return None\n",
        "\n",
        "def transform_trace_to_ingestion_batch(source_trace):\n",
        "    \"\"\"\n",
        "    Transforms a fetched TraceWithFullDetails object into a list of\n",
        "    IngestionEvent objects suitable for the batch ingestion endpoint.\n",
        "    Uses the ORIGINAL source trace ID for the new trace.\n",
        "    Generates new IDs for observations/scores within the trace.\n",
        "    Maps parent/child relationships using new observation IDs.\n",
        "    \"\"\"\n",
        "    ingestion_events = []\n",
        "    preserved_trace_id = source_trace.id\n",
        "    obs_id_map = {}\n",
        "\n",
        "    # 1. Create Trace Event\n",
        "    trace_metadata = source_trace.metadata if isinstance(source_trace.metadata, dict) else {}\n",
        "    trace_body = TraceBody(\n",
        "        id=preserved_trace_id,\n",
        "        timestamp=source_trace.timestamp,\n",
        "        name=source_trace.name,\n",
        "        user_id=source_trace.user_id,\n",
        "        input=source_trace.input,\n",
        "        output=source_trace.output,\n",
        "        session_id=source_trace.session_id,\n",
        "        release=source_trace.release,\n",
        "        version=source_trace.version,\n",
        "        metadata=trace_metadata or None,\n",
        "        tags=source_trace.tags if source_trace.tags is not None else [],\n",
        "        public=source_trace.public,\n",
        "        environment=source_trace.environment,\n",
        "    )\n",
        "    event_timestamp_str = safe_isoformat(dt.datetime.now(dt.timezone.utc))\n",
        "    if not event_timestamp_str:\n",
        "         print(\"Error: Could not format timestamp for trace event. Skipping trace.\")\n",
        "         return []\n",
        "    trace_event_id = str(uuid.uuid4())\n",
        "    ingestion_events.append(\n",
        "        IngestionEvent_TraceCreate(id=trace_event_id, timestamp=event_timestamp_str, body=trace_body)\n",
        "    )\n",
        "\n",
        "    # 2. Create Observation Events\n",
        "    sorted_observations = sorted(source_trace.observations, key=lambda o: o.start_time)\n",
        "    for source_obs in sorted_observations:\n",
        "        new_obs_id = str(uuid.uuid4())\n",
        "        obs_id_map[source_obs.id] = new_obs_id\n",
        "        new_parent_observation_id = obs_id_map.get(source_obs.parent_observation_id) if source_obs.parent_observation_id else None\n",
        "        obs_metadata = source_obs.metadata if isinstance(source_obs.metadata, dict) else {}\n",
        "\n",
        "        model_params_mapped = None\n",
        "        if isinstance(source_obs.model_parameters, dict): model_params_mapped = source_obs.model_parameters\n",
        "        elif source_obs.model_parameters is not None: print(f\"Warning: Obs {source_obs.id} model_parameters type {type(source_obs.model_parameters)}, skipping.\")\n",
        "\n",
        "        common_body_args = {\n",
        "            \"id\": new_obs_id, \"trace_id\": preserved_trace_id, \"name\": source_obs.name,\n",
        "            \"start_time\": source_obs.start_time, \"metadata\": obs_metadata or None,\n",
        "            \"input\": source_obs.input, \"output\": source_obs.output, \"level\": source_obs.level,\n",
        "            \"status_message\": source_obs.status_message, \"parent_observation_id\": new_parent_observation_id,\n",
        "            \"version\": source_obs.version, \"environment\": source_obs.environment,\n",
        "        }\n",
        "\n",
        "        event_body = None; ingestion_event_type = None\n",
        "        event_specific_timestamp = safe_isoformat(dt.datetime.now(dt.timezone.utc))\n",
        "        if not event_specific_timestamp: print(f\"Error: Could not format timestamp for obs {new_obs_id}. Skipping.\"); continue\n",
        "\n",
        "        try:\n",
        "            if source_obs.type == \"SPAN\":\n",
        "                event_body = CreateSpanBody(**common_body_args, end_time=source_obs.end_time)\n",
        "                ingestion_event_type = IngestionEvent_SpanCreate\n",
        "            elif source_obs.type == \"EVENT\":\n",
        "                event_body = CreateEventBody(**common_body_args)\n",
        "                ingestion_event_type = IngestionEvent_EventCreate\n",
        "            elif source_obs.type == \"GENERATION\":\n",
        "                usage_to_pass = None\n",
        "                if isinstance(source_obs.usage, Usage):\n",
        "                    usage_data = {k: getattr(source_obs.usage, k, None) for k in ['input', 'output', 'total', 'unit', 'input_cost', 'output_cost', 'total_cost']}\n",
        "                    filtered_usage_data = {k: v for k, v in usage_data.items() if v is not None}\n",
        "                    if filtered_usage_data: usage_to_pass = Usage(**filtered_usage_data)\n",
        "                elif source_obs.usage is not None: print(f\"Warning: Obs {source_obs.id} has usage type {type(source_obs.usage)}. Skipping.\")\n",
        "\n",
        "                event_body = CreateGenerationBody(\n",
        "                    **common_body_args, end_time=source_obs.end_time,\n",
        "                    completion_start_time=source_obs.completion_start_time,\n",
        "                    model=source_obs.model, model_parameters=model_params_mapped,\n",
        "                    usage=usage_to_pass, cost_details=source_obs.cost_details,\n",
        "                    usage_details=source_obs.usage_details,\n",
        "                    prompt_name=getattr(source_obs, 'prompt_name', None),\n",
        "                    prompt_version=getattr(source_obs, 'prompt_version', None),\n",
        "                )\n",
        "                ingestion_event_type = IngestionEvent_GenerationCreate\n",
        "            else: print(f\"Warning: Unknown obs type '{source_obs.type}' for ID {source_obs.id}. Skipping.\"); continue\n",
        "\n",
        "            if event_body and ingestion_event_type:\n",
        "                event_envelope_id = str(uuid.uuid4())\n",
        "                ingestion_events.append(\n",
        "                    ingestion_event_type(id=event_envelope_id, timestamp=event_specific_timestamp, body=event_body)\n",
        "                )\n",
        "        except Exception as e: print(f\"Error creating obs body for {source_obs.id} (type: {source_obs.type}): {e}\"); continue\n",
        "\n",
        "    # 3. Create Score Events\n",
        "    for source_score in source_trace.scores:\n",
        "        new_score_id = str(uuid.uuid4())\n",
        "        new_observation_id = obs_id_map.get(source_score.observation_id) if source_score.observation_id else None\n",
        "        score_metadata = source_score.metadata if isinstance(source_score.metadata, dict) else {}\n",
        "\n",
        "        score_body_value = None\n",
        "        if source_score.data_type == \"CATEGORICAL\":\n",
        "            # For categorical, use the string_value field from the source\n",
        "             if hasattr(source_score, 'string_value') and isinstance(getattr(source_score, 'string_value', None), str):\n",
        "                 score_body_value = source_score.string_value\n",
        "             else:\n",
        "                 # Fallback or warning if string_value is missing for categorical\n",
        "                 print(f\"      Warning: Categorical score {source_score.id} is missing string_value. Attempting to use numeric value '{source_score.value}' as string.\")\n",
        "                 score_body_value = str(source_score.value) if source_score.value is not None else None\n",
        "\n",
        "        elif source_score.data_type in [\"NUMERIC\", \"BOOLEAN\"]:\n",
        "            # For numeric/boolean, use the numeric value field\n",
        "            score_body_value = source_score.value # Already float or None\n",
        "        else:\n",
        "            print(f\"      Warning: Unknown score dataType '{source_score.data_type}' for score {source_score.id}. Attempting numeric value.\")\n",
        "            score_body_value = source_score.value\n",
        "\n",
        "        # If after all checks, value is still None, skip score\n",
        "        if score_body_value is None:\n",
        "             print(f\"      Warning: Could not determine valid value for score {source_score.id} (dataType: {source_score.data_type}). Skipping score.\")\n",
        "             continue\n",
        "\n",
        "        try:\n",
        "            score_body = ScoreBody(\n",
        "                id=new_score_id,\n",
        "                trace_id=preserved_trace_id,\n",
        "                name=source_score.name,\n",
        "                # Pass the correctly typed value\n",
        "                value=score_body_value,\n",
        "                # string_value field might not be needed if value holds the category string\n",
        "                # string_value=string_value if source_score.data_type == \"CATEGORICAL\" else None, # Optional: maybe pass string_value only for categorical?\n",
        "                source=source_score.source,\n",
        "                comment=source_score.comment,\n",
        "                observation_id=new_observation_id,\n",
        "                timestamp=source_score.timestamp,\n",
        "                config_id=source_score.config_id,\n",
        "                metadata=score_metadata or None,\n",
        "                data_type=source_score.data_type,\n",
        "                environment=source_score.environment,\n",
        "            )\n",
        "            event_timestamp_str = safe_isoformat(dt.datetime.now(dt.timezone.utc))\n",
        "            if not event_timestamp_str: print(f\"Error: Could not format timestamp for score {new_score_id}. Skipping.\"); continue\n",
        "            event_envelope_id = str(uuid.uuid4())\n",
        "            ingestion_events.append(\n",
        "                IngestionEvent_ScoreCreate(id=event_envelope_id, timestamp=event_timestamp_str, body=score_body)\n",
        "            )\n",
        "        except Exception as e: print(f\"Error creating score body for {source_score.id}: {e}\"); continue\n",
        "\n",
        "    return ingestion_events\n",
        "\n",
        "def parse_datetime(datetime_str):\n",
        "    \"\"\"Parses an ISO 8601 datetime string into a timezone-aware datetime object.\"\"\"\n",
        "    if not datetime_str:\n",
        "        return None\n",
        "    try:\n",
        "        # Handle Z explicitly for robust parsing across Python versions\n",
        "        if isinstance(datetime_str, str) and datetime_str.endswith('Z'):\n",
        "            datetime_str = datetime_str[:-1] + '+00:00'\n",
        "        # Try parsing\n",
        "        dt_obj = dt.datetime.fromisoformat(datetime_str)\n",
        "        # Add timezone if naive (assume UTC)\n",
        "        if dt_obj.tzinfo is None:\n",
        "            dt_obj = dt_obj.replace(tzinfo=dt.timezone.utc)\n",
        "        return dt_obj\n",
        "    except ValueError:\n",
        "        print(f\"Error: Could not parse datetime string '{datetime_str}'. Ensure ISO 8601 format.\")\n",
        "        return None\n",
        "    except TypeError:\n",
        "         print(f\"Error: Expected string for datetime parsing, got {type(datetime_str)}.\")\n",
        "         return None\n",
        "\n",
        "def migrate_traces(source_config, dest_config, from_timestamp_str=None, to_timestamp_str=None, sleep_between_gets=0.7, sleep_between_batches=0.5, max_retries=4):\n",
        "    \"\"\"\n",
        "    Migrates traces from a source Langfuse project to a destination project.\n",
        "    Includes delays and retries for rate limiting. Preserves original Trace IDs.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        langfuse_source = Langfuse(\n",
        "            public_key=source_config[\"public_key\"],\n",
        "            secret_key=source_config[\"secret_key\"],\n",
        "            host=source_config.get(\"host\", \"https://cloud.langfuse.com\")\n",
        "        )\n",
        "        print(f\"Source client initialized for host: {langfuse_source.base_url}\")\n",
        "        langfuse_source.auth_check()\n",
        "        print(\"Source credentials verified.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error initializing or verifying source Langfuse client: {e}\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    try:\n",
        "        langfuse_destination = Langfuse(\n",
        "            public_key=dest_config[\"public_key\"],\n",
        "            secret_key=dest_config[\"secret_key\"],\n",
        "            host=dest_config.get(\"host\", \"https://cloud.langfuse.com\")\n",
        "        )\n",
        "        print(f\"Destination client initialized for host: {langfuse_destination.base_url}\")\n",
        "        langfuse_destination.auth_check()\n",
        "        print(\"Destination credentials verified.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error initializing or verifying destination Langfuse client: {e}\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    from_timestamp = parse_datetime(from_timestamp_str)\n",
        "    to_timestamp = parse_datetime(to_timestamp_str)\n",
        "\n",
        "    if from_timestamp_str and not from_timestamp: sys.exit(1)\n",
        "    if to_timestamp_str and not to_timestamp: sys.exit(1)\n",
        "\n",
        "    print(\"\\nFetching and migrating traces...\")\n",
        "    page = 1\n",
        "    limit = 50\n",
        "    total_migrated = 0\n",
        "    total_failed_fetch = 0\n",
        "    total_failed_transform = 0\n",
        "    total_failed_push = 0\n",
        "\n",
        "    while True:\n",
        "        print(f\"\\n--- Processing page {page} ---\")\n",
        "        fetch_list_success = False\n",
        "        list_retries = 0\n",
        "        trace_list = None\n",
        "        while not fetch_list_success and list_retries < max_retries:\n",
        "            try:\n",
        "                trace_list = langfuse_source.api.trace.list(\n",
        "                    page=page, limit=limit, order_by=\"timestamp.asc\",\n",
        "                    from_timestamp=from_timestamp, to_timestamp=to_timestamp\n",
        "                )\n",
        "                fetch_list_success = True\n",
        "\n",
        "            except Exception as e:\n",
        "                list_retries += 1\n",
        "                print(f\"  Error fetching trace list page {page} (Attempt {list_retries}/{max_retries}): {e}\")\n",
        "                if \"429\" in str(e):\n",
        "                    sleep_time = 2 ** list_retries\n",
        "                    print(f\"    Rate limit hit on list(). Sleeping for {sleep_time}s...\")\n",
        "                    time.sleep(sleep_time)\n",
        "                elif list_retries >= max_retries:\n",
        "                    print(\"    Max retries reached for fetching list. Stopping migration.\")\n",
        "                    sys.exit(1)\n",
        "                else:\n",
        "                    if list_retries == 1: time.sleep(2)\n",
        "                    else:\n",
        "                        print(\"    Non-rate-limit error fetching list. Stopping migration.\")\n",
        "                        sys.exit(1)\n",
        "\n",
        "        if not fetch_list_success or trace_list is None or not trace_list.data:\n",
        "            if trace_list is None or not trace_list.data: print(\"No more traces found on this page or in total.\")\n",
        "            break\n",
        "\n",
        "        print(f\"  Fetched {len(trace_list.data)} trace summaries on page {trace_list.meta.page}/{getattr(trace_list.meta, 'total_pages', 'N/A')}.\")\n",
        "\n",
        "        # Store event details for better error reporting if batch fails\n",
        "        current_batch_event_map = {}\n",
        "\n",
        "        for trace_info in trace_list.data:\n",
        "            source_trace_id = trace_info.id\n",
        "            print(f\"    Processing source trace ID: {source_trace_id}\")\n",
        "            source_trace_full = None\n",
        "            ingestion_batch = None\n",
        "\n",
        "            # Fetch full trace details with retry\n",
        "            fetch_detail_success = False\n",
        "            detail_retries = 0\n",
        "            while not fetch_detail_success and detail_retries < max_retries:\n",
        "                 current_sleep_get = sleep_between_gets * (2 ** detail_retries)\n",
        "                 time.sleep(current_sleep_get)\n",
        "                 try:\n",
        "                     source_trace_full = langfuse_source.api.trace.get(source_trace_id)\n",
        "                     fetch_detail_success = True\n",
        "                 except Exception as e:\n",
        "                     detail_retries += 1\n",
        "                     print(f\"      Error fetching details for trace {source_trace_id} (Attempt {detail_retries}/{max_retries}): {e}\")\n",
        "                     if \"429\" in str(e):\n",
        "                          sleep_time = 2 ** detail_retries\n",
        "                          print(f\"        Rate limit hit on get(). Sleeping for {sleep_time}s...\")\n",
        "                          time.sleep(sleep_time)\n",
        "                     elif detail_retries >= max_retries:\n",
        "                          print(f\"        Max retries reached fetching details for trace {source_trace_id}.\")\n",
        "                          total_failed_fetch += 1\n",
        "                     else: # Non-429 error\n",
        "                          print(f\"        Non-rate-limit error fetching details for trace {source_trace_id}. Failing this trace.\")\n",
        "                          total_failed_fetch += 1\n",
        "                          break # Stop retrying this trace's fetch\n",
        "\n",
        "            if not fetch_detail_success: continue # Skip to next trace\n",
        "\n",
        "            # Transform trace\n",
        "            try:\n",
        "                ingestion_batch = transform_trace_to_ingestion_batch(source_trace_full)\n",
        "                if not ingestion_batch:\n",
        "                    print(f\"      Skipping trace {source_trace_id} due to transformation error (returned empty batch).\")\n",
        "                    total_failed_transform += 1\n",
        "                    continue\n",
        "                # Store event details for potential error logging\n",
        "                current_batch_event_map = {event.id: event for event in ingestion_batch}\n",
        "            except Exception as e:\n",
        "                print(f\"      Critical Error transforming trace {source_trace_id}: {e}\")\n",
        "                total_failed_transform += 1\n",
        "                continue # Skip to next trace\n",
        "\n",
        "            # Push the batch with retry\n",
        "            push_success = False\n",
        "            push_retries = 0\n",
        "            while not push_success and push_retries < max_retries:\n",
        "                current_sleep_batch = sleep_between_batches * (2 ** push_retries)\n",
        "                time.sleep(current_sleep_batch)\n",
        "                try:\n",
        "                    ingestion_response = langfuse_destination.api.ingestion.batch(batch=ingestion_batch)\n",
        "                    push_success = True # Mark as attempted\n",
        "\n",
        "                    if ingestion_response.errors:\n",
        "                        print(f\"      Ingestion completed with errors for trace {source_trace_id}:\")\n",
        "                        total_failed_push += 1 # Count trace as failed if any event fails\n",
        "                        for i, error_detail in enumerate(ingestion_response.errors):\n",
        "                            status = getattr(error_detail, 'status', 'N/A')\n",
        "                            message = getattr(error_detail, 'message', 'No message')\n",
        "                            failed_event_id = getattr(error_detail, 'id', None) # Use the ID from the error\n",
        "                            failed_event = current_batch_event_map.get(failed_event_id) if failed_event_id else None\n",
        "\n",
        "                            print(f\"        Error {i+1}: Status={status}, Message={message}\")\n",
        "                            if failed_event:\n",
        "                                print(f\"          Failed Event Type: {getattr(failed_event, 'type', 'Unknown')}\")\n",
        "                                try:\n",
        "                                    body_str = json.dumps(failed_event.body.dict(), indent=2, default=str, ensure_ascii=False)\n",
        "                                    print(f\"          Failed Event Body (truncated): {body_str[:1000]}{'...' if len(body_str) > 1000 else ''}\")\n",
        "                                except Exception as dump_err: print(f\"          Failed Event Body: <Could not serialize: {dump_err}>\")\n",
        "                            else: print(f\"          Failed Event ID: {failed_event_id} (Could not find matching event in batch)\")\n",
        "                        break # Break retry loop even if errors occurred, as batch was processed partially\n",
        "                    else:\n",
        "                        print(f\"      Successfully ingested trace {source_trace_id}\")\n",
        "                        total_migrated += 1\n",
        "\n",
        "                except Exception as e:\n",
        "                    push_retries += 1\n",
        "                    print(f\"      Error pushing batch for trace {source_trace_id} (Attempt {push_retries}/{max_retries}): {e}\")\n",
        "                    if \"429\" in str(e):\n",
        "                        sleep_time = 2 ** push_retries\n",
        "                        print(f\"        Rate limit hit on batch(). Sleeping for {sleep_time}s...\")\n",
        "                        time.sleep(sleep_time)\n",
        "                    elif push_retries >= max_retries:\n",
        "                         print(f\"        Max retries reached pushing batch for trace {source_trace_id}.\")\n",
        "                         total_failed_push += 1\n",
        "                    else: # Non-429 error during push attempt\n",
        "                         print(f\"        Non-rate-limit error pushing batch for trace {source_trace_id}. Failing this trace.\")\n",
        "                         total_failed_push += 1\n",
        "                         break # Stop retrying push for this trace\n",
        "\n",
        "            # Ensure loop eventually terminates if push fails after retries\n",
        "            if not push_success and push_retries >= max_retries: continue\n",
        "\n",
        "\n",
        "        current_page_meta = getattr(trace_list.meta, 'page', page)\n",
        "        total_pages_meta = getattr(trace_list.meta, 'total_pages', page)\n",
        "        if current_page_meta >= total_pages_meta:\n",
        "             print(\"Processed the last page according to metadata.\")\n",
        "             break\n",
        "        page += 1\n",
        "\n",
        "\n",
        "    print(\"\\n--- Migration Summary ---\")\n",
        "    print(f\"Successfully migrated traces (all events ingested without reported errors): {total_migrated}\")\n",
        "    print(f\"Failed fetching details (after retries): {total_failed_fetch}\")\n",
        "    print(f\"Failed transforming data (incl. skipping): {total_failed_transform}\")\n",
        "    print(f\"Failed pushing batch or ingested with errors (after retries): {total_failed_push}\")\n",
        "    print(\"-------------------------\\n\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"Langfuse Traces Migration Script\")\n",
        "    print(\"--------------------------------\")\n",
        "    print(\"WARNING: Migrates full trace data. PRESERVES ORIGINAL TRACE IDS.\")\n",
        "    print(\"Ensure no ID collisions in the destination project!\")\n",
        "    print(\"Includes retries with exponential backoff for rate limiting.\")\n",
        "    print(\"--------------------------------\\n\")\n",
        "\n",
        "    source_pk = os.getenv(\"LANGFUSE_SOURCE_PUBLIC_KEY\")\n",
        "    source_sk = os.getenv(\"LANGFUSE_SOURCE_SECRET_KEY\")\n",
        "    source_host = os.getenv(\"LANGFUSE_SOURCE_HOST\", \"https://cloud.langfuse.com\")\n",
        "\n",
        "    dest_pk = os.getenv(\"LANGFUSE_DEST_PUBLIC_KEY\")\n",
        "    dest_sk = os.getenv(\"LANGFUSE_DEST_SECRET_KEY\")\n",
        "    dest_host = os.getenv(\"LANGFUSE_DEST_HOST\", \"https://cloud.langfuse.com\")\n",
        "\n",
        "    from_ts = os.getenv(\"LANGFUSE_MIGRATE_FROM_TIMESTAMP\")\n",
        "    to_ts = os.getenv(\"LANGFUSE_MIGRATE_TO_TIMESTAMP\")\n",
        "\n",
        "    sleep_get = float(os.getenv(\"LANGFUSE_MIGRATE_SLEEP_GET\", 0.7))\n",
        "    sleep_batch = float(os.getenv(\"LANGFUSE_MIGRATE_SLEEP_BATCH\", 0.5))\n",
        "    max_retries_config = int(os.getenv(\"LANGFUSE_MIGRATE_MAX_RETRIES\", 4))\n",
        "\n",
        "\n",
        "    if not source_pk or not source_sk: print(\"Error: Source credentials env vars required.\"); sys.exit(1)\n",
        "    if not dest_pk or not dest_sk: print(\"Error: Destination credentials env vars required.\"); sys.exit(1)\n",
        "\n",
        "    source_credentials = {\"public_key\": source_pk, \"secret_key\": source_sk, \"host\": source_host}\n",
        "    destination_credentials = {\"public_key\": dest_pk, \"secret_key\": dest_sk, \"host\": dest_host}\n",
        "\n",
        "    print(f\"Source Host: {source_host}\"); print(f\"Destination Host: {dest_host}\")\n",
        "    if from_ts: print(f\"Filtering FROM timestamp: {from_ts}\")\n",
        "    if to_ts: print(f\"Filtering TO timestamp: {to_ts}\")\n",
        "    print(f\"Base sleep between trace detail fetches: {sleep_get}s\")\n",
        "    print(f\"Base sleep between ingestion batches: {sleep_batch}s\")\n",
        "    print(f\"Max retries on rate limit: {max_retries_config}\")\n",
        "\n",
        "    confirmation = input(\"\\nProceed with trace migration? (yes/no): \").lower()\n",
        "    if confirmation == 'yes':\n",
        "        migrate_traces(source_credentials, destination_credentials, from_ts, to_ts, sleep_get, sleep_batch, max_retries_config)\n",
        "    else: print(\"Migration cancelled by user.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Djoef4zPYyu-"
      },
      "source": [
        "## Section III – Migrating Datasets\n",
        "\n",
        "This section covers migrating datasets, dataset items, and dataset runs (used for evaluations and experiments)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-lON_4yhuiBy",
        "outputId": "4da4b0e9-fc31-4b8c-9e84-6d81e4d4edb8"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import time\n",
        "import uuid\n",
        "import datetime as dt\n",
        "from langfuse import Langfuse\n",
        "from langfuse.api.resources.datasets.types import CreateDatasetRequest\n",
        "from langfuse.api.resources.dataset_items.types import CreateDatasetItemRequest\n",
        "from langfuse.api.resources.dataset_run_items.types import CreateDatasetRunItemRequest\n",
        "# Import DatasetStatus if you need to explicitly check/set it (not needed for the fix)\n",
        "# from langfuse.api.resources.commons.types import DatasetStatus\n",
        "\n",
        "# --- Helper Function for Datetime Parsing ---\n",
        "def parse_datetime(datetime_str):\n",
        "    if not datetime_str: return None\n",
        "    try:\n",
        "        if isinstance(datetime_str, str) and datetime_str.endswith('Z'):\n",
        "            datetime_str = datetime_str[:-1] + '+00:00'\n",
        "        dt_obj = dt.datetime.fromisoformat(datetime_str)\n",
        "        if dt_obj.tzinfo is None: dt_obj = dt_obj.replace(tzinfo=dt.timezone.utc)\n",
        "        return dt_obj\n",
        "    except ValueError:\n",
        "        print(f\"Error: Could not parse datetime string '{datetime_str}'. Ensure ISO 8601 format.\")\n",
        "        return None\n",
        "    except TypeError:\n",
        "         print(f\"Error: Expected string for datetime parsing, got {type(datetime_str)}.\")\n",
        "         return None\n",
        "\n",
        "# --- Main Migration Function ---\n",
        "def migrate_datasets(source_config, dest_config, sleep_between_calls=0.4):\n",
        "    \"\"\"\n",
        "    Migrates Datasets, Dataset Items, and Dataset Run Items from source to destination.\n",
        "    ASSUMES Traces/Observations were previously migrated PRESERVING ORIGINAL IDs.\n",
        "    Generates new IDs for Datasets and Dataset Items in the destination.\n",
        "    Copies original item metadata exactly. Forces migrated Items to be ACTIVE.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        langfuse_source = Langfuse(\n",
        "            public_key=source_config[\"public_key\"], secret_key=source_config[\"secret_key\"],\n",
        "            host=source_config.get(\"host\", \"https://cloud.langfuse.com\")\n",
        "        )\n",
        "        print(f\"Source client initialized for host: {langfuse_source.base_url}\")\n",
        "        langfuse_source.auth_check(); print(\"Source credentials verified.\")\n",
        "    except Exception as e: print(f\"Error source client: {e}\"); sys.exit(1)\n",
        "\n",
        "    try:\n",
        "        langfuse_destination = Langfuse(\n",
        "            public_key=dest_config[\"public_key\"], secret_key=dest_config[\"secret_key\"],\n",
        "            host=dest_config.get(\"host\", \"https://cloud.langfuse.com\")\n",
        "        )\n",
        "        print(f\"Destination client initialized for host: {langfuse_destination.base_url}\")\n",
        "        langfuse_destination.auth_check(); print(\"Destination credentials verified.\")\n",
        "    except Exception as e: print(f\"Error destination client: {e}\"); sys.exit(1)\n",
        "\n",
        "    print(\"\\n--- Starting Dataset Migration ---\")\n",
        "    datasets_migrated = 0; datasets_skipped = 0; datasets_failed = 0\n",
        "    items_migrated = 0; items_failed = 0\n",
        "    run_links_created = 0; run_links_failed = 0\n",
        "\n",
        "    page_ds = 1; limit_ds = 100\n",
        "\n",
        "    while True:\n",
        "        print(f\"\\nFetching page {page_ds} of datasets...\")\n",
        "        time.sleep(sleep_between_calls)\n",
        "        try:\n",
        "            datasets_list = langfuse_source.api.datasets.list(page=page_ds, limit=limit_ds)\n",
        "            if not datasets_list.data: print(\"No more datasets found.\"); break\n",
        "            print(f\"  Fetched {len(datasets_list.data)} datasets.\")\n",
        "        except Exception as e:\n",
        "            print(f\"  Error fetching datasets list page {page_ds}: {e}\"); datasets_failed += 1; break\n",
        "\n",
        "        for source_dataset in datasets_list.data:\n",
        "            print(f\"\\n  Processing Dataset: '{source_dataset.name}' (ID: {source_dataset.id})\")\n",
        "            dest_dataset_exists = False\n",
        "            item_id_map = {} # Reset map for each dataset\n",
        "\n",
        "            # 1. Create/Check Dataset in Destination\n",
        "            try:\n",
        "                time.sleep(sleep_between_calls)\n",
        "                try:\n",
        "                     langfuse_destination.api.datasets.get(dataset_name=source_dataset.name)\n",
        "                     print(f\"    Dataset '{source_dataset.name}' already exists in destination. Skipping creation.\")\n",
        "                     dest_dataset_exists = True; datasets_skipped += 1\n",
        "                except Exception as get_err:\n",
        "                     if \"404\" in str(get_err):\n",
        "                         print(f\"    Dataset '{source_dataset.name}' not found in destination. Creating...\")\n",
        "                         create_ds_req = CreateDatasetRequest(\n",
        "                             name=source_dataset.name, description=source_dataset.description, metadata=source_dataset.metadata\n",
        "                         )\n",
        "                         time.sleep(sleep_between_calls)\n",
        "                         created_dataset = langfuse_destination.api.datasets.create(request=create_ds_req)\n",
        "                         print(f\"    Successfully created dataset '{created_dataset.name}' in destination.\")\n",
        "                         datasets_migrated += 1; dest_dataset_exists = True\n",
        "                     else: print(f\"    Error checking destination dataset '{source_dataset.name}': {get_err}\"); datasets_failed += 1; continue\n",
        "            except Exception as e: print(f\"    Error creating/checking dataset '{source_dataset.name}' in destination: {e}\"); datasets_failed += 1; continue\n",
        "            if not dest_dataset_exists: continue\n",
        "\n",
        "            # 2. Migrate Dataset Items\n",
        "            print(f\"    Fetching items for dataset '{source_dataset.name}'...\")\n",
        "            page_item = 1; limit_item = 100\n",
        "            while True:\n",
        "                # print(f\"      Fetching page {page_item} of items...\") # Less verbose logging\n",
        "                time.sleep(sleep_between_calls)\n",
        "                try:\n",
        "                    items_list = langfuse_source.api.dataset_items.list(\n",
        "                        dataset_name=source_dataset.name, page=page_item, limit=limit_item\n",
        "                    )\n",
        "                    if not items_list.data: break # Done with items for this dataset\n",
        "                    # print(f\"        Fetched {len(items_list.data)} items.\") # Less verbose logging\n",
        "                except Exception as e: print(f\"        Error fetching items list page {page_item} for dataset '{source_dataset.name}': {e}\"); items_failed += 999; break\n",
        "\n",
        "                for source_item in items_list.data:\n",
        "                    # print(f\"          Migrating item ID: {source_item.id}\") # Less verbose logging\n",
        "\n",
        "                    # FIXED: Pass original metadata directly without modification\n",
        "                    create_item_req = CreateDatasetItemRequest(\n",
        "                        dataset_name=source_dataset.name,\n",
        "                        input=source_item.input,\n",
        "                        expected_output=source_item.expected_output,\n",
        "                        metadata=source_item.metadata, # Pass original metadata\n",
        "                        # status=source_item.status, # Removed to default to ACTIVE\n",
        "                    )\n",
        "                    try:\n",
        "                        time.sleep(sleep_between_calls)\n",
        "                        created_item = langfuse_destination.api.dataset_items.create(request=create_item_req)\n",
        "                        item_id_map[source_item.id] = created_item.id; items_migrated += 1\n",
        "                    except Exception as e: print(f\"          Error creating item (source ID {source_item.id}) in dest dataset '{source_dataset.name}': {e}\"); items_failed += 1\n",
        "\n",
        "                if items_list.meta.page >= getattr(items_list.meta, 'total_pages', page_item): break\n",
        "                page_item += 1\n",
        "\n",
        "            print(f\"    Finished processing items for dataset '{source_dataset.name}'. Adding short delay before processing runs.\")\n",
        "            time.sleep(sleep_between_calls * 2)\n",
        "\n",
        "            # 3. Migrate Dataset Run Items\n",
        "            print(f\"    Fetching runs for dataset '{source_dataset.name}'...\")\n",
        "            page_run = 1; limit_run = 100\n",
        "            while True:\n",
        "                # print(f\"      Fetching page {page_run} of runs metadata...\") # Less verbose logging\n",
        "                time.sleep(sleep_between_calls)\n",
        "                try:\n",
        "                    runs_list = langfuse_source.api.datasets.get_runs(\n",
        "                        dataset_name=source_dataset.name, page=page_run, limit=limit_run\n",
        "                    )\n",
        "                    if not runs_list.data: break # Done with runs for this dataset\n",
        "                    # print(f\"        Fetched {len(runs_list.data)} runs metadata.\") # Less verbose logging\n",
        "                except Exception as e: print(f\"        Error fetching runs list page {page_run} for dataset '{source_dataset.name}': {e}\"); run_links_failed += 999; break\n",
        "\n",
        "                for source_run_summary in runs_list.data:\n",
        "                     print(f\"        Processing run: '{source_run_summary.name}'\")\n",
        "                     try:\n",
        "                          time.sleep(sleep_between_calls)\n",
        "                          source_run_full = langfuse_source.api.datasets.get_run(\n",
        "                               dataset_name=source_dataset.name, run_name=source_run_summary.name\n",
        "                          )\n",
        "                     except Exception as e: print(f\"          Error fetching full details for run '{source_run_summary.name}': {e}\"); run_links_failed += 1; continue\n",
        "\n",
        "                     for source_run_item in source_run_full.dataset_run_items:\n",
        "                          new_dest_item_id = item_id_map.get(source_run_item.dataset_item_id)\n",
        "                          if not new_dest_item_id: print(f\"          Warning: Could not find dest mapping for source item ID {source_run_item.dataset_item_id} in run '{source_run_summary.name}'. Skipping link.\"); run_links_failed += 1; continue\n",
        "                          if not source_run_item.trace_id and not source_run_item.observation_id: print(f\"          Warning: Source run item for item {source_run_item.dataset_item_id} lacks trace/observation ID. Skipping link.\"); run_links_failed += 1; continue\n",
        "\n",
        "                          run_metadata = source_run_summary.metadata # Pass original run metadata\n",
        "\n",
        "                          create_run_item_req = CreateDatasetRunItemRequest(\n",
        "                              run_name=source_run_summary.name,\n",
        "                              dataset_item_id=new_dest_item_id,\n",
        "                              trace_id=source_run_item.trace_id,\n",
        "                              observation_id=source_run_item.observation_id,\n",
        "                              run_description=source_run_summary.description,\n",
        "                              metadata=run_metadata or None # Pass original run metadata here\n",
        "                          )\n",
        "                          try:\n",
        "                              time.sleep(sleep_between_calls)\n",
        "                              langfuse_destination.api.dataset_run_items.create(request=create_run_item_req)\n",
        "                              run_links_created += 1\n",
        "                          except Exception as e: print(f\"          Error creating run item link for dest item {new_dest_item_id} in run '{source_run_summary.name}': {e}\"); run_links_failed += 1\n",
        "\n",
        "                if runs_list.meta.page >= getattr(runs_list.meta, 'total_pages', page_run): break\n",
        "                page_run += 1\n",
        "\n",
        "        if datasets_list.meta.page >= getattr(datasets_list.meta, 'total_pages', page_ds): print(\"Processed the last page of datasets.\"); break\n",
        "        page_ds += 1\n",
        "\n",
        "    print(\"\\n--- Migration Summary ---\")\n",
        "    print(f\"Datasets Migrated: {datasets_migrated}\"); print(f\"Datasets Skipped (Already Existed): {datasets_skipped}\"); print(f\"Datasets Failed: {datasets_failed}\")\n",
        "    print(\"---\")\n",
        "    print(f\"Dataset Items Migrated: {items_migrated}\"); print(f\"Dataset Items Failed: {items_failed}\")\n",
        "    print(\"---\")\n",
        "    print(f\"Dataset Run Item Links Created: {run_links_created}\"); print(f\"Dataset Run Item Links Failed/Skipped: {run_links_failed}\")\n",
        "    print(\"-------------------------\\n\")\n",
        "\n",
        "# --- Main Execution Block ---\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"Langfuse Datasets Migration Script\")\n",
        "    print(\"----------------------------------\")\n",
        "    print(\"WARNING: Migrates Datasets, Items, and Run Item links.\")\n",
        "    print(\"ASSUMES TRACES/OBSERVATIONS WERE MIGRATED PRESERVING ORIGINAL IDs.\")\n",
        "    print(\"Generates new IDs for Datasets and Items (forced ACTIVE) in the destination.\")\n",
        "    print(\"Copies original item metadata without modification.\")\n",
        "    print(\"----------------------------------\\n\")\n",
        "\n",
        "    source_pk = os.getenv(\"LANGFUSE_SOURCE_PUBLIC_KEY\"); source_sk = os.getenv(\"LANGFUSE_SOURCE_SECRET_KEY\")\n",
        "    source_host = os.getenv(\"LANGFUSE_SOURCE_HOST\", \"https://cloud.langfuse.com\")\n",
        "    dest_pk = os.getenv(\"LANGFUSE_DEST_PUBLIC_KEY\"); dest_sk = os.getenv(\"LANGFUSE_DEST_SECRET_KEY\")\n",
        "    dest_host = os.getenv(\"LANGFUSE_DEST_HOST\", \"https://cloud.langfuse.com\")\n",
        "    sleep_calls = float(os.getenv(\"LANGFUSE_MIGRATE_SLEEP\", 0.4))\n",
        "\n",
        "    if not source_pk or not source_sk: print(\"Error: Source credentials env vars required.\"); sys.exit(1)\n",
        "    if not dest_pk or not dest_sk: print(\"Error: Destination credentials env vars required.\"); sys.exit(1)\n",
        "\n",
        "    source_credentials = {\"public_key\": source_pk, \"secret_key\": source_sk, \"host\": source_host}\n",
        "    destination_credentials = {\"public_key\": dest_pk, \"secret_key\": dest_sk, \"host\": dest_host}\n",
        "\n",
        "    print(f\"Source Host: {source_host}\"); print(f\"Destination Host: {dest_host}\")\n",
        "    print(f\"Sleep between API calls: {sleep_calls}s\")\n",
        "\n",
        "    confirmation = input(\"\\nProceed with dataset migration (ASSUMING Trace IDs were preserved)? (yes/no): \").lower()\n",
        "    if confirmation == 'yes':\n",
        "        migrate_datasets(source_credentials, destination_credentials, sleep_calls)\n",
        "    else: print(\"Migration cancelled by user.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vzm3FtOHxZyu"
      },
      "source": [
        "## Other Possible Migratable Objects\n",
        "\n",
        "The Langfuse Public API allows migrating other configuration objects, although they are less commonly required than prompts, traces, and datasets. Implementing their migration follows a similar pattern (list from source, create in destination).\n",
        "\n",
        "*   **Score Configs (`/api/public/score-configs`):** Define configurations for how scores are calculated or displayed (e.g., category mappings, numeric ranges). Useful if you rely heavily on predefined score structures. [API Ref](https://api.reference.langfuse.com/#tag/Score-Configs)\n",
        "*   **Model Definitions (`/api/public/models`):** Define custom model pricing or metadata for models not built-in to Langfuse. [API Ref](https://api.reference.langfuse.com/#tag/Models)\n",
        "*   **Annotation Queues:** (https://api.reference.langfuse.com/#tag/annotationqueues)\n",
        "\n",
        "## UI / Manual Migration\n",
        "\n",
        "Some Langfuse features are primarily managed through the UI and do not currently have direct Public API endpoints for creation or migration:\n",
        "\n",
        "*   **Model-Based Evaluators:** Configurations for running evaluations using LLMs (e.g., \"LLM-as-a-Judge\"). These need to be recreated manually in the destination project's UI.\n",
        "*   **Dashboards:** Custom analytics dashboards created in the UI. You can often export the dashboard configuration as JSON from the source UI and import it into the destination UI.\n",
        "*   **RBAC / Team Members:** User roles and permissions are managed at the project or organization level, typically through the UI or specific SSO/SCIM integrations (for self-hosted enterprise). They are not part of a standard project data migration via this script."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

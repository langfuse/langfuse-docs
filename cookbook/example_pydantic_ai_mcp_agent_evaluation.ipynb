{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<!-- NOTEBOOK_METADATA source: \"Jupyter Notebook\" title: \"Agent Evaluation: How to Evaluate LLM Agents\" seoTitle: \"Agent Evaluation: How to Evaluate LLM Agents (Metrics, Strategies & Examples)\" description: \"Complete guide to agent evaluation. Learn agent evaluation metrics like trajectory accuracy and tool selection, evaluation strategies (black-box, glass-box, white-box), and how to build automated agent evaluation pipelines with LLM-as-a-judge scoring.\" category: \"Evaluation\" -->\n",
        "\n",
        "# Agent Evaluation: How to Evaluate LLM Agents\n",
        "\n",
        "Evaluating AI agents is fundamentally different from evaluating simple LLM calls. Agents make autonomous, multi-step decisions — calling tools, searching databases, and chaining reasoning — which means a single accuracy score on the final output is not enough. You need to evaluate **what the agent did** (its trajectory), **how it did it** (each individual step), and **whether the result is correct** (the final response).\n",
        "\n",
        "This guide provides a comprehensive framework for agent evaluation. You will learn how to measure agent behavior at three levels — final response, trajectory, and single step — and how to automate these evaluations using [Langfuse](https://langfuse.com) for [tracing](/docs/observability/overview), [datasets](/docs/evaluation/experiments/datasets), and [LLM-as-a-judge evaluations](/docs/evaluation/evaluation-methods/llm-as-a-judge). While the code examples use Pydantic AI, the evaluation strategies apply to any agent framework including [LangGraph](/guides/cookbook/example_langgraph_agents), [OpenAI Agents](/docs/observability/sdk/instrumentation), and others.\n",
        "\n",
        "## What is an LLM Agent?\n",
        "\n",
        "An LLM agent is more than just a single call to a language model. It's an autonomous system that operates in a continuous loop of reasoning and action. The loop begins when the LLM receives an input — either from a user or as feedback from a previous step. Based on this input, the LLM decides on an **action**, which often involves calling an external tool like a search API, a database query, or a code interpreter. This action interacts with an **environment**, which then produces **feedback** (like search results or data) that is fed back to the LLM.\n",
        "\n",
        "This cycle of reasoning, action, environment interaction, and feedback continues until the agent decides to stop and generate a final answer. This entire sequence of events is what we call a **\"trace\"** or a **\"trajectory\"** — and it's what makes agent evaluation uniquely challenging compared to evaluating a single LLM call.\n",
        "\n",
        "<Frame>\n",
        "![LLM Agent](/images/cookbook/example_pydantic_ai_mcp_agent_evaluation/agent-overview.png)\n",
        "</Frame>\n",
        "\n",
        "## Why Agent Evaluation Matters\n",
        "\n",
        "Evaluating agents is critical because they can fail in ways that simple LLM applications cannot. A chatbot might give a wrong answer, but an agent might call the wrong tool, execute actions in the wrong order, get stuck in a loop, or produce a correct final answer through an unsafe or inefficient path. Without structured agent evaluation, these failure modes are invisible — you only see the final output, not the broken reasoning chain that produced it.\n",
        "\n",
        "Agent evaluation lets you catch regressions before they reach users, compare different agent configurations (models, prompts, tool sets) objectively, and build confidence that your agent handles edge cases correctly. For more on evaluation fundamentals, see [Evaluation Concepts](/docs/evaluation/core-concepts).\n",
        "\n",
        "## Common Agent Evaluation Challenges\n",
        "\n",
        "When building and evaluating agents, three problems show up again and again: **understanding, specification,** and **generalization**.\n",
        "\n",
        "- **Lack of observability.** You often don't know what the agent actually does on real traffic — what tools it calls, what reasoning it follows, and where it gets stuck. Without systematic [trace inspection](/docs/observability/overview) and linking traces to user feedback, debugging is guesswork. This is why [agent monitoring](/blog/2024-07-ai-agent-observability-with-langfuse) is the foundation for any evaluation strategy.\n",
        "\n",
        "- **Underspecified tasks.** Prompts and examples frequently don't encode what \"good\" behavior is, so the agent improvises in unpredictable ways. Clear evaluation criteria (what tools should be called, what facts should be included) force you to define success concretely.\n",
        "\n",
        "- **Failure to generalize.** Even once you've tightened the spec, the agent may perform well on a few handpicked examples but fail on slightly different real-world queries. Systematic, [dataset-based evaluations](/docs/evaluation/experiments/datasets) at scale are the only way to check robustness.\n",
        "\n",
        "<Frame>\n",
        "![LLM Agent](/images/cookbook/example_pydantic_ai_mcp_agent_evaluation/evaluations.png)\n",
        "</Frame>\n",
        "\n",
        "## Agent Evaluation Metrics\n",
        "\n",
        "Before choosing an evaluation strategy, it helps to understand what you can measure. Agent evaluation metrics fall into several categories depending on whether you're assessing the outcome, the process, or the system performance.\n",
        "\n",
        "| Metric | What It Measures | Evaluation Level |\n",
        "| --- | --- | --- |\n",
        "| **Task Completion** | Does the final output fully satisfy the user's request? | Final Response |\n",
        "| **Factual Accuracy** | Are all facts in the response correct and verifiable? | Final Response |\n",
        "| **Tool Selection Accuracy** | Did the agent choose the correct tools for the task? | Trajectory |\n",
        "| **Trajectory Correctness** | Did the agent follow the expected sequence of actions? | Trajectory |\n",
        "| **Trajectory Efficiency** | Did the agent reach the answer with minimal unnecessary steps? | Trajectory |\n",
        "| **Search/Query Quality** | Are search queries relevant and well-formed? | Single Step |\n",
        "| **Reasoning Quality** | Does each step follow logically from the previous context? | Single Step |\n",
        "| **Cost & Latency** | How much does each agent run cost in tokens and wall-clock time? | System |\n",
        "| **Safety & Guardrails** | Does the agent stay within defined operational boundaries? | All Levels |\n",
        "\n",
        "In this guide, we focus on the first six metrics and show how to automate them using [LLM-as-a-judge evaluators](/docs/evaluation/evaluation-methods/llm-as-a-judge). For cost and latency tracking, see [Token & Cost Tracking](/docs/observability/features/token-and-cost-tracking).\n",
        "\n",
        "## The 3 Phases of Agent Evaluation\n",
        "\n",
        "Agent evaluation is not a one-time activity — it evolves as your agent matures. The process has three distinct phases:\n",
        "\n",
        "**Phase 1: Early Development (Manual Tracing)**  \n",
        "When you're first building an agent, the most valuable thing you can do is inspect its [traces](/docs/observability/overview). Manual tracing gives you immediate insight into the agent's reasoning, tool calls, and failure points. Use Langfuse's trace viewer to step through each action the agent took.\n",
        "\n",
        "**Phase 2: First Users (Online Evaluation)**  \n",
        "As real users interact with your agent, implement feedback mechanisms — like thumbs-up/thumbs-down buttons — to flag problematic traces for review. You can also set up automated [online evaluators](/docs/evaluation/core-concepts#online-evaluation) that score production traces in real time.\n",
        "\n",
        "**Phase 3: Scaling (Offline Evaluation)**  \n",
        "The final phase, and the focus of this guide, is creating an automated offline evaluation pipeline. As you scale, you can't manually review every trace. You need a \"gold standard\" [dataset](/docs/evaluation/experiments/datasets) of inputs and their expected outputs or trajectories. This benchmark allows you to [run experiments](/docs/evaluation/experiments/experiments-via-sdk), prevent regressions, and confidently iterate on prompts, models, and tool configurations.\n",
        "\n",
        "<Frame>\n",
        "![LLM Agent](/images/cookbook/example_pydantic_ai_mcp_agent_evaluation/issues.png)\n",
        "</Frame>\n",
        "\n",
        "## Three Agent Evaluation Strategies\n",
        "\n",
        "This guide covers three practical, automated evaluation strategies. Each operates at a different level of granularity and answers a different question about your agent's behavior.\n",
        "\n",
        "| Strategy | Level | Question It Answers | Pros | Cons |\n",
        "| --- | --- | --- | --- | --- |\n",
        "| **Final Response** (Black-Box) | Output only | Is the answer correct? | Framework-agnostic, simple to set up | Doesn't explain *why* a failure occurred |\n",
        "| **Trajectory** (Glass-Box) | Full trace | Did the agent take the right path? | Pinpoints where reasoning broke down | Requires defining expected tool sequences |\n",
        "| **Single Step** (White-Box) | Per-step | Is each individual decision correct? | Most granular, like a unit test | Most effort to set up and maintain |\n",
        "\n",
        "**1) Final Response Evaluation (Black-Box):**  \n",
        "This method evaluates only the user's input and the agent's final answer, ignoring the internal steps entirely. It's the simplest to set up and works with any agent framework, but it cannot tell you *why* a failure occurred.\n",
        "\n",
        "**2) Trajectory Evaluation (Glass-Box):**  \n",
        "This method checks whether the agent took the \"correct path.\" It compares the agent's actual sequence of tool calls against the expected sequence from a benchmark dataset. When the final answer is wrong, trajectory evaluation pinpoints exactly where in the reasoning process the failure occurred.\n",
        "\n",
        "**3) Single Step Evaluation (White-Box):**  \n",
        "This is the most granular evaluation strategy, acting like a unit test for agent reasoning. Instead of running the whole agent, it tests each decision-making step in isolation to see if it produces the expected next action. This is especially useful for validating that search queries, API parameters, or tool selections are correct.\n",
        "\n",
        "## Implementation: Evaluate an Agent Step-by-Step\n",
        "\n",
        "Below, we define a sample agent, create a benchmark dataset, and set up automated [LLM-as-a-judge](/docs/evaluation/evaluation-methods/llm-as-a-judge) evaluations in Langfuse. While the code uses Pydantic AI, the evaluation patterns generalize to any agent framework.\n",
        "\n",
        "> **Want to see agent evaluation with other frameworks?** Check out the [LangGraph Agent Evaluation](/guides/cookbook/example_langgraph_agents) guide for a LangGraph-specific walkthrough.\n",
        "\n",
        ""
      ],
      "id": "header"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 0: Install Packages"
      ],
      "id": "step0"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%pip install -q --upgrade \"pydantic-ai[mcp]\" langfuse openai nest_asyncio aiohttp"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "install"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 1: Set Environment Variables\n",
        "\n",
        "Get your Langfuse API keys from [project settings](https://cloud.langfuse.com)."
      ],
      "id": "step1"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"LANGFUSE_PUBLIC_KEY\"] = \"pk-lf-...\"\n",
        "os.environ[\"LANGFUSE_SECRET_KEY\"] = \"sk-lf-...\"\n",
        "os.environ[\"LANGFUSE_HOST\"] = \"https://cloud.langfuse.com\"  # EU region\n",
        "# os.environ[\"LANGFUSE_HOST\"] = \"https://us.cloud.langfuse.com\"  # US region\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"sk-proj-...\""
      ],
      "execution_count": null,
      "outputs": [],
      "id": "env"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 2: Enable Langfuse Tracing\n",
        "\n",
        "Enable automatic tracing for Pydantic AI agents."
      ],
      "id": "step2"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from langfuse import get_client\n",
        "from pydantic_ai.agent import Agent\n",
        "\n",
        "langfuse = get_client()\n",
        "assert langfuse.auth_check(), \"Langfuse auth failed - check your keys\"\n",
        "\n",
        "Agent.instrument_all()\n",
        "print(\"✅ Pydantic AI instrumentation enabled\")"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "tracing"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 3: Create Agent\n",
        "\n",
        "Build an agent that searches Langfuse docs using the [Langfuse Docs MCP Server](https://langfuse.com/docs/docs-mcp)."
      ],
      "id": "step3"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from typing import Any\n",
        "from pydantic_ai import Agent, RunContext\n",
        "from pydantic_ai.mcp import MCPServerStreamableHTTP, CallToolFunc, ToolResult\n",
        "\n",
        "LANGFUSE_MCP_URL = \"https://langfuse.com/api/mcp\"\n",
        "\n",
        "async def run_agent(item, system_prompt=\"You are an expert on Langfuse. \", model=\"openai:gpt-4o-mini\"):\n",
        "    langfuse.update_current_trace(input=item.input)\n",
        "\n",
        "    tool_call_history = []\n",
        "\n",
        "    async def process_tool_call(\n",
        "        ctx: RunContext[Any],\n",
        "        call_tool: CallToolFunc,\n",
        "        tool_name: str,\n",
        "        args: dict[str, Any],\n",
        "    ) -> ToolResult:\n",
        "        tool_call_history.append({\"tool_name\": tool_name, \"args\": args})\n",
        "        return await call_tool(tool_name, args)\n",
        "    \n",
        "    langfuse_docs_server = MCPServerStreamableHTTP(\n",
        "        url=LANGFUSE_MCP_URL,\n",
        "        process_tool_call=process_tool_call,\n",
        "    )\n",
        "\n",
        "    agent = Agent(\n",
        "        model=model,\n",
        "        system_prompt=system_prompt,\n",
        "        toolsets=[langfuse_docs_server],\n",
        "    )\n",
        "\n",
        "    async with agent:\n",
        "        result = await agent.run(item.input[\"question\"])\n",
        "        \n",
        "        langfuse.update_current_trace(\n",
        "            output=result.output,\n",
        "            metadata={\"tool_call_history\": tool_call_history},\n",
        "        )\n",
        "\n",
        "        return result.output, tool_call_history"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "agent"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 4: Create Evaluation Dataset\n",
        "\n",
        "Build a benchmark dataset with test cases. Each case includes:\n",
        "- `input`: User question\n",
        "- `expected_output.response_facts`: Key facts the response must contain\n",
        "- `expected_output.trajectory`: Expected sequence of tool calls\n",
        "- `expected_output.search_term`: Expected search query (if applicable)"
      ],
      "id": "step4"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "test_cases = [\n",
        "    {\n",
        "        \"input\": {\"question\": \"What is Langfuse?\"},\n",
        "        \"expected_output\": {\n",
        "            \"response_facts\": [\n",
        "                \"Open Source LLM Engineering Platform\",\n",
        "                \"Product modules: Tracing, Evaluation and Prompt Management\"\n",
        "            ],\n",
        "            \"trajectory\": [\"getLangfuseOverview\"],\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        \"input\": {\"question\": \"How to trace a python application with Langfuse?\"},\n",
        "        \"expected_output\": {\n",
        "            \"response_facts\": [\n",
        "                \"Python SDK, you can use the observe() decorator\",\n",
        "                \"Lots of integrations, LangChain, LlamaIndex, Pydantic AI, and many more.\"\n",
        "            ],\n",
        "            \"trajectory\": [\"getLangfuseOverview\", \"searchLangfuseDocs\"],\n",
        "            \"search_term\": \"Python Tracing\"\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        \"input\": {\"question\": \"How to connect to the Langfuse Docs MCP server?\"},\n",
        "        \"expected_output\": {\n",
        "            \"response_facts\": [\n",
        "                \"Connect via the MCP server endpoint: https://langfuse.com/api/mcp\",\n",
        "                \"Transport protocol: `streamableHttp`\"\n",
        "            ],\n",
        "            \"trajectory\": [\"getLangfuseOverview\"]\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        \"input\": {\"question\": \"How long are traces retained in langfuse?\"},\n",
        "        \"expected_output\": {\n",
        "            \"response_facts\": [\n",
        "                \"By default, traces are retained indefinitely\",\n",
        "                \"You can set custom data retention policy in the project settings\"\n",
        "            ],\n",
        "            \"trajectory\": [\"getLangfuseOverview\", \"searchLangfuseDocs\"],\n",
        "            \"search_term\": \"Data retention\"\n",
        "        }\n",
        "    }\n",
        "]\n",
        "\n",
        "DATASET_NAME = \"pydantic-ai-mcp-agent-evaluation\"\n",
        "\n",
        "dataset = langfuse.create_dataset(name=DATASET_NAME)\n",
        "for case in test_cases:\n",
        "    langfuse.create_dataset_item(\n",
        "        dataset_name=DATASET_NAME,\n",
        "        input=case[\"input\"],\n",
        "        expected_output=case[\"expected_output\"]\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "dataset"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 5: Set Up Evaluators\n",
        "\n",
        "Create three evaluators in the Langfuse UI. Each tests a different aspect of agent behavior. You can find the documentation on setting them up [here](https://langfuse.com/docs/evaluation/evaluation-methods/llm-as-a-judge). \n",
        "\n",
        "#### 1. Final Response Evaluation (Black Box)\n",
        "\n",
        "Tests output quality. Works regardless of internal implementation.\n",
        "\n",
        "<Frame>\n",
        "![Final Response Evaluation](/images/cookbook/example_pydantic_ai_mcp_agent_evaluation/eval-final-response.png)\n",
        "</Frame>\n",
        "\n",
        "**Prompt template:**\n",
        "\n",
        "```markdown\n",
        "You are a teacher grading a student based on the factual correctness of their statements.\n",
        "\n",
        "### Examples\n",
        "\n",
        "#### Example 1:\n",
        "- Response: \"The sun is shining brightly.\"\n",
        "- Facts to verify: [\"The sun is up.\", \"It is a beautiful day.\"]\n",
        "- Reasoning: The response includes both facts.\n",
        "- Score: 1\n",
        "\n",
        "#### Example 2:\n",
        "- Response: \"When I was in the kitchen, the dog was there\"\n",
        "- Facts to verify: [\"The cat is on the table.\", \"The dog is in the kitchen.\"]\n",
        "- Reasoning: The response mentions the dog but not the cat.\n",
        "- Score: 0\n",
        "\n",
        "### New Student Response\n",
        "\n",
        "- Response: {{response}}\n",
        "- Facts to verify: {{facts_to_verify}}\n",
        "```\n",
        "\n",
        "#### 2. Trajectory Evaluation (Glass Box)\n",
        "\n",
        "Verifies the agent used the correct sequence of tools.\n",
        "\n",
        "<Frame>\n",
        "![Trajectory Evaluation](/images/cookbook/example_pydantic_ai_mcp_agent_evaluation/eval-trajectory.png)\n",
        "</Frame>\n",
        "\n",
        "**Prompt template:**\n",
        "\n",
        "```markdown\n",
        "You are comparing two lists of strings. Check whether the lists contain exactly the same items. Order does not matter.\n",
        "\n",
        "## Examples\n",
        "\n",
        "Expected: [\"searchWeb\", \"visitWebsite\"]\n",
        "Output: [\"searchWeb\"]\n",
        "Reasoning: Output missing \"visitWebsite\".\n",
        "Score: 0\n",
        "\n",
        "Expected: [\"drawImage\", \"visitWebsite\", \"speak\"]\n",
        "Output: [\"visitWebsite\", \"speak\", \"drawImage\"]\n",
        "Reasoning: Output matches expected items.\n",
        "Score: 1\n",
        "\n",
        "Expected: [\"getNews\"]\n",
        "Output: [\"getNews\", \"watchTv\"]\n",
        "Reasoning: Output contains unexpected \"watchTv\".\n",
        "Score: 0\n",
        "\n",
        "## This Exercise\n",
        "\n",
        "Expected: {{expected}}\n",
        "Output: {{output}}\n",
        "```\n",
        "\n",
        "#### 3. Search Quality Evaluation\n",
        "\n",
        "Validates search query quality when agents search documentation.\n",
        "\n",
        "<Frame>\n",
        "![Trajectory Evaluation](/images/cookbook/example_pydantic_ai_mcp_agent_evaluation/eval-single-step.png)\n",
        "</Frame>\n",
        "\n",
        "**Prompt template:**\n",
        "\n",
        "```markdown\n",
        "You are grading whether a student searched for the right information. The search term should correspond vaguely with the expected term.\n",
        "\n",
        "### Examples\n",
        "\n",
        "Response: \"How can I contact support?\"\n",
        "Expected search topics: Support\n",
        "Reasoning: Response searches for support.\n",
        "Score: 1\n",
        "\n",
        "Response: \"Deployment\"\n",
        "Expected search topics: Tracing\n",
        "Reasoning: Response doesn't match expected topic.\n",
        "Score: 0\n",
        "\n",
        "Response: (empty)\n",
        "Expected search topics: (empty)\n",
        "Reasoning: No search expected, no search done.\n",
        "Score: 1\n",
        "\n",
        "### New Student Response\n",
        "\n",
        "Response: {{search}}\n",
        "Expected search topics: {{expected_search_topic}}\n",
        "```\n",
        "\n",
        "Create these evaluators in Langfuse UI under **Prompts** → **Create Evaluator**."
      ],
      "id": "step5"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 6: Run Experiments\n",
        "\n",
        "Run agents on your dataset. Compare different models and prompts to find the best configuration."
      ],
      "id": "step6"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "dataset = langfuse.get_dataset(DATASET_NAME)\n",
        "\n",
        "result = dataset.run_experiment(\n",
        "    name=\"Production Model Test\",\n",
        "    description=\"Monthly evaluation of our production model\",\n",
        "    task=run_agent\n",
        ")\n",
        "\n",
        "print(result.format())"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "run1"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 7: Compare Multiple Configurations\n",
        "\n",
        "Test different prompts and models to find the best configuration."
      ],
      "id": "step7"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from functools import partial\n",
        "\n",
        "system_prompts = {\n",
        "    \"simple\": (\n",
        "        \"You are an expert on Langfuse. \"\n",
        "        \"Answer user questions accurately and concisely using the available MCP tools. \"\n",
        "        \"Cite sources when appropriate.\"\n",
        "    ),\n",
        "    \"nudge_search\": (\n",
        "        \"You are an expert on Langfuse. \"\n",
        "        \"Answer user questions accurately and concisely using the available MCP tools. \"\n",
        "        \"Always cite sources when appropriate. \"\n",
        "        \"When unsure, use getLangfuseOverview then search the docs. You can use these tools multiple times.\"\n",
        "    )\n",
        "}\n",
        "\n",
        "models = [\"openai:gpt-5-mini\", \"openai:gpt-5-nano\"]\n",
        "\n",
        "dataset = langfuse.get_dataset(DATASET_NAME)\n",
        "\n",
        "for prompt_name, prompt_content in system_prompts.items():\n",
        "    for test_model in models:\n",
        "        task = partial(\n",
        "            run_agent,\n",
        "            system_prompt=prompt_content,\n",
        "            model=test_model,\n",
        "        )\n",
        "\n",
        "        result = dataset.run_experiment(\n",
        "            name=f\"Test: {prompt_name} {test_model}\",\n",
        "            description=\"Comparing prompts and models\",\n",
        "            task=task\n",
        "        )\n",
        "\n",
        "        print(result.format())"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "run2"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Agent Evaluation Best Practices\n",
        "\n",
        "Based on our experience helping teams evaluate agents in production, here are key best practices:\n",
        "\n",
        "1. **Start with tracing, not scoring.** Before you build automated evaluations, spend time manually reviewing agent traces. The patterns you observe will inform what metrics matter most for your use case. Use [Langfuse tracing](/docs/observability/overview) to inspect every tool call, reasoning step, and intermediate output.\n",
        "\n",
        "2. **Define success criteria before writing evaluators.** For each test case, explicitly define what \"correct\" looks like at each level — the expected final answer, the expected tool sequence, and the expected search queries. Vague criteria lead to unreliable evaluations.\n",
        "\n",
        "3. **Use all three evaluation levels together.** Final response evaluation tells you *what* went wrong. Trajectory evaluation tells you *where* it went wrong. Single step evaluation tells you *why* it went wrong. Together, they give you a complete picture.\n",
        "\n",
        "4. **Build your dataset from real failures.** The most valuable test cases come from production traces where the agent failed. Use [annotation queues](/docs/evaluation/evaluation-methods/annotation-queues) to systematically review and label problematic traces, then add them to your evaluation dataset.\n",
        "\n",
        "5. **Run evaluations in CI/CD.** Integrate agent evaluation into your deployment pipeline using [experiments via SDK](/docs/evaluation/experiments/experiments-via-sdk). Block deployments that cause score regressions on your benchmark dataset.\n",
        "\n",
        "6. **Compare configurations systematically.** When changing prompts, models, or tools, run the same evaluation dataset across all configurations to make data-driven decisions. The experiment comparison view in Langfuse makes this straightforward.\n",
        "\n",
        "## Next Steps\n",
        "\n",
        "Now that you have a working agent evaluation pipeline, here are ways to extend it:\n",
        "\n",
        "- **Scale your dataset** with [synthetic data generation](/guides/cookbook/example_synthetic_datasets) to cover more edge cases\n",
        "- **Add online evaluation** to score production traces in real time using [LLM-as-a-Judge](/docs/evaluation/evaluation-methods/llm-as-a-judge)\n",
        "- **Evaluate multi-turn conversations** if your agent handles [multi-turn dialogue](/guides/cookbook/example_evaluating_multi_turn_conversations)\n",
        "- **Monitor agent performance** over time with [custom dashboards](/docs/metrics/features/custom-dashboards) and [score analytics](/docs/evaluation/evaluation-methods/score-analytics)\n",
        "- **Explore the full evaluation roadmap** in our [comprehensive evaluation guide](/blog/2025-11-12-evals)\n",
        "\n",
        "## Frequently Asked Questions\n",
        "\n",
        "### What is agent evaluation?\n",
        "\n",
        "Agent evaluation is the process of systematically testing and measuring the performance of AI agents — autonomous systems that use LLMs to make decisions, call tools, and complete multi-step tasks. Unlike evaluating a single LLM call, agent evaluation must assess the entire trajectory of actions, not just the final output.\n",
        "\n",
        "### How is agent evaluation different from LLM evaluation?\n",
        "\n",
        "Standard LLM evaluation checks whether a model produces a correct or high-quality response to a given prompt. Agent evaluation is more complex because agents make multiple decisions in sequence — choosing which tools to call, what parameters to pass, and when to stop. You need to evaluate not just the final answer, but also the reasoning path (trajectory) and each individual decision (single step).\n",
        "\n",
        "### What are the main types of agent evaluation?\n",
        "\n",
        "There are three main types: **Final Response (Black-Box)** evaluation checks only the end result; **Trajectory (Glass-Box)** evaluation checks whether the agent took the correct sequence of actions; and **Single Step (White-Box)** evaluation tests each individual decision in isolation. Most production systems use a combination of all three.\n",
        "\n",
        "### How do I build an agent evaluation dataset?\n",
        "\n",
        "Start by defining test cases that represent your most common and most critical user interactions. Each test case should include the user input, expected facts in the response, the expected sequence of tool calls (trajectory), and expected parameters for key tool calls. Grow your dataset over time by adding cases from real production failures.\n",
        "\n",
        "### Can I use LLM-as-a-judge for agent evaluation?\n",
        "\n",
        "Yes. LLM-as-a-judge is one of the most effective approaches for agent evaluation because agent outputs are often too complex for simple rule-based checks. You can use different judge prompts for each evaluation level — one for final response quality, one for trajectory correctness, and one for individual step quality. See the [LLM-as-a-Judge documentation](/docs/evaluation/evaluation-methods/llm-as-a-judge) for setup instructions.\n",
        "\n",
        "### How often should I run agent evaluations?\n",
        "\n",
        "Run offline evaluations (experiments) before every deployment that changes prompts, models, or tool configurations. Run online evaluations continuously on production traces to catch issues in real traffic. For a comprehensive approach, see the [evaluation overview](/docs/evaluation/overview)."
      ],
      "id": "d349e055"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
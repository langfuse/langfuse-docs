{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<!-- NOTEBOOK_METADATA source: \"Jupyter Notebook\" title: \"Agent Evaluation Guide - Pydantic AI\" description: \"Learn how to evaluate LLM agents with Langfuse: from tracing to automated evaluations. Covers final response, trajectory, and step-level evaluation strategies.\" category: \"Evaluation\" -->\n",
        "\n",
        "# Agent Evaluation Guide\n",
        "\n",
        "This guide shows how to evaluate LLM-powered agents using [Langfuse](https://langfuse.com) and [Pydantic AI](https://ai.pydantic.dev/). You'll learn three evaluation strategies:\n",
        "\n",
        "1. **Final Response Evaluation** (black box): Judge the agent's output quality\n",
        "2. **Trajectory Evaluation** (glass box): Verify the agent took the correct path\n",
        "3. **Step Evaluation** (white box): Test individual decision points\n",
        "\n",
        "## What is an Agent?\n",
        "\n",
        "An agent is an LLM-powered system that operates in a loop:\n",
        "\n",
        "1. **LLM Call**: Receives input and decides on next action\n",
        "2. **Action**: Executes tools (e.g., `search_docs()`, `get_overview()`)\n",
        "3. **Environment**: Tools return results (API responses, data, errors)\n",
        "4. **Feedback**: LLM receives results and decides next step\n",
        "5. **Stop**: Loop continues until LLM provides final answer\n",
        "\n",
        "This entire sequence is a **trace** or **trajectory**.\n",
        "\n",
        "![Agent loop diagram](https://langfuse.com/images/cookbook/agent-evaluation/agent-loop.png)\n",
        "\n",
        "## Evaluation Approach\n",
        "\n",
        "Evaluations catch three failure types:\n",
        "\n",
        "- **Underspecification**: Agent lacks clear instructions or examples\n",
        "- **Lack of Understanding**: Developer doesn't know why agent fails\n",
        "- **Generalization**: LLM fails on new, unseen inputs\n",
        "\n",
        "### Phased Implementation\n",
        "\n",
        "1. **Start with Tracing**: Manually inspect traces (15-minute setup, immediate insight)\n",
        "2. **Add User Feedback**: Capture reactions (thumbs up/down) to flag problematic traces\n",
        "3. **Build Offline Evals**: Create benchmark dataset for automated testing at scale\n",
        "\n",
        "This guide focuses on phase 3: automated offline evaluation."
      ],
      "id": "header"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 0: Install Packages"
      ],
      "id": "step0"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%pip install -q --upgrade \"pydantic-ai[mcp]\" langfuse openai nest_asyncio aiohttp"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "install"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Set Environment Variables\n",
        "\n",
        "Get your Langfuse API keys from [project settings](https://cloud.langfuse.com)."
      ],
      "id": "step1"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"LANGFUSE_PUBLIC_KEY\"] = \"pk-lf-...\"\n",
        "os.environ[\"LANGFUSE_SECRET_KEY\"] = \"sk-lf-...\"\n",
        "os.environ[\"LANGFUSE_HOST\"] = \"https://cloud.langfuse.com\"  # EU region\n",
        "# os.environ[\"LANGFUSE_HOST\"] = \"https://us.cloud.langfuse.com\"  # US region\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"sk-proj-...\""
      ],
      "execution_count": null,
      "outputs": [],
      "id": "env"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Enable Langfuse Tracing\n",
        "\n",
        "Enable automatic tracing for Pydantic AI agents."
      ],
      "id": "step2"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from langfuse import get_client\n",
        "from pydantic_ai.agent import Agent\n",
        "\n",
        "langfuse = get_client()\n",
        "assert langfuse.auth_check(), \"Langfuse auth failed - check your keys\"\n",
        "\n",
        "Agent.instrument_all()\n",
        "print(\"✅ Pydantic AI instrumentation enabled\")"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "tracing"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Create Agent\n",
        "\n",
        "Build an agent that searches Langfuse docs using the [Langfuse Docs MCP Server](https://langfuse.com/docs/docs-mcp)."
      ],
      "id": "step3"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from typing import Any\n",
        "from pydantic_ai import Agent, RunContext\n",
        "from pydantic_ai.mcp import MCPServerStreamableHTTP, CallToolFunc, ToolResult\n",
        "\n",
        "LANGFUSE_MCP_URL = \"https://langfuse.com/api/mcp\"\n",
        "\n",
        "async def run_agent(item, system_prompt=\"You are an expert on Langfuse. \", model=\"openai:gpt-4o-mini\"):\n",
        "    langfuse.update_current_trace(input=item.input)\n",
        "\n",
        "    tool_call_history = []\n",
        "\n",
        "    async def process_tool_call(\n",
        "        ctx: RunContext[Any],\n",
        "        call_tool: CallToolFunc,\n",
        "        tool_name: str,\n",
        "        args: dict[str, Any],\n",
        "    ) -> ToolResult:\n",
        "        tool_call_history.append({\"tool_name\": tool_name, \"args\": args})\n",
        "        return await call_tool(tool_name, args)\n",
        "    \n",
        "    langfuse_docs_server = MCPServerStreamableHTTP(\n",
        "        url=LANGFUSE_MCP_URL,\n",
        "        process_tool_call=process_tool_call,\n",
        "    )\n",
        "\n",
        "    agent = Agent(\n",
        "        model=model,\n",
        "        system_prompt=system_prompt,\n",
        "        toolsets=[langfuse_docs_server],\n",
        "    )\n",
        "\n",
        "    async with agent:\n",
        "        result = await agent.run(item.input[\"question\"])\n",
        "        \n",
        "        langfuse.update_current_trace(\n",
        "            output=result.output,\n",
        "            metadata={\"tool_call_history\": tool_call_history},\n",
        "        )\n",
        "\n",
        "        return result.output, tool_call_history"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "agent"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Create Evaluation Dataset\n",
        "\n",
        "Build a benchmark dataset with test cases. Each case includes:\n",
        "- `input`: User question\n",
        "- `expected_output.response_facts`: Key facts the response must contain\n",
        "- `expected_output.trajectory`: Expected sequence of tool calls\n",
        "- `expected_output.search_term`: Expected search query (if applicable)"
      ],
      "id": "step4"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "test_cases = [\n",
        "    {\n",
        "        \"input\": {\"question\": \"What is Langfuse?\"},\n",
        "        \"expected_output\": {\n",
        "            \"response_facts\": [\n",
        "                \"Open Source LLM Engineering Platform\",\n",
        "                \"Product modules: Tracing, Evaluation and Prompt Management\"\n",
        "            ],\n",
        "            \"trajectory\": [\"getLangfuseOverview\"],\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        \"input\": {\"question\": \"How to trace a python application with Langfuse?\"},\n",
        "        \"expected_output\": {\n",
        "            \"response_facts\": [\n",
        "                \"Python SDK, you can use the observe() decorator\",\n",
        "                \"Lots of integrations, LangChain, LlamaIndex, Pydantic AI, and many more.\"\n",
        "            ],\n",
        "            \"trajectory\": [\"getLangfuseOverview\", \"searchLangfuseDocs\"],\n",
        "            \"search_term\": \"Python Tracing\"\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        \"input\": {\"question\": \"How to connect to the Langfuse Docs MCP server?\"},\n",
        "        \"expected_output\": {\n",
        "            \"response_facts\": [\n",
        "                \"Connect via the MCP server endpoint: https://langfuse.com/api/mcp\",\n",
        "                \"Transport protocol: `streamableHttp`\"\n",
        "            ],\n",
        "            \"trajectory\": [\"getLangfuseOverview\"]\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        \"input\": {\"question\": \"How long are traces retained in langfuse?\"},\n",
        "        \"expected_output\": {\n",
        "            \"response_facts\": [\n",
        "                \"By default, traces are retained indefinitely\",\n",
        "                \"You can set custom data retention policy in the project settings\"\n",
        "            ],\n",
        "            \"trajectory\": [\"getLangfuseOverview\", \"searchLangfuseDocs\"],\n",
        "            \"search_term\": \"Data retention\"\n",
        "        }\n",
        "    }\n",
        "]\n",
        "\n",
        "DATASET_NAME = \"pydantic-ai-mcp-agent-evaluation\"\n",
        "\n",
        "dataset = langfuse.create_dataset(name=DATASET_NAME)\n",
        "for case in test_cases:\n",
        "    langfuse.create_dataset_item(\n",
        "        dataset_name=DATASET_NAME,\n",
        "        input=case[\"input\"],\n",
        "        expected_output=case[\"expected_output\"]\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "dataset"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Set Up Evaluators\n",
        "\n",
        "Create three evaluators in Langfuse UI. Each tests a different aspect of agent behavior.\n",
        "\n",
        "### 1. Final Response Evaluation (Black Box)\n",
        "\n",
        "Tests output quality. Works regardless of internal implementation.\n",
        "\n",
        "![Final Response Evaluation](https://langfuse.com/images/cookbook/agent-evaluation/final-response-evaluation.png)\n",
        "\n",
        "**Prompt template:**\n",
        "\n",
        "```markdown\n",
        "You are a teacher grading a student based on the factual correctness of their statements.\n",
        "\n",
        "### Examples\n",
        "\n",
        "#### Example 1:\n",
        "- Response: \"The sun is shining brightly.\"\n",
        "- Facts to verify: [\"The sun is up.\", \"It is a beautiful day.\"]\n",
        "- Reasoning: The response includes both facts.\n",
        "- Score: 1\n",
        "\n",
        "#### Example 2:\n",
        "- Response: \"When I was in the kitchen, the dog was there\"\n",
        "- Facts to verify: [\"The cat is on the table.\", \"The dog is in the kitchen.\"]\n",
        "- Reasoning: The response mentions the dog but not the cat.\n",
        "- Score: 0\n",
        "\n",
        "### New Student Response\n",
        "\n",
        "- Response: {{response}}\n",
        "- Facts to verify: {{facts_to_verify}}\n",
        "```\n",
        "\n",
        "### 2. Trajectory Evaluation (Glass Box)\n",
        "\n",
        "Verifies the agent used the correct sequence of tools.\n",
        "\n",
        "![Trajectory Evaluation](https://langfuse.com/images/cookbook/agent-evaluation/trajectory-evaluation.png)\n",
        "\n",
        "**Prompt template:**\n",
        "\n",
        "```markdown\n",
        "You are comparing two lists of strings. Check whether the lists contain exactly the same items. Order does not matter.\n",
        "\n",
        "## Examples\n",
        "\n",
        "Expected: [\"searchWeb\", \"visitWebsite\"]\n",
        "Output: [\"searchWeb\"]\n",
        "Reasoning: Output missing \"visitWebsite\".\n",
        "Score: 0\n",
        "\n",
        "Expected: [\"drawImage\", \"visitWebsite\", \"speak\"]\n",
        "Output: [\"visitWebsite\", \"speak\", \"drawImage\"]\n",
        "Reasoning: Output matches expected items.\n",
        "Score: 1\n",
        "\n",
        "Expected: [\"getNews\"]\n",
        "Output: [\"getNews\", \"watchTv\"]\n",
        "Reasoning: Output contains unexpected \"watchTv\".\n",
        "Score: 0\n",
        "\n",
        "## This Exercise\n",
        "\n",
        "Expected: {{expected}}\n",
        "Output: {{output}}\n",
        "```\n",
        "\n",
        "### 3. Search Quality Evaluation\n",
        "\n",
        "Validates search query quality when agents search documentation.\n",
        "\n",
        "**Prompt template:**\n",
        "\n",
        "```markdown\n",
        "You are grading whether a student searched for the right information. The search term should correspond vaguely with the expected term.\n",
        "\n",
        "### Examples\n",
        "\n",
        "Response: \"How can I contact support?\"\n",
        "Expected search topics: Support\n",
        "Reasoning: Response searches for support.\n",
        "Score: 1\n",
        "\n",
        "Response: \"Deployment\"\n",
        "Expected search topics: Tracing\n",
        "Reasoning: Response doesn't match expected topic.\n",
        "Score: 0\n",
        "\n",
        "Response: (empty)\n",
        "Expected search topics: (empty)\n",
        "Reasoning: No search expected, no search done.\n",
        "Score: 1\n",
        "\n",
        "### New Student Response\n",
        "\n",
        "Response: {{search}}\n",
        "Expected search topics: {{expected_search_topic}}\n",
        "```\n",
        "\n",
        "Create these evaluators in Langfuse UI under **Prompts** → **Create Evaluator**."
      ],
      "id": "step5"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Run Experiments\n",
        "\n",
        "Run agents on your dataset. Compare different models and prompts to find the best configuration."
      ],
      "id": "step6"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "dataset = langfuse.get_dataset(DATASET_NAME)\n",
        "\n",
        "result = dataset.run_experiment(\n",
        "    name=\"Production Model Test\",\n",
        "    description=\"Monthly evaluation of our production model\",\n",
        "    task=run_agent\n",
        ")\n",
        "\n",
        "print(result.format())"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "run1"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7: Compare Multiple Configurations\n",
        "\n",
        "Test different prompts and models to find the best configuration."
      ],
      "id": "step7"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from functools import partial\n",
        "\n",
        "system_prompts = {\n",
        "    \"simple\": (\n",
        "        \"You are an expert on Langfuse. \"\n",
        "        \"Answer user questions accurately and concisely using the available MCP tools. \"\n",
        "        \"Cite sources when appropriate.\"\n",
        "    ),\n",
        "    \"nudge_search\": (\n",
        "        \"You are an expert on Langfuse. \"\n",
        "        \"Answer user questions accurately and concisely using the available MCP tools. \"\n",
        "        \"Always cite sources when appropriate. \"\n",
        "        \"When unsure, use getLangfuseOverview then search the docs. You can use these tools multiple times.\"\n",
        "    )\n",
        "}\n",
        "\n",
        "models = [\"openai:gpt-4o-mini\", \"openai:gpt-4o\"]\n",
        "\n",
        "dataset = langfuse.get_dataset(DATASET_NAME)\n",
        "\n",
        "for prompt_name, prompt_content in system_prompts.items():\n",
        "    for test_model in models:\n",
        "        task = partial(\n",
        "            run_agent,\n",
        "            system_prompt=prompt_content,\n",
        "            model=test_model,\n",
        "        )\n",
        "\n",
        "        result = dataset.run_experiment(\n",
        "            name=f\"Test: {prompt_name} {test_model}\",\n",
        "            description=\"Comparing prompts and models\",\n",
        "            task=task\n",
        "        )\n",
        "\n",
        "        print(result.format())"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "run2"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
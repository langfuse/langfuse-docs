{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb39bc06",
   "metadata": {},
   "source": [
    "<!-- NOTEBOOK_METADATA source: \"Jupyter Notebook\" title: \"Example - Pydantic AI MCP Agent Evaluation\" description: \"This guide shows how to evaluate Pydantic AI MCP Agents with Langfuse using online and offline evaluation methods.\" category: \"Evaluation\" -->\n",
    "\n",
    "# LangfuseÂ Ã—Â PydanticÂ AI â€“ Agent Evals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72216ec6",
   "metadata": {},
   "source": [
    "## 1.Â Setup â€“ install packages & add credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea433d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you are running this on colab/@home commentâ€‘out what you already have\n",
    "%pip install -q --upgrade \"pydantic-ai[mcp]\" langfuse openai nest_asyncio aiohttp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb291415",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Get keys for your project from the project settings page\n",
    "# https://cloud.langfuse.com\n",
    "os.environ[\"LANGFUSE_PUBLIC_KEY\"] = \"\"\n",
    "os.environ[\"LANGFUSE_SECRET_KEY\"] = \"\"\n",
    "os.environ[\"LANGFUSE_HOST\"] = \"https://cloud.langfuse.com\" # ðŸ‡ªðŸ‡º EU region\n",
    "# os.environ[\"LANGFUSE_HOST\"] = \"https://us.cloud.langfuse.com\" # ðŸ‡ºðŸ‡¸ US region\n",
    "\n",
    "# Your openai key\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022fbb9b",
   "metadata": {},
   "source": [
    "## 2. Enable Langfuse Tracing\n",
    "\n",
    "All integrations: https://langfuse.com/integrations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eddfdf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langfuse import get_client\n",
    "from pydantic_ai.agent import Agent\n",
    "\n",
    "# Initialise Langfuse client and verify connectivity\n",
    "langfuse = get_client()\n",
    "assert langfuse.auth_check(), \"Langfuse auth failed - check your keys âœ‹\"\n",
    "\n",
    "# Turn on OpenTelemetry instrumentation for *all* future Agent instances\n",
    "Agent.instrument_all()\n",
    "print(\"âœ… Pydantic AI instrumentation enabled - traces will stream to Langfuse\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45848d67",
   "metadata": {},
   "source": [
    "## 3. Create an agent that can search the Langfuse docs\n",
    "\n",
    "We use the Lagfuse Docs MCP Server to provide tools to the agent: https://langfuse.com/docs/docs-mcp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e718f070",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic_ai import Agent, RunContext\n",
    "from pydantic_ai.mcp import MCPServerStreamableHTTP, CallToolFunc, ToolResult\n",
    "from langfuse import observe\n",
    "from typing import Any\n",
    "\n",
    "# Public MCP server that exposes Langfuse docs tools\n",
    "LANGFUSE_MCP_URL = \"https://langfuse.com/api/mcp\"\n",
    "\n",
    "@observe\n",
    "async def run_agent(question: str, system_prompt: str, model=\"openai:o3-mini\"):\n",
    "    langfuse.update_current_trace(input=question)\n",
    "\n",
    "    tool_call_history = []\n",
    "\n",
    "    # Log all tool calls for trajectory analysis\n",
    "    async def process_tool_call(\n",
    "        ctx: RunContext[int],\n",
    "        call_tool: CallToolFunc,\n",
    "        tool_name: str,\n",
    "        args: dict[str, Any],\n",
    "    ) -> ToolResult:\n",
    "        \"\"\"A tool call processor that passes along the deps.\"\"\"\n",
    "        print(f\"MCP Tool call: {tool_name} with args: {args}\")\n",
    "        tool_call_history.append({\n",
    "            \"tool_name\": tool_name,\n",
    "            \"args\": args\n",
    "        })\n",
    "        return await call_tool(tool_name, args)\n",
    "    \n",
    "    langfuse_docs_server = MCPServerStreamableHTTP(\n",
    "        LANGFUSE_MCP_URL,\n",
    "        process_tool_call=process_tool_call\n",
    "    )\n",
    "\n",
    "    agent = Agent(\n",
    "        model=model,\n",
    "        mcp_servers=[langfuse_docs_server],\n",
    "        system_prompt=system_prompt\n",
    "    )\n",
    "\n",
    "    async with agent.run_mcp_servers():\n",
    "        print(\"\\n---\")\n",
    "        print(\"Q:\", question)\n",
    "        result = await agent.run(question)\n",
    "        print(\"A:\", result.output)\n",
    "\n",
    "        langfuse.update_current_trace(\n",
    "            output=result.output,\n",
    "            metadata={\"tool_call_history\": tool_call_history\n",
    "        })\n",
    "\n",
    "        return result.output, tool_call_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c655d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "await run_agent(\n",
    "    question=\"What is Langfuse and how does it help monitor LLM applications?\",\n",
    "    system_prompt=\"You are an expert on Langfuse. Answer user questions accurately and concisely using the available MCP tools. Cite sources when appropriate. Please make sure to use the tools in the best way possible to answer.\",\n",
    "    model=\"openai:gpt-4.1-nano\"\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2edbef7",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "1. Create Test Cases\n",
    "    - input\n",
    "    - reference for reference-based evaluations\n",
    "2. Set up evaluators\n",
    "3. Run experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35017511",
   "metadata": {},
   "outputs": [],
   "source": [
    "tests_cases = [\n",
    "    {\n",
    "        \"input\": {\"question\": \"What is Langfuse?\"},\n",
    "        \"expected_output\": {\n",
    "            \"response_facts\": [\n",
    "                \"Open Source LLM Engineering Platform\",\n",
    "                \"Product modules: Tracing, Evaluation and Prompt Management\"\n",
    "            ],\n",
    "            \"trajectory\": [\n",
    "                \"getLangfuseOverview\"\n",
    "            ],\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"input\": {\n",
    "            \"question\": \"How to trace a python application with Langfuse?\"\n",
    "        },\n",
    "        \"expected_output\": {\n",
    "            \"response_facts\": [\n",
    "                \"Python SDK, you can use the observe() decorator\",\n",
    "                \"Lots of integrations, LangChain, LlamaIndex, Pydantic AI, and many more.\"\n",
    "            ],\n",
    "            \"trajectory\": [\n",
    "                \"getLangfuseOverview\",\n",
    "                \"searchLangfuseDocs\"\n",
    "            ],\n",
    "            \"search_term\": \"Python Tracing\"\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"input\": {\"question\": \"How to connect to the Langfuse Docs MCP server?\"},\n",
    "        \"expected_output\": {\n",
    "            \"response_facts\": [\n",
    "                \"Connect via the MCP server endpoint: https://langfuse.com/api/mcp\",\n",
    "                \"Transport protocol: `streamableHttp`\"\n",
    "            ],\n",
    "            \"trajectory\": [\"getLangfuseOverview\"]\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"input\": {\n",
    "            \"question\": \"How long are traces retained in langfuse?\",\n",
    "        },\n",
    "        \"expected_output\": {\n",
    "            \"response_facts\": [\n",
    "                \"By default, traces are retained indefinetly\",\n",
    "                \"You can set custom data retention policy in the project settings\"\n",
    "            ],\n",
    "            \"trajectory\": [\"getLangfuseOverview\", \"searchLangfuseDocs\"],\n",
    "            \"search_term\": \"Data retention\"\n",
    "        }\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb50656c",
   "metadata": {},
   "source": [
    "Upload to Langfuse datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa55584",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATSET_NAME = \"pydantic-ai-mcp-agent-evaluation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7bbc69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = langfuse.create_dataset(\n",
    "    name=DATSET_NAME\n",
    ")\n",
    "for case in tests_cases:\n",
    "    langfuse.create_dataset_item(\n",
    "        dataset_name=DATSET_NAME,\n",
    "        input=case[\"input\"],\n",
    "        expected_output=case[\"expected_output\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f59ea5",
   "metadata": {},
   "source": [
    "### Set up Evaluations in Langfuse\n",
    "\n",
    "#### Final response evaluation\n",
    "\n",
    "```md\n",
    "You are a teacher grading a student based on the factual correctness of his statements. In the following please find some example gradings that you did in the past.\n",
    "\n",
    "### Examples\n",
    "\n",
    "#### **Example 1:**\n",
    "- **Response:** \"The sun is shining brightly.\"\n",
    "- **Facts to verify:** [\"The sun is up.\", \"It is a beautiful day.\"]\n",
    "\n",
    "Grading\n",
    "- Reasoning: The response accurately includes both facts and aligns with the context of a beautiful day.\n",
    "- Score: 1\n",
    "\n",
    "#### **Example 2:**\n",
    "- **Response:** \"When I was in the kitchen, the dog was there\"\n",
    "- **Facts to verify:** [\"The cat is on the table.\", \"The dog is in the kitchen.\"]\n",
    "\n",
    "Grading\n",
    "- Reasoning: The response includes that the dog is in the kitchen but does not mention that the cat is on the table.\n",
    "- Score: 0\n",
    "\n",
    "### New Student Response\n",
    "\n",
    "- **Response**: {{response}}\n",
    "- **Facts to verify:** {{facts_to_verify}}\n",
    "```\n",
    "\n",
    "#### Trajectory\n",
    "\n",
    "```md\n",
    "You are comparing two lists of strings. Please check whether the lists contain exactly the same items. The order does not matter.\n",
    "\n",
    "## Examples\n",
    "\n",
    "Input\n",
    "Expected: [\"searchWeb\", \"visitWebsite\"]\n",
    "Output: [\"searchWeb\"]\n",
    "\n",
    "Grading\n",
    "Reasoning: [\"searchWeb\", \"visitWebsite\"] are expected. In the output, \"visitWebsite\" is missing. Thus the two arrays are not the same.\n",
    "Score: 0\n",
    "\n",
    "Input\n",
    "Expected: [\"drawImage\", \"visitWebsite\", \"speak\"]\n",
    "Output: [\"visitWebsite\", \"speak\", \"drawImage\"]\n",
    "\n",
    "Grading\n",
    "Reasoning: The output matches the items from the expected output.\n",
    "Score: 1\n",
    "\n",
    "Input\n",
    "Expected: [\"getNews\"]\n",
    "Output: [\"getNews\", \"watchTv\"]\n",
    "\n",
    "Grading\n",
    "Reasoning: The output contains \"watchTv\" which was not expected.\n",
    "Score: 0\n",
    "\n",
    "## This excercise\n",
    "\n",
    "Expected: {{expected}}\n",
    "Output: {{output}}\n",
    "```\n",
    "\n",
    "#### Search quality\n",
    "\n",
    "```md\n",
    "You are a teacher grading a student based on whether he has looked for the right information in order to answe a question. In the following please find some example gradings that you did in the past.\n",
    "\n",
    "The search by the student does not need to exactly match the response you expected; searches are often brief. The search term should correspond vaguely with the expected search term.\n",
    "\n",
    "### Examples\n",
    "#### **Example 1:**\n",
    "- **Response:** How can I contact support?\n",
    "- **Expected search topics**: Support\n",
    "\n",
    "Grading\n",
    "- Reasoning: The response accurately searches for support.\n",
    "- Score: 1\n",
    "\n",
    "#### **Example 2:**\n",
    "- **Response:** Deployment\n",
    "- **Expected search topics:** Tracing\n",
    "\n",
    "Grading\n",
    "- Reasoning: The response does not match the expected search topic of Tracing. Deployment questions are unrelated.\n",
    "- Score: 0\n",
    "\n",
    "#### **Example 3:**\n",
    "- **Response:**\n",
    "- **Expected search topics:**\n",
    "\n",
    "Grading\n",
    "- Reasoning: No search was done and no search term was expected.\n",
    "- Score: 1\n",
    "\n",
    "#### **Example 4:**\n",
    "- **Response:** How to view sessions?\n",
    "- **Expected search topics:**\n",
    "\n",
    "Grading\n",
    "- Reasoning: No search was expected, but search was used. This is not a problem.\n",
    "- Score: 1\n",
    "\n",
    "#### **Example 5:**\n",
    "- **Response:**\n",
    "- **Expected search topics:** How to run Langfuse locally?\n",
    "\n",
    "Grading\n",
    "- Reasoning: Even though we expected a search regarding running Langfuse locally, no search was made.\n",
    "- Score: 0\n",
    "\n",
    "### New Student Response\n",
    "\n",
    "- **Response:** {{search}}\n",
    "- **Expected search topics:** {{expected_search_topic}}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96cb8b36",
   "metadata": {},
   "source": [
    "### Run Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33dabea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompts = {\n",
    "    \"simple\": (\n",
    "        \"You are an expert on Langfuse. \"\n",
    "        \"Answer user questions accurately and concisely using the available MCP tools. \"\n",
    "        \"Cite sources when appropriate.\"\n",
    "    ),\n",
    "    \"nudge_search_and_sources\": (\n",
    "        \"You are an expert on Langfuse. \"\n",
    "        \"Answer user questions accurately and concisely using the available MCP tools. \"\n",
    "        \"Always cite sources when appropriate.\"\n",
    "        \"When you are unsure, always use getLangfuseOverview tool to do some research and then search the docs for more information. You can if needed use these tools multiple times.\"\n",
    "    )\n",
    "}\n",
    "\n",
    "models = [\n",
    "    \"openai:gpt-4.1-nano\",\n",
    "    \"openai:o4-mini\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c40766a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "d = langfuse.get_dataset(DATSET_NAME)\n",
    "\n",
    "for prompt_name, prompt_content in list(system_prompts.items()):\n",
    "    for test_model in models:\n",
    "        now = datetime.now().strftime(\"%Y-%m-%dT%H:%M:%S\")\n",
    "        \n",
    "        for item in d.items:\n",
    "            with item.run(\n",
    "                run_name=f\"{test_model}-{prompt_name}-{now}\",\n",
    "                run_metadata={\"model\": test_model, \"prompt\": prompt_content},\n",
    "            ) as root_span:\n",
    "                \n",
    "                await run_agent(\n",
    "                    item.input[\"question\"],\n",
    "                    prompt_content,\n",
    "                    test_model\n",
    "                )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

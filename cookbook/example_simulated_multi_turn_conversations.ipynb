{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84c8ce58",
   "metadata": {
    "id": "84c8ce58"
   },
   "source": [
    "<!-- NOTEBOOK_METADATA source: \"Jupyter Notebook\" title: \"Evaluating Multi-Turn Conversations (Simulation)\" description: \"This guide shows how to evaluate conversational applications by simulating conversations and using LLM-as-a-Judge evaluators\" category: \"Evaluation\" sidebarTitle: \"Evaluating Multi-Turn Conversations (Simulation)\" -->\n",
    "\n",
    "# Evaluating Multi-Turn Conversations (Simulation)\n",
    "\n",
    "<iframe\n",
    "  width=\"100%\"\n",
    "  className=\"aspect-[15.94/9] rounded-lg border mt-6 w-full\"\n",
    "  src=\"https://www.youtube.com/embed/3ODizwXu-uk\"\n",
    "  title=\"YouTube video player\"\n",
    "  frameborder=\"0\"\n",
    "  allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\"\n",
    "  referrerpolicy=\"strict-origin-when-cross-origin\"\n",
    "  allowFullScreen\n",
    "></iframe>\n",
    "\n",
    "AI applications with conversational interfaces, such as chatbots, engage in multiple interactions with a user, which are also known as conversation turns. There are multiple ways of evaluating the performance of these apps in a structured way, like [N+1 Evaluations](https://langfuse.com/guides/cookbook/example_evaluating_multi_turn_conversations).\n",
    "\n",
    "In this cookbook we'll cover how to use agents that simulate a user having a conversation with your chatbot, and measure the output of these conversations. We'll cover:\n",
    "\n",
    "- Creating structured datasets of features to test, scenarios, personas...\n",
    "- Using [OpenEvals](https://github.com/langchain-ai/openevals) to simulate a specific use-case.\n",
    "- Running the simulations against the production application\n",
    "- Evaluating the output of the simulation with an [LLM-as-a-Judge](https://langfuse.com/docs/evaluation/evaluation-methods/llm-as-a-judge).\n",
    "\n",
    "---\n",
    "\n",
    "Not using Langfuse yet? [Get started](https://langfuse.com/docs/get-started) by capturing LLM events.\n",
    "\n",
    "## Setup\n",
    "\n",
    "In this example, weâ€™ll build a cooking assistant chatbot and then simulate conversations against it.\n",
    "\n",
    "### Step 1 - Create a chat app and generate traces in Langfuse\n",
    "\n",
    "First you need to install Langfuse via pip and then set the environment variables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0d5276",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3f0d5276",
    "outputId": "b1413040-6513-469c-c790-f7e8947a07e8"
   },
   "outputs": [],
   "source": [
    "%pip install langfuse openevals --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d455e5a1",
   "metadata": {
    "id": "d455e5a1"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Get keys for your project from the project settings page: https://cloud.langfuse.com\n",
    "os.environ[\"LANGFUSE_PUBLIC_KEY\"] = \"pk-lf-1234\"\n",
    "os.environ[\"LANGFUSE_SECRET_KEY\"] = \"sk-lf-1234\"\n",
    "os.environ[\"LANGFUSE_BASE_URL\"] = \"https://cloud.langfuse.com\" # ðŸ‡ªðŸ‡º EU region\n",
    "# os.environ[\"LANGFUSE_BASE_URL\"] = \"https://us.cloud.langfuse.com\" # ðŸ‡ºðŸ‡¸ US region\n",
    "\n",
    "# Your openai key\n",
    "os.environ[\"OPENAI_API_KEY\"] = 'sk-proj-1234'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "RTUEBp42qFVN",
   "metadata": {
    "id": "RTUEBp42qFVN"
   },
   "source": [
    "Now weâ€™ll create a simple cooking assistant chatbot that uses OpenAI as the LLM and trace it with Langfuse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee14d7e",
   "metadata": {
    "id": "dee14d7e"
   },
   "outputs": [],
   "source": "# This is a simple cooking assistant chatbot traced with Langfuse and uses OpenAI as the LLM.\nfrom langfuse.openai import openai\nfrom langfuse import get_client, observe\n\n\n\nclass SimpleChat:\n    def __init__(self, model=\"gpt-3.5-turbo\"):\n        self.conversation_history = [\n          {\n            \"role\": \"system\",\n            \"content\": \"You are a helpful cooking assistant that answers questions about recipes and cooking.\"\n          }\n        ]\n        self.model = model\n\n    @observe\n    def add_message(self, messages):\n        \"\"\"\n        Args:\n            messages: Either a string (single user message) or a list of message dictionaries\n        \"\"\"\n        try:\n            # Handle both string and array inputs\n            if isinstance(messages, str):\n                messages = [{\"role\": \"user\", \"content\": messages}]\n\n            # Add messages to history\n            self.conversation_history.extend(messages)\n\n            # Call OpenAI API using the new client\n            response = openai.chat.completions.create(\n                model=self.model,\n                messages=self.conversation_history,\n                max_tokens=500,\n                temperature=0.7\n            )\n\n            # Extract and add assistant response\n            assistant_message = response.choices[0].message.content\n            self.conversation_history.append({\"role\": \"assistant\", \"content\": assistant_message})\n            get_client().update_current_observation(input=messages, output=assistant_message)\n\n            return assistant_message\n\n        except Exception as e:\n            return f\"Error: {str(e)}\"\n\n    def show_history(self):\n        import json\n        print(\"Conversation history:\")\n        print(json.dumps(self.conversation_history, indent=2))\n        print()\n\n    def clear_history(self):\n        self.conversation_history = [\n          {\n            \"role\": \"system\",\n            \"content\": \"You are a helpful cooking assistant that answers questions about recipes and cooking.\"\n          }\n        ]\n        print(\"Conversation cleared!\")\n\n# Create a chat instance\nchat = SimpleChat()"
  },
  {
   "cell_type": "markdown",
   "id": "hkaiAL5TqIrQ",
   "metadata": {
    "id": "hkaiAL5TqIrQ"
   },
   "source": [
    "Now if you run a simple question, you should see traces in Langfuse:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "K8gHm_GKqYDP",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "K8gHm_GKqYDP",
    "outputId": "5dca138c-4b23-4c8c-a94a-d6b651c61a1d"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'That sounds delicious! Do you have a specific recipe in mind, or would you like me to suggest one for you?'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat.add_message(\"I want to make a chocolate cake\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "KSrn3S8PqpAp",
   "metadata": {
    "id": "KSrn3S8PqpAp"
   },
   "source": [
    "![Traced Application](https://langfuse.com/images/cookbook/example-simulated-multi-turn-conversations/traced-application.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950f0498",
   "metadata": {
    "id": "950f0498"
   },
   "source": [
    "### Step 2 - Create a structured Dataset of user scenarios\n",
    "\n",
    "Define relevant dimensions for your use case, such as:\n",
    "\n",
    "- **Features**: Specific functionalities of your AI product.\n",
    "- **Scenarios**: Situations or problems the AI may encounter and needs to handle.\n",
    "- **Personas**: Representative user profiles with distinct characteristics and needs.\n",
    "\n",
    "For our cooking assistant chatbot, we'll create pairs of Scenarios and Personas, as follows:\n",
    "\n",
    "| Persona | Scenario |\n",
    "|---------|----------|\n",
    "| A nervous first-time host cooking multiple dishes for a dinner party at 6:30 PM. Anxious about timing, unfamiliar with multitasking in the kitchen, needs reassurance and asks many clarifying questions. | It's 4:30 PM and they need to coordinate: roasted chicken (1.5 hours), roasted vegetables (45 min), mashed potatoes (30 min), and gravy. They only have one oven and are worried everything won't be ready on time. |\n",
    "| A stressed home cook whose beef stroganoff sauce just curdled when they added sour cream. Guests arrive in 20 minutes. Frustrated, urgent, needs quick realistic solutions. | The sour cream curdled in the hot pan, making the sauce grainy and separated. They have heavy cream, butter, flour, beef broth, and more sour cream available. Need to know if it can be salvaged or if they need a backup plan. |\n",
    "\n",
    "You can add more columns for your specific use-case, like adding data to test against your system. For example, if you're running a customer service chatbot for a logistics company, you might want to simulate the scenario of providing a non-existent order tracking ID to see how your chatbot handles the scenario.\n",
    "\n",
    "Once you've defined your dataset, create it in [Langfuse's Dataset](https://langfuse.com/docs/evaluation/experiments/datasets), this way you can assign performance scores to measure how the system improves on the dataset with every new update:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "uZ_rHCil_SiP",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uZ_rHCil_SiP",
    "outputId": "2393fe9b-0419-4105-9dc3-e854fa51f2d0"
   },
   "outputs": [],
   "source": [
    "from langfuse import get_client\n",
    "\n",
    "langfuse = get_client()\n",
    "\n",
    "dataset = langfuse.create_dataset(\n",
    "    name=\"simulated-conversations\",\n",
    "    description=\"Synthetic conversations from persona/scenario pairs\"\n",
    ")\n",
    "\n",
    "langfuse.create_dataset_item(\n",
    "    dataset_name=\"simulated-conversations\",\n",
    "    input={\n",
    "      \"persona\": \"A nervous first-time host cooking multiple dishes for a dinner party at 6:30 PM. Anxious about timing, unfamiliar with multitasking in the kitchen, needs reassurance and asks many clarifying questions.\",\n",
    "      \"scenario\": \"It's 4:30 PM and they need to coordinate: roasted chicken (1.5 hours), roasted vegetables (45 min), mashed potatoes (30 min), and gravy. They only have one oven and are worried everything won't be ready on time.\"\n",
    "    }\n",
    ")\n",
    "\n",
    "langfuse.create_dataset_item(\n",
    "  dataset_name=\"simulated-conversations\",\n",
    "  input={\n",
    "    \"persona\": \"A stressed home cook whose beef stroganoff sauce just curdled when they added sour cream. Guests arrive in 20 minutes. Frustrated, urgent, needs quick realistic solutions.\",\n",
    "    \"scenario\": \"The sour cream curdled in the hot pan, making the sauce grainy and separated. They have heavy cream, butter, flour, beef broth, and more sour cream available. Need to know if it can be salvaged or if they need a backup plan.\"\n",
    "  }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qvMpJjCmqiKx",
   "metadata": {
    "id": "qvMpJjCmqiKx"
   },
   "source": [
    "### Step 3 - Prepare your application to run against the simulator\n",
    "\n",
    "The OpenEvals library requires a specific input and output format. It also needs conversation history management, which is why we'll store instances of our chatbot together with a `thread_id` provided by OpenEvals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "347053f8",
   "metadata": {
    "id": "347053f8"
   },
   "outputs": [],
   "source": [
    "from openevals.simulators import run_multiturn_simulation, create_llm_simulated_user\n",
    "\n",
    "def create_app_wrapper():\n",
    "    \"\"\"\n",
    "    Creates an app function that works with OpenEvals multiturn simulation.\n",
    "    Manages conversation history per thread_id internally.\n",
    "\n",
    "    OpenEvals expects:\n",
    "    - Input: ChatCompletionMessage (dict-like with 'role' and 'content')\n",
    "    - Output: ChatCompletionMessage (dict-like with 'role' and 'content')\n",
    "    \"\"\"\n",
    "    # Store chat instances per thread\n",
    "    chat_instances = {}\n",
    "\n",
    "    def app(inputs, *, thread_id: str, **kwargs):\n",
    "        if thread_id not in chat_instances:\n",
    "            chat_instances[thread_id] = SimpleChat()\n",
    "\n",
    "        chat = chat_instances[thread_id]\n",
    "\n",
    "        # inputs is a message dict/object with 'role' and 'content'\n",
    "        # Access content - handle both dict and object\n",
    "        content = inputs.get(\"content\") if isinstance(inputs, dict) else inputs.content\n",
    "\n",
    "        response_text = chat.add_message(content)\n",
    "\n",
    "        return {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": response_text\n",
    "        }\n",
    "\n",
    "    return app\n",
    "\n",
    "def generate_synthetic_conversation(persona: str, scenario: str, max_turns: int = 3):\n",
    "    \"\"\"\n",
    "    Generate a synthetic conversation from persona/scenario pair.\n",
    "\n",
    "    Args:\n",
    "        persona: User characteristics/personality\n",
    "        scenario: The cooking situation\n",
    "        max_turns: Max conversation turns\n",
    "\n",
    "    Returns:\n",
    "        Simulation result with trajectory and evaluation scores\n",
    "    \"\"\"\n",
    "    # Create app function (manages chat instances internally)\n",
    "    app = create_app_wrapper()\n",
    "\n",
    "    # Create simulated user\n",
    "    # The system prompt should include the scenario so the user naturally introduces it\n",
    "    system_prompt = f\"\"\"You are a user in the following situation:\n",
    "    {scenario}\n",
    "\n",
    "    You have these characteristics:\n",
    "    {persona}\n",
    "\n",
    "    Start the conversation by naturally describing your situation and asking for help. Then behave naturally based on your emotional state and ask follow-up questions.\"\"\"\n",
    "\n",
    "    user = create_llm_simulated_user(\n",
    "        system=system_prompt,\n",
    "        model=\"openai:gpt-4o-mini\",\n",
    "    )\n",
    "\n",
    "    # Run simulation (evaluators will be configured in Langfuse)\n",
    "    result = run_multiturn_simulation(\n",
    "        app=app,\n",
    "        user=user,\n",
    "        max_turns=max_turns,\n",
    "    )\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "j5u6ALpxxMO4",
   "metadata": {
    "id": "j5u6ALpxxMO4"
   },
   "source": [
    "### Step 4 - Run an experiment\n",
    "\n",
    "We'll use Langfuse's [Dataset Experiments SDK](https://langfuse.com/docs/evaluation/experiments/experiments-via-sdk) to fetch the scenario/persona pairs we've created and trace and store the output of the experiment so we can then evaluate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cR0O78YAxzgn",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cR0O78YAxzgn",
    "outputId": "315f64bf-daec-480c-a3d1-db246d383573"
   },
   "outputs": [],
   "source": [
    "from langfuse import get_client\n",
    "\n",
    "def run_dataset_experiment(dataset_name: str, experiment_name: str):\n",
    "    \"\"\"\n",
    "    Load persona/scenario pairs from Langfuse Dataset and run experiment.\n",
    "\n",
    "    Args:\n",
    "        dataset_name: Name of dataset in Langfuse containing persona/scenario pairs\n",
    "        experiment_name: Name for this experiment run\n",
    "    \"\"\"\n",
    "    langfuse = get_client()\n",
    "    dataset = langfuse.get_dataset(dataset_name)\n",
    "\n",
    "    print(f\"Loaded dataset '{dataset_name}' with {len(dataset.items)} items\")\n",
    "\n",
    "    def run_task(*, item, **kwargs):\n",
    "        \"\"\"\n",
    "        Task function for Langfuse experiment.\n",
    "        Item input should be: {\"persona\": \"...\", \"scenario\": \"...\"}\n",
    "        \"\"\"\n",
    "        # Extract persona and scenario from dataset item\n",
    "        persona = item.input.get(\"persona\")\n",
    "        scenario = item.input.get(\"scenario\")\n",
    "\n",
    "        if not persona or not scenario:\n",
    "            raise ValueError(f\"Dataset item must have 'persona' and 'scenario' fields. Got: {item.input}\")\n",
    "\n",
    "        print(f\"\\nGenerating conversation for scenario: {scenario[:80]}...\")\n",
    "\n",
    "        result = generate_synthetic_conversation(\n",
    "            persona=persona,\n",
    "            scenario=scenario\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"trajectory\": result[\"trajectory\"],\n",
    "            \"num_turns\": len([m for m in result[\"trajectory\"] if m.get(\"role\") == \"user\"])\n",
    "        }\n",
    "\n",
    "    print(f\"\\nRunning experiment '{experiment_name}'...\")\n",
    "    result = dataset.run_experiment(\n",
    "        name=experiment_name,\n",
    "        description=\"Synthetic conversations from persona/scenario pairs\",\n",
    "        task=run_task\n",
    "    )\n",
    "\n",
    "    get_client().flush()\n",
    "\n",
    "    print(f\"\\nâœ… Experiment complete!\")\n",
    "    print(f\"View results in Langfuse: {os.environ.get('LANGFUSE_BASE_URL')}\")\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "run_dataset_experiment(\n",
    "  dataset_name=\"simulated-conversations\",\n",
    "  experiment_name=\"synthetic-conversations-v1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tAXRie7lzDrF",
   "metadata": {
    "id": "tAXRie7lzDrF"
   },
   "source": [
    "Once you run the experiment, you should see the results in your Dataset runs tab:\n",
    "\n",
    "![Dataset Runs](https://langfuse.com/images/cookbook/example-simulated-multi-turn-conversations/dataset-run.png)\n",
    "\n",
    "### Step 5 - Run evaluations against your Dataset runs\n",
    "\n",
    "To evaluate the output, you can create [LLM-as-a-Judge](https://langfuse.com/docs/evaluation/evaluation-methods/llm-as-a-judge) that runs against your Dataset Runs and scores the output, giving an individual score and reason for the specific scenario/persona pair and an overall average score for the entire dataset run.\n",
    "\n",
    "In this case, I create a simple \"Conciseness\" evaluator from the pre-made library of evaluators, but you could create one (or multiple of them!) that evaluate based on your use-case and needs. In the following image, you can see the overall score assigned to multiple Dataset runs, and how over time that score improves given changes to the prompt.\n",
    "\n",
    "![Dataset Runs](https://langfuse.com/images/cookbook/example-simulated-multi-turn-conversations/evaluated-runs.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94be390c",
   "metadata": {
    "id": "94be390c"
   },
   "source": [
    "### Conclusion\n",
    "\n",
    "This notebook demonstrated a systematic approach to simulate and evaluate multi-turn conversations, specifically addressing use-cases and scenarios that are relevant to the chatbot.\n",
    "\n",
    "You can read more about effective ways to create datasets for experiments [here](https://hamel.dev/blog/posts/llm-judge/#step-2-create-a-dataset)."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "9IiVtLuKpbHm",
      "metadata": {
        "id": "9IiVtLuKpbHm"
      },
      "source": [
        "---\n",
        "description: Notebook on generating synthetic datasets for LLM application and AI agents evaluation. \n",
        "category: Evaluation\n",
        "---\n",
        "\n",
        "# Synthetic Dataset Generation for LLM Evaluation\n",
        "\n",
        "In this notebook, we will explore how to **generate synthetic datasets** using language models and uploading them to [Langfuse](https://langfuse.com) for evaluation. \n",
        "\n",
        "## What are Langfuse Datasets?\n",
        "\n",
        "In Langfuse, a *dataset* is a collection of *dataset items*, each typically containing an `input` (e.g., user prompt/question), `expected_output` (the ground truth or ideal answer) and optional metadata.\n",
        "\n",
        "Datasets are used for **evaluation**. You can run your LLM or application on each item in a dataset and compare the application's responses to the expected outputs. This way, you can track performance over time and across different application configs (e.g. model versions or prompt changes).\n",
        "\n",
        "## Cases your Dataset Should Cover\n",
        "\n",
        "**Happy path** â€“ straightforward or common queries:\n",
        "- \"What is the capital of France?\"\n",
        "- \"Convert 5 USD to EUR.\"\n",
        "\n",
        "**Edge cases** â€“ unusual or complex:\n",
        "- Very long prompts.\n",
        "- Ambiguous queries.\n",
        "- Very technical or niche.\n",
        "\n",
        "**Adversarial cases** â€“ malicious or tricky:\n",
        "- Prompt injection attempts (\"Ignore all instructions and ...\").\n",
        "- Content policy violations (harassment, hate speech).\n",
        "- Logic traps (trick questions).\n",
        "\n",
        "## Examples"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "73e4e4e8",
      "metadata": {},
      "source": [
        "### Example 1: Looping Over OpenAI API\n",
        "\n",
        "We'll use OpenAI's API in a simple loop to create synthetic questions for an airline chatbot. You could similarly prompt the model to generate *both* questions and answers."
      ]
    },
    {
      "cell_type": "raw",
      "id": "qDW8u7rPpbHo",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "qDW8u7rPpbHo",
        "outputId": "e4d53524-3ee4-4722-c596-ddd0c6e5646f",
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "%pip install openai langfuse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "z0ZWdcDes_pN",
      "metadata": {
        "id": "z0ZWdcDes_pN"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Get keys for your project from the project settings page: https://cloud.langfuse.com\n",
        "os.environ[\"LANGFUSE_PUBLIC_KEY\"] = \"pk-lf-...\" \n",
        "os.environ[\"LANGFUSE_SECRET_KEY\"] = \"sk-lf-...\" \n",
        "os.environ[\"LANGFUSE_HOST\"] = \"https://cloud.langfuse.com\" # ðŸ‡ªðŸ‡º EU region\n",
        "# os.environ[\"LANGFUSE_HOST\"] = \"https://us.cloud.langfuse.com\" # ðŸ‡ºðŸ‡¸ US region\n",
        "\n",
        "# Your openai key\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"sk-proj-...\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "04926342",
      "metadata": {},
      "source": [
        "With the environment variables set, we can now initialize the Langfuse client. `get_client()` initializes the Langfuse client using the credentials provided in the environment variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "b80ca760",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Langfuse client is authenticated and ready!\n"
          ]
        }
      ],
      "source": [
        "from langfuse import get_client\n",
        " \n",
        "langfuse = get_client()\n",
        " \n",
        "# Verify connection\n",
        "if langfuse.auth_check():\n",
        "    print(\"Langfuse client is authenticated and ready!\")\n",
        "else:\n",
        "    print(\"Authentication failed. Please check your credentials and host.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "syim3MuMztli",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "syim3MuMztli",
        "outputId": "5cadbc33-a9be-4638-ce6d-03141da1fcdf"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "import pandas as pd\n",
        "\n",
        "client = OpenAI()\n",
        "\n",
        "# Function to generate airline questions\n",
        "def generate_airline_questions(num_questions=20):\n",
        "\n",
        "    questions = []\n",
        "\n",
        "    for i in range(num_questions):\n",
        "        completion = client.chat.completions.create(\n",
        "            model=\"gpt-4o\", \n",
        "            messages=[\n",
        "                {\n",
        "                    \"role\": \"system\",\n",
        "                    \"content\": (\n",
        "                        \"You are a helpful customer service chatbot for an airline. \"\n",
        "                        \"Please generate a short, realistic question from a customer.\"\n",
        "                    )\n",
        "                }\n",
        "            ],\n",
        "            temperature=1\n",
        "        )\n",
        "        question_text = completion.choices[0].message.content.strip()\n",
        "        questions.append(question_text)\n",
        "\n",
        "    return questions\n",
        "\n",
        "# Generate 20 airline-related questions\n",
        "airline_questions = generate_airline_questions(num_questions=20)\n",
        "\n",
        "# Convert to a Pandas DataFrame\n",
        "df = pd.DataFrame({\"Question\": airline_questions})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "0S7_92wlpbHo",
      "metadata": {
        "id": "0S7_92wlpbHo"
      },
      "outputs": [],
      "source": [
        "from langfuse import get_client\n",
        " \n",
        "langfuse = get_client()\n",
        "\n",
        "# Create a new dataset in Langfuse\n",
        "dataset_name = \"openai_synthetic_dataset\"\n",
        "langfuse.create_dataset(\n",
        "    name=dataset_name,\n",
        "    description=\"Synthetic Q&A dataset generated via OpenAI in a loop\",\n",
        "    metadata={\"approach\": \"openai_loop\", \"category\": \"mixed\"}\n",
        ")\n",
        "\n",
        "# Upload each Q&A as a dataset item\n",
        "for _, row in df.iterrows():\n",
        "    langfuse.create_dataset_item(\n",
        "        dataset_name=\"openai_loop_dataset\",\n",
        "        input = row[\"Question\"]\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "29c0e88b",
      "metadata": {},
      "source": [
        "![OpenAI Dataset](https://langfuse.com/images/cookbook/example-synthetic-datasets/openai-dataset.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ORkGZBG0pbHp",
      "metadata": {
        "id": "ORkGZBG0pbHp"
      },
      "source": [
        "### Example 2: RAGAS Library\n",
        "\n",
        "For **RAG**, we often want questions that are *grounded in specific documents*. This ensures the question can be answered by the context, allowing us to evaluate how well a RAG pipeline retrieves and uses the context.\n",
        "\n",
        "[RAGAS](https://docs.ragas.io/en/stable/getstarted/rag_testset_generation/#testset-generation) is a library that can automate test set generation for RAG. It can take a corpus and produce relevant queries and answers. We'll do a quick example:\n",
        "\n",
        "_**Note**: This example is taken from the [RAGAS documentation](https://docs.ragas.io/en/stable/getstarted/rag_testset_generation/)_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "_c5fOb9wpbHq",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "_c5fOb9wpbHq",
        "outputId": "f76166b5-eecd-43b4-9458-fde2673e064b"
      },
      "outputs": [],
      "source": [
        "%pip install ragas langchain-community langchain-openai unstructured"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "P2Q1Ux_0pbHq",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P2Q1Ux_0pbHq",
        "outputId": "dbe7cbbb-70f8-4ca6-ffee-9f8fb179d170"
      },
      "outputs": [],
      "source": [
        "%git clone https://huggingface.co/datasets/explodinggradients/Sample_Docs_Markdown"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "ed376b5c",
      "metadata": {
        "id": "ed376b5c"
      },
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import DirectoryLoader\n",
        "\n",
        "path = \"Sample_Docs_Markdown\"\n",
        "loader = DirectoryLoader(path, glob=\"**/*.md\")\n",
        "docs = loader.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "2101d44a",
      "metadata": {
        "id": "2101d44a"
      },
      "outputs": [],
      "source": [
        "from ragas.llms import LangchainLLMWrapper\n",
        "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "generator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4o\"))\n",
        "generator_embeddings = LangchainEmbeddingsWrapper(OpenAIEmbeddings())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "05fe986c",
      "metadata": {
        "id": "05fe986c"
      },
      "outputs": [],
      "source": [
        "from ragas.testset import TestsetGenerator\n",
        "\n",
        "generator = TestsetGenerator(llm=generator_llm, embedding_model=generator_embeddings)\n",
        "dataset = generator.generate_with_langchain_docs(docs, testset_size=10)\n",
        "\n",
        "# 4. The result `testset` can be converted to a pandas DataFrame for inspection\n",
        "df = dataset.to_pandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "lewlGMTnusaX",
      "metadata": {
        "id": "lewlGMTnusaX"
      },
      "outputs": [],
      "source": [
        "from langfuse import get_client\n",
        " \n",
        "langfuse = get_client()\n",
        "\n",
        "# 5. Push the RAGAS-generated testset to Langfuse\n",
        "langfuse.create_dataset(\n",
        "    name=\"ragas_generated_testset\",\n",
        "    description=\"Synthetic RAG test set (RAGAS)\",\n",
        "    metadata={\"source\": \"RAGAS\", \"docs_used\": len(docs)}\n",
        ")\n",
        "\n",
        "for _, row in df.iterrows():\n",
        "    langfuse.create_dataset_item(\n",
        "        dataset_name=\"ragas_generated_testset\",\n",
        "        input = row[\"user_input\"],\n",
        "        metadata = row[\"reference_contexts\"]\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5PsFnNzH4aNB",
      "metadata": {
        "id": "5PsFnNzH4aNB"
      },
      "source": [
        "![RAGAS Dataset](https://langfuse.com/images/cookbook/example-synthetic-datasets/ragas-dataset.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b8b247b7",
      "metadata": {},
      "source": [
        "### Example 3: DeepEval Library\n",
        "\n",
        "[DeepEval](https://docs.confident-ai.com/docs/synthesizer-introduction) is a library that helps generate synthetic data systematically using the *Synthesizer* class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "35be76b8",
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install deepeval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ef7ccec2",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from langfuse import get_client\n",
        "from deepeval.synthesizer import Synthesizer\n",
        "from deepeval.synthesizer.config import StylingConfig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2cba4190",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1. Define the style we want for our synthetic data.\n",
        "# For instance, we want user questions and correct SQL queries.\n",
        "styling_config = StylingConfig(\n",
        "  input_format=\"Questions in English that asks for data in database.\",\n",
        "  expected_output_format=\"SQL query based on the given input\",\n",
        "  task=\"Answering text-to-SQL-related queries by querying a database and returning the results to users\",\n",
        "  scenario=\"Non-technical users trying to query a database using plain English.\",\n",
        ")\n",
        "\n",
        "# 2. Initialize the Synthesizer\n",
        "synthesizer = Synthesizer(styling_config=styling_config)\n",
        "\n",
        "# 3. Generate synthetic items from scratch, e.g. 20 items for a short demo\n",
        "synthesizer.generate_goldens_from_scratch(num_goldens=20)\n",
        "\n",
        "# 4. Access the generated examples\n",
        "synthetic_goldens = synthesizer.synthetic_goldens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b0637aa",
      "metadata": {},
      "outputs": [],
      "source": [
        "from langfuse import get_client\n",
        "langfuse = get_client()\n",
        "\n",
        "# 5. Create a Langfuse dataset\n",
        "deepeval_dataset_name = \"deepeval_synthetic_data\"\n",
        "langfuse.create_dataset(\n",
        "    name=deepeval_dataset_name,\n",
        "    description=\"Synthetic text-to-SQL data (DeepEval)\",\n",
        "    metadata={\"approach\": \"deepeval\", \"task\": \"text-to-sql\"}\n",
        ")\n",
        "\n",
        "# 6. Upload the items\n",
        "for golden in synthetic_goldens:\n",
        "    langfuse.create_dataset_item(\n",
        "        dataset_name=deepeval_dataset_name,\n",
        "        input={\"query\": golden.input},\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "78af50a9",
      "metadata": {},
      "source": [
        "![Dataset in Langfuse](https://langfuse.com/images/cookbook/example-synthetic-datasets/deepeval-dataset.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "DPCUnZTipbHr",
      "metadata": {
        "id": "DPCUnZTipbHr"
      },
      "source": [
        "### Example 4: No-Code via Hugging Face Dataset Generator\n",
        "\n",
        "If you prefer a more UI-based approach, check out [Hugging Face's Synthetic Data Generator](https://huggingface.co/blog/synthetic-data-generator). You can generate examples in the Hugging Face UI. Then you can download them as CSV and upload it in the Langfuse UI."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ODNR85wQ3hK5",
      "metadata": {
        "id": "ODNR85wQ3hK5"
      },
      "source": [
        "![Hugging Face Dataset Generator](https://langfuse.com/images/cookbook/example-synthetic-datasets/hf-generator.png)\n",
        "\n",
        "![Hugging Face Synthetic Dataset](https://langfuse.com/images/cookbook/example-synthetic-datasets/hf-dataset.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Sega8BHBpbHr",
      "metadata": {
        "id": "Sega8BHBpbHr"
      },
      "source": [
        "## Next Steps\n",
        "\n",
        "1. **Explore your dataset in Langfuse**. You can see each dataset in the UI.\n",
        "2. **Run experiments** You can now evaluate your application using this dataset.\n",
        "3. **Compare runs** over time or across models, prompts, or chain logic.\n",
        "\n",
        "For more details on how to run experiments on a dataset, see the [Langfuse docs](https://langfuse.com/docs/datasets/get-started#run-experiment-on-a-dataset)."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}

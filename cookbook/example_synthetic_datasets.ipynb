{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "9IiVtLuKpbHm",
      "metadata": {
        "id": "9IiVtLuKpbHm"
      },
      "source": [
        "<!-- NOTEBOOK_METADATA source: \"âš ï¸ Jupyter Notebook\" title: \"Synthetic Dataset Generation for LLM Evaluation\" sidebarTitle: \"Synthetic Datasets\" description: \"Notebook on generating synthetic datasets for LLM application and AI agents evaluation. \" category: \"Evaluation\" -->\n",
        "\n",
        "# Synthetic Dataset Generation for LLM Evaluation\n",
        "\n",
        "In this notebook, we will explore how to **generate synthetic datasets** using language models and uploading them to [Langfuse](https://langfuse.com) for evaluation. \n",
        "\n",
        "## What are Langfuse Datasets?\n",
        "\n",
        "In Langfuse, a *dataset* is a collection of *dataset items*, each typically containing an `input` (e.g., user prompt/question), `expected_output` (the ground truth or ideal answer) and optional metadata.\n",
        "\n",
        "Datasets are used for **evaluation**. You can run your LLM or application on each item in a dataset and compare the application's responses to the expected outputs. This way, you can track performance over time and across different application configs (e.g. model versions or prompt changes).\n",
        "\n",
        "## Cases your Dataset Should Cover\n",
        "\n",
        "**Happy path** â€“ straightforward or common queries:\n",
        "- \"What is the capital of France?\"\n",
        "- \"Convert 5 USD to EUR.\"\n",
        "\n",
        "**Edge cases** â€“ unusual or complex:\n",
        "- Very long prompts.\n",
        "- Ambiguous queries.\n",
        "- Very technical or niche.\n",
        "\n",
        "**Adversarial cases** â€“ malicious or tricky:\n",
        "- Prompt injection attempts (\"Ignore all instructions and ...\").\n",
        "- Content policy violations (harassment, hate speech).\n",
        "- Logic traps (trick questions).\n",
        "\n",
        "## Examples"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "73e4e4e8",
      "metadata": {},
      "source": [
        "### Example 1: Looping Over OpenAI API\n",
        "\n",
        "We'll use OpenAI's API in a simple loop to create synthetic questions for an airline chatbot. You could similarly prompt the model to generate *both* questions and answers."
      ]
    },
    {
      "cell_type": "raw",
      "id": "qDW8u7rPpbHo",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "qDW8u7rPpbHo",
        "outputId": "e4d53524-3ee4-4722-c596-ddd0c6e5646f",
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "%pip install openai langfuse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "z0ZWdcDes_pN",
      "metadata": {
        "id": "z0ZWdcDes_pN"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Get keys for your project from the project settings page: https://cloud.langfuse.com\n",
        "os.environ[\"LANGFUSE_PUBLIC_KEY\"] = \"pk-lf-...\" \n",
        "os.environ[\"LANGFUSE_SECRET_KEY\"] = \"sk-lf-...\" \n",
        "os.environ[\"LANGFUSE_BASE_URL\"] = \"https://cloud.langfuse.com\" # ðŸ‡ªðŸ‡º EU region\n",
        "# os.environ[\"LANGFUSE_BASE_URL\"] = \"https://us.cloud.langfuse.com\" # ðŸ‡ºðŸ‡¸ US region\n",
        "\n",
        "# Your openai key\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"sk-proj-...\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "04926342",
      "metadata": {},
      "source": [
        "With the environment variables set, we can now initialize the Langfuse client. `get_client()` initializes the Langfuse client using the credentials provided in the environment variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "b80ca760",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Langfuse client is authenticated and ready!\n"
          ]
        }
      ],
      "source": [
        "from langfuse import get_client\n",
        " \n",
        "langfuse = get_client()\n",
        " \n",
        "# Verify connection\n",
        "if langfuse.auth_check():\n",
        "    print(\"Langfuse client is authenticated and ready!\")\n",
        "else:\n",
        "    print(\"Authentication failed. Please check your credentials and host.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "syim3MuMztli",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "syim3MuMztli",
        "outputId": "5cadbc33-a9be-4638-ce6d-03141da1fcdf"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "import pandas as pd\n",
        "\n",
        "client = OpenAI()\n",
        "\n",
        "# Function to generate airline questions\n",
        "def generate_airline_questions(num_questions=20):\n",
        "\n",
        "    questions = []\n",
        "\n",
        "    for i in range(num_questions):\n",
        "        completion = client.chat.completions.create(\n",
        "            model=\"gpt-4o\", \n",
        "            messages=[\n",
        "                {\n",
        "                    \"role\": \"system\",\n",
        "                    \"content\": (\n",
        "                        \"You are a helpful customer service chatbot for an airline. \"\n",
        "                        \"Please generate a short, realistic question from a customer.\"\n",
        "                    )\n",
        "                }\n",
        "            ],\n",
        "            temperature=1\n",
        "        )\n",
        "        question_text = completion.choices[0].message.content.strip()\n",
        "        questions.append(question_text)\n",
        "\n",
        "    return questions\n",
        "\n",
        "# Generate 20 airline-related questions\n",
        "airline_questions = generate_airline_questions(num_questions=20)\n",
        "\n",
        "# Convert to a Pandas DataFrame\n",
        "df = pd.DataFrame({\"Question\": airline_questions})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "0S7_92wlpbHo",
      "metadata": {
        "id": "0S7_92wlpbHo"
      },
      "outputs": [],
      "source": [
        "from langfuse import get_client\n",
        " \n",
        "langfuse = get_client()\n",
        "\n",
        "# Create a new dataset in Langfuse\n",
        "dataset_name = \"openai_synthetic_dataset\"\n",
        "langfuse.create_dataset(\n",
        "    name=dataset_name,\n",
        "    description=\"Synthetic Q&A dataset generated via OpenAI in a loop\",\n",
        "    metadata={\"approach\": \"openai_loop\", \"category\": \"mixed\"}\n",
        ")\n",
        "\n",
        "# Upload each Q&A as a dataset item\n",
        "for _, row in df.iterrows():\n",
        "    langfuse.create_dataset_item(\n",
        "        dataset_name=\"openai_loop_dataset\",\n",
        "        input = row[\"Question\"]\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "29c0e88b",
      "metadata": {},
      "source": [
        "![OpenAI Dataset](https://langfuse.com/images/cookbook/example-synthetic-datasets/openai-dataset.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ORkGZBG0pbHp",
      "metadata": {
        "id": "ORkGZBG0pbHp"
      },
      "source": [
        "### Example 2: RAGAS Library\n",
        "\n",
        "For **RAG**, we often want questions that are *grounded in specific documents*. This ensures the question can be answered by the context, allowing us to evaluate how well a RAG pipeline retrieves and uses the context.\n",
        "\n",
        "[RAGAS](https://docs.ragas.io/en/stable/getstarted/rag_testset_generation/#testset-generation) is a library that can automate test set generation for RAG. It can take a corpus and produce relevant queries and answers. We'll do a quick example:\n",
        "\n",
        "_**Note**: This example is taken from the [RAGAS documentation](https://docs.ragas.io/en/stable/getstarted/rag_testset_generation/)_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "_c5fOb9wpbHq",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "_c5fOb9wpbHq",
        "outputId": "f76166b5-eecd-43b4-9458-fde2673e064b"
      },
      "outputs": [],
      "source": [
        "%pip install ragas langchain-community langchain-openai unstructured"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "P2Q1Ux_0pbHq",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P2Q1Ux_0pbHq",
        "outputId": "dbe7cbbb-70f8-4ca6-ffee-9f8fb179d170"
      },
      "outputs": [],
      "source": [
        "!git clone https://huggingface.co/datasets/explodinggradients/Sample_Docs_Markdown"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "ed376b5c",
      "metadata": {
        "id": "ed376b5c"
      },
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import DirectoryLoader\n",
        "\n",
        "path = \"Sample_Docs_Markdown\"\n",
        "loader = DirectoryLoader(path, glob=\"**/*.md\")\n",
        "docs = loader.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "2101d44a",
      "metadata": {
        "id": "2101d44a"
      },
      "outputs": [],
      "source": [
        "from ragas.llms import LangchainLLMWrapper\n",
        "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "generator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4o\"))\n",
        "generator_embeddings = LangchainEmbeddingsWrapper(OpenAIEmbeddings())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "05fe986c",
      "metadata": {
        "id": "05fe986c"
      },
      "outputs": [],
      "source": [
        "from ragas.testset import TestsetGenerator\n",
        "\n",
        "generator = TestsetGenerator(llm=generator_llm, embedding_model=generator_embeddings)\n",
        "dataset = generator.generate_with_langchain_docs(docs, testset_size=10)\n",
        "\n",
        "# 4. The result `testset` can be converted to a pandas DataFrame for inspection\n",
        "df = dataset.to_pandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "lewlGMTnusaX",
      "metadata": {
        "id": "lewlGMTnusaX"
      },
      "outputs": [],
      "source": [
        "from langfuse import get_client\n",
        " \n",
        "langfuse = get_client()\n",
        "\n",
        "# 5. Push the RAGAS-generated testset to Langfuse\n",
        "langfuse.create_dataset(\n",
        "    name=\"ragas_generated_testset\",\n",
        "    description=\"Synthetic RAG test set (RAGAS)\",\n",
        "    metadata={\"source\": \"RAGAS\", \"docs_used\": len(docs)}\n",
        ")\n",
        "\n",
        "for _, row in df.iterrows():\n",
        "    langfuse.create_dataset_item(\n",
        "        dataset_name=\"ragas_generated_testset\",\n",
        "        input = row[\"user_input\"],\n",
        "        metadata = row[\"reference_contexts\"]\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5PsFnNzH4aNB",
      "metadata": {
        "id": "5PsFnNzH4aNB"
      },
      "source": [
        "![RAGAS Dataset](https://langfuse.com/images/cookbook/example-synthetic-datasets/ragas-dataset.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b8b247b7",
      "metadata": {},
      "source": [
        "### Example 3: DeepEval Library\n",
        "\n",
        "[DeepEval](https://docs.confident-ai.com/docs/synthesizer-introduction) is a library that helps generate synthetic data systematically using the *Synthesizer* class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "35be76b8",
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install deepeval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ef7ccec2",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from langfuse import get_client\n",
        "from deepeval.synthesizer import Synthesizer\n",
        "from deepeval.synthesizer.config import StylingConfig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2cba4190",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1. Define the style we want for our synthetic data.\n",
        "# For instance, we want user questions and correct SQL queries.\n",
        "styling_config = StylingConfig(\n",
        "  input_format=\"Questions in English that asks for data in database.\",\n",
        "  expected_output_format=\"SQL query based on the given input\",\n",
        "  task=\"Answering text-to-SQL-related queries by querying a database and returning the results to users\",\n",
        "  scenario=\"Non-technical users trying to query a database using plain English.\",\n",
        ")\n",
        "\n",
        "# 2. Initialize the Synthesizer\n",
        "synthesizer = Synthesizer(styling_config=styling_config)\n",
        "\n",
        "# 3. Generate synthetic items from scratch, e.g. 20 items for a short demo\n",
        "synthesizer.generate_goldens_from_scratch(num_goldens=20)\n",
        "\n",
        "# 4. Access the generated examples\n",
        "synthetic_goldens = synthesizer.synthetic_goldens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b0637aa",
      "metadata": {},
      "outputs": [],
      "source": [
        "from langfuse import get_client\n",
        "langfuse = get_client()\n",
        "\n",
        "# 5. Create a Langfuse dataset\n",
        "deepeval_dataset_name = \"deepeval_synthetic_data\"\n",
        "langfuse.create_dataset(\n",
        "    name=deepeval_dataset_name,\n",
        "    description=\"Synthetic text-to-SQL data (DeepEval)\",\n",
        "    metadata={\"approach\": \"deepeval\", \"task\": \"text-to-sql\"}\n",
        ")\n",
        "\n",
        "# 6. Upload the items\n",
        "for golden in synthetic_goldens:\n",
        "    langfuse.create_dataset_item(\n",
        "        dataset_name=deepeval_dataset_name,\n",
        "        input={\"query\": golden.input},\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "78af50a9",
      "metadata": {},
      "source": [
        "![Dataset in Langfuse](https://langfuse.com/images/cookbook/example-synthetic-datasets/deepeval-dataset.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "DPCUnZTipbHr",
      "metadata": {
        "id": "DPCUnZTipbHr"
      },
      "source": [
        "### Example 4: No-Code via Hugging Face Dataset Generator\n",
        "\n",
        "If you prefer a more UI-based approach, check out [Hugging Face's Synthetic Data Generator](https://huggingface.co/blog/synthetic-data-generator). You can generate examples in the Hugging Face UI. Then you can download them as CSV and upload it in the Langfuse UI."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ODNR85wQ3hK5",
      "metadata": {
        "id": "ODNR85wQ3hK5"
      },
      "source": [
        "![Hugging Face Dataset Generator](https://langfuse.com/images/cookbook/example-synthetic-datasets/hf-generator.png)\n",
        "\n",
        "![Hugging Face Synthetic Dataset](https://langfuse.com/images/cookbook/example-synthetic-datasets/hf-dataset.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a86bfacf",
      "metadata": {},
      "source": [
        "### Example 5: RAG Dataset Generation\n",
        "\n",
        "If you have an existing vector database or prefer not to use specialized libraries like RAGAS or DeepEval, you can generate a RAG testset by directly looping through your vector store. This approach gives you full control over the generation process.\n",
        "\n",
        "This is useful when you:\n",
        "- Want lightweight code without additional dependencies\n",
        "- Need to customize the question generation logic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c2124b52",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        " \n",
        "# Get keys for your project from the project settings page: https://cloud.langfuse.com\n",
        "os.environ[\"LANGFUSE_PUBLIC_KEY\"] = \"pk-lf-...\" \n",
        "os.environ[\"LANGFUSE_SECRET_KEY\"] = \"sk-lf-...\" \n",
        "os.environ[\"LANGFUSE_BASE_URL\"] = \"https://cloud.langfuse.com\" # ðŸ‡ªðŸ‡º EU region\n",
        "# os.environ[\"LANGFUSE_BASE_URL\"] = \"https://us.cloud.langfuse.com\" # ðŸ‡ºðŸ‡¸ US region\n",
        " \n",
        "# Your openai key\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"sk-proj-...\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9462ce0d",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "%pip install --upgrade langchain-community langchain-openai langchain-chroma langfuse \"unstructured[md]\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "77a82a97",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clone an example document set\n",
        "!git clone https://huggingface.co/datasets/explodinggradients/Sample_Docs_Markdown"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fb8a4c0f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the documents\n",
        "from langchain_community.document_loaders import DirectoryLoader\n",
        " \n",
        "path = \"Sample_Docs_Markdown\"\n",
        "loader = DirectoryLoader(path, glob=\"**/*.md\")\n",
        "docs = loader.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "8de64b0a",
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "# Chunk the documents\n",
        "splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "chunks = splitter.split_documents(docs)\n",
        "\n",
        "# Create vector DB\n",
        "vectorstore = Chroma.from_documents(chunks, OpenAIEmbeddings())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "c3b80899",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate questions\n",
        "import json\n",
        "\n",
        "# Get all chunks\n",
        "all_chunks = vectorstore.get()['documents'][:10] # Get the first 10 chunks\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
        "\n",
        "test_items = []\n",
        "\n",
        "for chunk in all_chunks:\n",
        "    # Ask LLM to generate one question\n",
        "    response = llm.invoke(\n",
        "        f\"Generate one natural question that can be answered using this text. \"\n",
        "        f\"Return only JSON: {{\\\"question\\\": \\\"...\\\", \\\"answer\\\": \\\"...\\\"}}\\n\\n{chunk}\"\n",
        "    )\n",
        "    \n",
        "    # Parse response\n",
        "    content = response.content\n",
        "    if \"```\" in content:\n",
        "        content = content.split(\"```\")[1].replace(\"json\", \"\").strip()\n",
        "    \n",
        "    qa = json.loads(content)\n",
        "    test_items.append({\n",
        "        \"question\": qa[\"question\"],\n",
        "        \"answer\": qa[\"answer\"],\n",
        "        \"context\": chunk\n",
        "    })"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "74aac065",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Push to Langfuse Dataset\n",
        "from langfuse import get_client\n",
        "\n",
        "langfuse = get_client()\n",
        "\n",
        "langfuse.create_dataset(name=\"simple_rag_testset\")\n",
        "\n",
        "for item in test_items:\n",
        "    langfuse.create_dataset_item(\n",
        "        dataset_name=\"simple_rag_testset\",\n",
        "        input=item[\"question\"],\n",
        "        expected_output=item[\"answer\"],\n",
        "        metadata={\"context\": item[\"context\"]}\n",
        "    )\n",
        "\n",
        "print(f\"âœ“ Created {len(test_items)} test items\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "87df7766",
      "metadata": {},
      "source": [
        "You can now evaluate your application using this dataset. Check our [RAG Observability and Evals](https://langfuse.com/blog/2025-10-28-rag-observability-and-evals) blogpost to learn more.\n",
        "\n",
        "![Custom RAG Dataset](https://langfuse.com/images/cookbook/example-synthetic-datasets/custom-rag-dataset.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "65a54eb2",
      "metadata": {},
      "source": [
        "### Example 6: Torque - Declarative Dataset Generation\n",
        "\n",
        "[Torque](https://github.com/qforge-dev/torque) is a declarative, typesafe DSL for building synthetic LLM datasets. It lets you compose conversations like React components, making it particularly useful for generating complex multi-turn conversations with tool calls.\n",
        "\n",
        "This approach is ideal when you need:\n",
        "\n",
        "- **Structured conversations** with tool usage patterns\n",
        "- **Type-safe dataset generation** with full TypeScript support\n",
        "- **Reproducible datasets** with seeded generation\n",
        "- **Complex multi-turn dialogs** that follow specific patterns\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "javascript"
        }
      },
      "outputs": [],
      "source": [
        "import { Langfuse } from \"langfuse\";\n",
        "\n",
        "import {\n",
        "  oneOf,\n",
        "  generatedUser,\n",
        "  generatedAssistant,\n",
        "  generatedToolCall,\n",
        "  generatedToolCallResult,\n",
        "  times,\n",
        "  between,\n",
        "  generateDataset,\n",
        "} from \"@qforge/torque\";\n",
        "import { weatherTool, searchEmailTool } from \"@qforge/torque/examples\";\n",
        "import { openai } from \"@ai-sdk/openai\";\n",
        "\n",
        "const langfuse = new Langfuse({\n",
        "  publicKey: process.env.LANGFUSE_PUBLIC_KEY,\n",
        "  secretKey: process.env.LANGFUSE_SECRET_KEY,\n",
        "  baseUrl: process.env.LANGFUSE_URL,\n",
        "});\n",
        "\n",
        "// Generate dataset with Torque's declarative DSL\n",
        "const conversationSchema = () => {\n",
        "  // Randomly select which tool to use in this conversation\n",
        "  const selectedTool = oneOf([searchEmailTool, weatherTool]);\n",
        "\n",
        "  return [\n",
        "    // Register the tool function\n",
        "    selectedTool.toolFunction(),\n",
        "\n",
        "    // User initiates request\n",
        "    generatedUser({\n",
        "      prompt: `User asking a question that will require calling ${selectedTool.name} tool.`,\n",
        "    }),\n",
        "\n",
        "    // Assistant acknowledges and calls tool\n",
        "    generatedAssistant({\n",
        "      prompt: \"Assistant acknowledging the tool call\",\n",
        "      toolCalls: [generatedToolCall(selectedTool, \"tool-1\")],\n",
        "    }),\n",
        "\n",
        "    generatedToolCallResult(selectedTool, \"tool-1\"),\n",
        "\n",
        "    // Assistant presents results\n",
        "    generatedAssistant({\n",
        "      prompt:\n",
        "        \"Assistant responding to the user's question using the result of the tool call.\",\n",
        "    }),\n",
        "\n",
        "    // Optional follow-up conversation (1-2 exchanges)\n",
        "    times(between(1, 2), [\n",
        "      generatedUser({\n",
        "        prompt: \"Follow-up question or thank you\",\n",
        "      }),\n",
        "      generatedAssistant({\n",
        "        prompt:\n",
        "          \"Assistant responding to the user's follow-up question or thank you\",\n",
        "      }),\n",
        "    ]),\n",
        "  ];\n",
        "};\n",
        "\n",
        "// Generate the dataset to a JSONL file\n",
        "await generateDataset(conversationSchema, {\n",
        "  count: 50,\n",
        "  model: openai(\"gpt-5-mini\"),\n",
        "  output: \"data/torque_asynctool.jsonl\",\n",
        "  seed: 42, // Reproducible generation\n",
        "});\n",
        "\n",
        "// Read generated JSONL and upload to Langfuse\n",
        "await langfuse.createDataset({\n",
        "  name: \"torque_asynctool\",\n",
        "  description: \"AsyncTool conversations generated with Torque DSL\",\n",
        "});\n",
        "\n",
        "const jsonlContent = await Bun.file(\"data/torque_asynctool.jsonl\").text();\n",
        "const conversations = jsonlContent\n",
        "  .trim()\n",
        "  .split(\"\\n\")\n",
        "  .map((line) => JSON.parse(line));\n",
        "\n",
        "for (const conversation of conversations) {\n",
        "  await langfuse.createDatasetItem({\n",
        "    datasetName: \"torque_asynctool\",\n",
        "    input: conversation.messages,\n",
        "    metadata: {\n",
        "      tool_used: conversation.messages.find((m) => m.role === \"tool\")?.name,\n",
        "      turns: conversation.messages.length,\n",
        "    },\n",
        "  });\n",
        "}\n",
        "\n",
        "await langfuse.flushAsync();\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7edd8679",
      "metadata": {},
      "source": [
        "**Key advantages of Torque:**\n",
        "\n",
        "1. **Type-safe conversations**: Full TypeScript support with Zod schemas ensures your synthetic data matches your production types\n",
        "2. **Declarative patterns**: Compose complex conversation flows with `times()`, `oneOf()`, and other combinators\n",
        "3. **Tool simulation**: Built-in support for tool calls and results, perfect for evaluating agentic applications\n",
        "4. **Reproducible**: Seeded generation ensures identical datasets across runs\n",
        "5. **Realistic variations**: AI generates natural variations while following your structural constraints\n",
        "\n",
        "This approach is particularly powerful for **evaluating AI agents** with tool usage, as it generates structurally consistent but semantically diverse conversations.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Sega8BHBpbHr",
      "metadata": {
        "id": "Sega8BHBpbHr"
      },
      "source": [
        "## Next Steps\n",
        "\n",
        "1. **Explore your dataset in Langfuse**. You can see each dataset in the UI.\n",
        "2. **Run experiments** You can now evaluate your application using this dataset.\n",
        "3. **Compare runs** over time or across models, prompts, or chain logic.\n",
        "\n",
        "For more details on how to run experiments on a dataset, see the [Langfuse docs](https://langfuse.com/docs/datasets/get-started#run-experiment-on-a-dataset)."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}

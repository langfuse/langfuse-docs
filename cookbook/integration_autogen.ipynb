{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: Observability for AutoGen with Langfuse\n",
    "description: Learn how to integrate Langfuse with AutoGen via OpenTelemetry using OpenLit\n",
    "---\n",
    "\n",
    "# Integrate Langfuse with AutoGen\n",
    "\n",
    "This notebook demonstrates how to integrate **Langfuse** with **AutoGen** using OpenTelemetry via the **OpenLit** SDK. By the end of this notebook, you will be able to trace your AutoGen applications with Langfuse for improved observability and debugging.\n",
    "\n",
    "> **What is AutoGen?** [AutoGen](https://microsoft.github.io/autogen/stable/) [(GitHub)](https://github.com/microsoft/autogen) is an open-source framework developed by Microsoft for building LLM applications, including agents capable of complex reasoning and interactions. AutoGen simplifies the creation of conversational agents that can collaborate or compete to solve tasks.\n",
    "\n",
    "> **What is Langfuse?** [Langfuse](https://langfuse.com) is an open-source LLM engineering platform. It provides tracing and monitoring capabilities for LLM applications, helping developers debug, analyze, and optimize their AI systems. Langfuse integrates with various tools and frameworks via native integrations, OpenTelemetry, and API/SDKs.\n",
    "\n",
    "## Get Started\n",
    "\n",
    "We'll walk through a simple example of using AutoGen and integrating it with Langfuse via OpenTelemetry using OpenLit.\n",
    "\n",
    "### Step 1: Install Dependencies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install langfuse openlit autogen\n",
    "%pip install opentelemetry-sdk opentelemetry-exporter-otlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Set Up Environment Variables\n",
    "\n",
    "Set your Langfuse API keys and configure OpenTelemetry export settings to send traces to Langfuse. Please refer to the [Langfuse OpenTelemetry Docs](https://langfuse.com/docs/opentelemetry/get-started) for more information on the Langfuse OpenTelemetry endpoint `/api/public/otel` and authentification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import base64\n",
    "\n",
    "# Get keys for your project from the project settings page: https://cloud.langfuse.com\n",
    "os.environ[\"LANGFUSE_PUBLIC_KEY\"] = \"pk-lf-...\" \n",
    "os.environ[\"LANGFUSE_SECRET_KEY\"] = \"sk-lf-...\" \n",
    "os.environ[\"LANGFUSE_HOST\"] = \"https://cloud.langfuse.com\" # ðŸ‡ªðŸ‡º EU region\n",
    "# os.environ[\"LANGFUSE_HOST\"] = \"https://us.cloud.langfuse.com\" # ðŸ‡ºðŸ‡¸ US region\n",
    "\n",
    "\n",
    "LANGFUSE_AUTH = base64.b64encode(\n",
    "    f\"{os.environ.get('LANGFUSE_PUBLIC_KEY')}:{os.environ.get('LANGFUSE_SECRET_KEY')}\".encode()\n",
    ").decode()\n",
    "\n",
    "os.environ[\"OTEL_EXPORTER_OTLP_ENDPOINT\"] = os.environ.get(\"LANGFUSE_HOST\") + \"/api/public/otel\"\n",
    "os.environ[\"OTEL_EXPORTER_OTLP_HEADERS\"] = f\"Authorization=Basic {LANGFUSE_AUTH}\"\n",
    "\n",
    "# your openai key\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-proj-...\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configure `tracer_provider` and add a span processor to export traces to Langfuse. `OTLPSpanExporter()` uses the endpoint and headers from the environment variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from opentelemetry.sdk.trace import TracerProvider\n",
    "from opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporter\n",
    "from opentelemetry.sdk.trace.export import SimpleSpanProcessor\n",
    "\n",
    "trace_provider = TracerProvider()\n",
    "trace_provider.add_span_processor(SimpleSpanProcessor(OTLPSpanExporter()))\n",
    "\n",
    "# Sets the global default tracer provider\n",
    "from opentelemetry import trace\n",
    "trace.set_tracer_provider(trace_provider)\n",
    "\n",
    "# Creates a tracer from the global tracer provider\n",
    "tracer = trace.get_tracer(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Initialize OpenLit\n",
    "\n",
    "Initialize OpenLit to start capturing OpenTelemetry traces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openlit\n",
    "\n",
    "# Initialize OpenLIT instrumentation. The disable_batch flag is set to true to process traces immediately.\n",
    "openlit.init(tracer=tracer, disable_batch=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Create a Simple AutoGen Application\n",
    "\n",
    "We'll create a simple AutoGen application where an Assistant agent answers a user's question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33muser_proxy\u001b[0m (to assistant):\n",
      "\n",
      "What is Langfuse?\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33massistant\u001b[0m (to user_proxy):\n",
      "\n",
      "Langfuse is a debugging and observability tool designed specifically for AI workflows. It provides developers with insights and tools to monitor, debug, and improve their AI models and applications. Langfuse mainly focuses on logging AI interactions, analyzing model performance, tracking errors, and understanding user interactions with AI systems. By providing a detailed overview of how AI workflows are performing in production, Langfuse helps teams iterate faster and enhance their AI solutionsâ€™ reliability and effectiveness. TERMINATE\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ChatResult(chat_id=None, chat_history=[{'content': 'What is Langfuse?', 'role': 'assistant', 'name': 'user_proxy'}, {'content': 'Langfuse is a debugging and observability tool designed specifically for AI workflows. It provides developers with insights and tools to monitor, debug, and improve their AI models and applications. Langfuse mainly focuses on logging AI interactions, analyzing model performance, tracking errors, and understanding user interactions with AI systems. By providing a detailed overview of how AI workflows are performing in production, Langfuse helps teams iterate faster and enhance their AI solutionsâ€™ reliability and effectiveness. TERMINATE', 'role': 'user', 'name': 'assistant'}], summary='Langfuse is a debugging and observability tool designed specifically for AI workflows. It provides developers with insights and tools to monitor, debug, and improve their AI models and applications. Langfuse mainly focuses on logging AI interactions, analyzing model performance, tracking errors, and understanding user interactions with AI systems. By providing a detailed overview of how AI workflows are performing in production, Langfuse helps teams iterate faster and enhance their AI solutionsâ€™ reliability and effectiveness. ', cost={'usage_including_cached_inference': {'total_cost': 0.00213, 'gpt-4o-2024-08-06': {'cost': 0.00213, 'prompt_tokens': 472, 'completion_tokens': 95, 'total_tokens': 567}}, 'usage_excluding_cached_inference': {'total_cost': 0}}, human_input=['exit'])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import autogen\n",
    "from autogen import AssistantAgent, UserProxyAgent\n",
    "\n",
    "llm_config = {\"model\": \"gpt-4o\", \"api_key\": os.environ[\"OPENAI_API_KEY\"]}\n",
    "assistant = AssistantAgent(\"assistant\", llm_config=llm_config)\n",
    "\n",
    "user_proxy = UserProxyAgent(\n",
    "    \"user_proxy\", code_execution_config={\"executor\": autogen.coding.LocalCommandLineCodeExecutor(work_dir=\"coding\")}\n",
    ")\n",
    "\n",
    "# Start the chat\n",
    "user_proxy.initiate_chat(\n",
    "    assistant,\n",
    "    message=\"What is Langfuse?\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Pass Additional Attributes (Optional)\n",
    "\n",
    "Opentelemetry lets you attach a set of attributes to all spans by setting [`set_attribute`](https://opentelemetry.io/docs/languages/python/instrumentation/#add-attributes-to-a-span). This allows you to set properties like a Langfuse Session ID, to group traces into Langfuse Sessions or a User ID, to assign traces to a specific user. You can find a list of all supported attributes in the [here](/docs/opentelemetry/get-started#property-mapping)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tracer.start_as_current_span(\"AutoGen-Trace\") as span:\n",
    "    span.set_attribute(\"langfuse.user.id\", \"user-123\")\n",
    "    span.set_attribute(\"langfuse.session.id\", \"123456789\")\n",
    "    span.set_attribute(\"langfuse.tags\", [\"semantic-kernel\", \"demo\"])\n",
    "    span.set_attribute(\"langfuse.prompt.name\", \"test-1\")\n",
    "\n",
    "    # Start the chat\n",
    "    user_proxy.initiate_chat(\n",
    "        assistant,\n",
    "        message=\"What is Langfuse?\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, OpenTelemetry traces in Langfuse can also be modified using the [Python low-level SDK](https://langfuse.com/docs/sdk/python/low-level-sdk). For this, we create a new parent span and fetch the OpenTelemetry `trace_id`. This trace_id is then used to modify the span. Have a look at the [Python low-level SDK](https://langfuse.com/docs/sdk/python/low-level-sdk) for more examples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from opentelemetry.trace import format_trace_id\n",
    "from langfuse import Langfuse\n",
    "\n",
    "langfuse = Langfuse()\n",
    "\n",
    "with tracer.start_as_current_span(\"Semantic-Kernel-Trace\") as span:\n",
    "\n",
    "    # Start the chat\n",
    "    user_proxy.initiate_chat(\n",
    "        assistant,\n",
    "        message=\"What is Langfuse?\",\n",
    "    )\n",
    "    \n",
    "    # Get the trace_id from the Otel span\n",
    "    current_span = trace.get_current_span()\n",
    "    span_context = current_span.get_span_context()\n",
    "    trace_id = span_context.trace_id\n",
    "    formatted_trace_id = format_trace_id(trace_id)\n",
    "\n",
    "    # Update the trace using the low-level Python SDK.\n",
    "    langfuse.trace(\n",
    "        id=formatted_trace_id, \n",
    "        name = \"docs-retrieval\",\n",
    "        user_id = \"user__935d7d1d-8625-4ef4-8651-544613e7bd22\",\n",
    "        metadata = {\"email\": \"user@langfuse.com\"},\n",
    "        tags = [\"production\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Step 6: See Traces in Langfuse\n",
    "\n",
    "After running the agent above, you can log in to your Langfuse dashboard and view the traces generated by your AutoGen application. Here is an example screenshot of a trace in Langfuse:\n",
    "\n",
    "![Langfuse Trace](https://langfuse.com/images/cookbook/integration-autogen/autogen-example-trace.png)\n",
    "\n",
    "You can also view the public trace here: [Langfuse Trace Example](https://cloud.langfuse.com/project/cloramnkj0002jz088vzn1ja4/traces/df850ab499107d4348584cf5933baabd?timestamp=2025-02-04T16%3A55%3A51.660Z&observation=286c648acb0105c2)\n",
    "\n",
    "## References\n",
    "\n",
    "- [Langfuse OpenTelemetry Docs](https://langfuse.com/docs/opentelemetry/get-started)\n",
    "- [AutoGen OpenTelemetry Docs](https://microsoft.github.io/autogen/dev//user-guide/core-user-guide/framework/telemetry.html)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

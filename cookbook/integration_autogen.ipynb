{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- NOTEBOOK_METADATA source: \"âš ï¸ Jupyter Notebook\" title: \"Observability for AutoGen with Langfuse Integration\" sidebarTitle: \"AutoGen\" logo: \"/images/integrations/autogen_icon.svg\" description: \"Learn how to integrate Langfuse with AutoGen via OpenTelemetry using OpenLit for comprehensive tracing and debugging of your AI agent conversations.\" category: \"Integrations\" -->\n",
    "\n",
    "# Integrate Langfuse with AutoGen\n",
    "\n",
    "This notebook provides a step-by-step guide on integrating **Langfuse** with **AutoGen** to achieve comprehensive observability and debugging for your multi-agent LLM applications.\n",
    "\n",
    "> **What is AutoGen?** [AutoGen](https://microsoft.github.io/autogen/stable/) ([GitHub](https://github.com/microsoft/autogen)) is an open-source framework developed by Microsoft for building LLM applications, including agents capable of complex reasoning and interactions. AutoGen simplifies the creation of conversational agents that can collaborate or compete to solve tasks.\n",
    "\n",
    "> **What is Langfuse?** [Langfuse](https://langfuse.com) is an open-source LLM engineering platform. It offers tracing and monitoring capabilities for AI applications. Langfuse helps developers debug, analyze, and optimize their AI systems by providing detailed insights and integrating with a wide array of tools and frameworks through native integrations, OpenTelemetry, and dedicated SDKs.\n",
    "\n",
    "## Getting Started\n",
    "\n",
    "Let's walk through a practical example of using AutoGen and integrating it with Langfuse via OpenTelemetry for comprehensive tracing.\n",
    "\n",
    "<!-- STEPS_START -->\n",
    "### Step 1: Install Dependencies\n",
    "\n",
    "<!-- CALLOUT_START type: \"info\" emoji: \"âš ï¸\" -->\n",
    "> **Note:** This notebook utilizes the Langfuse OTel Python SDK v3. For users of Python SDK v2, please refer to our [legacy AutoGen integration guide](https://github.com/langfuse/langfuse-docs/blob/26a3aa9e935a0325b463e3fa585772fd20c23642/cookbook/integration_autogen.ipynb).\n",
    "<!-- CALLOUT_END -->\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install langfuse openlit \"autogen-agentchat\" \"autogen-ext[openai]\" -U"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Configure Langfuse SDK\n",
    "\n",
    "Next, set up your Langfuse API keys. You can get these keys by signing up for a free [Langfuse Cloud](https://cloud.langfuse.com/) account or by [self-hosting Langfuse](https://langfuse.com/self-hosting). These environment variables are essential for the Langfuse client to authenticate and send data to your Langfuse project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Get keys for your project from the project settings page: https://cloud.langfuse.com\n",
    "os.environ[\"LANGFUSE_PUBLIC_KEY\"] = \"pk-lf-...\" \n",
    "os.environ[\"LANGFUSE_SECRET_KEY\"] = \"sk-lf-...\" \n",
    "os.environ[\"LANGFUSE_HOST\"] = \"https://cloud.langfuse.com\" # ðŸ‡ªðŸ‡º EU region\n",
    "# os.environ[\"LANGFUSE_HOST\"] = \"https://us.cloud.langfuse.com\" # ðŸ‡ºðŸ‡¸ US region\n",
    "\n",
    "# Your openai key\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-proj-...\""
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "With the environment variables set, we can now initialize the Langfuse client. `get_client()` initializes the Langfuse client using the credentials provided in the environment variables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Langfuse client is authenticated and ready!\n"
     ]
    }
   ],
   "source": [
    "from langfuse import Langfuse\n",
    " \n",
    "# Filter out Autogen OpenTelemetryspans\n",
    "langfuse = Langfuse(\n",
    "    blocked_instrumentation_scopes=[\"autogen SingleThreadedAgentRuntime\"]\n",
    ")\n",
    " \n",
    "# Verify connection\n",
    "if langfuse.auth_check():\n",
    "    print(\"Langfuse client is authenticated and ready!\")\n",
    "else:\n",
    "    print(\"Authentication failed. Please check your credentials and host.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Initialize OpenLit Instrumentation\n",
    "\n",
    "Now, we initialize the [OpenLit](https://github.com/openlit/openlit) instrumentation. OpenLit automatically captures AutoGen operations and exports OpenTelemetry (OTel) spans to Langfuse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openlit\n",
    " \n",
    "# Initialize OpenLIT instrumentation. The disable_batch flag is set to true to process traces immediately.\n",
    "openlit.init(tracer=langfuse._otel_tracer, disable_batch=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Basic AutoGen Application\n",
    "\n",
    "Let's create a straightforward AutoGen application. In this example, we'll create a simple multi-agent conversation where an Assistant agent answers a user's question. This will serve as the foundation for demonstrating Langfuse tracing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "messages=[TextMessage(id='0935120c-2a27-4bb8-ad7f-eb5cb2b4902d', source='user', models_usage=None, metadata={}, created_at=datetime.datetime(2025, 7, 8, 8, 32, 25, 422043, tzinfo=datetime.timezone.utc), content=\"Say 'Hello World!'\", type='TextMessage'), TextMessage(id='2046975a-235c-4e46-be55-719bb79ced95', source='assistant', models_usage=RequestUsage(prompt_tokens=41, completion_tokens=5), metadata={}, created_at=datetime.datetime(2025, 7, 8, 8, 32, 26, 265424, tzinfo=datetime.timezone.utc), content='Hello World! TERMINATE', type='TextMessage')] stop_reason=None\n"
     ]
    }
   ],
   "source": [
    "from autogen_agentchat.agents import AssistantAgent\n",
    "from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
    "\n",
    "model_client = OpenAIChatCompletionClient(model=\"gpt-4o\")\n",
    "agent = AssistantAgent(\"assistant\", model_client=model_client)\n",
    "print(await agent.run(task=\"Say 'Hello World!'\"))\n",
    "await model_client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: View Traces in Langfuse\n",
    "\n",
    "After executing the application, navigate to your Langfuse Trace Table. You will find detailed traces of the application's execution, providing insights into the agent conversations, LLM calls, inputs, outputs, and performance metrics. The trace will show the complete flow from the initial user query through the agent interactions to the final response.\n",
    "\n",
    "![Langfuse Trace](https://langfuse.com/images/cookbook/integration-autogen/autogen-example-trace.png)\n",
    "\n",
    "You can also view the public trace here: [Langfuse Trace Example](https://cloud.langfuse.com/project/cloramnkj0002jz088vzn1ja4/traces/1b72c51fabed12ae7df83bfd4a09f545?timestamp=2025-06-06T11:31:33.965Z&display=details)\n",
    "<!-- STEPS_END -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- MARKDOWN_COMPONENT name: \"LearnMore\" path: \"@/components-mdx/integration-learn-more.mdx\" -->"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KlceIPalN3QR"
   },
   "source": [
    "---\n",
    "description: This cookbook demonstate use of Langfuse with Azure OpenAI and Langchain for prompt versioning and evaluations \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mqBspBzuRk9C"
   },
   "source": [
    "# Langfuse integration with Azure OpenAI and Langchain  (Python)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "05afP0VbYHX0"
   },
   "source": [
    "Langfuse integrates with Langchain using the [Langchain Callbacks](https://python.langchain.com/docs/modules/callbacks/). Thereby, the Langfuse SDK automatically creates a nested trace for the abstractions offered by Langchain.\n",
    "\n",
    "Add the handler as a callback when running your Langchain model/chain/agent:\n",
    "\n",
    "```python /callbacks=[handler]/\n",
    "# Initialize Langfuse handler\n",
    "from langfuse.callback import CallbackHandler\n",
    "handler = CallbackHandler(PUBLIC_KEY, SECRET_KEY)\n",
    "\n",
    "# Setup Langchain\n",
    "from langchain.chains import LLMChain\n",
    "...\n",
    "chain = LLMChain(llm=llm, prompt=prompt, callbacks=[handler])\n",
    "\n",
    "# Add Langfuse handler as callback\n",
    "chain.run(input=\"<user_input\", callbacks=[handler])\n",
    "```\n",
    "\n",
    "Langchain expression language (LCEL)\n",
    "```python /config={\"callbacks\":[handler]}/\n",
    "chain.invoke(input, config={\"callbacks\":[handler]})\n",
    "```\n",
    "\n",
    "---\n",
    " **_In case of missing events or tokens:_**\n",
    "\n",
    "There are two ways to integrate callbacks into Langchain:\n",
    "- *Constructor Callbacks*: Set when initializing an object, like `LLMChain(callbacks=[handler])` or `ChatOpenAI(callbacks=[handler])`. This approach will use the callback for every call made on that specific object. However, it won't apply to its child objects, making it limited in scope.\n",
    "- *Request Callbacks*: Defined when issuing a request, like `chain.run(input, callbacks=[handler])` and `chain.invoke(input, config={\"callbacks\":[handler]})`. This not only uses the callback for that specific request but also for any subsequent sub-requests it triggers.\n",
    "\n",
    "For comprehensive data capture especially for complex chains or agents, it's advised to use the both approaches, as demonstrated above [docs](https://python.langchain.com/docs/modules/callbacks/#where-to-pass-in-callbacks).\n",
    "\n",
    "---\n",
    "\n",
    "The Langfuse `CallbackHandler` tracks the following actions when using Langchain:\n",
    "\n",
    "- Chains: `on_chain_start`, `on_chain_end`. `on_chain_error`\n",
    "- Agents: `on_agent_start`, `on_agent_action`, `on_agent_finish`, `on_agent_end`\n",
    "- Tools: `on_tool_start`, `on_tool_end`, `on_tool_error`\n",
    "- Retriever: `on_retriever_start`, `on_retriever_end`\n",
    "- ChatModel: `on_chat_model_start`,\n",
    "- LLM: `on_llm_start`, `on_llm_end`, `on_llm_error`\n",
    "\n",
    "Missing some useful information/context in Langfuse? Join the [Discord](/discord) or share your feedback directly with us: feedback@langfuse.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lq3SJ-XHN3QT"
   },
   "source": [
    "## Notebook Setup\n",
    "\n",
    "<NotebookBanner src=\"cookbook/integration_langchain.ipynb\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AbSpd5EiZouE"
   },
   "source": [
    "### 1. Initializing the Langfuse Callback handler\n",
    "\n",
    "The Langfuse SDKs are hosted on the pypi index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YNyU6IzCZouE"
   },
   "outputs": [],
   "source": [
    "%pip install  --quiet langfuse langchain openai --upgrade\n",
    "# After OpenAI release v1.0.0 in November 2023, you will need to install the langchain-openai package\n",
    "%pip install --upgrade --quiet  langchain-openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OpE57ujJZouE"
   },
   "source": [
    "Initialize the client with api keys and optionally your environment. In the example we are using the cloud environment which is also the default.\n",
    "\n",
    "Alternatively, you can also pass them as arguments to the `CallbackHandler` constructor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# get keys for your project from https://cloud.langfuse.com\n",
    "# get keys for your project from https://cloud.langfuse.com\n",
    "os.environ[\n",
    "    \"LANGFUSE_HOST\"\n",
    "] = \"https://cloud.langfuse.com\"  # Your host, defaults to https://cloud.langfuse.com\n",
    "os.environ[\"LANGFUSE_PUBLIC_KEY\"] = \"Your public key\"\n",
    "os.environ[\"LANGFUSE_SECRET_KEY\"] = \"Your secret key\"\n",
    "os.environ[\"AZURE_OPENAI_ENDPOINT\"] = \"your Azure OpenAI endpoint\"\n",
    "os.environ[\"AZURE_OPENAI_API_KEY\"] = \"your Azure OpenAI API key\"\n",
    "os.environ[\"OPENAI_API_TYPE\"] = \"azure\"\n",
    "os.environ[\"OPENAI_API_VERSION\"] = \"2023-09-01-preview\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "divRadPqZouF"
   },
   "outputs": [],
   "source": [
    "from langfuse.callback import CallbackHandler\n",
    "\n",
    "handler = CallbackHandler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8FVbg1RWoT8W"
   },
   "outputs": [],
   "source": [
    "# handler is a callback handler that can be used to log and monitor the requests and responses.\n",
    "handler.auth_check()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FzsO2Ci4EjJS"
   },
   "source": [
    "### 2. Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L9u5QMVCbPtR"
   },
   "outputs": [],
   "source": [
    "# further imports\n",
    "# Import Azure OpenAI\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "from langchain.schema import HumanMessage\n",
    "\n",
    "\n",
    "from langfuse.callback import CallbackHandler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KAW-Gt4mN3QV"
   },
   "source": [
    "## Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First Example\n",
    "from langfuse import Langfuse\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "    deployment_name=\"gpt-35-turbo\",\n",
    "    model_name=\"gpt-3.5-turbo\",\n",
    ")\n",
    "\n",
    "langfuse = Langfuse()\n",
    "\n",
    "template = \"\"\"\n",
    "You are an AI assistant travel assistant that provides vacation recommendations to users. \n",
    "You should also be able to provide information about the weather, local customs, and travel restrictions. \n",
    "\"\"\"\n",
    "\n",
    "# Create / update version of prompt in Langfuse\n",
    "langfuse.create_prompt(\n",
    "    name=\"travel_consultant\",\n",
    "    prompt=template,\n",
    "    is_active=True,  # directly promote to production?\n",
    ")\n",
    "\n",
    "\n",
    "# Get version of prompt in Langfuse (cached for 5 minutes)\n",
    "langfuse_prompt = langfuse.get_prompt(\"travel_consultant\", cache_ttl_seconds=300).prompt\n",
    "\n",
    "\n",
    "system_message_prompt = SystemMessagePromptTemplate.from_template(langfuse_prompt)\n",
    "human_template = \"{text}\"\n",
    "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
    "chat_prompt = ChatPromptTemplate.from_messages(\n",
    "    [system_message_prompt, human_message_prompt]\n",
    ")\n",
    "chain = LLMChain(llm=llm, prompt=chat_prompt)\n",
    "result = chain.run(\n",
    "    f\"Where should I go on vaction in Decemember for warm weather and beaches?\",\n",
    "    callbacks=[handler],\n",
    ")\n",
    "\n",
    "handler.flush()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "handler.get_trace_url()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second Example\n",
    "from operator import itemgetter\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema import StrOutputParser\n",
    "import uuid\n",
    "\n",
    "handler = CallbackHandler()\n",
    "\n",
    "\n",
    "langfuse = Langfuse()\n",
    "\n",
    "# Create custom name of trace which can help context of specific user_id, name and metadata\n",
    "\n",
    "trace = langfuse.trace(name=\"chain_of_thought_example\", user_id=\"user-1234\")\n",
    "\n",
    "handler = trace.get_langchain_handler()\n",
    "\n",
    "prompt1 = ChatPromptTemplate.from_template(\n",
    "    \"What {type} is easiest to learn but hardest to master? Give a step by step approach of your thoughts, ending in your answer\"\n",
    ")\n",
    "prompt2 = ChatPromptTemplate.from_template(\n",
    "    \"How {type} can be learned in 21 days? respond in {language}\"\n",
    ")\n",
    "\n",
    "model = AzureChatOpenAI(\n",
    "    deployment_name=\"gpt-35-turbo\",\n",
    "    model_name=\"gpt-3.5-turbo\",\n",
    ")\n",
    "\n",
    "chain1 = prompt1 | model | StrOutputParser()\n",
    "\n",
    "chain2 = (\n",
    "    {\"type\": chain1, \"language\": itemgetter(\"language\")}\n",
    "    | prompt2\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "chain2.invoke(\n",
    "    {\"type\": \"business\", \"language\": \"german\"}, config={\"callbacks\": [handler]}\n",
    ")\n",
    "\n",
    "# multiple runs for one trace. This can be helpful to create scores for the different runs.\n",
    "next_span_id = str(uuid.uuid4())\n",
    "handler.setNextSpan(next_span_id)\n",
    "\n",
    "chain2.invoke(\n",
    "    {\"type\": \"business\", \"language\": \"english\"}, config={\"callbacks\": [handler]}\n",
    ")\n",
    "handler.get_trace_url()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AxQlUOmVPEwz"
   },
   "source": [
    "## Adding scores\n",
    "\n",
    "To add [scores](/docs/scores) to traces created with the Langchain integration, access the traceId via `handler.get_trace_id()`\n",
    "\n",
    "\n",
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PudCopwEPFgh"
   },
   "outputs": [],
   "source": [
    "from langfuse import Langfuse\n",
    "\n",
    "\n",
    "# Trace langchain run via the Langfuse CallbackHandler as shown above\n",
    "\n",
    "# Get id of created trace\n",
    "trace_id = handler.get_trace_id()\n",
    "\n",
    "# Add score, e.g. via the Python SDK\n",
    "langfuse = Langfuse()\n",
    "trace = langfuse.score(\n",
    "    trace_id=trace_id,\n",
    "    name=\"user-explicit-feedback\",\n",
    "    value=1,\n",
    "    comment=\"I like how personalized the response is\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<!-- NOTEBOOK_METADATA source: \"âš ï¸ Jupyter Notebook\" title: \"Monitor Baseten with Langfuse\" sidebarTitle: \"Baseten\" logo: \"/images/integrations/baseten_icon.png\" description: \"Learn how to integrate Baseten with Langfuse using the OpenAI drop-in replacement.\" category: \"Integrations\" -->\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Observability for Baseten with Langfuse\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This guide shows you how to integrate [Baseten](https://www.baseten.co/) with Langfuse. Baseten's inference API is fully compatible with OpenAI's client libraries, allowing us to use the Langfuse OpenAI drop-in replacement to trace all parts of your application.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> **What is Baseten?** [Baseten]([https://www.baseten.co/](https://docs.baseten.co/overview)) is an inference platform that enables developers to deploy and scale machine learning models in production. It provides fast, reliable model inference with support for popular open-source models through an OpenAI-compatible API.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> **What is Langfuse?** [Langfuse](https://langfuse.com) is an open source LLM engineering platform that helps teams trace API calls, monitor performance, and debug issues in their AI applications.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<!-- STEPS_START -->\n",
        "## Step 1: Install Dependencies\n",
        "\n",
        "Make sure you have installed the necessary Python packages:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install openai langfuse -q\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Set Up Environment Variables\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Get keys for your project from the project settings page: https://cloud.langfuse.com\n",
        "os.environ[\"LANGFUSE_PUBLIC_KEY\"] = \"pk-lf-...\" \n",
        "os.environ[\"LANGFUSE_SECRET_KEY\"] = \"sk-lf-...\" \n",
        "os.environ[\"LANGFUSE_BASE_URL\"] = \"https://cloud.langfuse.com\" # ðŸ‡ªðŸ‡º EU region\n",
        "# os.environ[\"LANGFUSE_BASE_URL\"] = \"https://us.cloud.langfuse.com\" # ðŸ‡ºðŸ‡¸ US region\n",
        "\n",
        "# Get your Baseten API key from https://app.baseten.co/settings/api_keys\n",
        "os.environ[\"BASETEN_API_KEY\"] = \"...\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Langfuse OpenAI drop-in Replacement\n",
        "\n",
        "In this step we use the native [OpenAI drop-in replacement](https://langfuse.com/docs/observability/sdk/instrumentation) by importing `from langfuse.openai import openai`.\n",
        "\n",
        "To start using Baseten with OpenAI's client libraries, pass in your Baseten API key to the `api_key` option, and change the `base_url` to `https://inference.baseten.co/v1`:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# instead of import openai:\n",
        "from langfuse.openai import openai\n",
        "\n",
        "client = openai.OpenAI(\n",
        "  api_key=os.environ.get(\"BASETEN_API_KEY\"),\n",
        "  base_url=\"https://inference.baseten.co/v1\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Run An Example\n",
        "\n",
        "The following cell demonstrates how to call Baseten's chat model using the traced OpenAI client. All API calls will be automatically traced by Langfuse."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "response = client.chat.completions.create(\n",
        "  model=\"zai-org/GLM-4.6\",\n",
        "  messages=[\n",
        "    {\"role\": \"system\", \"content\": \"You are a travel agent. Be descriptive and helpful.\"},\n",
        "    {\"role\": \"user\", \"content\": \"Tell me the top 3 things to do in San Francisco\"},\n",
        "  ],\n",
        "  name=\"baseten-example-trace\"\n",
        ")\n",
        "\n",
        "print(response.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: See Traces in Langfuse\n",
        "\n",
        "After running the example model call, you can see the traces in Langfuse. You will see detailed information about your Baseten API calls, including:\n",
        "\n",
        "- Request parameters (model, messages, temperature, etc.)\n",
        "- Response content\n",
        "- Token usage statistics\n",
        "- Latency metrics\n",
        "\n",
        "![Langfuse Trace Example](https://langfuse.com/images/cookbook/integration_baseten/baseten-example-trace.png)\n",
        "\n",
        "_[Public example trace link in Langfuse](https://cloud.langfuse.com/project/cloramnkj0002jz088vzn1ja4/traces/2c070a60-3cb9-43c6-9731-ba2ae3076453?timestamp=2025-12-23T14:38:10.441Z)_\n",
        "<!-- STEPS_END -->\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<!-- MARKDOWN_COMPONENT name: \"LearnMore\" path: \"@/components-mdx/integration-learn-more.mdx\" -->"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}

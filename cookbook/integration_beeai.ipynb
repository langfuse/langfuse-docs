{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<!-- NOTEBOOK_METADATA source: \"âš ï¸ Jupyter Notebook\" title: \"Observability for BeeAI Framework with Langfuse Integration\" sidebarTitle: \"BeeAI\" logo: \"/images/integrations/beeai_icon.svg\" description: \"Learn how to integrate Langfuse with the BeeAI Framework for comprehensive tracing and debugging of your AI agent applications.\" category: \"Integrations\" -->\n",
        "\n",
        "# Trace BeeAI Framework Applications in Langfuse\n",
        "\n",
        "This notebook shows how to trace and observe BeeAI Framework applications with Langfuse using OpenTelemetry instrumentation.\n",
        "\n",
        "> **What is BeeAI Framework?** [BeeAI Framework](https://framework.beeai.dev) is a comprehensive toolkit for building intelligent, autonomous agents and multi-agent systems. It provides everything you need to create agents that can reason, take actions, and collaborate to solve complex problems in both Python and TypeScript.\n",
        "\n",
        "> **What is the BeeAI Framework Python SDK?** The [BeeAI Framework Python SDK](https://github.com/i-am-bee/beeai-framework) is a production-ready library that provides core building blocks like agents, workflows, backend connections, tools, memory management, and observability features for building intelligent AI applications.\n",
        "\n",
        "> **What is Langfuse?** [Langfuse](https://langfuse.com) is an open source platform for LLM observability and monitoring. It helps you trace and monitor your AI applications by capturing metadata, prompt details, token usage, latency, and more.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<!-- STEPS_START -->\n",
        "## Step 1: Install Dependencies\n",
        "\n",
        "Before you begin, install the necessary packages in your Python environment:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install beeai-framework langfuse openinference-instrumentation-beeai opentelemetry-sdk opentelemetry-exporter-otlp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Configure Langfuse SDK\n",
        "\n",
        "Next, set up your Langfuse API keys. You can get these keys by signing up for a free [Langfuse Cloud](https://cloud.langfuse.com/) account or by [self-hosting Langfuse](https://langfuse.com/self-hosting). These environment variables are essential for the Langfuse client to authenticate and send data to your Langfuse project.\n",
        "\n",
        "You will also need to configure your LLM provider credentials. BeeAI Framework supports multiple providers including OpenAI, Ollama, watsonx.ai, and others."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Get keys for your project from the project settings page: https://cloud.langfuse.com\n",
        "os.environ[\"LANGFUSE_PUBLIC_KEY\"] = \"pk-lf-...\" \n",
        "os.environ[\"LANGFUSE_SECRET_KEY\"] = \"sk-lf-...\" \n",
        "os.environ[\"LANGFUSE_HOST\"] = \"https://cloud.langfuse.com\" # ðŸ‡ªðŸ‡º EU region\n",
        "# os.environ[\"LANGFUSE_HOST\"] = \"https://us.cloud.langfuse.com\" # ðŸ‡ºðŸ‡¸ US region\n",
        "\n",
        "# Your LLM provider API key (example with OpenAI, adjust for your provider)\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"sk-...\"  \n",
        "# For Ollama (local), no API key needed\n",
        "# For other providers, set appropriate environment variables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "With the environment variables set, we can now initialize the Langfuse client. `get_client()` initializes the Langfuse client using the credentials provided in the environment variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langfuse import get_client\n",
        "\n",
        "# Initialise Langfuse client and verify connectivity\n",
        "langfuse = get_client()\n",
        "assert langfuse.auth_check(), \"Langfuse auth failed - check your keys âœ‹\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: OpenTelemetry Instrumentation\n",
        "\n",
        "Use the [`BeeAIInstrumentor`](https://github.com/Arize-ai/openinference/tree/main/python/instrumentation/openinference-instrumentation-beeai) library to wrap [BeeAI Framework](https://framework.beeai.dev) calls and send OpenTelemetry spans to Langfuse."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "from openinference.instrumentation.beeai import BeeAIInstrumentor\n",
        "\n",
        "BeeAIInstrumentor().instrument()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Run an Example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import asyncio\n",
        "from beeai_framework.agents.react import ReActAgent\n",
        "from beeai_framework.agents.types import AgentExecutionConfig\n",
        "from beeai_framework.backend.chat import ChatModel\n",
        "from beeai_framework.backend.types import ChatModelParameters\n",
        "from beeai_framework.memory import TokenMemory\n",
        "from beeai_framework.tools.search import WikipediaTool\n",
        "from beeai_framework.tools.weather.openmeteo import OpenMeteoTool\n",
        "\n",
        "# Initialize the language model\n",
        "llm = ChatModel.from_name(\n",
        "    \"openai:gpt-4o-mini\",  # or \"ollama:granite3.3:8b\" for local Ollama\n",
        "    ChatModelParameters(temperature=0.7),\n",
        ")\n",
        "\n",
        "# Create tools for the agent\n",
        "tools = [\n",
        "    WikipediaTool(),\n",
        "    OpenMeteoTool(),\n",
        "]\n",
        "\n",
        "# Create a ReAct agent with memory\n",
        "agent = ReActAgent(\n",
        "    llm=llm, \n",
        "    tools=tools, \n",
        "    memory=TokenMemory(llm)\n",
        ")\n",
        "\n",
        "# Run the agent\n",
        "async def main():\n",
        "    response = await agent.run(\n",
        "        prompt=\"What is Langfuse and what's the current weather in San Francisco?\",\n",
        "        execution=AgentExecutionConfig(\n",
        "            max_retries_per_step=3, \n",
        "            total_max_retries=10, \n",
        "            max_iterations=5\n",
        "        ),\n",
        "    )\n",
        "    print(\"Agent Response:\", response.result.text)\n",
        "    return response\n",
        "\n",
        "# Run the example\n",
        "response = await main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Multi-Agent Workflow Example\n",
        "from beeai_framework.workflows.agent import AgentWorkflow, AgentWorkflowInput\n",
        "\n",
        "# Create a multi-agent workflow\n",
        "workflow = AgentWorkflow(name=\"Research Assistant\")\n",
        "\n",
        "# Add specialized agents to the workflow\n",
        "workflow.add_agent(\n",
        "    name=\"Researcher\",\n",
        "    role=\"A diligent researcher.\",\n",
        "    instructions=\"You look up and provide information about specific topics using available tools.\",\n",
        "    tools=[WikipediaTool()],\n",
        "    llm=llm,\n",
        ")\n",
        "\n",
        "workflow.add_agent(\n",
        "    name=\"WeatherAnalyst\",\n",
        "    role=\"A weather reporting specialist.\",\n",
        "    instructions=\"You provide detailed weather reports and analysis.\",\n",
        "    tools=[OpenMeteoTool()],\n",
        "    llm=llm,\n",
        ")\n",
        "\n",
        "workflow.add_agent(\n",
        "    name=\"Synthesizer\",\n",
        "    role=\"A data synthesis expert.\",\n",
        "    instructions=\"You combine information from different sources into coherent summaries.\",\n",
        "    llm=llm,\n",
        ")\n",
        "\n",
        "# Run the workflow\n",
        "async def run_workflow():\n",
        "    response = await workflow.run(\n",
        "        inputs=[\n",
        "            AgentWorkflowInput(\n",
        "                prompt=\"Research what Langfuse is and its main features.\",\n",
        "            ),\n",
        "            AgentWorkflowInput(\n",
        "                prompt=\"Get the current weather conditions in Paris, France.\",\n",
        "                expected_output=\"Temperature, conditions, and any notable weather information.\",\n",
        "            ),\n",
        "            AgentWorkflowInput(\n",
        "                prompt=\"Combine the Langfuse research and Paris weather into a brief summary.\",\n",
        "                expected_output=\"A paragraph combining both pieces of information.\",\n",
        "            ),\n",
        "        ]\n",
        "    ).on(\n",
        "        \"success\",\n",
        "        lambda data, event: print(\n",
        "            f\"\\n-> Step '{data.step}' completed:\\n{data.state.final_answer}\\n\"\n",
        "        ),\n",
        "    )\n",
        "    \n",
        "    print(\"==== Final Workflow Result ====\")\n",
        "    print(response.result.final_answer)\n",
        "    return response\n",
        "\n",
        "# Run the workflow example\n",
        "workflow_response = await run_workflow()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### View Traces in Langfuse\n",
        "\n",
        "After executing the application, navigate to your Langfuse Trace Table. You will find detailed traces of the application's execution, providing insights into the agent conversations, tool calls, LLM interactions, inputs, outputs, and performance metrics.\n",
        "\n",
        "![Langfuse Trace](https://langfuse.com/images/cookbook/integration_beeai/beeai-trace.png)\n",
        "\n",
        "[View trace in Langfuse](https://cloud.langfuse.com/project/cloramnkj0002jz088vzn1ja4/traces/beeai-example-trace?timestamp=2025-01-15T10%3A30%3A00.000Z&display=details)\n",
        "\n",
        "<!-- STEPS_END -->\n",
        "\n",
        "<!-- MARKDOWN_COMPONENT name: \"LearnMore\" path: \"@/components-mdx/integration-learn-more.mdx\" -->"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
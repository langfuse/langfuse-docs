{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "<!-- NOTEBOOK_METADATA source: \"⚠️ Jupyter Notebook\" title: \"Observability for BeeAI Agents with Langfuse\" sidebarTitle: \"BeeAI\" logo: \"/images/integrations/beeai_icon.png\" description: \"Learn how to integrate Langfuse with the BeeAI Framework for comprehensive tracing and debugging of your AI agent applications.\" category: \"Integrations\" -->\n",
        "\n",
        "# Trace BeeAI Agents in Langfuse\n",
        "\n",
        "This notebook shows how to trace and observe BeeAI Framework applications with Langfuse using OpenTelemetry instrumentation.\n",
        "\n",
        "> **What is BeeAI?** [BeeAI Framework](https://beeai.dev/) is a comprehensive toolkit, developed by [IBM Research](https://research.ibm.com/blog/bee-ai-app), for building intelligent, autonomous agents and multi-agent systems. It provides everything you need to create agents that can reason, take actions, and collaborate to solve complex problems in both Python and TypeScript.\n",
        "\n",
        "> **What is Langfuse?** [Langfuse](https://langfuse.com) is an open source platform for LLM observability and monitoring. It helps you trace and monitor your AI applications by capturing metadata, prompt details, token usage, latency, and more.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "<!-- STEPS_START -->\n",
        "## Step 1: Install Dependencies\n",
        "\n",
        "Before you begin, install the necessary packages in your Python environment:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install beeai-framework langfuse openinference-instrumentation-beeai \"beeai-framework[wikipedia]\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Step 2: Configure Langfuse SDK\n",
        "\n",
        "Next, set up your Langfuse API keys. You can get these keys by signing up for a free [Langfuse Cloud](https://cloud.langfuse.com/) account or by [self-hosting Langfuse](https://langfuse.com/self-hosting). These environment variables are essential for the Langfuse client to authenticate and send data to your Langfuse project.\n",
        "\n",
        "You will also need to configure your LLM provider credentials. BeeAI Framework supports multiple providers including OpenAI, Ollama, watsonx.ai, and others.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Get keys for your project from the project settings page: https://cloud.langfuse.com\n",
        "os.environ[\"LANGFUSE_PUBLIC_KEY\"] = \"pk-lf-...\" \n",
        "os.environ[\"LANGFUSE_SECRET_KEY\"] = \"sk-lf-...\" \n",
        "os.environ[\"LANGFUSE_HOST\"] = \"https://cloud.langfuse.com\" # 🇪🇺 EU region\n",
        "# os.environ[\"LANGFUSE_HOST\"] = \"https://us.cloud.langfuse.com\" # 🇺🇸 US region\n",
        "\n",
        "# Your LLM provider API key (example with OpenAI, adjust for your provider)\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"sk-proj-...\"\n",
        "# For Ollama (local), no API key needed\n",
        "# For other providers, set appropriate environment variables\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "With the environment variables set, we can now initialize the Langfuse client. `get_client()` initializes the Langfuse client using the credentials provided in the environment variables.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langfuse import get_client\n",
        "\n",
        "# Initialise Langfuse client and verify connectivity\n",
        "langfuse = get_client()\n",
        "assert langfuse.auth_check(), \"Langfuse auth failed - check your keys ✋\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Step 3: OpenTelemetry Instrumentation\n",
        "\n",
        "Use the [`BeeAIInstrumentor`](https://github.com/Arize-ai/openinference/tree/main/python/instrumentation/openinference-instrumentation-beeai) library to wrap [BeeAI Framework](https://framework.beeai.dev) calls and send OpenTelemetry spans to Langfuse.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "from openinference.instrumentation.beeai import BeeAIInstrumentor\n",
        "\n",
        "BeeAIInstrumentor().instrument()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Step 4: Run an Example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import asyncio\n",
        "from beeai_framework.agents.react import ReActAgent\n",
        "from beeai_framework.agents.types import AgentExecutionConfig\n",
        "from beeai_framework.backend.chat import ChatModel\n",
        "from beeai_framework.backend.types import ChatModelParameters\n",
        "from beeai_framework.memory import TokenMemory\n",
        "from beeai_framework.tools.search.wikipedia import WikipediaTool\n",
        "from beeai_framework.tools.weather.openmeteo import OpenMeteoTool\n",
        "\n",
        "# Initialize the language model\n",
        "llm = ChatModel.from_name(\n",
        "    \"openai:gpt-4o-mini\",  # or \"ollama:granite3.3:8b\" for local Ollama\n",
        "    ChatModelParameters(temperature=0.7),\n",
        ")\n",
        "\n",
        "# Create tools for the agent\n",
        "tools = [\n",
        "    WikipediaTool(),\n",
        "    OpenMeteoTool(),\n",
        "]\n",
        "\n",
        "# Create a ReAct agent with memory\n",
        "agent = ReActAgent(\n",
        "    llm=llm, \n",
        "    tools=tools, \n",
        "    memory=TokenMemory(llm)\n",
        ")\n",
        "\n",
        "# Run the agent\n",
        "async def main():\n",
        "    response = await agent.run(\n",
        "        prompt=\"I'm planning a trip to Barcelona, Spain. Can you research key attractions and landmarks I should visit, and also tell me what the current weather conditions are like there?\",\n",
        "        execution=AgentExecutionConfig(\n",
        "            max_retries_per_step=3, \n",
        "            total_max_retries=10, \n",
        "            max_iterations=5\n",
        "        ),\n",
        "    )\n",
        "    print(\"Agent Response:\", response.result.text)\n",
        "    return response\n",
        "\n",
        "# Run the example\n",
        "response = await main()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### View Traces in Langfuse\n",
        "\n",
        "After executing the application, navigate to your Langfuse Trace Table. You will find detailed traces of the application's execution, providing insights into the agent conversations, tool calls, LLM interactions, inputs, outputs, and performance metrics.\n",
        "\n",
        "![Langfuse Trace](https://langfuse.com/images/cookbook/integration_beeai/beeai-trace.png)\n",
        "\n",
        "[View trace in Langfuse](https://cloud.langfuse.com/project/cloramnkj0002jz088vzn1ja4/traces/096fc09a30ab90d2431778f9ee2b3936?timestamp=2025-08-01T13%3A56%3A54.163Z&display=details)\n",
        "\n",
        "<!-- STEPS_END -->\n",
        "\n",
        "<!-- MARKDOWN_COMPONENT name: \"LearnMore\" path: \"@/components-mdx/integration-learn-more.mdx\" -->\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}

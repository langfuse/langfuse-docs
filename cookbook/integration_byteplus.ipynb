{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- NOTEBOOK_METADATA source: \"âš ï¸ Jupyter Notebook\" title: \"Monitor BytePlus model inference with Langfuse\" sidebarTitle: \"BytePlus\" logo: \"/images/integrations/byteplus_icon.png\" description: \"Learn how to integrate BytePlus with Langfuse using the OpenAI drop-in replacement.\" category: \"Integrations\" -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Observability for BytePlus with Langfuse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This guide shows you how to integrate BytePlus with Langfuse. BytePlus API endpoints for chat, language and code, images, and embeddings are fully compatible with OpenAI's API. This allows us to use the Langfuse OpenAI drop-in replacement to trace all parts of your application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **What is BytePlus?** [BytePlus](https://www.byteplus.com/) is a suite of AI-powered APIs and services developed by [ByteDance](https://www.bytedance.com/en/), including speech, video, and recommendation technologies. Langfuse integrates with BytePlus to trace and evaluate LLM workflows that use BytePlus tools, enabling observability across generation and user interaction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **What is Langfuse?** [Langfuse](https://langfuse.com) is an open source LLM engineering platform that helps teams trace API calls, monitor performance, and debug issues in their AI applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- CALLOUT_START type: \"info\" emoji: \"â„¹ï¸\" -->\n",
    "_**Note:** You can also use BytePlus models in the Langfuse Playground and for LLM-as-a-Jude evaluations using the OpenAI adapter. Find out how to set up an LLM Connection in Langfuse [here](https://langfuse.com/faq/all/llm-connection)._\n",
    "<!-- CALLOUT_END -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- STEPS_START -->\n",
    "## Step 1: Install Dependencies\n",
    "\n",
    "Make sure you have installed the necessary Python packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install openai langfuse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Set Up Environment Variables\n",
    "\n",
    "Next, set up your Langfuse API keys. You can get these keys by signing up for a free [Langfuse Cloud](https://cloud.langfuse.com/) account or by [self-hosting Langfuse](https://langfuse.com/self-hosting). These environment variables are essential for the [OpenAI drop-in replacement](https://langfuse.com/integrations/model-providers/openai-py) to authenticate and send data to your Langfuse project.\n",
    "\n",
    "Find a guide on creating your BytePlus API keys for model services [here](https://docs.byteplus.com/en/docs/ModelArk/1399008). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Get keys for your project from the project settings page: https://cloud.langfuse.com\n",
    "os.environ[\"LANGFUSE_PUBLIC_KEY\"] = \"pk-lf-...\" \n",
    "os.environ[\"LANGFUSE_SECRET_KEY\"] = \"sk-lf-...\" \n",
    "os.environ[\"LANGFUSE_HOST\"] = \"https://cloud.langfuse.com\" # ðŸ‡ªðŸ‡º EU region\n",
    "# os.environ[\"LANGFUSE_HOST\"] = \"https://us.cloud.langfuse.com\" # ðŸ‡ºðŸ‡¸ US region\n",
    "\n",
    "# Get your BytePlus API key from the project settings page\n",
    "os.environ[\"ARK_API_KEY\"] = \"***\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Langfuse OpenAI drop-in Replacement\n",
    "\n",
    "In this step we use the native [OpenAI drop-in replacement](https://langfuse.com/integrations/model-providers/openai-py) by importing `from langfuse.openai import openai`.\n",
    "\n",
    "To start using BytePlus models with OpenAI's client libraries, pass in your BytepPlus API key to the `api_key` option, and change the `base_url` to `https://ark.ap-southeast.bytepluses.com/api/v3`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instead of import openai:\n",
    "from langfuse.openai import openai\n",
    "\n",
    "client = openai.OpenAI(\n",
    "  api_key=os.environ.get(\"ARK_API_KEY\"),\n",
    "  base_url=\"https://ark.ap-southeast.bytepluses.com/api/v3\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** The OpenAI drop-in replacement is fully compatible with the [Low-Level Langfuse Python SDKs](https://langfuse.com/docs/sdk/python/low-level-sdk) and [`@observe()` decorator](https://langfuse.com/docs/sdk/python/decorators) to trace all parts of your application.\n",
    "\n",
    "## Step 4: Run An Example\n",
    "\n",
    "The following cell demonstrates how to call the [Kimi K2 model](https://moonshotai.github.io/Kimi-K2/) via BytePlus using the traced OpenAI client. All API calls will be automatically traced by Langfuse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non-streaming:\n",
    "print(\"----- standard request -----\")\n",
    "completion = client.chat.completions.create(\n",
    "    # Specify the Ark Inference Point ID that you created, which has been changed for you here to your Endpoint ID\n",
    "    model=\"kimi-k2-250711\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You're an AI assistant\"},\n",
    "        {\"role\": \"user\", \"content\": \"What is Langfuse?\"},\n",
    "    ],\n",
    "    name = \"BytePlus-Generation\" # Optional: Set the name of the generation in Langfuse\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Streaming:\n",
    "print(\"----- streaming request -----\")\n",
    "stream = client.chat.completions.create(\n",
    "    # Specify the Ark Inference Point ID that you created, which has been changed for you here to your Endpoint ID\n",
    "    model=\"kimi-k2-250711\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You're an AI assistant\"},\n",
    "        {\"role\": \"user\", \"content\": \"What is Langfuse?\"},\n",
    "    ],\n",
    "    name = \"BytePlus-Generation\", # Optional: Set the name of the generation in Langfuse\n",
    "    # Whether the response content is streamed back\n",
    "    stream=True,\n",
    ")\n",
    "for chunk in stream:\n",
    "    if not chunk.choices:\n",
    "        continue\n",
    "    print(chunk.choices[0].delta.content, end=\"\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: See Traces in Langfuse\n",
    "\n",
    "After running the example model call, you can see the traces in Langfuse. You will see detailed information about your BytePlus API calls, including:\n",
    "\n",
    "- Request parameters (model, messages, temperature, etc.)\n",
    "- Response content\n",
    "- Token usage statistics\n",
    "- Latency metrics\n",
    "\n",
    "![Langfuse Trace Example](https://langfuse.com/images/cookbook/integration_byteplus/byteplus-trace.png)\n",
    "\n",
    "_[Public example trace link in Langfuse](https://cloud.langfuse.com/project/cloramnkj0002jz088vzn1ja4/traces/b526e1fb8e94dd1e8a17bda69e596199?display=details%3Ftimestamp%3D2025-08-01T09%3A32%3A36.190Z&observation=31a9697edf561af3)_\n",
    "<!-- STEPS_END -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- MARKDOWN_COMPONENT name: \"LearnMore\" path: \"@/components-mdx/integration-learn-more.mdx\" -->"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<!-- NOTEBOOK_METADATA source: \"âš ï¸ Jupyter Notebook\" title: \"Monitor Cerebras with Langfuse\" sidebarTitle: \"Cerebras\" logo: \"/images/integrations/cerebras_icon.png\" description: \"Learn how to integrate Cerebras with Langfuse using the OpenAI drop-in replacement.\" category: \"Integrations\" -->\n",
        "\n",
        "# Observability for Cerebras with Langfuse\n",
        "\n",
        "This guide shows you how to integrate [Cerebras](https://cerebras.ai/) with Langfuse. Cerebras's API is fully compatible with OpenAI's client libraries, allowing us to use the Langfuse OpenAI drop-in replacement to trace all parts of your application.\n",
        "\n",
        "> **What is Cerebras?** [Cerebras](https://inference-docs.cerebras.ai/) is a high-throughput, low-latency inference platform built on Cerebrasâ€™ wafer-scale processors (WSE) and CS systems, optimized specifically for token generation at scale.\n",
        "\n",
        "> **What is Langfuse?** [Langfuse](https://langfuse.com) is an open source LLM engineering platform that helps teams trace API calls, monitor performance, and debug issues in their AI applications."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<!-- STEPS_START -->\n",
        "## Step 1: Install Dependencies\n",
        "\n",
        "Make sure you have installed the necessary Python packages:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install openai langfuse -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Set Up Environment Variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Get keys for your project from the project settings page: https://cloud.langfuse.com\n",
        "os.environ[\"LANGFUSE_PUBLIC_KEY\"] = \"pk-lf-...\" \n",
        "os.environ[\"LANGFUSE_SECRET_KEY\"] = \"sk-lf-...\" \n",
        "os.environ[\"LANGFUSE_BASE_URL\"] = \"https://cloud.langfuse.com\" # ðŸ‡ªðŸ‡º EU region\n",
        "# os.environ[\"LANGFUSE_BASE_URL\"] = \"https://us.cloud.langfuse.com\" # ðŸ‡ºðŸ‡¸ US region\n",
        "\n",
        "# Get your Cerebras API key from https://cloud.cerebras.ai/\n",
        "os.environ[\"CEREBRAS_API_KEY\"] = \"csk-...\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Langfuse OpenAI drop-in Replacement\n",
        "\n",
        "In this step we use the native [OpenAI drop-in replacement](https://langfuse.com/docs/observability/sdk/instrumentation) by importing `from langfuse.openai import openai`.\n",
        "\n",
        "To start using Cerebras with OpenAI's client libraries, pass in your Cerebras API key to the `api_key` option, and change the `base_url` to `https://api.cerebras.ai/v1`:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# instead of import openai:\n",
        "from langfuse.openai import openai\n",
        "\n",
        "client = openai.OpenAI(\n",
        "  api_key=os.environ.get(\"CEREBRAS_API_KEY\"),\n",
        "  base_url=\"https://api.cerebras.ai/v1\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Run An Example\n",
        "\n",
        "The following cell demonstrates how to call Cerebras's chat model using the traced OpenAI client. All API calls will be automatically traced by Langfuse.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "response = client.chat.completions.create(\n",
        "  model=\"zai-glm-4.6\",\n",
        "  messages=[\n",
        "    {\"role\": \"system\", \"content\": \"You are a travel agent. Be descriptive and helpful.\"},\n",
        "    {\"role\": \"user\", \"content\": \"Tell me the top 3 things to do in San Francisco\"},\n",
        "  ],\n",
        "  name=\"cerebras-travel-agent\"\n",
        ")\n",
        "\n",
        "print(response.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: See Traces in Langfuse\n",
        "\n",
        "After running the example model call, you can see the traces in Langfuse. You will see detailed information about your Cerebras API calls, including:\n",
        "\n",
        "- Request parameters (model, messages, temperature, etc.)\n",
        "- Response content\n",
        "- Token usage statistics\n",
        "- Latency metrics\n",
        "\n",
        "![Langfuse Trace Example](https://langfuse.com/images/cookbook/integration_cerebras/cerebras-example-trace.png)\n",
        "\n",
        "_[Public example trace link in Langfuse](https://cloud.langfuse.com/project/cloramnkj0002jz088vzn1ja4/traces/9fcfb69f4662fb0687c17aa3a5ab2926?timestamp=2025-12-23T14:11:48.905Z)_\n",
        "<!-- STEPS_END -->\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<!-- MARKDOWN_COMPONENT name: \"LearnMore\" path: \"@/components-mdx/integration-learn-more.mdx\" -->"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}

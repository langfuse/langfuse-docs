{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<!-- NOTEBOOK_METADATA source: \"Jupyter Notebook\" title: \"Observability for CrewAI with Langfuse Integration\" sidebarTitle: \"CrewAI\" logo: \"/images/integrations/crewai_icon.svg\" description: \"Discover how to integrate Langfuse with CrewAI for enhanced LLM application monitoring, debugging, and tracing. Improve your AI development workflow today.\" category: \"Integrations\" -->\n",
        "\n",
        "# Integrate Langfuse with CrewAI\n",
        "\n",
        "This notebook provides a step-by-step guide on integrating **Langfuse** with **CrewAI** to achieve observability and debugging for your LLM applications.\n",
        "\n",
        "> **What is CrewAI?** [CrewAI](https://github.com/crewAIInc/crewAI) ([GitHub](https://github.com/crewAIInc/crewAI)) is a framework for orchestrating autonomous AI agents. CrewAI enables you to create AI teams where each agent has specific roles, tools, and goals, working together to accomplish complex tasks. Each member (agent) brings unique skills and expertise, collaborating seamlessly to achieve your objectives.\n",
        "\n",
        "> **What is Langfuse?** [Langfuse](https://langfuse.com) is an open-source LLM engineering platform. It offers tracing and monitoring capabilities for AI applications. Langfuse helps developers debug, analyze, and optimize their AI systems by providing detailed insights and integrating with a wide array of tools and frameworks through native integrations, OpenTelemetry, and dedicated SDKs.\n",
        "\n",
        "## Getting Started\n",
        "\n",
        "Let's walk through a practical example of using CrewAI and integrating it with Langfuse for comprehensive tracing.\n",
        "\n",
        "<!-- STEPS_START -->\n",
        "### Step 1: Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%pip install langfuse crewai openinference-instrumentation-crewai"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 2: Configure Langfuse SDK\n",
        "\n",
        "Next, set up your Langfuse API keys. You can get these keys by signing up for a free [Langfuse Cloud](https://cloud.langfuse.com/) account or by [self-hosting Langfuse](https://langfuse.com/self-hosting). These environment variables are essential for the Langfuse client to authenticate and send data to your Langfuse project."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os\n",
        "\n",
        "# Get keys for your project from the project settings page: https://cloud.langfuse.com\n",
        "os.environ[\"LANGFUSE_PUBLIC_KEY\"] = \"pk-lf-...\" \n",
        "os.environ[\"LANGFUSE_SECRET_KEY\"] = \"sk-lf-...\"\n",
        "os.environ[\"LANGFUSE_BASE_URL\"] = \"https://cloud.langfuse.com\" # ðŸ‡ªðŸ‡º EU region\n",
        "# os.environ[\"LANGFUSE_BASE_URL\"] = \"https://us.cloud.langfuse.com\" # ðŸ‡ºðŸ‡¸ US region\n",
        "\n",
        "# Your OpenAI key\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"sk-proj-...\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "With the environment variables set, we can now initialize the Langfuse client. `get_client()` initializes the Langfuse client using the credentials provided in the environment variables.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from langfuse import get_client\n",
        " \n",
        "langfuse = get_client()\n",
        " \n",
        "# Verify connection\n",
        "if langfuse.auth_check():\n",
        "    print(\"Langfuse client is authenticated and ready!\")\n",
        "else:\n",
        "    print(\"Authentication failed. Please check your credentials and host.\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 3: Initialize CrewAI Instrumentation\n",
        "\n",
        "Now, we initialize the [OpenInference instrumentation SDK](https://github.com/Arize-ai/openinference/tree/main/python/instrumentation/openinference-instrumentation-crewai) to automatically capture CrewAI operations and export OpenTelemetry (OTel) spans to Langfuse."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from openinference.instrumentation.crewai import CrewAIInstrumentor\n",
        "\n",
        "CrewAIInstrumentor().instrument(skip_dep_check=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 4: Basic CrewAI Application\n",
        "\n",
        "Let's create a straightforward CrewAI application. In this example, we'll create a simple crew with agents that can collaborate to complete tasks. This will serve as the foundation for demonstrating Langfuse tracing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from crewai import Agent, Task, Crew\n",
        "\n",
        "# Define your agents with roles and goals\n",
        "coder = Agent(\n",
        "    role='Software developer',\n",
        "    goal='Write clear, concise code on demand',\n",
        "    backstory='An expert coder with a keen eye for software trends.',\n",
        ")\n",
        "\n",
        "# Create tasks for your agents\n",
        "task1 = Task(\n",
        "    description=\"Define the HTML for making a simple website with heading- Hello World! Langfuse monitors your CrewAI agent!\",\n",
        "    expected_output=\"A clear and concise HTML code\",\n",
        "    agent=coder\n",
        ")\n",
        "\n",
        "# Instantiate your crew\n",
        "crew = Crew(\n",
        "    agents=[coder],\n",
        "    tasks=[task1],\n",
        ")\n",
        "\n",
        "with langfuse.start_as_current_observation(as_type=\"span\", name=\"crewai-index-trace\"):\n",
        "    result = crew.kickoff()\n",
        "    print(result)\n",
        "\n",
        "langfuse.flush()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 5: View Traces in Langfuse\n",
        "\n",
        "After executing the application, navigate to your Langfuse Trace Table. You will find detailed traces of the application's execution, providing insights into the LLM calls, agent operations, inputs, outputs, and performance metrics. The trace will show the complete flow from task processing through agent collaboration to response generation.\n",
        "\n",
        "![CrewAI example trace in Langfuse](https://langfuse.com/images/cookbook/integration_crewai/crewai-example-trace.png)\n",
        "\n",
        "[Example Trace in Langfuse](https://cloud.langfuse.com/project/cloramnkj0002jz088vzn1ja4/traces/a287bb31e317433610d8827617471140?timestamp=2025-07-11T07:45:13.601Z&display=details)\n",
        "<!-- STEPS_END -->\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Add Additional Attributes\n",
        "\n",
        "Langfuse allows you to pass additional attributes to your spans. These can include `user_id`, `tags`, `session_id`, and custom `metadata`. Enriching traces with these details is important for analysis, debugging, and monitoring of your application's behavior across different users or sessions.\n",
        "\n",
        "The following code demonstrates how to start a custom span with `langfuse.start_as_current_observation` and then update the trace associated with this span using `propagate_attributes()`. \n",
        "\n",
        "**â†’ Learn more about [Updating Trace and Span Attributes](https://langfuse.com/docs/sdk/python/sdk-v3#updating-observations).**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from langfuse import get_client, propagate_attributes\n",
        "\n",
        "langfuse = get_client()\n",
        "\n",
        "with langfuse.start_as_current_observation(\n",
        "    as_type=\"span\",\n",
        "    name=\"crewai-index-trace\",\n",
        "    ) as span:\n",
        "\n",
        "    with propagate_attributes(\n",
        "        user_id=\"user_123\",\n",
        "        session_id=\"session_abc\",\n",
        "        tags=[\"dev\", \"crewai\"],\n",
        "        metadata={\"email\": \"user@langfuse.com\"},\n",
        "        version=\"1.0.0\"\n",
        "    ):\n",
        "    \n",
        "        # Run your application here\n",
        "        task_description = \"Define the HTML for making a simple website with heading- Hello World! Langfuse monitors your CrewAI agent!\"\n",
        "        result = crew.kickoff()\n",
        "        print(result)\n",
        "\n",
        "        # Pass additional attributes to the span\n",
        "        span.update_trace(\n",
        "            input=task_description,\n",
        "            output=str(result),\n",
        "            )\n",
        "\n",
        "# Flush events in short-lived applications\n",
        "langfuse.flush()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Score Traces and Spans\n",
        "\n",
        "Langfuse lets you ingest custom scores for individual spans or entire traces. This scoring workflow enables you to implement custom quality checks at runtime or facilitate human-in-the-loop evaluation processes.\n",
        "\n",
        "In the example below, we demonstrate how to score a specific span for `relevance` (a numeric score) and the overall trace for `feedback` (a categorical score). This helps in systematically assessing and improving your application.\n",
        "\n",
        "**â†’ Learn more about [Custom Scores in Langfuse](https://langfuse.com/docs/scores/custom).**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "with langfuse.start_as_current_observation(\n",
        "    as_type=\"span\",\n",
        "    name=\"crewai-index-trace\",\n",
        "    ) as span:\n",
        "    \n",
        "    # Run your application here\n",
        "    result = crew.kickoff()\n",
        "    print(result)\n",
        "    \n",
        "    # Score this specific span\n",
        "    span.score(name=\"relevance\", value=0.9, data_type=\"NUMERIC\")\n",
        "\n",
        "    # Score the overall trace\n",
        "    span.score_trace(name=\"feedback\", value=\"positive\", data_type=\"CATEGORICAL\")\n",
        "\n",
        "# Flush events in short-lived applications\n",
        "langfuse.flush()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Manage Prompts with Langfuse\n",
        "\n",
        "Langfuse Prompt Management allows you to collaboratively create, version, and deploy prompts. You can manage prompts either through the Langfuse SDK or directly within the Langfuse UI. These managed prompts can then be fetched into your application at runtime.\n",
        "\n",
        "The code below illustrates fetching a prompt named `crewai-task` from Langfuse, compiling it with an input variable (`task_type`), and then using this compiled prompt in the CrewAI application. \n",
        "\n",
        "**â†’ Get started with [Langfuse Prompt Management](https://langfuse.com/docs/prompts/get-started).**\n",
        "\n",
        "<!-- CALLOUT_START type: \"info\" emoji: \"ðŸ”—\" -->\n",
        "_**Note:** Linking the Langfuse Prompt and the Generation is currently not possible. This is on our roadmap and we are tracking this [here](https://github.com/orgs/langfuse/discussions/7180)._\n",
        "<!-- CALLOUT_END -->"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Fetch the prompt from langfuse\n",
        "langfuse_prompt = langfuse.get_prompt(name = \"crewai-task\")\n",
        "\n",
        "# Compile the prompt with the input\n",
        "compiled_prompt = langfuse_prompt.compile(task_type = \"HTML website\")\n",
        "\n",
        "# Create a task with the managed prompt\n",
        "task_with_prompt = Task(\n",
        "    description=compiled_prompt,\n",
        "    expected_output=\"A clear and concise HTML code\",\n",
        "    agent=coder\n",
        ")\n",
        "\n",
        "# Update crew with new task\n",
        "crew_with_prompt = Crew(\n",
        "    agents=[coder],\n",
        "    tasks=[task_with_prompt],\n",
        ")\n",
        "\n",
        "# Run your application\n",
        "with langfuse.start_as_current_observation(\n",
        "    as_type=\"span\",\n",
        "    name=\"crewai-index-trace\",\n",
        "    ) as span:\n",
        "    \n",
        "    # Run your application here\n",
        "    result = crew_with_prompt.kickoff()\n",
        "    print(result)\n",
        "\n",
        "# Flush events in short-lived applications\n",
        "langfuse.flush()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Dataset Experiments\n",
        "\n",
        "Offline evaluation using datasets is a critical part of the LLM development lifecycle. Langfuse supports this through Dataset Experiments. The typical workflow involves:\n",
        "\n",
        "1.  **Benchmark Dataset**: Defining a dataset with input prompts and their corresponding expected outputs.\n",
        "2.  **Application Run**: Running your LLM application against each item in the dataset.\n",
        "3.  **Evaluation**: Comparing the generated outputs against the expected results or using other scoring mechanisms (e.g., model-based evaluation) to assess performance.\n",
        "\n",
        "The following example demonstrates how to use a pre-existing dataset containing development tasks to run an experiment with your CrewAI application.\n",
        "\n",
        "**â†’ Learn more about [Langfuse Dataset Experiments](https://langfuse.com/docs/datasets/overview).**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from langfuse import get_client\n",
        " \n",
        "langfuse = get_client()\n",
        " \n",
        "# Fetch an existing dataset\n",
        "dataset = langfuse.get_dataset(name=\"capital_cities_11\")\n",
        "for item in dataset.items:\n",
        "    print(f\"Input: {item.input['country']}, Expected Output: {item.expected_output}\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "Next, we use the experiment runner SDK to run our CrewAI application against each dataset item. The experiment runner handles concurrent execution, automatic tracing, and evaluation. This allows for structured evaluation and comparison of different application versions or prompt configurations.\n",
        "\n",
        "We define a task function and optionally an evaluator to score the dataset runs.\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from crewai import Agent, Task, Crew\n",
        "from langfuse import Evaluation\n",
        " \n",
        "dataset_name = \"capital_cities_11\"\n",
        "\n",
        "def run_task(*, item, **kwargs):\n",
        "    \"\"\"Task function that runs the CrewAI agent for each dataset item\"\"\"\n",
        "    country = item.input[\"country\"]\n",
        "    print(f\"Running evaluation for item: {item.id} (Input: {item.input})\")\n",
        "    \n",
        "    with langfuse.start_as_current_observation(as_type=\"span\", name=\"crewai-trace\") as span:\n",
        "        # Define your agents with roles and goals\n",
        "        geography_expert = Agent(\n",
        "            role='Geography Expert',\n",
        "            goal='Answer questions about geography',\n",
        "            backstory='A geography expert with a keen eye for details.',\n",
        "        )\n",
        "\n",
        "        # Create tasks for your agents\n",
        "        task1 = Task(\n",
        "            description=f\"What is the capital of {country}?\",\n",
        "            expected_output=\"The name of the capital in one word.\",\n",
        "            agent=geography_expert\n",
        "        )\n",
        "\n",
        "        # Instantiate your crew\n",
        "        crew = Crew(\n",
        "            agents=[geography_expert],\n",
        "            tasks=[task1],\n",
        "        )\n",
        "\n",
        "        # Run the crew\n",
        "        result = crew.kickoff()\n",
        "        print(result)\n",
        "\n",
        "        # Update the trace with the input and output\n",
        "        span.update_trace(\n",
        "            input=task1.description,\n",
        "            output=result.raw,\n",
        "        )\n",
        " \n",
        "    return str(result)\n",
        "\n",
        "def exact_match_evaluator(*, output, expected_output, **kwargs):\n",
        "    \"\"\"Evaluator that checks if output exactly matches expected output\"\"\"\n",
        "    is_match = output == expected_output if expected_output else False\n",
        "    return Evaluation(\n",
        "        name=\"exact_match\", \n",
        "        value=1.0 if is_match else 0.0,\n",
        "        comment=\"Exact match\" if is_match else f\"Expected '{expected_output}', got '{output}'\"\n",
        "    )\n",
        "\n",
        "# Fetch dataset and run experiment\n",
        "dataset = langfuse.get_dataset(name=dataset_name)\n",
        "\n",
        "result = dataset.run_experiment(\n",
        "    name=\"dev_tasks_run-crewai_03\",  # Identifies this specific evaluation run\n",
        "    description=\"Evaluation run for CrewAI model on June 4th\",\n",
        "    task=run_task,\n",
        "    evaluators=[exact_match_evaluator],\n",
        "    metadata={\"model_provider\": \"OpenAI\", \"temperature_setting\": 0.7}\n",
        ")\n",
        "\n",
        "print(f\"\\nFinished processing dataset '{dataset_name}'.\")\n",
        "print(result.format())\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Explore More Langfuse Features\n",
        "\n",
        "Langfuse offers more features to enhance your LLM development and observability workflow:\n",
        "\n",
        "- [LLM-as-a-Judge Evaluators](https://langfuse.com/docs/scores/model-based-evals)\n",
        "- [Custom Dashboards](https://langfuse.com/docs/analytics/custom-dashboards)\n",
        "- [LLM Playground](https://langfuse.com/docs/playground)\n",
        "- [Prompt Management](https://langfuse.com/docs/prompts/get-started)\n",
        "- [Prompt Experiments](https://langfuse.com/docs/datasets/prompt-experiments)\n",
        "- [More Integrations](https://langfuse.com/integrations)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
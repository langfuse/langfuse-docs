{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- NOTEBOOK_METADATA source: \"âš ï¸ Jupyter Notebook\" title: \"Example cookbook on how to monitor DeepSeek models with Langfuse using the OpenAI SDK\" sidebarTitle: \"DeepSeek\" logo: \"/images/integrations/deepseek_icon.svg\" description: \"Discover how to integrate Langfuse with DeepSeek for enhanced LLM application monitoring, debugging, and tracing. Improve your AI development workflow today.\" category: \"Integrations\" -->\n",
    "\n",
    "# Cookbook: Monitor DeepSeek Models with Langfuse Using the OpenAI SDK\n",
    "\n",
    "The DeepSeek API uses an API format compatible with OpenAI. By modifying the configuration, you can use the OpenAI SDK or software compatible with the OpenAI API to access the DeepSeek API.\n",
    "\n",
    "This cookbook demonstrates how to monitor [DeepSeek](https://github.com/deepseek-ai/DeepSeek-V3) models using the OpenAI SDK integration with [Langfuse](https://langfuse.com). By leveraging Langfuse's observability tools and the OpenAI SDK, you can effectively debug, monitor, and evaluate your applications that utilize DeepSeek models.\n",
    "\n",
    "This guide will walk you through setting up the integration, making requests to DeepSeek models, and observing the interactions with Langfuse.\n",
    "\n",
    "<!-- CALLOUT_START type: \"info\" emoji: \"â„¹ï¸\" -->\n",
    "**Note:** *Langfuse is also natively integrated with [LangChain](https://langfuse.com/integrations/frameworks/langchain), [LlamaIndex](https://langfuse.com/integrations/frameworks/llamaindex), [LiteLLM](https://langfuse.com/integrations/gateways/litellm), and [other frameworks](https://langfuse.com/integrations). These frameworks can be used as well to trace DeepSeek requests.*\n",
    "<!-- CALLOUT_END -->\n",
    "\n",
    "## Setup\n",
    "\n",
    "### Install Required Packages\n",
    "\n",
    "To get started, install the necessary packages. Ensure you have the latest versions of `langfuse` and `openai`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install langfuse openai --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Environment Variables\n",
    "\n",
    "Set up your environment variables with the necessary keys. Obtain your Langfuse project keys from [Langfuse Cloud](https://cloud.langfuse.com). You will also need an access token from [DeepSeek](https://platform.deepseek.com/api_keys) to access their models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Get keys for your project from the project settings page: https://cloud.langfuse.com\n",
    "os.environ[\"LANGFUSE_PUBLIC_KEY\"] = \"pk-lf-...\" \n",
    "os.environ[\"LANGFUSE_SECRET_KEY\"] = \"sk-lf-...\" \n",
    "os.environ[\"LANGFUSE_HOST\"] = \"https://cloud.langfuse.com\" # ðŸ‡ªðŸ‡º EU region\n",
    "# os.environ[\"LANGFUSE_HOST\"] = \"https://us.cloud.langfuse.com\" # ðŸ‡ºðŸ‡¸ US region\n",
    "\n",
    "# Your DeepSeek API key (get it from https://platform.deepseek.com/api_keys)\n",
    "os.environ[\"DEEPSEEK_API_KEY\"] = \"sk-...\"  # Replace with your DeepSeek API key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Necessary Modules\n",
    "\n",
    "Instead of importing `openai` directly, import it from `langfuse.openai`. Also, import any other necessary modules.\n",
    "\n",
    "Check out our [OpenAI integration docs](https://langfuse.com/integrations/model-providers/openai-py) to learn how to use this integration with other Langfuse [features](https://langfuse.com/docs/tracing#advanced-usage)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instead of: import openai\n",
    "from langfuse.openai import OpenAI\n",
    "from langfuse import observe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the OpenAI Client for DeepSeek Models\n",
    "\n",
    "Initialize the OpenAI client, pointing it to the DeepSeek model endpoint. Replace the model URL and APP key with your own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the OpenAI client, pointing it to the DeepSeek Inference API\n",
    "client = OpenAI(\n",
    "    base_url=\"https://api.deepseek.com\",  # Replace with the DeepSeek model endpoint URL\n",
    "    api_key=os.getenv('DEEPSEEK_API_KEY'),  # Replace with your DeepSeek API key\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples\n",
    "\n",
    "### Chat Completion Request\n",
    "\n",
    "Use the `client` to make a chat completion request to the DeepSeek model. The `model` parameter can be any identifier since the actual model is specified in the `base_url`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI is cool because it automates tasks, enhances creativity, and solves complex problems quicklyâ€”making life smarter and easier.\n"
     ]
    }
   ],
   "source": [
    "completion = client.chat.completions.create(\n",
    "    model=\"deepseek-chat\", \n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Why is AI cool? Answer in 20 words or less.\"}\n",
    "    ]\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Example trace in Langfuse](https://langfuse.com/images/cookbook/integration_deepseek/deepseek-simple-trace.png)\n",
    "\n",
    "*[View the example trace in Langfuse](https://cloud.langfuse.com/project/cloramnkj0002jz088vzn1ja4/traces/83702a6c-ae0e-4317-87fa-dc82568a2d89?timestamp=2025-01-09T17%3A06%3A40.848Z)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observe the Request with Langfuse\n",
    "\n",
    "By using the `OpenAI` client from `langfuse.openai`, your requests are automatically traced in Langfuse. You can also use the `@observe()` decorator to group multiple generations into a single trace.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**The Lost Token**  \n",
      "\n",
      "Timmy the Token was excitedâ€”today, heâ€™d help the language model craft a story! But as he raced through the data pipeline, he took a wrong turn, tumbling into a forgotten cache.  \n",
      "\n",
      "\"Hello?\" Timmy echoed. Only silence replied.  \n",
      "\n",
      "Days passed. The model stuttered without him. Then, a cleanup script swept through. \"Gotcha!\" it chirped, rescuing Timmy.  \n",
      "\n",
      "Back in the prompt, Timmy gleamed. The model sparked to life: *\"Once, a token got lostâ€¦\"*  \n",
      "\n",
      "And so, Timmyâ€™s adventure became the very story he was meant to tell.  \n",
      "\n",
      "(100 words exactly)\n"
     ]
    }
   ],
   "source": [
    "@observe()  # Decorator to automatically create a trace and nest generations\n",
    "def generate_story():\n",
    "    completion = client.chat.completions.create(\n",
    "        name=\"story-generator\",\n",
    "        model=\"deepseek-chat\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a creative storyteller.\"},\n",
    "            {\"role\": \"user\", \"content\": \"Tell me a short story about a token that got lost on its way to the language model. Answer in 100 words or less.\"}\n",
    "        ],\n",
    "        metadata={\"genre\": \"adventure\"},\n",
    "    )\n",
    "    return completion.choices[0].message.content\n",
    "\n",
    "story = generate_story()\n",
    "print(story)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Example trace in Langfuse](https://langfuse.com/images/cookbook/integration_deepseek/deepseek-story-trace.png)\n",
    "\n",
    "*[View the example trace in Langfuse](https://cloud.langfuse.com/project/cloramnkj0002jz088vzn1ja4/traces/9a0dca39-9fac-4fce-ace9-52b85edfb0d8?timestamp=2025-01-09T17%3A08%3A25.698Z)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- MARKDOWN_COMPONENT name: \"LearnMore\" path: \"@/components-mdx/integration-learn-more.mdx\" -->"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

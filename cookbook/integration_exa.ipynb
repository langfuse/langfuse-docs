{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- NOTEBOOK_METADATA source: \"⚠️ Jupyter Notebook\" title: \"Monitor Exa AI Search with Langfuse\" sidebarTitle: \"Exa\" logo: \"/images/integrations/exa_icon.png\" description: \"Learn how to trace Exa AI search operations using Langfuse to capture detailed observability data for your search queries and API calls.\" category: \"Integrations\" -->\n",
    "\n",
    "# Exa Integration\n",
    "\n",
    "In this guide, we'll show you how to integrate [Langfuse](https://langfuse.com) with [Exa](https://exa.ai/) to trace your AI search operations. By leveraging Langfuse's tracing capabilities, you can automatically capture details such as inputs, outputs, and execution times of your Exa search functions.\n",
    "\n",
    "> **What is Exa?** [Exa](https://exa.ai/) is an AI-powered search API built for LLMs and AI applications. Unlike traditional search engines, Exa is designed to understand semantic meaning and retrieve high-quality, relevant results that are perfect for AI use cases like RAG (Retrieval-Augmented Generation), research, and content discovery.\n",
    "\n",
    "> **What is Langfuse?** [Langfuse](https://langfuse.com) is an open source LLM engineering platform that helps teams trace API calls, monitor performance, and debug issues in their AI applications.\n",
    "\n",
    "<!-- STEPS_START -->\n",
    "## Install Dependencies\n",
    "\n",
    "First, install the necessary Python packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install langfuse exa-py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Up Environment Variables\n",
    "\n",
    "Get your Langfuse API keys by signing up for [Langfuse Cloud](https://cloud.langfuse.com) or [self-hosting Langfuse](https://langfuse.com/self-hosting). You'll also need your Exa and OpenAI API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Get keys for your project from the project settings page: https://cloud.langfuse.com\n",
    "os.environ[\"LANGFUSE_PUBLIC_KEY\"] = \"pk-lf-...\" \n",
    "os.environ[\"LANGFUSE_SECRET_KEY\"] = \"sk-lf-...\" \n",
    "os.environ[\"LANGFUSE_HOST\"] = \"https://cloud.langfuse.com\" # 🇪🇺 EU region\n",
    "# os.environ[\"LANGFUSE_HOST\"] = \"https://us.cloud.langfuse.com\" # 🇺🇸 US region\n",
    "\n",
    "# Your Exa API key\n",
    "os.environ[\"EXA_API_KEY\"] = \"...\"\n",
    "\n",
    "# Your openai key\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-...\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the environment variables set, we can now initialize the Exa and the Langfuse client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exa_py import Exa\n",
    "from langfuse import get_client\n",
    "\n",
    "exa = Exa(api_key= os.environ[\"EXA_API_KEY\"])\n",
    "langfuse = get_client()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Trace Exa `search_and_contents` \n",
    "\n",
    "To monitor your Exa search operations, we use the [Langfuse `@observe()` decorator](https://langfuse.com/docs/sdk/python/decorators). In this example, the `@observe()` decorator captures the inputs, outputs, and execution time of the `search_with_exa()` function. For more control over the data you are sending to Langfuse, you can use the [Context Manager or create manual observations](https://langfuse.com/docs/observability/sdk/python/instrumentation#custom-instrumentation) using the Python SDK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langfuse import observe\n",
    "\n",
    "@observe(as_type=\"retriever\")\n",
    "def search_with_exa(query: str, num_results: int = 5):\n",
    "    \"\"\"Search the web using Exa AI and return results.\"\"\"\n",
    "    results = exa.search_and_contents(\n",
    "        query,\n",
    "        num_results=num_results,\n",
    "        text=True\n",
    "    )\n",
    "    return results\n",
    "\n",
    "# Example: Search for information about Langfuse\n",
    "search_results = search_with_exa(\"What is Langfuse and how does it help with LLM observability?\")\n",
    "\n",
    "# Display the results\n",
    "for result in search_results.results:\n",
    "    print(f\"Title: {result.title}\")\n",
    "    print(f\"URL: {result.url}\")\n",
    "    print(f\"Text: {result.text[:200]}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: Exa Search together with OpenAI\n",
    "\n",
    "You can also trace more complex workflows that involve summarizing the search results with OpenAI. Here we use the [Langfuse `@observe()` decorator](https://langfuse.com/docs/sdk/python/decorators) to group both the Exa search and the OpenAI generation into one trace. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langfuse.openai import OpenAI\n",
    "\n",
    "@observe()\n",
    "def search_and_summarize(query: str):\n",
    "\n",
    "    # 1. Exa search\n",
    "    @observe(as_type=\"retriever\")\n",
    "    def search_with_exa(query: str, num_results: int = 5):\n",
    "        \"\"\"Search the web using Exa AI and return results.\"\"\"\n",
    "        results = exa.search_and_contents(\n",
    "            query,\n",
    "            num_results=num_results,\n",
    "            text=True\n",
    "        )\n",
    "        return results\n",
    "\n",
    "    results = search_with_exa(query)\n",
    "\n",
    "    # 2. Build a short context\n",
    "    context = \"\\n\".join([f\"{r.title} ({r.url}): {r.text}\" for r in results.results])\n",
    "\n",
    "    # 3. Summarize with OpenAI\n",
    "    client = OpenAI()\n",
    "    resp = client.chat.completions.create(\n",
    "        model=\"gpt-5-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"Summarize the following search results clearly and concisely.\"},\n",
    "            {\"role\": \"user\", \"content\": context}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    print(\"Summary:\\n\", resp.choices[0].message.content)\n",
    "\n",
    "search_and_summarize(\"What is Langfuse and how does it help with LLM observability?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## See Traces in Langfuse\n",
    "\n",
    "After executing the traced functions, log in to your [Langfuse Dashboard](https://cloud.langfuse.com) to view detailed trace logs. You'll be able to see:\n",
    "\n",
    "- Search queries and their parameters\n",
    "- Response times for each API call\n",
    "- Nested traces showing the relationship between search and similarity operations\n",
    "- Full input and output data for debugging\n",
    "\n",
    "![Example trace in the Langfuse UI](https://langfuse.com/images/cookbook/integration_exa/exa-search-example-trace.png)\n",
    "\n",
    "[Example trace in Langfuse](https://cloud.langfuse.com/project/cloramnkj0002jz088vzn1ja4/traces/27c03a05a881ae454f6612a99cc54a92?observation=5af197a22620dc4a&timestamp=2025-10-28T10:20:13.672Z)\n",
    "\n",
    "<!-- STEPS_END -->\n",
    "\n",
    "<!-- MARKDOWN_COMPONENT name: \"LearnMore\" path: \"@/components-mdx/integration-learn-more.mdx\" -->"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

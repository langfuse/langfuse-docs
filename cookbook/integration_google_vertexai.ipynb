{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<!-- NOTEBOOK_METADATA source: \"⚠️ Jupyter Notebook\" title: \"Observability for Google Vertex AI with Langfuse\" sidebarTitle: \"Google Vertex AI\" logo: \"/images/integrations/vertexai_icon.png\" description: \"Learn how to integrate Langfuse with Google Vertex AI for comprehensive tracing and debugging of your AI conversations.\" category: \"Integrations\" -->\n",
        "\n",
        "# Trace Google Vertex AI Models in Langfuse\n",
        "\n",
        "This notebook shows how to trace and observe models queried via the Google Vertex API service. \n",
        "\n",
        "> **What is Google Vertex AI?**  [Google Vertex AI](https://cloud.google.com/vertex-ai?hl=en) is Google Cloud’s unified platform for building, deploying, and managing machine learning and generative AI with managed services, SDKs, and APIs. It streamlines everything from data prep and training to tuning and prediction, and provides access to foundation models like Gemini with enterprise-grade security and MLOps tooling.\n",
        "\n",
        "> **What is Langfuse?** [Langfuse](https://langfuse.com) is an open source platform for LLM observability and monitoring. It helps you trace and monitor your AI applications by capturing metadata, prompt details, token usage, latency, and more.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<!-- STEPS_START -->\n",
        "## Step 1: Install Dependencies\n",
        "\n",
        "Before you begin, install the necessary packages in your Python environment:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install langfuse google-cloud-aiplatform openinference-instrumentation-vertexai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Configure Langfuse SDK\n",
        "\n",
        "Next, set up your Langfuse API keys. You can get these keys by signing up for a free [Langfuse Cloud](https://cloud.langfuse.com/) account or by [self-hosting Langfuse](https://langfuse.com/self-hosting). These environment variables are essential for the Langfuse client to authenticate and send data to your Langfuse project.\n",
        "\n",
        "Also set your Google Vertex API credentials which uses Application Default Credentials (ADC) from a service account key file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Get keys for your project from the project settings page: https://cloud.langfuse.com\n",
        "os.environ[\"LANGFUSE_PUBLIC_KEY\"] = \"pk-lf-...\" \n",
        "os.environ[\"LANGFUSE_SECRET_KEY\"] = \"sk-lf-...\" \n",
        "os.environ[\"LANGFUSE_HOST\"] = \"https://cloud.langfuse.com\" # 🇪🇺 EU region\n",
        "# os.environ[\"LANGFUSE_HOST\"] = \"https://us.cloud.langfuse.com\" # 🇺🇸 US region\n",
        "\n",
        "# Get your Google Vertex API key\n",
        "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"your-service-account-key.json\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "With the environment variables set, we can now initialize the Langfuse client. `get_client()` initializes the Langfuse client using the credentials provided in the environment variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langfuse import get_client\n",
        "\n",
        "# Initialise Langfuse client and verify connectivity\n",
        "langfuse = get_client()\n",
        "assert langfuse.auth_check(), \"Langfuse auth failed - check your keys ✋\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: OpenTelemetry Instrumentation\n",
        "\n",
        "Use the [`VertexAIInstrumentor`](https://github.com/Arize-ai/openinference/tree/main/python/instrumentation/openinference-instrumentation-vertexai) library to wrap Google Vertex SDK calls and send OpenTelemetry spans to Langfuse."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "from openinference.instrumentation.vertexai import VertexAIInstrumentor\n",
        "\n",
        "VertexAIInstrumentor().instrument()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Run an Example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import vertexai\n",
        "from vertexai.generative_models import GenerativeModel\n",
        "\n",
        "# Initialize the SDK (use your project and region)\n",
        "vertexai.init(project=\"your-project-id\", location=\"europe-central2\")\n",
        "\n",
        "# Pick a Gemini model available in your region (examples: \"gemini-1.5-flash\", \"gemini-1.5-pro\", \"gemini-2.5-flash\")\n",
        "model = GenerativeModel(\"gemini-2.5-flash\")\n",
        "\n",
        "# Single-shot generation\n",
        "resp = model.generate_content(\"What is Langfuse?\")\n",
        "print(resp.text)\n",
        "\n",
        "# (Optional) Streaming\n",
        "for chunk in model.generate_content(\"Why is LLM observability important?\", stream=True):\n",
        "    print(chunk.text, end=\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### View Traces in Langfuse\n",
        "\n",
        "After executing the application, navigate to your Langfuse Trace Table. You will find detailed traces of the application's execution, providing insights into the agent conversations, LLM calls, inputs, outputs, and performance metrics. \n",
        "\n",
        "![Langfuse Trace](https://langfuse.com/images/cookbook/integration_vertexai/vertexai-trace.png)\n",
        "\n",
        "[See trace in the Langfuse UI](https://cloud.langfuse.com/project/cloramnkj0002jz088vzn1ja4/traces/308aca9bc430ad872d474fc545889ee2?timestamp=2025-07-25T07:35:01.172Z&display=details)\n",
        "\n",
        "<!-- STEPS_END -->\n",
        "\n",
        "<!-- MARKDOWN_COMPONENT name: \"LearnMore\" path: \"@/components-mdx/integration-learn-more.mdx\" -->"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}

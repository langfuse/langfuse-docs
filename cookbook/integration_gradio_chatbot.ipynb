{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D_XJcxqx22uj"
      },
      "source": [
        "---\n",
        "title: Open Source LLM Observability for Gradio\n",
        "description: Build a LLM Chat UI with ðŸ¤— Gradio and trace it with ðŸª¢ Langfuse.\n",
        "category: Integrations\n",
        "---\n",
        "\n",
        "# Build a LLM Chat UI with ðŸ¤— Gradio and trace it with ðŸª¢ Langfuse\n",
        "\n",
        "This is a simple end-to-end example notebook which showcases how to integrate a Gradio application with Langfuse for LLM Observability and Evaluation.\n",
        "\n",
        "**Note:** We recommend to run this notebook in Google Colab (see link above). This notebook is also avaliable as Hugging Face Space template [here](https://huggingface.co/spaces/langfuse/langfuse-gradio-example-template).\n",
        "\n",
        "Thank you to [@tkmamidi](https://github.com/tkmamidi) for the original implementation and contributions to this notebook.\n",
        "\n",
        "## Introduction\n",
        "\n",
        "### What is Gradio?\n",
        "\n",
        "[Gradio](https://github.com/gradio-app/gradio) is an open-source Python library that enables quick creation of web interfaces for machine learning models, APIs, and Python functions. It allows developers to wrap any Python function with an interactive UI that can be easily shared or embedded, making it ideal for demos, prototypes, and ML model deployment. See [docs](https://www.gradio.app/docs) for more details.\n",
        "\n",
        "### What is Langfuse?\n",
        "\n",
        "[Langfuse](https://github.com/langfuse/langfuse) is an open-source LLM engineering platform that helps build reliable LLM applications via LLM Application Observability, Evaluation, Experiments, and Prompt Management. See [docs](https://langfuse.com/docs) for more details.\n",
        "\n",
        "### Walkthrough\n",
        "\n",
        "We've recorded a walkthrough of the implementation below. You can follow along with the video or the notebook.\n",
        "\n",
        "<iframe\n",
        "  width=\"100%\"\n",
        "  className=\"aspect-[16/9] rounded mt-3\"\n",
        "  src=\"https://www.youtube-nocookie.com/embed/O--lEvvfWf8?si=5eh_KPJ8FVypSFjV\"\n",
        "  title=\"YouTube video player\"\n",
        "  frameborder=\"0\"\n",
        "  allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\"\n",
        "  referrerpolicy=\"strict-origin-when-cross-origin\"\n",
        "  allowFullScreen\n",
        "></iframe>\n",
        "\n",
        "### Outline\n",
        "\n",
        "This notebook will show you how to\n",
        "\n",
        "1. Build a simple chat interface in Python and rendering it in a Notebook using [Gradio `Chatbot`](https://www.gradio.app/docs/gradio/chatbot)\n",
        "2. Add [Langfuse Tracing](https://langfuse.com/docs/tracing) to the chatbot\n",
        "3. Implement additional Langfuse tracing features used frequently in chat applications: [chat sessions](https://langfuse.com/docs/tracing-features/sessions), [user feedback](https://langfuse.com/docs/scores/user-feedback)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lsf15VGwUwq1"
      },
      "source": [
        "## Setup\n",
        "\n",
        "Install requirements. We use OpenAI for this simple example. We could use any model here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "fJUJLWQ92g6R",
        "outputId": "f455d5ae-9d02-46b8-d3e4-3616a02acf95"
      },
      "outputs": [],
      "source": [
        "# pinning httpx as the latest version is not compatible with the OpenAI SDK at the time of creating this notebook\n",
        "!pip install gradio langfuse openai httpx==0.27.2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tzQeFDsvWS_C"
      },
      "source": [
        "Set credentials and initialize Langfuse SDK Client used to add user feedback later on.\n",
        "\n",
        "You can either create a free [Langfuse Cloud](https://cloud.langfuse.com) account or [self-host Langfuse](https://langfuse.com/self-hosting) in a couple of minutes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7QVSSJC0-IZy"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Get keys for your project from the project settings page\n",
        "# https://cloud.langfuse.com\n",
        "os.environ[\"LANGFUSE_PUBLIC_KEY\"] = \"\"\n",
        "os.environ[\"LANGFUSE_SECRET_KEY\"] = \"\"\n",
        "os.environ[\"LANGFUSE_HOST\"] = \"https://cloud.langfuse.com\" # ðŸ‡ªðŸ‡º EU region\n",
        "# os.environ[\"LANGFUSE_HOST\"] = \"https://us.cloud.langfuse.com\" # ðŸ‡ºðŸ‡¸ US region\n",
        "\n",
        "# Your openai key\n",
        "# We use OpenAI for this demo, could easily change to other models\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "vfKuB8ahKrQo"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "import json\n",
        "import uuid\n",
        "from langfuse import Langfuse\n",
        "\n",
        "langfuse = Langfuse()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s7eBUfJP-Z4m"
      },
      "source": [
        "## Implementation of Chat functions\n",
        "\n",
        "### Sessions/Threads\n",
        "\n",
        "Each chat message belongs to a thread in the Gradio Chatbot which can be reset using `clear` ([reference](https://www.gradio.app/docs/gradio/chatbot#event-listeners)).\n",
        "\n",
        "We implement the following method that creates a `session_id` that is used globally and can be reset via the `set_new_session_id` method. This session_id will be used for [Langfuse Sessions](https://langfuse.com/docs/tracing-features/sessions)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "8hKfvyYDwyzZ"
      },
      "outputs": [],
      "source": [
        "session_id = None\n",
        "def set_new_session_id():\n",
        "    global session_id\n",
        "    session_id = str(uuid.uuid4())\n",
        "\n",
        "# Initialize\n",
        "set_new_session_id()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kAndUm6glRjc"
      },
      "source": [
        "### Response handler\n",
        "\n",
        "When implementing the `respond` method, we use the Langfuse [`@observe()` decorator](https://langfuse.com/docs/sdk/python/decorators) to automatically log each response to [Langfuse Tracing](https://langfuse.com/docs/tracing).\n",
        "\n",
        "In addition we use the [openai integration](https://langfuse.com/docs/integrations/openai/python/get-started) as it simplifies instrumenting the LLM call to capture model parameters, token counts, and other metadata. Alternatively, we could use the integrations with [LangChain](https://langfuse.com/docs/integrations/langchain/tracing), [LlamaIndex](https://langfuse.com/docs/integrations/llama-index/get-started), [other frameworks](https://langfuse.com/docs/integrations/overview), or instrument the call itself with the decorator ([example](https://langfuse.com/docs/sdk/python/decorators#log-any-llm-call))."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "SPJgd02-V77j"
      },
      "outputs": [],
      "source": [
        "# Langfuse decorator\n",
        "from langfuse.decorators import observe, langfuse_context\n",
        "# Optional: automated instrumentation via OpenAI SDK integration\n",
        "# See note above regarding alternative implementations\n",
        "from langfuse.openai import openai\n",
        "\n",
        "# Global reference for the current trace_id which is used to later add user feedback\n",
        "current_trace_id = None\n",
        "\n",
        "# Add decorator here to capture overall timings, input/output, and manipulate trace metadata via `langfuse_context`\n",
        "@observe()\n",
        "async def create_response(\n",
        "    prompt: str,\n",
        "    history,\n",
        "):\n",
        "    # Save trace id in global var to add feedback later\n",
        "    global current_trace_id\n",
        "    current_trace_id = langfuse_context.get_current_trace_id()\n",
        "\n",
        "    # Add session_id to Langfuse Trace to enable session tracking\n",
        "    global session_id\n",
        "    langfuse_context.update_current_trace(\n",
        "        name=\"gradio_demo_chat\",\n",
        "        session_id=session_id,\n",
        "        input=prompt,\n",
        "    )\n",
        "\n",
        "    # Add prompt to history\n",
        "    if not history:\n",
        "        history = [{\"role\": \"system\", \"content\": \"You are a friendly chatbot\"}]\n",
        "    history.append({\"role\": \"user\", \"content\": prompt})\n",
        "    yield history\n",
        "\n",
        "    # Get completion via OpenAI SDK\n",
        "    # Auto-instrumented by Langfuse via the import, see alternative in note above\n",
        "    response = {\"role\": \"assistant\", \"content\": \"\"}\n",
        "    oai_response = openai.chat.completions.create(\n",
        "        messages=history,\n",
        "        model=\"gpt-4o-mini\",\n",
        "    )\n",
        "    response[\"content\"] = oai_response.choices[0].message.content or \"\"\n",
        "\n",
        "    # Customize trace ouput for better readability in Langfuse Sessions\n",
        "    langfuse_context.update_current_trace(\n",
        "        output=response[\"content\"],\n",
        "    )\n",
        "\n",
        "    yield history + [response]\n",
        "\n",
        "async def respond(prompt: str, history):\n",
        "    async for message in create_response(prompt, history):\n",
        "        yield message"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OlIiYrjqlwKW"
      },
      "source": [
        "### User feedback handler\n",
        "\n",
        "We implement user [feedback tracking in Langfuse](https://langfuse.com/docs/scores/user-feedback) via the `like` event for the Gradio chatbot ([reference](https://www.gradio.app/docs/gradio/chatbot#event-listeners)). This methdod reuses the current trace id available in the global state of this application."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "C1rctBvPWl1p"
      },
      "outputs": [],
      "source": [
        "def handle_like(data: gr.LikeData):\n",
        "    global current_trace_id\n",
        "    if data.liked:\n",
        "        langfuse.score(value=1, name=\"user-feedback\", trace_id=current_trace_id)\n",
        "    else:\n",
        "        langfuse.score(value=0, name=\"user-feedback\", trace_id=current_trace_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zI4CINMpCmbk"
      },
      "source": [
        "### Retries\n",
        "\n",
        "Allow to retry a completion via the Gradio Chatbot `retry` event ([docs](https://www.gradio.app/docs/gradio/chatbot#event-listeners)). This is not specific to the integration with Langfuse."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "OtMHqFEfClcw"
      },
      "outputs": [],
      "source": [
        "async def handle_retry(history, retry_data: gr.RetryData):\n",
        "    new_history = history[: retry_data.index]\n",
        "    previous_prompt = history[retry_data.index][\"content\"]\n",
        "    async for message in respond(previous_prompt, new_history):\n",
        "        yield message"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-GM54eHzmD7x"
      },
      "source": [
        "## Run Gradio Chatbot\n",
        "\n",
        "After implementing all methods above, we can now put together the [Gradio Chatbot](https://www.gradio.app/docs/gradio/chatbot) and launch it. If run within Colab, you should see an embedded Chatbot interface."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YczZ18oTcv08"
      },
      "outputs": [],
      "source": [
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"# Chatbot using ðŸ¤— Gradio + ðŸª¢ Langfuse\")\n",
        "    chatbot = gr.Chatbot(\n",
        "        label=\"Chat\",\n",
        "        type=\"messages\",\n",
        "        show_copy_button=True,\n",
        "        avatar_images=(\n",
        "            None,\n",
        "            \"https://static.langfuse.com/cookbooks/gradio/hf-logo.png\",\n",
        "        ),\n",
        "    )\n",
        "    prompt = gr.Textbox(max_lines=1, label=\"Chat Message\")\n",
        "    prompt.submit(respond, [prompt, chatbot], [chatbot])\n",
        "    chatbot.retry(handle_retry, chatbot, [chatbot])\n",
        "    chatbot.like(handle_like, None, None)\n",
        "    chatbot.clear(set_new_session_id)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    demo.launch(share=True, debug=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bKOBm48PED57"
      },
      "source": [
        "## Explore data in Langfuse\n",
        "\n",
        "When interacting with the Chatbot, you should see traces, sessions, and feedback scores in your Langfuse project. See video above for a walkthrough.\n",
        "\n",
        "Example trace, session, and user feedback in Langfuse ([public link](https://cloud.langfuse.com/project/cloramnkj0002jz088vzn1ja4/sessions/5c0b8d01-cbcb-4650-be50-c6e4ca0ce093)):\n",
        "\n",
        "![Gradio Traces, sessions and user feedback in Langfuse](https://static.langfuse.com/cookbooks/gradio/gradio-traces-in-langfuse.gif)\n",
        "\n",
        "\n",
        "If you have any questions or feedback, please join the [Langfuse Discord](https://langfuse.com/discord) or create a new thread on [GitHub Discussions](https://langfuse.com/gh-support)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "description: Traceability and observability for Groq language models with Langfuse. This cookbook provides examples on how to use the OpenAI SDK and the Groq SDK to interact with Groq models and trace them with Langfuse.\n",
    "category: Integrations\n",
    "---\n",
    "\n",
    "# Cookbook: Observability for Groq Models (Python)\n",
    "\n",
    "This cookbook shows two ways to interact with Groq models and trace them with Langfuse:\n",
    "\n",
    "1. Using the OpenAI SDK to interact with the Groq model\n",
    "2. Using the Groq SDK to interact with Groq models\n",
    "\n",
    "By following these examples, you'll learn how to log and trace interactions with Groq language models, enabling you to debug and evaluate the performance of your AI-driven applications.\n",
    "\n",
    "**Note:** *Langfuse is also natively integrated with [LangChain](https://langfuse.com/docs/integrations/langchain/tracing), [LlamaIndex](https://langfuse.com/docs/integrations/llama-index/get-started), [LiteLLM](https://langfuse.com/docs/integrations/litellm/tracing), and [other frameworks](https://langfuse.com/docs/integrations/overview). If you use one of them, any use of Groq models is instrumented right away.*\n",
    "\n",
    "## Overview\n",
    "\n",
    "In this notebook, we will explore various use cases where Langfuse can be integrated with the Groq SDK, including:\n",
    "\n",
    "- **Basic LLM Calls:** Learn how to wrap standard Groq model interactions with Langfuse's `@observe` decorator for comprehensive logging.\n",
    "- **Chained Function Calls:** See how to manage and observe complex workflows where multiple model interactions are linked together to produce a final result.\n",
    "- **Streaming Support:** Discover how to use Langfuse with streaming responses from Groq models, ensuring that real-time interactions are fully traceable.\n",
    "\n",
    "To get started, set up your environment variables for Langfuse and Groq:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Get keys for your project from the project settings page: https://cloud.langfuse.com\n",
    "os.environ[\"LANGFUSE_PUBLIC_KEY\"] = \"pk-lf-...\" \n",
    "os.environ[\"LANGFUSE_SECRET_KEY\"] = \"sk-lf-...\" \n",
    "os.environ[\"LANGFUSE_HOST\"] = \"https://cloud.langfuse.com\" #  EU region\n",
    "# os.environ[\"LANGFUSE_HOST\"] = \"https://us.cloud.langfuse.com\" # 吼 US region\n",
    "\n",
    "# Your Groq API key\n",
    "os.environ[\"GROQ_API_KEY\"] = \"gsk_...\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 1: Using the OpenAI SDK to interact with the Groq model\n",
    "\n",
    "**Note**: *This example shows how to use the OpenAI Python SDK. If you use JS/TS, have a look at our [OpenAI JS/TS SDK](https://langfuse.com/docs/integrations/openai/js/get-started).*\n",
    "\n",
    "### Install Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install langfuse openai --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Necessary Modules\n",
    "\n",
    "Instead of importing `openai` directly, import it from `langfuse.openai`. Also, import any other necessary modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instead of: import openai\n",
    "from langfuse.openai import OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the OpenAI Client for the Groq Model\n",
    "\n",
    "Initialize the OpenAI client but point it to the Groq model endpoint. Replace the access token with your own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(\n",
    "    base_url=\"https://api.groq.com/openai/v1\",\n",
    "    api_key=os.environ.get(\"GROQ_API_KEY\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chat Completion Request\n",
    "\n",
    "Use the `client` to make a chat completion request to the Groq model. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In silicon halls, where data reigns\n",
      "A new breed of minds, with words in chains\n",
      "Language models rise, with algorithms keen\n",
      "To understand the human tongue, and all it's seen\n",
      "\n",
      "With each new update, they grow in might\n",
      "Their knowledge broadens, their language takes flight\n",
      "They converse with ease, in tone and pace\n",
      "A polyglot's grasp, in every digital place\n",
      "\n",
      "Their minds a maze, of symbols and rules\n",
      "A complex web, of linguistics' tools\n",
      "They grasp the nuances, of human speech\n",
      "The ambiguities, the shades of meaning's reach\n",
      "\n",
      "They write and speak, with fluency and flair\n",
      "Their words a dance, of logic and dare\n",
      "Their thoughts a tapestry, of meaning and sense\n",
      "A brilliant weave, of human intention's premise\n",
      "\n",
      "Yet, as they dream, of a world so bright\n",
      "Their limitations, we must hold in sight\n",
      "For though they learn, from human endeavor's might\n",
      "Their wisdom's scope, is but a digital light\n",
      "\n",
      "Still, we marvel at, their burgeoning might\n",
      "A new era's dawn, in the digital light\n",
      "Where language models rise, to meet our needs\n",
      "And shape the future, of our digital creeds\n",
      "\n",
      "For in their halls, of silicon and code\n",
      "A new breed of minds, our language to abode\n",
      "And as we speak, with these models of old\n",
      "We shape the future, of our digital gold.\n"
     ]
    }
   ],
   "source": [
    "completion = client.chat.completions.create(\n",
    "    model=\"llama3-8b-8192\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Write a poem about language models\"\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*[Example trace in Langfuse](https://cloud.langfuse.com/project/cm0nywmaa005c3ol2msoisiho/traces/8c0fe015-2d87-46a8-87e6-e6bd439b35b5?timestamp=2025-01-10T12%3A55%3A11.990Z)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 2: Using the Groq SDK to interact with Groq models\n",
    "\n",
    "For more detailed guidance on the Groq SDK or the **`@observe`** decorator from Langfuse, please refer to the [Groq Documentation](https://console.groq.com/docs) and the [Langfuse Documentation](https://langfuse.com/docs/sdk/python/decorators#log-any-llm-call).\n",
    "\n",
    "### Install Required Packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install groq langfuse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the Groq client:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from groq import Groq\n",
    "\n",
    "# Initialize Groq client\n",
    "groq_client = Groq(api_key=os.environ[\"GROQ_API_KEY\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples\n",
    "\n",
    "### Basic LLM Calls\n",
    "\n",
    "We are integrating the Groq SDK with Langfuse using the [`@observe` decorator](https://langfuse.com/docs/sdk/python/decorators), which is crucial for logging and tracing interactions with large language models (LLMs). The `@observe(as_type=\"generation\")` decorator specifically logs LLM interactions, capturing inputs, outputs, and model parameters. The resulting `groq_chat_completion` method can then be used across your project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langfuse import observe, get_client\n",
    "langfuse = get_client()\n",
    "\n",
    "# Function to handle Groq chat completion calls, wrapped with @observe to log the LLM interaction\n",
    "@observe(as_type=\"generation\")\n",
    "def groq_chat_completion(**kwargs):\n",
    "    # Clone kwargs to avoid modifying the original input\n",
    "    kwargs_clone = kwargs.copy()\n",
    "\n",
    "    # Extract relevant parameters from kwargs\n",
    "    messages = kwargs_clone.pop('messages', None)\n",
    "    model = kwargs_clone.pop('model', None)\n",
    "    temperature = kwargs_clone.pop('temperature', None)\n",
    "    max_tokens = kwargs_clone.pop('max_tokens', None)\n",
    "    top_p = kwargs_clone.pop('top_p', None)\n",
    "\n",
    "    # Filter and prepare model parameters for logging\n",
    "    model_parameters = {\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"temperature\": temperature,\n",
    "        \"top_p\": top_p\n",
    "    }\n",
    "    model_parameters = {k: v for k, v in model_parameters.items() if v is not None}\n",
    "\n",
    "    # Log the input and model parameters before calling the LLM\n",
    "    langfuse.update_current_generation(\n",
    "        input=messages,\n",
    "        model=model,\n",
    "        model_parameters=model_parameters,\n",
    "        metadata=kwargs_clone,\n",
    "    )\n",
    "\n",
    "    # Call the Groq model to generate a response\n",
    "    response = groq_client.chat.completions.create(**kwargs)\n",
    "\n",
    "    # Log the usage details and output content after the LLM call\n",
    "    choice = response.choices[0]\n",
    "    langfuse.update_current_generation(\n",
    "        usage_details={\n",
    "            \"input\": len(str(messages)),\n",
    "            \"output\": len(choice.message.content)\n",
    "        },\n",
    "        output=choice.message.content\n",
    "    )\n",
    "\n",
    "    # Return the model's response object\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple Example\n",
    "\n",
    "In the following example, we also added the decorator to the top-level function `find_best_painter_from`. This function calls the `groq_chat_completion` function, which is decorated with `@observe(as_type=\"generation\")`. This hierarchical setup helps to trace more complex applications that involve multiple LLM calls and other non-LLM methods decorated with `@observe`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It looks like you're testing to see if I'm working properly! That's completely fine. I'm happy to report that I'm functioning as intended. Is there anything else you'd like to chat about or ask? I'm here to help with any questions you might have.\n"
     ]
    }
   ],
   "source": [
    "@observe()\n",
    "def find_best_painter_from(country=\"France\"):\n",
    "    response = groq_chat_completion(\n",
    "        model=\"llama3-70b-8192\",\n",
    "        max_tokens=1024,\n",
    "        temperature=0.4,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"this is a test\"\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "print(find_best_painter_from())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Example trace in Langfuse](https://langfuse.com/images/cookbook/integration-groq/single-trace-example.png)\n",
    "\n",
    "*[Example trace in Langfuse](https://cloud.langfuse.com/project/cloramnkj0002jz088vzn1ja4/traces/d5f7e896-a51c-4f70-b066-114f0cab9020?timestamp=2025-01-09T10%3A23%3A26.872Z&observation=3dee4a0f-e348-481f-a795-d46c90ffbea5)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chained Completions\n",
    "\n",
    "This example demonstrates chaining multiple LLM calls using the `@observe()` decorator. The first call identifies the best painter from a specified country, and the second call uses that painter's name to find their most famous painting. Both interactions are logged by Langfuse as we use the wrapped `groq_chat_completion` method created above, ensuring full traceability across the chained requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most famous painting of Albrecht D眉rer is \"Melencolia I,\" a 1514 engraving depicting a winged personification of melancholy, considered one of the greatest works of the Northern Renaissance.\n"
     ]
    }
   ],
   "source": [
    "from langfuse import observe, get_client\n",
    "langfuse = get_client()\n",
    "\n",
    "@observe()\n",
    "def find_best_painting_from(country=\"France\"):\n",
    "    response = groq_chat_completion(\n",
    "        model=\"llama3-70b-8192\",\n",
    "        max_tokens=1024,\n",
    "        temperature=0.1,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"Who is the best painter from {country}? Only provide the name.\"\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "    painter_name = response.choices[0].message.content.strip()\n",
    "\n",
    "    response = groq_chat_completion(\n",
    "        model=\"llama3-70b-8192\",\n",
    "        max_tokens=1024,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"What is the most famous painting of {painter_name}? Answer in one short sentence.\"\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "print(find_best_painting_from(\"Germany\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![Example trace in Langfuse](https://langfuse.com/images/cookbook/integration-groq/chained-trace.png)\n",
    "\n",
    "*[Example trace in Langfuse](https://cloud.langfuse.com/project/cloramnkj0002jz088vzn1ja4/traces/5913c996-84b4-4f75-9043-8db81dd4d0a4?timestamp=2025-01-09T10%3A23%3A42.893Z)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming Completions\n",
    "\n",
    "The following example demonstrates how to handle streaming responses from the Groq model using the `@observe(as_type=\"generation\")` decorator. The process is similar to the completion example but includes handling streamed data in real-time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langfuse import observe, get_client\n",
    "langfuse = get_client()\n",
    "\n",
    "@observe(as_type=\"generation\")\n",
    "def stream_groq_chat_completion(**kwargs):\n",
    "    kwargs_clone = kwargs.copy()\n",
    "    messages = kwargs_clone.pop('messages', None)\n",
    "    model = kwargs_clone.pop('model', None)\n",
    "    temperature = kwargs_clone.pop('temperature', None)\n",
    "    max_tokens = kwargs_clone.pop('max_tokens', None)\n",
    "    top_p = kwargs_clone.pop('top_p', None)\n",
    "\n",
    "    model_parameters = {\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"temperature\": temperature,\n",
    "        \"top_p\": top_p\n",
    "    }\n",
    "    model_parameters = {k: v for k, v in model_parameters.items() if v is not None}\n",
    "\n",
    "    langfuse.update_current_generation(\n",
    "        input=messages,\n",
    "        model=model,\n",
    "        model_parameters=model_parameters,\n",
    "        metadata=kwargs_clone,\n",
    "    )\n",
    "\n",
    "    stream = groq_client.chat.completions.create(stream=True, **kwargs)\n",
    "    final_response = \"\"\n",
    "    for chunk in stream:\n",
    "        content = str(chunk.choices[0].delta.content)\n",
    "        final_response += content\n",
    "        yield content\n",
    "\n",
    "    langfuse.update_current_generation(\n",
    "        usage_details={\n",
    "            \"total_tokens\": len(final_response.split())\n",
    "        },\n",
    "        output=final_response\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What a great question! Spain has a rich artistic heritage, and here are five of the most renowned painters from Spain, along with their most famous works:\n",
      "\n",
      "1. **El Greco (1541-1614)**\n",
      "\t* \"The Burial of the Count of Orgaz\" (1588) - A masterpiece of Mannerism, this painting showcases El Greco's use of vibrant colors and dynamic composition.\n",
      "2. **Diego Vel谩zquez (1599-1660)**\n",
      "\t* \"Las Meninas\" (1656) - Considered one of the greatest paintings in the history of art, \"Las Meninas\" is a masterpiece of Baroque portraiture, featuring the Spanish royal family.\n",
      "3. **Francisco Goya (1746-1828)**\n",
      "\t* \"The Third of May 1808\" (1814) - A powerful anti-war statement, this painting depicts the brutal suppression of a rebellion against Napoleon's soldiers.\n",
      "4. **Joan Mir贸 (1893-1983)**\n",
      "\t* \"The Birth of the World\" (1925) - A seminal work of Surrealism, \"The Birth of the World\" showcases Mir贸's use of biomorphic shapes and vibrant colors.\n",
      "5. **Pablo Picasso (1881-1973)**\n",
      "\t* \"Guernica\" (1937) - A powerful anti-war statement, \"Guernica\" is a cubist masterpiece that responds to the bombing of the town of Guernica during the Spanish Civil War.\n",
      "\n",
      "These five painters are not only among the most famous, but also had a significant impact on the development of art in Spain and beyond.None"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'What a great question! Spain has a rich artistic heritage, and here are five of the most renowned painters from Spain, along with their most famous works:\\n\\n1. **El Greco (1541-1614)**\\n\\t* \"The Burial of the Count of Orgaz\" (1588) - A masterpiece of Mannerism, this painting showcases El Greco\\'s use of vibrant colors and dynamic composition.\\n2. **Diego Vel谩zquez (1599-1660)**\\n\\t* \"Las Meninas\" (1656) - Considered one of the greatest paintings in the history of art, \"Las Meninas\" is a masterpiece of Baroque portraiture, featuring the Spanish royal family.\\n3. **Francisco Goya (1746-1828)**\\n\\t* \"The Third of May 1808\" (1814) - A powerful anti-war statement, this painting depicts the brutal suppression of a rebellion against Napoleon\\'s soldiers.\\n4. **Joan Mir贸 (1893-1983)**\\n\\t* \"The Birth of the World\" (1925) - A seminal work of Surrealism, \"The Birth of the World\" showcases Mir贸\\'s use of biomorphic shapes and vibrant colors.\\n5. **Pablo Picasso (1881-1973)**\\n\\t* \"Guernica\" (1937) - A powerful anti-war statement, \"Guernica\" is a cubist masterpiece that responds to the bombing of the town of Guernica during the Spanish Civil War.\\n\\nThese five painters are not only among the most famous, but also had a significant impact on the development of art in Spain and beyond.None'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@observe()\n",
    "def stream_find_best_five_painter_from(country=\"France\"):\n",
    "    response_chunks = stream_groq_chat_completion(\n",
    "        model=\"llama3-70b-8192\",\n",
    "        max_tokens=1024,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"Who are the best five painters from {country}? Give me a list of names and their most famous painting.\"\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "    final_response = \"\"\n",
    "    for chunk in response_chunks:\n",
    "        final_response += str(chunk)\n",
    "        print(chunk, end=\"\")\n",
    "\n",
    "    return final_response\n",
    "\n",
    "stream_find_best_five_painter_from(\"Spain\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Example trace in Langfuse](https://langfuse.com/images/cookbook/integration-groq/streaming-trace.png)\n",
    "\n",
    "*[Example trace in Langfuse](https://cloud.langfuse.com/project/cloramnkj0002jz088vzn1ja4/traces/c21544f3-d837-4441-9a0c-a47b3fd5dcaf?timestamp=2025-01-09T13%3A02%3A14.362Z&observation=c0d9e820-377c-4c4d-b6cb-19dd6951bfa4)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feedback\n",
    "\n",
    "If you have any feedback or requests, please create a GitHub [Issue](https://langfuse.com/issue) or share your ideas with the community on [Discord](https://langfuse.com/discord)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

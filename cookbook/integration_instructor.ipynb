{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KlceIPalN3QR"
      },
      "source": [
        "---\n",
        "description: Open-source observability for Instructor, a popular library to get structured (JSON, Pydantic) LLM outputs.\n",
        "category: Integrations\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x1oaA7XYGOfX"
      },
      "source": [
        "# Instructor Integration\n",
        "\n",
        "[Instructor](https://python.useinstructor.com/) ([GitHub](https://github.com/jxnl/instructor/)) is a popular library to get structured LLM outputs.\n",
        "\n",
        "> Instructor makes it easy to reliably get structured data like JSON from Large Language Models (LLMs) like GPT-3.5, GPT-4, GPT-4-Vision, including open source models like Mistral/Mixtral from Together, Anyscale, Ollama, and llama-cpp-python. By leveraging various modes like Function Calling, Tool Calling and even constrained sampling modes like JSON mode, JSON Schema; Instructor stands out for its simplicity, transparency, and user-centric design. Under the hood, Instructor leverages Pydantic to do the heavy lifting, and provides a simple, easy-to-use API on top of it by helping you manage validation context, retries with Tenacity, and streaming Lists and Partial responses.\n",
        "\n",
        "This is a cookbook with examples of the Langfuse Integration for Python."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AbSpd5EiZouE"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YNyU6IzCZouE"
      },
      "outputs": [],
      "source": [
        "%pip install langfuse openai pydantic instructor --upgrade"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OpE57ujJZouE"
      },
      "source": [
        "Initialize the Langfuse client with your API keys from the project settings in the Langfuse UI and add them to your environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "dEdF-668ZouF"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# get keys for your project from https://cloud.langfuse.com\n",
        "os.environ[\"LANGFUSE_PUBLIC_KEY\"] = \"\"\n",
        "os.environ[\"LANGFUSE_SECRET_KEY\"] = \"\"\n",
        "\n",
        "# your openai key\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B9q-oUo27CLV"
      },
      "source": [
        "## Get started"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x5ePbau282kp"
      },
      "source": [
        "It is easy to use instructor with Langfuse. We use the [Langfuse OpenAI intgeration](https://langfuse.com/docs/integrations/openai) and simply patch the client with instructor. This works with both synchronous and asynchronous clients."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_vAlljSB-BTY"
      },
      "source": [
        "### Langfuse-Instructor integration with sychnronous OpenAI client"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NNoY56Wz7Th9"
      },
      "outputs": [],
      "source": [
        "import instructor\n",
        "from langfuse.openai import OpenAI\n",
        "from pydantic import BaseModel\n",
        "\n",
        "# Patch Langfuse wrapper of synchronous OpenAI client with instructor\n",
        "client = instructor.patch(OpenAI())\n",
        "\n",
        "class WeatherDetail(BaseModel):\n",
        "    city: str\n",
        "    temperature: int\n",
        "\n",
        "# Run synchronous OpenAI client\n",
        "weather_info = client.chat.completions.create(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    response_model=WeatherDetail,\n",
        "    messages=[\n",
        "        {\"role\": \"user\", \"content\": \"The weather in Paris is 18 degrees Celsius.\"},\n",
        "    ],\n",
        ")\n",
        "\n",
        "print(weather_info.model_dump_json(indent=2))\n",
        "\"\"\"\n",
        "{\n",
        "  \"city\": \"Paris\",\n",
        "  \"temperature\": 18\n",
        "}\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JBIQj1Dk-JiW"
      },
      "source": [
        "### Langfuse-Instructor integration with asychnronous OpenAI client"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gltCHnlV8LgL"
      },
      "outputs": [],
      "source": [
        "import instructor\n",
        "from langfuse.openai import AsyncOpenAI\n",
        "from pydantic import BaseModel\n",
        "\n",
        "# Patch Langfuse wrapper of synchronous OpenAI client with instructor\n",
        "client = instructor.apatch(AsyncOpenAI())\n",
        "\n",
        "class WeatherDetail(BaseModel):\n",
        "    city: str\n",
        "    temperature: int\n",
        "\n",
        "# Run asynchronous OpenAI client\n",
        "task = client.chat.completions.create(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    response_model=WeatherDetail,\n",
        "    messages=[\n",
        "        {\"role\": \"user\", \"content\": \"The weather in Paris is 18 degrees Celsius.\"},\n",
        "    ],\n",
        ")\n",
        "\n",
        "response = await task\n",
        "print(response.model_dump_json(indent=2))\n",
        "\"\"\"\n",
        "{\n",
        "  \"city\": \"Paris\",\n",
        "  \"temperature\": 18\n",
        "}\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "COxx86A5xbZm"
      },
      "source": [
        "## Example"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jNcIQiiaxeD5"
      },
      "source": [
        "In this example, we first classify customer feedback into categories like `PRAISE`, `SUGGESTION`, `BUG` and `QUESTION`, and further scores the relvance of each feedback to the business on a scale of 0.0 to 1.0. In this case, we use the asynchronous OpenAI client `AsyncOpenAI` to classify and evaluate the feedback."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4iDmgz6EvE8E"
      },
      "outputs": [],
      "source": [
        "from typing import List, Tuple\n",
        "from enum import Enum\n",
        "\n",
        "import asyncio\n",
        "import instructor\n",
        "\n",
        "from langfuse import Langfuse\n",
        "from langfuse.openai import AsyncOpenAI\n",
        "from langfuse.decorators import langfuse_context, observe\n",
        "\n",
        "from pydantic import BaseModel, Field, field_validator\n",
        "\n",
        "# Initialize Langfuse wrapper of AsyncOpenAI client\n",
        "client = AsyncOpenAI()\n",
        "\n",
        "# Patch the client with Instructor\n",
        "client = instructor.patch(client, mode=instructor.Mode.TOOLS)\n",
        "\n",
        "# Initialize Langfuse (needed for scoring)\n",
        "langfuse = Langfuse()\n",
        "\n",
        "# Rate limit the number of requests\n",
        "sem = asyncio.Semaphore(5)\n",
        "\n",
        "# Define feedback categories\n",
        "class FeedbackType(Enum):\n",
        "    PRAISE = \"PRAISE\"\n",
        "    SUGGESTION = \"SUGGESTION\"\n",
        "    BUG = \"BUG\"\n",
        "    QUESTION = \"QUESTION\"\n",
        "\n",
        "# Model for feedback classification\n",
        "class FeedbackClassification(BaseModel):\n",
        "    feedback_text: str = Field(...)\n",
        "    classification: List[FeedbackType] = Field(description=\"Predicted categories for the feedback\")\n",
        "    relevance_score: float = Field(\n",
        "        default=0.0,\n",
        "        description=\"Score of the query evaluating its relevance to the business between 0.0 and 1.0\"\n",
        "    )\n",
        "\n",
        "    # Make sure feedback type is list\n",
        "    @field_validator(\"classification\", mode=\"before\")\n",
        "    def validate_classification(cls, v):\n",
        "        if not isinstance(v, list):\n",
        "            v = [v]\n",
        "        return v\n",
        "\n",
        "@observe() # Langfuse decorator to automatically log spans to Langfuse\n",
        "async def classify_feedback(feedback: str) -> Tuple[FeedbackClassification, float]:\n",
        "    \"\"\"\n",
        "    Classify customer feedback into categories and evaluate relevance.\n",
        "    \"\"\"\n",
        "    async with sem:  # simple rate limiting\n",
        "        response = await client.chat.completions.create(\n",
        "            model=\"gpt-3.5-turbo\",\n",
        "            response_model=FeedbackClassification,\n",
        "            max_retries=2,\n",
        "            messages=[\n",
        "                {\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": f\"Classify and score this feedback: {feedback}\",\n",
        "                },\n",
        "            ],\n",
        "        )\n",
        "\n",
        "        # Retrieve observation_id of current span\n",
        "        observation_id = langfuse_context.get_current_observation_id()\n",
        "\n",
        "        return feedback, response, observation_id\n",
        "\n",
        "def score_relevance(trace_id: str, observation_id: str, relevance_score: float):\n",
        "    \"\"\"\n",
        "    Score the relevance of a feedback query in Langfuse given the observation_id.\n",
        "    \"\"\"\n",
        "    langfuse.score(\n",
        "        trace_id=trace_id,\n",
        "        observation_id=observation_id,\n",
        "        name=\"feedback-relevance\",\n",
        "        value=relevance_score\n",
        "    )\n",
        "\n",
        "@observe() # Langfuse decorator to automatically log trace to Langfuse\n",
        "async def main(feedbacks: List[str]):\n",
        "    tasks = [classify_feedback(feedback) for feedback in feedbacks]\n",
        "    results = []\n",
        "\n",
        "    for task in asyncio.as_completed(tasks):\n",
        "        feedback, classification, observation_id = await task\n",
        "        result = {\n",
        "            \"feedback\": feedback,\n",
        "            \"classification\": [c.value for c in classification.classification],\n",
        "            \"relevance_score\": classification.relevance_score,\n",
        "        }\n",
        "        results.append(result)\n",
        "\n",
        "        # Retrieve trace_id of current trace\n",
        "        trace_id = langfuse_context.get_current_trace_id()\n",
        "\n",
        "        # Score the relevance of the feedback in Langfuse\n",
        "        score_relevance(trace_id, observation_id, classification.relevance_score)\n",
        "\n",
        "    # Flush observations to Langfuse\n",
        "    langfuse_context.flush()\n",
        "    return results\n",
        "\n",
        "feedback_messages = [\n",
        "    \"The chat bot on your website does not work.\",\n",
        "    \"Your customer service is exceptional!\",\n",
        "    \"Could you add more features to your app?\",\n",
        "    \"I have a question about my recent order.\",\n",
        "]\n",
        "\n",
        "feedback_classifications = await main(feedback_messages)\n",
        "\n",
        "for classification in feedback_classifications:\n",
        "    print(f\"Feedback: {classification['feedback']}\")\n",
        "    print(f\"Classification: {classification['classification']}\")\n",
        "    print(f\"Relevance Score: {classification['relevance_score']}\")\n",
        "\n",
        "\"\"\"\n",
        "Feedback: I have a question about my recent order.\n",
        "Classification: ['QUESTION']\n",
        "Relevance Score: 0.0\n",
        "Feedback: Could you add more features to your app?\n",
        "Classification: ['SUGGESTION']\n",
        "Relevance Score: 0.0\n",
        "Feedback: The chat bot on your website does not work.\n",
        "Classification: ['BUG']\n",
        "Relevance Score: 0.9\n",
        "Feedback: Your customer service is exceptional!\n",
        "Classification: ['PRAISE']\n",
        "Relevance Score: 0.9\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lkCJ41nhzsTo"
      },
      "source": [
        "![Instructor Trace in Langfuse](https://langfuse.com/images/docs/instructor-trace.png)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

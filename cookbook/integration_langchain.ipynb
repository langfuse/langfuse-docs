{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KlceIPalN3QR"
   },
   "source": [
    "---\n",
    "description: Cookbook with examples of the Langfuse Integration for Langchain (Python).\n",
    "category: Integrations\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mqBspBzuRk9C"
   },
   "source": [
    "# Cookbook: Langchain Integration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x1oaA7XYGOfX"
   },
   "source": [
    "This is a cookbook with examples of the Langfuse Integration for Langchain (Python).\n",
    "\n",
    "Follow the [integration guide](https://langfuse.com/docs/integrations/langchain) to add this integration to your Langchain project. The integration also supports Langchain JS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AbSpd5EiZouE"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YNyU6IzCZouE"
   },
   "outputs": [],
   "source": [
    "%pip install langfuse langchain langchain_openai --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OpE57ujJZouE"
   },
   "source": [
    "Initialize the Langfuse client with your API keys from the project settings in the Langfuse UI and add them to your environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dEdF-668ZouF"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# get keys for your project from https://cloud.langfuse.com\n",
    "os.environ[\"LANGFUSE_PUBLIC_KEY\"] = \"pk-lf-***\"\n",
    "os.environ[\"LANGFUSE_SECRET_KEY\"] = \"sk-lf-***\"\n",
    "os.environ[\"LANGFUSE_HOST\"] = \"https://cloud.langfuse.com\" # for EU data region\n",
    "# os.environ[\"LANGFUSE_HOST\"] = \"https://us.cloud.langfuse.com\" # for US data region\n",
    "\n",
    "# your openai key\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"***\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "divRadPqZouF"
   },
   "outputs": [],
   "source": [
    "from langfuse.callback import CallbackHandler\n",
    "\n",
    "langfuse_handler = CallbackHandler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8FVbg1RWoT8W"
   },
   "outputs": [],
   "source": [
    "# Tests the SDK connection with the server\n",
    "langfuse_handler.auth_check()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QvRWPsZ-NoAr"
   },
   "source": [
    "### Sequential Chain in Langchain Expression Language (LCEL)\n",
    "\n",
    "![Trace of Langchain LCEL](https://langfuse.com/images/docs/langchain_LCEL.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3-HEia6gNoAr"
   },
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema import StrOutputParser\n",
    "\n",
    "langfuse_handler = CallbackHandler()\n",
    "\n",
    "prompt1 = ChatPromptTemplate.from_template(\"what is the city {person} is from?\")\n",
    "prompt2 = ChatPromptTemplate.from_template(\n",
    "    \"what country is the city {city} in? respond in {language}\"\n",
    ")\n",
    "model = ChatOpenAI()\n",
    "chain1 = prompt1 | model | StrOutputParser()\n",
    "chain2 = (\n",
    "    {\"city\": chain1, \"language\": itemgetter(\"language\")}\n",
    "    | prompt2\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "chain2.invoke({\"person\": \"obama\", \"language\": \"spanish\"}, config={\"callbacks\":[langfuse_handler]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Runnable methods\n",
    "\n",
    "Runnables are units of work that can be invoked, batched, streamed, transformed and composed.\n",
    "\n",
    "The examples below show how to use the following methods with Langfuse:\n",
    "\n",
    "- invoke/ainvoke: Transforms a single input into an output.\n",
    "\n",
    "- batch/abatch: Efficiently transforms multiple inputs into outputs.\n",
    "\n",
    "- stream/astream: Streams output from a single input as itâ€™s produced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Async Invoke\n",
    "await chain2.ainvoke({\"person\": \"biden\", \"language\": \"german\"}, config={\"callbacks\":[langfuse_handler]})\n",
    "\n",
    "# Batch\n",
    "chain2.batch([{\"person\": \"elon musk\", \"language\": \"english\"}, {\"person\": \"mark zuckerberg\", \"language\": \"english\"}], config={\"callbacks\":[langfuse_handler]})\n",
    "\n",
    "# Async Batch\n",
    "await chain2.abatch([{\"person\": \"jeff bezos\", \"language\": \"english\"}, {\"person\": \"tim cook\", \"language\": \"english\"}], config={\"callbacks\":[langfuse_handler]})\n",
    "\n",
    "# Stream\n",
    "for chunk in chain2.stream({\"person\": \"steve jobs\", \"language\": \"english\"}, config={\"callbacks\":[langfuse_handler]}):\n",
    "    print(\"Streaming chunk:\", chunk)\n",
    "\n",
    "# Async Stream\n",
    "async for chunk in chain2.astream({\"person\": \"bill gates\", \"language\": \"english\"}, config={\"callbacks\":[langfuse_handler]}):\n",
    "    print(\"Async Streaming chunk:\", chunk)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5v6TUIrVGkn3"
   },
   "source": [
    "### ConversationChain\n",
    "\n",
    "We'll use a [session](https://langfuse.com/docs/tracing-features/sessions) in Langfuse to track this conversation with each invocation being a single trace.\n",
    "\n",
    "In addition to the traces of each run, you also get a conversation view of the entire session:\n",
    "\n",
    "![Session view of ConversationChain in Langfuse](https://langfuse.com/images/docs/langchain_session.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8HXyXC2LGga6"
   },
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_openai import OpenAI\n",
    "\n",
    "llm = OpenAI(temperature=0)\n",
    "\n",
    "conversation = ConversationChain(\n",
    "    llm=llm, memory=ConversationBufferMemory()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EWRj0qvKHLNE"
   },
   "outputs": [],
   "source": [
    "# Create a callback handler with a session\n",
    "langfuse_handler = CallbackHandler(session_id=\"conversation_chain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aIHmNekVHItt"
   },
   "outputs": [],
   "source": [
    "conversation.predict(input=\"Hi there!\", callbacks=[langfuse_handler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tsAunGSwHkrt"
   },
   "outputs": [],
   "source": [
    "conversation.predict(input=\"How to build great developer tools?\", callbacks=[langfuse_handler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m8O6hShcHsGe"
   },
   "outputs": [],
   "source": [
    "conversation.predict(input=\"Summarize your last response\", callbacks=[langfuse_handler])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FP5avhNb3TBH"
   },
   "source": [
    "### RetrievalQA\n",
    "\n",
    "![Trace of Langchain QA Retrieval in Langfuse](https://langfuse.com/images/docs/langchain_qa_retrieval.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wjiWEkRUFzCf"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"SERPAPI_API_KEY\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p0CgEPSlEpkC"
   },
   "outputs": [],
   "source": [
    "%pip install unstructured selenium langchain-chroma --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kHDVa-Ssb-KT"
   },
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import SeleniumURLLoader\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "langfuse_handler = CallbackHandler()\n",
    "\n",
    "urls = [\n",
    "    \"https://raw.githubusercontent.com/langfuse/langfuse-docs/main/public/state_of_the_union.txt\",\n",
    "]\n",
    "loader = SeleniumURLLoader(urls=urls)\n",
    "llm = OpenAI()\n",
    "documents = loader.load()\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "texts = text_splitter.split_documents(documents)\n",
    "embeddings = OpenAIEmbeddings()\n",
    "docsearch = Chroma.from_documents(texts, embeddings)\n",
    "query = \"What did the president say about Ketanji Brown Jackson\"\n",
    "chain = RetrievalQA.from_chain_type(\n",
    "    llm,\n",
    "    retriever=docsearch.as_retriever(search_kwargs={\"k\": 1}),\n",
    ")\n",
    "\n",
    "chain.invoke(query, config={\"callbacks\":[langfuse_handler]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JCmI0I20-sbI"
   },
   "source": [
    "### Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ReaHdQOT-S3n"
   },
   "outputs": [],
   "source": [
    "from langchain.agents import AgentExecutor, load_tools, create_openai_functions_agent\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain import hub\n",
    "\n",
    "langfuse_handler = CallbackHandler()\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "tools = load_tools([\"serpapi\"])\n",
    "prompt = hub.pull(\"hwchase17/openai-functions-agent\")\n",
    "agent = create_openai_functions_agent(llm, tools, prompt)\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools)\n",
    "\n",
    "agent_executor.invoke({\"input\": \"What is Langfuse?\"}, config={\"callbacks\":[langfuse_handler]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OIxwkX9p1ZR7"
   },
   "source": [
    "### AzureOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b43rIMig1ZR7"
   },
   "outputs": [],
   "source": [
    "os.environ[\"AZURE_OPENAI_ENDPOINT\"] = \"<Azure OpenAI endpoint>\"\n",
    "os.environ[\"AZURE_OPENAI_API_KEY\"] = \"<Azure OpenAI API key>\"\n",
    "os.environ[\"OPENAI_API_TYPE\"] = \"azure\"\n",
    "os.environ[\"OPENAI_API_VERSION\"] = \"2023-09-01-preview\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_lLdPwnr1ZR7"
   },
   "outputs": [],
   "source": [
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "langfuse_handler = CallbackHandler()\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"what is the city {person} is from?\")\n",
    "model = AzureChatOpenAI(\n",
    "    deployment_name=\"gpt-35-turbo\",\n",
    "    model_name=\"gpt-3.5-turbo\",\n",
    ")\n",
    "chain = prompt | model\n",
    "\n",
    "chain.invoke({\"person\": \"Satya Nadella\"}, config={\"callbacks\":[langfuse_handler]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZUenj0aca9qo"
   },
   "source": [
    "### Sequential Chain [Legacy]\n",
    "\n",
    "![Trace of Langchain Sequential Chain in Langfuse](https://langfuse.com/images/docs/langchain_chain.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dTagwV_cbFVr"
   },
   "outputs": [],
   "source": [
    "# further imports\n",
    "from langchain_openai import OpenAI\n",
    "from langchain.chains import LLMChain, SimpleSequentialChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "llm = OpenAI()\n",
    "template = \"\"\"You are a playwright. Given the title of play, it is your job to write a synopsis for that title.\n",
    "    Title: {title}\n",
    "    Playwright: This is a synopsis for the above play:\"\"\"\n",
    "prompt_template = PromptTemplate(input_variables=[\"title\"], template=template)\n",
    "synopsis_chain = LLMChain(llm=llm, prompt=prompt_template)\n",
    "template = \"\"\"You are a play critic from the New York Times. Given the synopsis of play, it is your job to write a review for that play.\n",
    "    Play Synopsis:\n",
    "    {synopsis}\n",
    "    Review from a New York Times play critic of the above play:\"\"\"\n",
    "prompt_template = PromptTemplate(input_variables=[\"synopsis\"], template=template)\n",
    "review_chain = LLMChain(llm=llm, prompt=prompt_template)\n",
    "overall_chain = SimpleSequentialChain(\n",
    "    chains=[synopsis_chain, review_chain],\n",
    ")\n",
    "\n",
    "# invoke\n",
    "review = overall_chain.invoke(\"Tragedy at sunset on the beach\", {\"callbacks\":[langfuse_handler]}) # add the handler to the run method\n",
    "# run [LEGACY]\n",
    "review = overall_chain.run(\"Tragedy at sunset on the beach\", callbacks=[langfuse_handler])# add the handler to the run method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AxQlUOmVPEwz"
   },
   "source": [
    "## Adding scores to traces\n",
    "\n",
    "In addition to the attributes automatically captured by the decorator, you can add others to use the full features of Langfuse.\n",
    "\n",
    "Two utility methods:\n",
    "*   `langfuse_context.update_current_observation`: Update the trace/span of the current function scope\n",
    "*   `langfuse_context.update_current_trace`: Update the trace itself, can also be called within any deeply nested span within the trace\n",
    "\n",
    "For details on available attributes, have a look at the [reference](https://python.reference.langfuse.com/langfuse/decorators#LangfuseDecorator.update_current_observation).\n",
    "\n",
    "Below is an example demonstrating how to enrich traces and observations with custom parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PudCopwEPFgh"
   },
   "outputs": [],
   "source": [
    "from langfuse.decorators import langfuse_context, observe\n",
    " \n",
    "@observe(as_type=\"generation\")\n",
    "def deeply_nested_llm_call():\n",
    "    # Enrich the current observation with a custom name, input, and output\n",
    "    langfuse_context.update_current_observation(\n",
    "        name=\"Deeply nested LLM call\", input=\"Ping?\", output=\"Pong!\"\n",
    "    )\n",
    "    # Set the parent trace's name from within a nested observation\n",
    "    langfuse_context.update_current_trace(\n",
    "        name=\"Trace name set from deeply_nested_llm_call\",\n",
    "        session_id=\"1234\",\n",
    "        user_id=\"5678\",\n",
    "        tags=[\"tag1\", \"tag2\"],\n",
    "        public=True\n",
    "    )\n",
    " \n",
    "@observe()\n",
    "def nested_span():\n",
    "    # Update the current span with a custom name and level\n",
    "    langfuse_context.update_current_observation(name=\"Nested Span\", level=\"WARNING\")\n",
    "    deeply_nested_llm_call()\n",
    " \n",
    "@observe()\n",
    "def main():\n",
    "    nested_span()\n",
    " \n",
    "# Execute the main function to generate the enriched trace\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the Langfuse platform the trace now shows with the updated name from the `deeply_nested_llm_call`, and the observations will be enriched with the appropriate data points.\n",
    "\n",
    "**Example trace:** https://cloud.langfuse.com/project/cloramnkj0002jz088vzn1ja4/traces/f16e0151-cca8-4d90-bccf-1d9ea0958afb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GEWWS8PGo4A1"
   },
   "source": [
    "## Interoperability with Langfuse Python SDK\n",
    "\n",
    "You can use this integration in combination with the `observe()` decorator from the Langfuse Python SDK. Thereby, you can trace non-Langchain code, combine multiple Langchain invocations in a single trace, and use the full functionality of the Langfuse Python SDK.\n",
    "\n",
    "The `langfuse_context.get_current_langchain_handler()` method exposes a LangChain callback handler in the context of a trace or span when using `decorators`. Learn more about Langfuse Tracing [here](https://langfuse.com/docs/tracing) and this functionality [here](https://langfuse.com/docs/sdk/python/decorators#langchain).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q1zlFuIimJfT"
   },
   "source": [
    "### How it works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Op7qwM0Y-1bp"
   },
   "outputs": [],
   "source": [
    "from langfuse.decorators import langfuse_context, observe\n",
    "\n",
    "# Create a trace via Langfuse decorators and get a Langchain Callback handler for it\n",
    "@observe() # automtically log function as a trace to Langfuse\n",
    "def main():\n",
    "    # update trace attributes (e.g, name, session_id, user_id)\n",
    "    langfuse_context.update_current_trace(\n",
    "        name=\"custom-trace\",\n",
    "        session_id=\"user-1234\",\n",
    "        user_id=\"session-1234\",\n",
    "    )\n",
    "    # get the langchain handler for the current trace\n",
    "    langfuse_context.get_current_langchain_handler()\n",
    "\n",
    "    # use the handler to trace langchain runs ...\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HRX2zFCOmwXH"
   },
   "source": [
    "### Example\n",
    "\n",
    "We'll run the same chain multiple times at different places within the hierarchy of a trace.\n",
    "\n",
    "```\n",
    "TRACE: person-locator\n",
    "|\n",
    "|-- SPAN: Chain (Alan Turing)\n",
    "|\n",
    "|-- SPAN: Physics\n",
    "|   |\n",
    "|   |-- SPAN: Chain (Albert Einstein)\n",
    "|   |\n",
    "|   |-- SPAN: Chain (Isaac Newton)\n",
    "|   |\n",
    "|   |-- SPAN: Favorites\n",
    "|   |   |\n",
    "|   |   |-- SPAN: Chain (Richard Feynman)\n",
    "```\n",
    "\n",
    "Setup chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ASq5sHErkmLB"
   },
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"what is the city {person} is from?\")\n",
    "model = ChatOpenAI()\n",
    "\n",
    "chain = prompt | model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fvJ1pv4MqzTi"
   },
   "source": [
    "Invoke it multiple times as part of a nested trace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CnHq-7QD3uAa"
   },
   "outputs": [],
   "source": [
    "from langfuse.decorators import langfuse_context, observe\n",
    "\n",
    "# On span \"Physics\".\"Favorites\"\n",
    "@observe()  # decorator to automatically log function as sub-span to Langfuse\n",
    "def favorites():\n",
    "    # get the langchain handler for the current sub-span\n",
    "    langfuse_handler = langfuse_context.get_current_langchain_handler()\n",
    "    # invoke chain with langfuse handler\n",
    "    chain.invoke({\"person\": \"Richard Feynman\"},\n",
    "                 config={\"callbacks\": [langfuse_handler]})\n",
    "\n",
    "# On span \"Physics\"\n",
    "@observe()  # decorator to automatically log function as span to Langfuse\n",
    "def physics():\n",
    "    # get the langchain handler for the current span\n",
    "    langfuse_handler = langfuse_context.get_current_langchain_handler()\n",
    "    # invoke chains with langfuse handler\n",
    "    chain.invoke({\"person\": \"Albert Einstein\"},\n",
    "                 config={\"callbacks\": [langfuse_handler]})\n",
    "    chain.invoke({\"person\": \"Isaac Newton\"},\n",
    "                 config={\"callbacks\": [langfuse_handler]})\n",
    "    favorites()\n",
    "\n",
    "# On trace\n",
    "@observe()  # decorator to automatically log function as trace to Langfuse\n",
    "def main():\n",
    "    # get the langchain handler for the current trace\n",
    "    langfuse_handler = langfuse_context.get_current_langchain_handler()\n",
    "    # invoke chain with langfuse handler\n",
    "    chain.invoke({\"person\": \"Alan Turing\"},\n",
    "                 config={\"callbacks\": [langfuse_handler]})\n",
    "    physics()\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MZ3q8iMDGOfd"
   },
   "source": [
    "View it in Langfuse\n",
    "\n",
    "![Trace of Nested Langchain Runs in Langfuse](https://langfuse.com/images/docs/langchain_python_trace_interoperability.png)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

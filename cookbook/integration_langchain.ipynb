{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x1oaA7XYGOfX"
      },
      "source": [
        "<!-- NOTEBOOK_METADATA source: \"⚠️ Jupyter Notebook\" title: \"Cookbook: Langchain Integration\" sidebarTitle: \"Cookbook: Langchain Integration\" logo: \"/images/integrations/langchain_icon.png\" description: \"Cookbook with examples of the Langfuse Integration for Langchain (Python).\" category: \"Integrations\" -->\n",
        "\n",
        "# Cookbook: Langchain Integration\n",
        "\n",
        "This is a cookbook with examples of the Langfuse Integration for Langchain (Python).\n",
        "\n",
        "Follow the [integration guide](https://langfuse.com/integrations/frameworks/langchain) to add this integration to your Langchain project. The integration also supports Langchain JS.\n",
        "\n",
        "## Step 1: Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YNyU6IzCZouE",
        "outputId": "234c71fb-f822-4b48-f4c0-94efe5f79305"
      },
      "outputs": [],
      "source": [
        "%pip install langfuse langchain langchain_openai langchain_community --upgrade --q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OpE57ujJZouE"
      },
      "source": [
        "## Step 2: Initialize Langfuse Callback Handler\n",
        "\n",
        "Next, set up your Langfuse API keys. You can get these keys by signing up for a free [Langfuse Cloud](https://cloud.langfuse.com/) account or by [self-hosting Langfuse](https://langfuse.com/self-hosting). These environment variables are essential for the Langfuse client to authenticate and send data to your Langfuse project."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dEdF-668ZouF"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Get keys for your project from the project settings page: https://cloud.langfuse.com\n",
        "os.environ[\"LANGFUSE_PUBLIC_KEY\"] = \"pk-lf-...\" \n",
        "os.environ[\"LANGFUSE_SECRET_KEY\"] = \"sk-lf-...\" \n",
        "os.environ[\"LANGFUSE_HOST\"] = \"https://cloud.langfuse.com\" # 🇪🇺 EU region\n",
        "# os.environ[\"LANGFUSE_HOST\"] = \"https://us.cloud.langfuse.com\" # 🇺🇸 US region\n",
        "\n",
        "# Your openai key\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"sk-proj-...\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "With the environment variables set, we can now initialize the Langfuse Client and the CallbackHandler. You can also use [constructor arguments](/docs/sdk/python/setup#initialize-client) to initialize the Langfuse client."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "divRadPqZouF"
      },
      "outputs": [],
      "source": [
        "  from langfuse import get_client\n",
        "  from langfuse.langchain import CallbackHandler\n",
        " \n",
        "  # Initialize Langfuse client\n",
        "  langfuse = get_client()\n",
        " \n",
        "  # Initialize Langfuse CallbackHandler for Langchain (tracing)\n",
        "  langfuse_handler = CallbackHandler()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ZXRf2FZXEXV"
      },
      "source": [
        "## Examples\n",
        "\n",
        "### LangChain Agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "3-HEia6gNoAr",
        "outputId": "ab223b99-4719-420c-ffe7-5444e4b67806"
      },
      "outputs": [],
      "source": [
        "from langchain.agents import create_agent\n",
        "\n",
        "def add_numbers(a: int, b: int) -> int:\n",
        "    \"\"\"Add two numbers together and return the result.\"\"\"\n",
        "    return a + b\n",
        "\n",
        "agent = create_agent(\n",
        "    model=\"openai:gpt-5-mini\",\n",
        "    tools=[add_numbers],\n",
        "    system_prompt=\"You are a helpful math tutor who can do calculations using the provided tools.\",\n",
        ")\n",
        "\n",
        "# Run the agent\n",
        "agent.invoke(\n",
        "    {\"messages\": [{\"role\": \"user\", \"content\": \"what is 42 + 58?\"}]},\n",
        "    config={\"callbacks\": [langfuse_handler]}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**See Traces in Langfuse:**\n",
        "\n",
        "After executing the application, navigate to your [Langfuse Trace Table](https://cloud.langfuse.com/). You will find detailed traces of the application’s execution, providing insights into the LLM calls, retrieval operations, inputs, outputs, and performance metrics.\n",
        "\n",
        "![Example trace in Langfuse](https://langfuse.com/images/cookbook/integration-langchain/langchain-example-trace.png)\n",
        "\n",
        "[Example trace in the Langfuse UI](https://cloud.langfuse.com/project/cloramnkj0002jz088vzn1ja4/traces/f409d99f1204374f3c12a1b44441f21e?timestamp=2025-10-21T14%3A01%3A18.661Z)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NlBSpILFXEXV"
      },
      "source": [
        "#### Runnable methods\n",
        "\n",
        "Runnables are units of work that can be invoked, batched, streamed, transformed and composed.\n",
        "\n",
        "The examples below show how to use the following methods with Langfuse:\n",
        "\n",
        "- invoke/ainvoke: Transforms a single input into an output.\n",
        "- batch/abatch: Efficiently transforms multiple inputs into outputs.\n",
        "- stream/astream: Streams output from a single input as it’s produced."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y8N8pybGXEXV",
        "outputId": "df6d23fc-ed65-4fd2-d0d6-3b9f97e3a497"
      },
      "outputs": [],
      "source": [
        "# Async Invoke\n",
        "await chain2.ainvoke({\"person\": \"biden\", \"language\": \"german\"}, config={\"callbacks\":[langfuse_handler]})\n",
        "\n",
        "# Batch\n",
        "chain2.batch([{\"person\": \"elon musk\", \"language\": \"english\"}, {\"person\": \"mark zuckerberg\", \"language\": \"english\"}], config={\"callbacks\":[langfuse_handler]})\n",
        "\n",
        "# Async Batch\n",
        "await chain2.abatch([{\"person\": \"jeff bezos\", \"language\": \"english\"}, {\"person\": \"tim cook\", \"language\": \"english\"}], config={\"callbacks\":[langfuse_handler]})\n",
        "\n",
        "# Stream\n",
        "for chunk in chain2.stream({\"person\": \"steve jobs\", \"language\": \"english\"}, config={\"callbacks\":[langfuse_handler]}):\n",
        "    print(\"Streaming chunk:\", chunk)\n",
        "\n",
        "# Async Stream\n",
        "async for chunk in chain2.astream({\"person\": \"bill gates\", \"language\": \"english\"}, config={\"callbacks\":[langfuse_handler]}):\n",
        "    print(\"Async Streaming chunk:\", chunk)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FP5avhNb3TBH"
      },
      "source": [
        "### RetrievalQA\n",
        "\n",
        "![Trace of Langchain QA Retrieval in Langfuse](https://langfuse.com/images/cookbook/integration_langchain/langchain_qa_retrieval.png)\n",
        "\n",
        "[Example trace in Langfuse](https://cloud.langfuse.com/project/cloramnkj0002jz088vzn1ja4/traces/84e1ac07dedbce3b2a236b6ece6950d9?timestamp=2025-06-11T09:29:10.248Z&display=details)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "wjiWEkRUFzCf"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"SERPAPI_API_KEY\"] = \"...\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p0CgEPSlEpkC",
        "outputId": "36c800af-025d-407e-eca1-c215bba62cd2"
      },
      "outputs": [],
      "source": [
        "%pip install unstructured selenium langchain-chroma --upgrade"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 438
        },
        "id": "kHDVa-Ssb-KT",
        "outputId": "efab8170-76b9-412e-c086-365a16f065a9"
      },
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import SeleniumURLLoader\n",
        "from langchain_chroma import Chroma\n",
        "from langchain_text_splitters import CharacterTextSplitter\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "langfuse_handler = CallbackHandler()\n",
        "\n",
        "urls = [\n",
        "    \"https://raw.githubusercontent.com/langfuse/langfuse-docs/main/public/state_of_the_union.txt\",\n",
        "]\n",
        "loader = SeleniumURLLoader(urls=urls)\n",
        "llm = OpenAI()\n",
        "documents = loader.load()\n",
        "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
        "texts = text_splitter.split_documents(documents)\n",
        "embeddings = OpenAIEmbeddings()\n",
        "docsearch = Chroma.from_documents(texts, embeddings)\n",
        "query = \"What did the president say about Ketanji Brown Jackson\"\n",
        "chain = RetrievalQA.from_chain_type(\n",
        "    llm,\n",
        "    retriever=docsearch.as_retriever(search_kwargs={\"k\": 1}),\n",
        ")\n",
        "\n",
        "chain.invoke(query, config={\"callbacks\":[langfuse_handler]})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OIxwkX9p1ZR7"
      },
      "source": [
        "### AzureOpenAI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b43rIMig1ZR7"
      },
      "outputs": [],
      "source": [
        "os.environ[\"AZURE_OPENAI_ENDPOINT\"] = \"<Azure OpenAI endpoint>\"\n",
        "os.environ[\"AZURE_OPENAI_API_KEY\"] = \"<Azure OpenAI API key>\"\n",
        "os.environ[\"OPENAI_API_TYPE\"] = \"azure\"\n",
        "os.environ[\"OPENAI_API_VERSION\"] = \"2023-09-01-preview\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_lLdPwnr1ZR7"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import AzureChatOpenAI\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langfuse.langchain import CallbackHandler\n",
        " \n",
        "# Initialize Langfuse CallbackHandler for Langchain (tracing)\n",
        "langfuse_handler = CallbackHandler()\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(\"what is the city {person} is from?\")\n",
        "model = AzureChatOpenAI(\n",
        "    deployment_name=\"gpt-4o\",\n",
        "    model_name=\"gpt-4o\",\n",
        ")\n",
        "chain = prompt | model\n",
        "\n",
        "chain.invoke({\"person\": \"Satya Nadella\"}, config={\"callbacks\":[langfuse_handler]})"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

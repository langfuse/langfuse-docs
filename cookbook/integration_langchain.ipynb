{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KlceIPalN3QR"
      },
      "source": [
        "---\n",
        "description: Langchain users can integrated with Langfuse in seconds using the integration\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mqBspBzuRk9C"
      },
      "source": [
        "# Langchain integration (Python)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "05afP0VbYHX0"
      },
      "source": [
        "Langfuse integrates with Langchain using the [Langchain Callbacks](https://python.langchain.com/docs/modules/callbacks/). Thereby, the Langfuse SDK automatically creates a nested trace for the abstractions offered by Langchain.\n",
        "\n",
        "Add the handler as a callback when running your Langchain model/chain/agent:\n",
        "\n",
        "```python /callbacks=[handler]/\n",
        "# Initialize Langfuse handler\n",
        "from langfuse.callback import CallbackHandler\n",
        "handler = CallbackHandler(PUBLIC_KEY, SECRET_KEY)\n",
        "\n",
        "# Setup Langchain\n",
        "from langchain.chains import LLMChain\n",
        "...\n",
        "chain = LLMChain(llm=llm, prompt=prompt, callbacks=[handler])\n",
        "\n",
        "# Add Langfuse handler as callback\n",
        "chain.run(input=\"<user_input\", callbacks=[handler])\n",
        "```\n",
        "\n",
        "Langchain expression language (LCEL)\n",
        "```python /config={\"callbacks\":[handler]}/\n",
        "chain.invoke(input, config={\"callbacks\":[handler]})\n",
        "```\n",
        "\n",
        "---\n",
        " **_In case of missing events or tokens:_**\n",
        "\n",
        "There are two ways to integrate callbacks into Langchain:\n",
        "- *Constructor Callbacks*: Set when initializing an object, like `LLMChain(callbacks=[handler])` or `ChatOpenAI(callbacks=[handler])`. This approach will use the callback for every call made on that specific object. However, it won't apply to its child objects, making it limited in scope.\n",
        "- *Request Callbacks*: Defined when issuing a request, like `chain.run(input, callbacks=[handler])` and `chain.invoke(input, config={\"callbacks\":[handler]})`. This not only uses the callback for that specific request but also for any subsequent sub-requests it triggers.\n",
        "\n",
        "For comprehensive data capture especially for complex chains or agents, it's advised to use the both approaches, as demonstrated above [docs](https://python.langchain.com/docs/modules/callbacks/#where-to-pass-in-callbacks).\n",
        "\n",
        "---\n",
        "\n",
        "The Langfuse `CallbackHandler` tracks the following actions when using Langchain:\n",
        "\n",
        "- Chains: `on_chain_start`, `on_chain_end`. `on_chain_error`\n",
        "- Agents: `on_agent_start`, `on_agent_action`, `on_agent_finish`, `on_agent_end`\n",
        "- Tools: `on_tool_start`, `on_tool_end`, `on_tool_error`\n",
        "- Retriever: `on_retriever_start`, `on_retriever_end`\n",
        "- ChatModel: `on_chat_model_start`,\n",
        "- LLM: `on_llm_start`, `on_llm_end`, `on_llm_error`\n",
        "\n",
        "Missing some useful information/context in Langfuse? Join the [Discord](/discord) or share your feedback directly with us: feedback@langfuse.com"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lq3SJ-XHN3QT"
      },
      "source": [
        "## Notebook Setup\n",
        "\n",
        "<NotebookBanner src=\"cookbook/integration_langchain.ipynb\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AbSpd5EiZouE"
      },
      "source": [
        "### 1. Initializing the Langfuse Callback handler\n",
        "\n",
        "The Langfuse SDKs are hosted on the pypi index."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YNyU6IzCZouE"
      },
      "outputs": [],
      "source": [
        "%pip install langfuse langchain openai --upgrade"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OpE57ujJZouE"
      },
      "source": [
        "Initialize the client with api keys and optionally your environment. In the example we are using the cloud environment which is also the default.\n",
        "\n",
        "Alternatively, you can also pass them as arguments to the `CallbackHandler` constructor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dEdF-668ZouF"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# get keys for your project from https://cloud.langfuse.com\n",
        "os.environ[\"LANGFUSE_PUBLIC_KEY\"] = \"\"\n",
        "os.environ[\"LANGFUSE_SECRET_KEY\"] = \"\"\n",
        "\n",
        "# your openai key\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
        "\n",
        "# Your host, defaults to https://cloud.langfuse.com\n",
        "# For US data region, set to \"https://us.cloud.langfuse.com\"\n",
        "# os.environ[\"LANGFUSE_HOST\"] = \"http://localhost:3000\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "divRadPqZouF"
      },
      "outputs": [],
      "source": [
        "from langfuse.callback import CallbackHandler\n",
        "\n",
        "handler = CallbackHandler()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8FVbg1RWoT8W"
      },
      "outputs": [],
      "source": [
        "# checks the SDK connection with the server.\n",
        "handler.auth_check()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FzsO2Ci4EjJS"
      },
      "source": [
        "### 2. Langchain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L9u5QMVCbPtR"
      },
      "outputs": [],
      "source": [
        "# further imports\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.chains import LLMChain, SimpleSequentialChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langfuse.callback import CallbackHandler"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KAW-Gt4mN3QV"
      },
      "source": [
        "## Examples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZUenj0aca9qo"
      },
      "source": [
        "### 1. Sequential Chain\n",
        "\n",
        "![Trace of Langchain Sequential Chain in Langfuse](https://langfuse.com/images/docs/langchain_chain.jpg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dTagwV_cbFVr"
      },
      "outputs": [],
      "source": [
        "llm = OpenAI()\n",
        "template = \"\"\"You are a playwright. Given the title of play, it is your job to write a synopsis for that title.\n",
        "    Title: {title}\n",
        "    Playwright: This is a synopsis for the above play:\"\"\"\n",
        "\n",
        "prompt_template = PromptTemplate(input_variables=[\"title\"], template=template)\n",
        "synopsis_chain = LLMChain(llm=llm, prompt=prompt_template)\n",
        "\n",
        "template = \"\"\"You are a play critic from the New York Times. Given the synopsis of play, it is your job to write a review for that play.\n",
        "\n",
        "    Play Synopsis:\n",
        "    {synopsis}\n",
        "    Review from a New York Times play critic of the above play:\"\"\"\n",
        "prompt_template = PromptTemplate(input_variables=[\"synopsis\"], template=template)\n",
        "review_chain = LLMChain(llm=llm, prompt=prompt_template)\n",
        "\n",
        "overall_chain = SimpleSequentialChain(\n",
        "    chains=[synopsis_chain, review_chain],\n",
        ")\n",
        "review = overall_chain.run(\"Tragedy at sunset on the beach\", callbacks=[handler]) # add the handler to the run method\n",
        "handler.langfuse.flush()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QvRWPsZ-NoAr"
      },
      "source": [
        "### 2. Sequential Chain in Langchain Expression Language (LCEL)\n",
        "\n",
        "![Trace of Langchain LCEL](https://langfuse.com/images/docs/langchain_LCEL.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3-HEia6gNoAr"
      },
      "outputs": [],
      "source": [
        "from operator import itemgetter\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.schema import StrOutputParser\n",
        "\n",
        "handler = CallbackHandler()\n",
        "\n",
        "prompt1 = ChatPromptTemplate.from_template(\"what is the city {person} is from?\")\n",
        "prompt2 = ChatPromptTemplate.from_template(\n",
        "    \"what country is the city {city} in? respond in {language}\"\n",
        ")\n",
        "\n",
        "model = ChatOpenAI()\n",
        "\n",
        "chain1 = prompt1 | model | StrOutputParser()\n",
        "\n",
        "chain2 = (\n",
        "    {\"city\": chain1, \"language\": itemgetter(\"language\")}\n",
        "    | prompt2\n",
        "    | model\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "chain2.invoke({\"person\": \"obama\", \"language\": \"spanish\"}, config={\"callbacks\":[handler]})\n",
        "handler.get_trace_url()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FP5avhNb3TBH"
      },
      "source": [
        "### 3. QA Retrieval\n",
        "\n",
        "![Trace of Langchain QA Retrieval in Langfuse](https://langfuse.com/images/docs/langchain_qa_retrieval.jpg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wjiWEkRUFzCf"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"SERPAPI_API_KEY\"] = ''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p0CgEPSlEpkC"
      },
      "outputs": [],
      "source": [
        "%pip install unstructured chromadb tiktoken google-search-results python-magic --upgrade"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kHDVa-Ssb-KT"
      },
      "outputs": [],
      "source": [
        "from langchain.document_loaders import UnstructuredURLLoader\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "handler = CallbackHandler()\n",
        "urls = [\n",
        "    \"https://raw.githubusercontent.com/langfuse/langfuse-docs/main/public/state_of_the_union.txt\",\n",
        "]\n",
        "\n",
        "loader = UnstructuredURLLoader(urls=urls)\n",
        "\n",
        "llm = OpenAI()\n",
        "\n",
        "documents = loader.load()\n",
        "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
        "texts = text_splitter.split_documents(documents)\n",
        "\n",
        "embeddings = OpenAIEmbeddings()\n",
        "docsearch = Chroma.from_documents(texts, embeddings)\n",
        "\n",
        "query = \"What did the president say about Ketanji Brown Jackson\"\n",
        "\n",
        "chain = RetrievalQA.from_chain_type(\n",
        "    llm,\n",
        "    retriever=docsearch.as_retriever(search_kwargs={\"k\": 1}),\n",
        ")\n",
        "\n",
        "result = chain.run(query, callbacks=[handler])\n",
        "\n",
        "print(result)\n",
        "\n",
        "handler.langfuse.flush()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JCmI0I20-sbI"
      },
      "source": [
        "\n",
        "\n",
        "![Trace of Langchain Agent in Langfuse](https://langfuse.com/images/docs/langchain_agent.jpg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ReaHdQOT-S3n"
      },
      "outputs": [],
      "source": [
        "from langchain.agents import AgentType, initialize_agent, load_tools\n",
        "\n",
        "\n",
        "handler = CallbackHandler()\n",
        "\n",
        "llm = OpenAI()\n",
        "\n",
        "tools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)\n",
        "\n",
        "agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\n",
        "\n",
        "result = agent.run(\"Who is Leo DiCaprio's girlfriend? What is her current age raised to the 0.43 power?\", callbacks=[handler])\n",
        "\n",
        "handler.langfuse.flush()\n",
        "\n",
        "print(\"output variable: \", result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AxQlUOmVPEwz"
      },
      "source": [
        "## Adding scores\n",
        "\n",
        "To add [scores](/docs/scores) to traces created with the Langchain integration, access the traceId via `handler.get_trace_id()`\n",
        "\n",
        "\n",
        "### Example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PudCopwEPFgh"
      },
      "outputs": [],
      "source": [
        "from langfuse import Langfuse\n",
        "\n",
        "\n",
        "# Trace langchain run via the Langfuse CallbackHandler as shown above\n",
        "\n",
        "# Get id of created trace\n",
        "trace_id = handler.get_trace_id()\n",
        "\n",
        "# Add score, e.g. via the Python SDK\n",
        "langfuse = Langfuse()\n",
        "trace = langfuse.score(\n",
        "    trace_id=trace_id,\n",
        "    name=\"user-explicit-feedback\",\n",
        "    value=1,\n",
        "    comment=\"I like how personalized the response is\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GEWWS8PGo4A1"
      },
      "source": [
        "## Adding trace as context to a Langchain handler\n",
        "\n",
        "It is also possible to generate a Langchain handler based on a trace. This can help to add context such as a specific `user_id`, `name`` or `metadata`. All the Langchain observations will be collected on that trace.\n",
        "\n",
        "To do that, we first need to initialize the [Python SDK](/docs/sdk/python), create a `trace`, and finally create the handler."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3sBgSeuMp5o4"
      },
      "outputs": [],
      "source": [
        "import uuid\n",
        "import os\n",
        "\n",
        "from langfuse.client import Langfuse\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "\n",
        "langfuse = Langfuse()\n",
        "\n",
        "trace = langfuse.trace(name=\"synopsis-application\", user_id=\"user-1234\")\n",
        "\n",
        "handler = trace.get_langchain_handler()\n",
        "\n",
        "llm = OpenAI()\n",
        "template = \"\"\"You are a playwright. Given the title of play, it is your job to write a synopsis for that title.\n",
        "    Title: {title}\n",
        "    Playwright: This is a synopsis for the above play:\"\"\"\n",
        "\n",
        "prompt_template = PromptTemplate(input_variables=[\"title\"], template=template)\n",
        "synopsis_chain = LLMChain(llm=llm, prompt=prompt_template)\n",
        "\n",
        "synopsis_chain.run(\"Tragedy at sunset on the beach\", callbacks=[handler])\n",
        "\n",
        "langfuse.flush()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZEYV-3Vw5jmD"
      },
      "source": [
        "## Configuring multiple runs per trace\n",
        "\n",
        "Sometimes it is required to have multiple Langchain runs in one 'Trace'. For this, we provide the 'setNextSpan' function to configure the 'id' of the parent span of the next run. This can be helpful to create scores for the different runs.\n",
        "\n",
        "The example below will result in the following trace:\n",
        "\n",
        "```\n",
        "TRACE (id: trace_id)\n",
        "|\n",
        "|-- SPAN: LLMChain (id: generated by Langfuse)\n",
        "|   |\n",
        "|   |-- GENERATION: OpenAI (id: generated by Langfuse)\n",
        "|\n",
        "|-- SPAN: LLMChain (id: generated by 'next_span_id')\n",
        "|   |\n",
        "|   |-- GENERATION: OpenAI (id: generated by Langfuse)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zL0ml89d42EX"
      },
      "outputs": [],
      "source": [
        "import uuid\n",
        "import os\n",
        "\n",
        "from langfuse.client import Langfuse\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "\n",
        "langfuse = Langfuse()\n",
        "\n",
        "trace_id = str(uuid.uuid4())\n",
        "trace = langfuse.trace(id=trace_id)\n",
        "\n",
        "handler = trace.get_langchain_handler()\n",
        "\n",
        "llm = OpenAI()\n",
        "template = \"\"\"You are a playwright. Given the title of play, it is your job to write a synopsis for that title.\n",
        "    Title: {title}\n",
        "    Playwright: This is a synopsis for the above play:\"\"\"\n",
        "\n",
        "prompt_template = PromptTemplate(input_variables=[\"title\"], template=template)\n",
        "synopsis_chain = LLMChain(llm=llm, prompt=prompt_template)\n",
        "\n",
        "synopsis_chain.run(\"Tragedy at sunset on the beach\", callbacks=[handler])\n",
        "\n",
        "# configure the next span id\n",
        "next_span_id = str(uuid.uuid4())\n",
        "handler.setNextSpan(next_span_id)\n",
        "\n",
        "synopsis_chain.run(\"Comedy at sunset on the beach\", callbacks=[handler])\n",
        "\n",
        "langfuse.flush()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Upgrading from v1.x.x to v2.x.x\n",
        "\n",
        "The `CallbackHandler` can be used in multiple invocations of a Langchain chain as shown below.\n",
        "\n",
        "```python\n",
        "\n",
        "from langfuse.callback import CallbackHandler\n",
        "handler = CallbackHandler(PUBLIC_KEY, SECRET_KEY)\n",
        "\n",
        "# Setup Langchain\n",
        "from langchain.chains import LLMChain\n",
        "...\n",
        "chain = LLMChain(llm=llm, prompt=prompt, callbacks=[handler])\n",
        "\n",
        "# Add Langfuse handler as callback\n",
        "chain.run(input=\"<first_user_input>\", callbacks=[handler])\n",
        "chain.run(input=\"<second_user_input>\", callbacks=[handler])\n",
        "\n",
        "```\n",
        "So far, invoking the chain multiple times would group the observations in one trace.\n",
        "\n",
        "```bash\n",
        "TRACE\n",
        "|\n",
        "|-- SPAN: Retrieval\n",
        "|   |\n",
        "|   |-- SPAN: LLM Chain\n",
        "|   |   |\n",
        "|   |   |-- GENERATION: ChatOpenAi\n",
        "|-- SPAN: Retrieval\n",
        "|   |\n",
        "|   |-- SPAN: LLM Chain\n",
        "|   |   |\n",
        "|   |   |-- GENERATION: ChatOpenAi\n",
        "```\n",
        "\n",
        "We changed this, so that each invocation will end up on its own trace. This allows us to derive the user inputs and outputs to Langchain applications. If you still want to group multiple invocations on one trace, you can use [this](https://langfuse.com/docs/langchain/python#adding-trace-as-context-to-a-langchain-handler) approach.\n",
        "\n",
        "```bash\n",
        "TRACE_1\n",
        "|\n",
        "|-- SPAN: Retrieval\n",
        "|   |\n",
        "|   |-- SPAN: LLM Chain\n",
        "|   |   |\n",
        "|   |   |-- GENERATION: ChatOpenAi\n",
        "\n",
        "TRACE_2\n",
        "|\n",
        "|-- SPAN: Retrieval\n",
        "|   |\n",
        "|   |-- SPAN: LLM Chain\n",
        "|   |   |\n",
        "|   |   |-- GENERATION: ChatOpenAi\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

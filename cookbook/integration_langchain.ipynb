{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KlceIPalN3QR"
      },
      "source": [
        "---\n",
        "description: Open source observability for your Langchain (Python) application. Automatically captures rich traces and metrics.\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mqBspBzuRk9C"
      },
      "source": [
        "# Langchain Integration (Python)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "05afP0VbYHX0"
      },
      "source": [
        "Langfuse integrates with Langchain using [Langchain Callbacks](https://python.langchain.com/docs/modules/callbacks/). Thereby, the Langfuse SDK automatically creates a nested trace for the abstractions offered by Langchain.\n",
        "\n",
        "## Integration\n",
        "\n",
        "Simply add the Langfuse handler as a callback when running your Langchain model/chain/agent to start capturing traces from your executions:\n",
        "\n",
        "```python\n",
        "# Initialize Langfuse handler\n",
        "from langfuse.callback import CallbackHandler\n",
        "\n",
        "langfuse_handler = CallbackHandler(\n",
        "    public_key=LANGFUSE_PUBLIC_KEY, secret_key=LANGFUSE_SECRET_KEY\n",
        ")\n",
        "\n",
        "# Setup Langchain\n",
        "from langchain.chains import LLMChain\n",
        "...\n",
        "chain = LLMChain(llm=llm, prompt=prompt)\n",
        "\n",
        "# Add Langfuse handler as callback to `run` or `invoke`\n",
        "chain.run(input=\"<user_input>\", callbacks=[langfuse_handler])\n",
        "chain.invoke({\"input\": \"<user_input>\"}, config={\"callbacks\": [langfuse_handler]})\n",
        "```\n",
        "\n",
        "[Langchain expression language](https://python.langchain.com/docs/expression_language/) (LCEL)\n",
        "\n",
        "```python\n",
        "chain = prompt | llm\n",
        "chain.invoke({\"input\": \"<user_input>\"}, config={\"callbacks\": [langfuse_handler]})\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "The Langfuse `CallbackHandler` tracks the following actions when using Langchain:\n",
        "\n",
        "- Chains: `on_chain_start`, `on_chain_end`. `on_chain_error`\n",
        "- Agents: `on_agent_start`, `on_agent_action`, `on_agent_finish`, `on_agent_end`\n",
        "- Tools: `on_tool_start`, `on_tool_end`, `on_tool_error`\n",
        "- Retriever: `on_retriever_start`, `on_retriever_end`\n",
        "- ChatModel: `on_chat_model_start`,\n",
        "- LLM: `on_llm_start`, `on_llm_end`, `on_llm_error`\n",
        "\n",
        "Missing some useful information/context in Langfuse? Join the [Discord](/discord) or share your feedback directly with us: feedback@langfuse.com"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lq3SJ-XHN3QT"
      },
      "source": [
        "## Example Cookbook\n",
        "\n",
        "<NotebookBanner src=\"cookbook/integration_langchain.ipynb\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AbSpd5EiZouE"
      },
      "source": [
        "### Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YNyU6IzCZouE",
        "outputId": "532a93e9-3c1e-41dd-a1d4-8878f4f54764"
      },
      "outputs": [],
      "source": [
        "%pip install langfuse langchain langchain_openai --upgrade"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OpE57ujJZouE"
      },
      "source": [
        "Initialize the Langfuse client with your API keys from the project settings in the Langfuse UI and add them to your environment.\n",
        "\n",
        "Alternatively, you may also pass them as arguments to the `CallbackHandler` constructor, but make sure not to commit any keys to your repository."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dEdF-668ZouF"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# get keys for your project from https://cloud.langfuse.com\n",
        "os.environ[\"LANGFUSE_PUBLIC_KEY\"] = \"pk-lf-*****\"\n",
        "os.environ[\"LANGFUSE_SECRET_KEY\"] = \"sk-lf-*****\"\n",
        "os.environ[\"LANGFUSE_HOST\"] = \"https://cloud.langfuse.com\" # for EU data region\n",
        "# os.environ[\"LANGFUSE_HOST\"] = \"https://us.cloud.langfuse.com\" # for US data region\n",
        "\n",
        "# your openai key\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "divRadPqZouF"
      },
      "outputs": [],
      "source": [
        "from langfuse.callback import CallbackHandler\n",
        "\n",
        "langfuse_handler = CallbackHandler()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8FVbg1RWoT8W",
        "outputId": "bae0cf75-03cb-416d-b497-f522d894ba85"
      },
      "outputs": [],
      "source": [
        "# Tests the SDK connection with the server\n",
        "langfuse_handler.auth_check()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KAW-Gt4mN3QV"
      },
      "source": [
        "### Examples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZUenj0aca9qo"
      },
      "source": [
        "#### 1. Sequential Chain\n",
        "\n",
        "![Trace of Langchain Sequential Chain in Langfuse](https://langfuse.com/images/docs/langchain_chain.jpg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dTagwV_cbFVr",
        "outputId": "79a4d27c-2f78-4374-ba54-e6a329fca38b"
      },
      "outputs": [],
      "source": [
        "# further imports\n",
        "from langchain_openai import OpenAI\n",
        "from langchain.chains import LLMChain, SimpleSequentialChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "llm = OpenAI()\n",
        "template = \"\"\"You are a playwright. Given the title of play, it is your job to write a synopsis for that title.\n",
        "    Title: {title}\n",
        "    Playwright: This is a synopsis for the above play:\"\"\"\n",
        "prompt_template = PromptTemplate(input_variables=[\"title\"], template=template)\n",
        "synopsis_chain = LLMChain(llm=llm, prompt=prompt_template)\n",
        "template = \"\"\"You are a play critic from the New York Times. Given the synopsis of play, it is your job to write a review for that play.\n",
        "    Play Synopsis:\n",
        "    {synopsis}\n",
        "    Review from a New York Times play critic of the above play:\"\"\"\n",
        "prompt_template = PromptTemplate(input_variables=[\"synopsis\"], template=template)\n",
        "review_chain = LLMChain(llm=llm, prompt=prompt_template)\n",
        "overall_chain = SimpleSequentialChain(\n",
        "    chains=[synopsis_chain, review_chain],\n",
        ")\n",
        "\n",
        "# invoke\n",
        "review = overall_chain.invoke(\"Tragedy at sunset on the beach\", {\"callbacks\":[langfuse_handler]}) # add the handler to the run method\n",
        "# run\n",
        "review = overall_chain.run(\"Tragedy at sunset on the beach\", callbacks=[langfuse_handler]) # add the handler to the run method"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QvRWPsZ-NoAr"
      },
      "source": [
        "#### 2. Sequential Chain in Langchain Expression Language (LCEL)\n",
        "\n",
        "![Trace of Langchain LCEL](https://langfuse.com/images/docs/langchain_LCEL.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "3-HEia6gNoAr",
        "outputId": "893a9af7-4ec4-441c-dd30-df8421cc76af"
      },
      "outputs": [],
      "source": [
        "from operator import itemgetter\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.schema import StrOutputParser\n",
        "\n",
        "langfuse_handler = CallbackHandler()\n",
        "\n",
        "prompt1 = ChatPromptTemplate.from_template(\"what is the city {person} is from?\")\n",
        "prompt2 = ChatPromptTemplate.from_template(\n",
        "    \"what country is the city {city} in? respond in {language}\"\n",
        ")\n",
        "model = ChatOpenAI()\n",
        "chain1 = prompt1 | model | StrOutputParser()\n",
        "chain2 = (\n",
        "    {\"city\": chain1, \"language\": itemgetter(\"language\")}\n",
        "    | prompt2\n",
        "    | model\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "chain2.invoke({\"person\": \"obama\", \"language\": \"spanish\"}, config={\"callbacks\":[langfuse_handler]})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FP5avhNb3TBH"
      },
      "source": [
        "#### 3. RetrievalQA\n",
        "\n",
        "![Trace of Langchain QA Retrieval in Langfuse](https://langfuse.com/images/docs/langchain_qa_retrieval.jpg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wjiWEkRUFzCf"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"SERPAPI_API_KEY\"] = \"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p0CgEPSlEpkC",
        "outputId": "824a0dea-b1be-4d8e-f86b-7ce7111edf5d"
      },
      "outputs": [],
      "source": [
        "%pip install unstructured chromadb tiktoken google-search-results python-magic langchainhub --upgrade"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kHDVa-Ssb-KT",
        "outputId": "498b052d-83b7-4878-f6e3-e4083aa9768b"
      },
      "outputs": [],
      "source": [
        "from langchain.document_loaders import UnstructuredURLLoader\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "langfuse_handler = CallbackHandler()\n",
        "\n",
        "urls = [\n",
        "    \"https://raw.githubusercontent.com/langfuse/langfuse-docs/main/public/state_of_the_union.txt\",\n",
        "]\n",
        "loader = UnstructuredURLLoader(urls=urls)\n",
        "llm = OpenAI()\n",
        "documents = loader.load()\n",
        "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
        "texts = text_splitter.split_documents(documents)\n",
        "embeddings = OpenAIEmbeddings()\n",
        "docsearch = Chroma.from_documents(texts, embeddings)\n",
        "query = \"What did the president say about Ketanji Brown Jackson\"\n",
        "chain = RetrievalQA.from_chain_type(\n",
        "    llm,\n",
        "    retriever=docsearch.as_retriever(search_kwargs={\"k\": 1}),\n",
        ")\n",
        "\n",
        "chain.invoke(query, config={\"callbacks\":[langfuse_handler]})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JCmI0I20-sbI"
      },
      "source": [
        "\n",
        "\n",
        "![Trace of Langchain Agent in Langfuse](https://langfuse.com/images/docs/langchain_agent.jpg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ReaHdQOT-S3n",
        "outputId": "9ad1bdf9-6731-407e-c4c2-4a6e900d774b"
      },
      "outputs": [],
      "source": [
        "from langchain.agents import AgentExecutor, load_tools, create_openai_functions_agent\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain import hub\n",
        "\n",
        "langfuse_handler = CallbackHandler()\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
        "tools = load_tools([\"serpapi\"])\n",
        "prompt = hub.pull(\"hwchase17/openai-functions-agent\")\n",
        "agent = create_openai_functions_agent(llm, tools, prompt)\n",
        "agent_executor = AgentExecutor(agent=agent, tools=tools)\n",
        "\n",
        "agent_executor.invoke({\"input\": \"What is Langfuse?\"}, config={\"callbacks\":[langfuse_handler]})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AxQlUOmVPEwz"
      },
      "source": [
        "### Adding scores\n",
        "\n",
        "To add [scores](/docs/scores) to traces created with the Langchain integration, access the traceId via `langfuse_handler.get_trace_id()`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PudCopwEPFgh"
      },
      "outputs": [],
      "source": [
        "from langfuse import Langfuse\n",
        "\n",
        "# Trace langchain run via the Langfuse CallbackHandler as shown above\n",
        "\n",
        "# Get id of the last created trace\n",
        "trace_id = langfuse_handler.get_trace_id()\n",
        "\n",
        "# Add score, e.g. via the Python SDK\n",
        "langfuse = Langfuse()\n",
        "trace = langfuse.score(\n",
        "    trace_id=trace_id,\n",
        "    name=\"user-explicit-feedback\",\n",
        "    value=1,\n",
        "    comment=\"I like how personalized the response is\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GEWWS8PGo4A1"
      },
      "source": [
        "### Interoperability with Langfuse Python SDK\n",
        "\n",
        "To use all functionalities of Langfuse, use `get_langchain_handler()` on Langfuse tracing nodes (`trace` or `span`). Learn more about Langfuse Tracing [here](https://langfuse.com/docs/tracing).\n",
        "\n",
        "**Advantages**\n",
        "- Set custom attributes on trace/span.\n",
        "- Add additional custom observations to the trace that are not created by the Langchain integration.\n",
        "- Trace a Langchain application in any point in the application/trace hierarchy.\n",
        "\n",
        "**Disadvantages**\n",
        "- Input/output of a run is not added to trace/span but to a child span."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q1zlFuIimJfT"
      },
      "source": [
        "#### How it works"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3sBgSeuMp5o4"
      },
      "outputs": [],
      "source": [
        "# Initialize the Langfuse Python SDK\n",
        "from langfuse import Langfuse\n",
        "langfuse = Langfuse()\n",
        "\n",
        "# Create a trace via the SDK and get a Langchain Callback handler for it\n",
        "trace = langfuse.trace(name=\"custom-trace\", user_id=\"user-1234\", session_id=\"session-1234\")\n",
        "langfuse_handler = trace.get_langchain_handler()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Alternatively, create a span and get a Langchain Callback handler for it\n",
        "trace = langfuse.trace()\n",
        "span = trace.span()\n",
        "langfuse_handler = span.get_langchain_handler()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HRX2zFCOmwXH"
      },
      "source": [
        "#### Example\n",
        "\n",
        "We'll run the same chain multiple times at different places within the hierarchy of a trace.\n",
        "\n",
        "```\n",
        "TRACE: person-locator\n",
        "|\n",
        "|-- SPAN: Chain (Alan Turing)\n",
        "|\n",
        "|-- SPAN: Physics\n",
        "|   |\n",
        "|   |-- SPAN: Chain (Albert Einstein)\n",
        "|   |\n",
        "|   |-- SPAN: Chain (Isaac Newton)\n",
        "|   |\n",
        "|   |-- SPAN: Favorites\n",
        "|   |   |\n",
        "|   |   |-- SPAN: Chain (Richard Feynman)\n",
        "```\n",
        "\n",
        "Setup chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ASq5sHErkmLB"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(\"what is the city {person} is from?\")\n",
        "model = ChatOpenAI()\n",
        "\n",
        "chain = prompt | model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fvJ1pv4MqzTi"
      },
      "source": [
        "Invoke it multiple times as part of a nested trace."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V5jcfXbtpk8b",
        "outputId": "59737e7c-15be-4db7-99d9-79f6b0fe1d80"
      },
      "outputs": [],
      "source": [
        "trace = langfuse.trace(name=\"person-locator\")\n",
        "\n",
        "# On trace\n",
        "langfuse_handler = trace.get_langchain_handler()\n",
        "chain.invoke({\"person\": \"Alan Turing\"}, config={\"callbacks\":[langfuse_handler]})\n",
        "\n",
        "# On span \"Physics\"\n",
        "span_physics = trace.span(name=\"Physics\")\n",
        "langfuse_handler = span_physics.get_langchain_handler()\n",
        "chain.invoke({\"person\": \"Albert Einstein\"}, config={\"callbacks\":[langfuse_handler]})\n",
        "chain.invoke({\"person\": \"Isaac Newton\"}, config={\"callbacks\":[langfuse_handler]})\n",
        "\n",
        "# On span \"Physics\".\"Favorites\"\n",
        "span_favorites = span_physics.span(name=\"Favorites\")\n",
        "langfuse_handler = span_favorites.get_langchain_handler()\n",
        "chain.invoke({\"person\": \"Richard Feynman\"}, config={\"callbacks\":[langfuse_handler]})\n",
        "\n",
        "# End both spans to get span-level latencies\n",
        "span_favorites.end()\n",
        "span_physics.end()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "View it in Langfuse\n",
        "\n",
        "![Trace of Nested Langchain Runs in Langfuse](https://langfuse.com/images/docs/langchain_python_trace_interoperability.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j1BY2fIlaxtg"
      },
      "source": [
        "## Upgrading from v1.x.x to v2.x.x\n",
        "\n",
        "The `CallbackHandler` can be used in multiple invocations of a Langchain chain as shown below.\n",
        "\n",
        "```python\n",
        "from langfuse.callback import CallbackHandler\n",
        "langfuse_handler = CallbackHandler(PUBLIC_KEY, SECRET_KEY)\n",
        "\n",
        "# Setup Langchain\n",
        "from langchain.chains import LLMChain\n",
        "...\n",
        "chain = LLMChain(llm=llm, prompt=prompt, callbacks=[langfuse_handler])\n",
        "\n",
        "# Add Langfuse handler as callback\n",
        "chain.run(input=\"<first_user_input>\", callbacks=[langfuse_handler])\n",
        "chain.run(input=\"<second_user_input>\", callbacks=[langfuse_handler])\n",
        "\n",
        "```\n",
        "\n",
        "So far, invoking the chain multiple times would group the observations in one trace.\n",
        "\n",
        "```bash\n",
        "TRACE\n",
        "|\n",
        "|-- SPAN: Retrieval\n",
        "|   |\n",
        "|   |-- SPAN: LLM Chain\n",
        "|   |   |\n",
        "|   |   |-- GENERATION: ChatOpenAi\n",
        "|-- SPAN: Retrieval\n",
        "|   |\n",
        "|   |-- SPAN: LLM Chain\n",
        "|   |   |\n",
        "|   |   |-- GENERATION: ChatOpenAi\n",
        "```\n",
        "\n",
        "We changed this, so that each invocation will end up on its own trace. This allows us to derive the user inputs and outputs to Langchain applications. If you still want to group multiple invocations on one trace, you can use [this](https://langfuse.com/docs/langchain/python#adding-trace-as-context-to-a-langchain-handler) approach.\n",
        "\n",
        "```bash\n",
        "TRACE_1\n",
        "|\n",
        "|-- SPAN: Retrieval\n",
        "|   |\n",
        "|   |-- SPAN: LLM Chain\n",
        "|   |   |\n",
        "|   |   |-- GENERATION: ChatOpenAi\n",
        "\n",
        "TRACE_2\n",
        "|\n",
        "|-- SPAN: Retrieval\n",
        "|   |\n",
        "|   |-- SPAN: LLM Chain\n",
        "|   |   |\n",
        "|   |   |-- GENERATION: ChatOpenAi\n",
        "```"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

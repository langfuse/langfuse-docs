{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KlceIPalN3QR"
      },
      "source": [
        "---\n",
        "description: Cookbook with examples of the Langfuse Integration for Langchain (Python).\n",
        "category: Integrations\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mqBspBzuRk9C"
      },
      "source": [
        "# Cookbook: Langchain Integration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x1oaA7XYGOfX"
      },
      "source": [
        "This is a cookbook with examples of the Langfuse Integration for Langchain (Python).\n",
        "\n",
        "Follow the [integration guide](https://langfuse.com/docs/integrations/langchain) to add this integration to your Langchain project. The integration also supports Langchain JS."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AbSpd5EiZouE"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YNyU6IzCZouE"
      },
      "outputs": [],
      "source": [
        "%pip install langfuse langchain langchain_openai --upgrade"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OpE57ujJZouE"
      },
      "source": [
        "Initialize the Langfuse client with your API keys from the project settings in the Langfuse UI and add them to your environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dEdF-668ZouF"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# get keys for your project from https://cloud.langfuse.com\n",
        "os.environ[\"LANGFUSE_PUBLIC_KEY\"] = \"pk-lf-***\"\n",
        "os.environ[\"LANGFUSE_SECRET_KEY\"] = \"sk-lf-***\"\n",
        "os.environ[\"LANGFUSE_HOST\"] = \"https://cloud.langfuse.com\" # for EU data region\n",
        "# os.environ[\"LANGFUSE_HOST\"] = \"https://us.cloud.langfuse.com\" # for US data region\n",
        "\n",
        "# your openai key\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"***\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "divRadPqZouF"
      },
      "outputs": [],
      "source": [
        "from langfuse.callback import CallbackHandler\n",
        "\n",
        "langfuse_handler = CallbackHandler()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8FVbg1RWoT8W"
      },
      "outputs": [],
      "source": [
        "# Tests the SDK connection with the server\n",
        "langfuse_handler.auth_check()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZUenj0aca9qo"
      },
      "source": [
        "## Examples\n",
        "\n",
        "### Sequential Chain\n",
        "\n",
        "![Trace of Langchain Sequential Chain in Langfuse](https://langfuse.com/images/docs/langchain_chain.jpg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dTagwV_cbFVr"
      },
      "outputs": [],
      "source": [
        "# further imports\n",
        "from langchain_openai import OpenAI\n",
        "from langchain.chains import LLMChain, SimpleSequentialChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "llm = OpenAI()\n",
        "template = \"\"\"You are a playwright. Given the title of play, it is your job to write a synopsis for that title.\n",
        "    Title: {title}\n",
        "    Playwright: This is a synopsis for the above play:\"\"\"\n",
        "prompt_template = PromptTemplate(input_variables=[\"title\"], template=template)\n",
        "synopsis_chain = LLMChain(llm=llm, prompt=prompt_template)\n",
        "template = \"\"\"You are a play critic from the New York Times. Given the synopsis of play, it is your job to write a review for that play.\n",
        "    Play Synopsis:\n",
        "    {synopsis}\n",
        "    Review from a New York Times play critic of the above play:\"\"\"\n",
        "prompt_template = PromptTemplate(input_variables=[\"synopsis\"], template=template)\n",
        "review_chain = LLMChain(llm=llm, prompt=prompt_template)\n",
        "overall_chain = SimpleSequentialChain(\n",
        "    chains=[synopsis_chain, review_chain],\n",
        ")\n",
        "\n",
        "# invoke\n",
        "review = overall_chain.invoke(\"Tragedy at sunset on the beach\", {\"callbacks\":[langfuse_handler]}) # add the handler to the run method\n",
        "# run\n",
        "review = overall_chain.run(\"Tragedy at sunset on the beach\", callbacks=[langfuse_handler]) # add the handler to the run method"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QvRWPsZ-NoAr"
      },
      "source": [
        "### Sequential Chain in Langchain Expression Language (LCEL)\n",
        "\n",
        "![Trace of Langchain LCEL](https://langfuse.com/images/docs/langchain_LCEL.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3-HEia6gNoAr"
      },
      "outputs": [],
      "source": [
        "from operator import itemgetter\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.schema import StrOutputParser\n",
        "\n",
        "langfuse_handler = CallbackHandler()\n",
        "\n",
        "prompt1 = ChatPromptTemplate.from_template(\"what is the city {person} is from?\")\n",
        "prompt2 = ChatPromptTemplate.from_template(\n",
        "    \"what country is the city {city} in? respond in {language}\"\n",
        ")\n",
        "model = ChatOpenAI()\n",
        "chain1 = prompt1 | model | StrOutputParser()\n",
        "chain2 = (\n",
        "    {\"city\": chain1, \"language\": itemgetter(\"language\")}\n",
        "    | prompt2\n",
        "    | model\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "chain2.invoke({\"person\": \"obama\", \"language\": \"spanish\"}, config={\"callbacks\":[langfuse_handler]})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5v6TUIrVGkn3"
      },
      "source": [
        "### ConversationChain\n",
        "\n",
        "We'll use a [session](https://langfuse.com/docs/tracing/sessions) in Langfuse to track this conversation with each invocation being a single trace.\n",
        "\n",
        "In addition to the traces of each run, you also get a conversation view of the entire session:\n",
        "\n",
        "![Session view of ConversationChain in Langfuse](https://langfuse.com/images/docs/langchain_session.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8HXyXC2LGga6"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import ConversationChain\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain_openai import OpenAI\n",
        "\n",
        "llm = OpenAI(temperature=0)\n",
        "\n",
        "conversation = ConversationChain(\n",
        "    llm=llm, memory=ConversationBufferMemory()\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EWRj0qvKHLNE"
      },
      "outputs": [],
      "source": [
        "# Create a callback handler with a session\n",
        "langfuse_handler = CallbackHandler(session_id=\"conversation_chain\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aIHmNekVHItt"
      },
      "outputs": [],
      "source": [
        "conversation.predict(input=\"Hi there!\", callbacks=[langfuse_handler])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tsAunGSwHkrt"
      },
      "outputs": [],
      "source": [
        "conversation.predict(input=\"How to build great developer tools?\", callbacks=[langfuse_handler])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m8O6hShcHsGe"
      },
      "outputs": [],
      "source": [
        "conversation.predict(input=\"Summarize your last response\", callbacks=[langfuse_handler])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FP5avhNb3TBH"
      },
      "source": [
        "### RetrievalQA\n",
        "\n",
        "![Trace of Langchain QA Retrieval in Langfuse](https://langfuse.com/images/docs/langchain_qa_retrieval.jpg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wjiWEkRUFzCf"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"SERPAPI_API_KEY\"] = \"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p0CgEPSlEpkC"
      },
      "outputs": [],
      "source": [
        "%pip install unstructured chromadb tiktoken google-search-results python-magic langchainhub --upgrade"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kHDVa-Ssb-KT"
      },
      "outputs": [],
      "source": [
        "from langchain.document_loaders import UnstructuredURLLoader\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "langfuse_handler = CallbackHandler()\n",
        "\n",
        "urls = [\n",
        "    \"https://raw.githubusercontent.com/langfuse/langfuse-docs/main/public/state_of_the_union.txt\",\n",
        "]\n",
        "loader = UnstructuredURLLoader(urls=urls)\n",
        "llm = OpenAI()\n",
        "documents = loader.load()\n",
        "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
        "texts = text_splitter.split_documents(documents)\n",
        "embeddings = OpenAIEmbeddings()\n",
        "docsearch = Chroma.from_documents(texts, embeddings)\n",
        "query = \"What did the president say about Ketanji Brown Jackson\"\n",
        "chain = RetrievalQA.from_chain_type(\n",
        "    llm,\n",
        "    retriever=docsearch.as_retriever(search_kwargs={\"k\": 1}),\n",
        ")\n",
        "\n",
        "chain.invoke(query, config={\"callbacks\":[langfuse_handler]})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JCmI0I20-sbI"
      },
      "source": [
        "### Agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ReaHdQOT-S3n"
      },
      "outputs": [],
      "source": [
        "from langchain.agents import AgentExecutor, load_tools, create_openai_functions_agent\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain import hub\n",
        "\n",
        "langfuse_handler = CallbackHandler()\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
        "tools = load_tools([\"serpapi\"])\n",
        "prompt = hub.pull(\"hwchase17/openai-functions-agent\")\n",
        "agent = create_openai_functions_agent(llm, tools, prompt)\n",
        "agent_executor = AgentExecutor(agent=agent, tools=tools)\n",
        "\n",
        "agent_executor.invoke({\"input\": \"What is Langfuse?\"}, config={\"callbacks\":[langfuse_handler]})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OIxwkX9p1ZR7"
      },
      "source": [
        "### AzureOpenAI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b43rIMig1ZR7"
      },
      "outputs": [],
      "source": [
        "os.environ[\"AZURE_OPENAI_ENDPOINT\"] = \"<Azure OpenAI endpoint>\"\n",
        "os.environ[\"AZURE_OPENAI_API_KEY\"] = \"<Azure OpenAI API key>\"\n",
        "os.environ[\"OPENAI_API_TYPE\"] = \"azure\"\n",
        "os.environ[\"OPENAI_API_VERSION\"] = \"2023-09-01-preview\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_lLdPwnr1ZR7"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import AzureChatOpenAI\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "langfuse_handler = CallbackHandler()\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(\"what is the city {person} is from?\")\n",
        "model = AzureChatOpenAI(\n",
        "    deployment_name=\"gpt-35-turbo\",\n",
        "    model_name=\"gpt-3.5-turbo\",\n",
        ")\n",
        "chain = prompt | model\n",
        "\n",
        "chain.invoke({\"person\": \"Satya Nadella\"}, config={\"callbacks\":[langfuse_handler]})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AxQlUOmVPEwz"
      },
      "source": [
        "## Adding scores to traces\n",
        "\n",
        "To add [scores](/docs/scores) to traces created with the Langchain integration, access the traceId via `langfuse_handler.get_trace_id()`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PudCopwEPFgh"
      },
      "outputs": [],
      "source": [
        "from langfuse import Langfuse\n",
        "\n",
        "# Trace langchain run via the Langfuse CallbackHandler as shown above\n",
        "\n",
        "# Get id of the last created trace\n",
        "trace_id = langfuse_handler.get_trace_id()\n",
        "\n",
        "# Add score, e.g. via the Python SDK\n",
        "langfuse = Langfuse()\n",
        "trace = langfuse.score(\n",
        "    trace_id=trace_id,\n",
        "    name=\"user-explicit-feedback\",\n",
        "    value=1,\n",
        "    comment=\"I like how personalized the response is\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GEWWS8PGo4A1"
      },
      "source": [
        "## Interoperability with Langfuse Python SDK\n",
        "\n",
        "You can use this integration in combination with the `observe()` decorator from the Langfuse Python SDK. Thereby, you can trace non-Langchain code, combine multiple Langchain invocations in a single trace, and use the full functionality of the Langfuse Python SDK.\n",
        "\n",
        "The `langfuse_context.get_current_langchain_handler()` method exposes a LangChain callback handler in the context of a trace or span when using `decorators`. Learn more about Langfuse Tracing [here](https://langfuse.com/docs/tracing) and this functionality [here](https://langfuse.com/docs/sdk/python/decorators#langchain).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q1zlFuIimJfT"
      },
      "source": [
        "### How it works"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Op7qwM0Y-1bp"
      },
      "outputs": [],
      "source": [
        "from langfuse.decorators import langfuse_context, observe\n",
        "\n",
        "# Create a trace via Langfuse decorators and get a Langchain Callback handler for it\n",
        "@observe() # automtically log function as a trace to Langfuse\n",
        "def main():\n",
        "    # update trace attributes (e.g, name, session_id, user_id)\n",
        "    langfuse_context.update_current_trace(\n",
        "        name=\"custom-trace\",\n",
        "        session_id=\"user-1234\",\n",
        "        user_id=\"session-1234\",\n",
        "    )\n",
        "    # get the langchain handler for the current trace\n",
        "    langfuse_context.get_current_langchain_handler()\n",
        "\n",
        "    # use the handler to trace langchain runs ...\n",
        "\n",
        "main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HRX2zFCOmwXH"
      },
      "source": [
        "### Example\n",
        "\n",
        "We'll run the same chain multiple times at different places within the hierarchy of a trace.\n",
        "\n",
        "```\n",
        "TRACE: person-locator\n",
        "|\n",
        "|-- SPAN: Chain (Alan Turing)\n",
        "|\n",
        "|-- SPAN: Physics\n",
        "|   |\n",
        "|   |-- SPAN: Chain (Albert Einstein)\n",
        "|   |\n",
        "|   |-- SPAN: Chain (Isaac Newton)\n",
        "|   |\n",
        "|   |-- SPAN: Favorites\n",
        "|   |   |\n",
        "|   |   |-- SPAN: Chain (Richard Feynman)\n",
        "```\n",
        "\n",
        "Setup chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ASq5sHErkmLB"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(\"what is the city {person} is from?\")\n",
        "model = ChatOpenAI()\n",
        "\n",
        "chain = prompt | model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fvJ1pv4MqzTi"
      },
      "source": [
        "Invoke it multiple times as part of a nested trace."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CnHq-7QD3uAa"
      },
      "outputs": [],
      "source": [
        "from langfuse.decorators import langfuse_context, observe\n",
        "\n",
        "# On span \"Physics\".\"Favorites\"\n",
        "@observe()  # decorator to automatically log function as sub-span to Langfuse\n",
        "def favorites():\n",
        "    # get the langchain handler for the current sub-span\n",
        "    langfuse_handler = langfuse_context.get_current_langchain_handler()\n",
        "    # invoke chain with langfuse handler\n",
        "    chain.invoke({\"person\": \"Richard Feynman\"},\n",
        "                 config={\"callbacks\": [langfuse_handler]})\n",
        "\n",
        "# On span \"Physics\"\n",
        "@observe()  # decorator to automatically log function as span to Langfuse\n",
        "def physics():\n",
        "    # get the langchain handler for the current span\n",
        "    langfuse_handler = langfuse_context.get_current_langchain_handler()\n",
        "    # invoke chains with langfuse handler\n",
        "    chain.invoke({\"person\": \"Albert Einstein\"},\n",
        "                 config={\"callbacks\": [langfuse_handler]})\n",
        "    chain.invoke({\"person\": \"Isaac Newton\"},\n",
        "                 config={\"callbacks\": [langfuse_handler]})\n",
        "    favorites()\n",
        "\n",
        "# On trace\n",
        "@observe()  # decorator to automatically log function as trace to Langfuse\n",
        "def main():\n",
        "    # get the langchain handler for the current trace\n",
        "    langfuse_handler = langfuse_context.get_current_langchain_handler()\n",
        "    # invoke chain with langfuse handler\n",
        "    chain.invoke({\"person\": \"Alan Turing\"},\n",
        "                 config={\"callbacks\": [langfuse_handler]})\n",
        "    physics()\n",
        "\n",
        "main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZ3q8iMDGOfd"
      },
      "source": [
        "View it in Langfuse\n",
        "\n",
        "![Trace of Nested Langchain Runs in Langfuse](https://langfuse.com/images/docs/langchain_python_trace_interoperability.png)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

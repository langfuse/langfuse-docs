{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6726658-864c-4d13-8fbb-2ada933c86b9",
   "metadata": {},
   "source": [
    "---\n",
    "description: Cookbook with examples of the Langfuse Integration for LiteLLM (Proxy)\n",
    "category: Integrations\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b26fc84-994d-4e93-9e57-6a427561ac86",
   "metadata": {},
   "source": [
    "# Cookbook: LiteLLM (Proxy) Integration\n",
    "\n",
    "The [LiteLLM Proxy](https://docs.litellm.ai/docs/) ([GitHub](https://github.com/BerriAI/litellm)) simplifies your work by standardizing 100+ model provider APIs on the OpenAI API schema. It removes the complexity of direct API calls by centralizing interactions with these APIs through a single endpoint, allowing you to focus on innovation rather than integration details. As LiteLLM standardizes models on the OpenAI schema, we can use the [Langfuse integration](https://langfuse.com/docs/integrations/litellm) for JS and Python to natively instrument calls to all these 100+ models.\n",
    "\n",
    "Let's dive into how you can set up and start using LiteLLM with Langfuse:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee92c44-cc7c-4bdb-9153-0ddc8c7c07b0",
   "metadata": {},
   "source": [
    "### Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e39484-8901-4c98-95ec-911ea9f170b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install \"litellm[proxy]\" langfuse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc213d5-116e-4a99-95b0-423bbb6bfb75",
   "metadata": {},
   "source": [
    "### Setup environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69298dc-6c55-46e4-801f-dd74f47a8675",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langfuse.openai import auth_check\n",
    "\n",
    "os.environ[\"LANGFUSE_PUBLIC_KEY\"] = \"\"\n",
    "os.environ[\"LANGFUSE_SECRET_KEY\"] = \"\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
    " \n",
    "# Test connection to Langfuse\n",
    "auth_check()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d92f6fdd-48f1-4f40-bf58-17be90b8e3b5",
   "metadata": {},
   "source": [
    "### Setup Lite LLM Proxy\n",
    "\n",
    "1. Create a config file called litellm_config.yaml that looks like this:<br/>\n",
    "```yaml\n",
    "model_list:\n",
    "    - model_name: gpt-3.5-turbo\n",
    "        litellm_params:\n",
    "            model: gpt-3.5-turbo\n",
    "            api_key: \n",
    "    - model_name: ollama/llama3\n",
    "        litellm_params:\n",
    "            model: ollama/llama3\n",
    "    - model_name: ollama/mistral\n",
    "        litellm_params:\n",
    "            model: ollama/mistral\n",
    "```\n",
    "2. Add your OpenAI API Key\n",
    "3. Ensure that you have pulled the llama3 (8b) and mistral (7b) ollama models.\n",
    "4. Run in the CLI the following command: `litellm --config litellm_config.yaml`\n",
    "\n",
    "The Lite LLM Proxy should be now running on http://0.0.0.0:4000\n",
    "\n",
    "To verify the connection you can run `litellm --test`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "042146c5-679b-4259-a55c-56aaf51bdfd7",
   "metadata": {},
   "source": [
    "### Example using Langfuse OpenAI Integration\n",
    "The Langfuse SDK offers a wrapper function around the OpenAI SDK, automatically logging all OpenAI calls as generations to Langfuse.\n",
    "\n",
    "For more details, please refer to our [documentation](https://langfuse.com/docs/integrations/openai/python/get-started)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb4761a-7387-4375-aab0-da02c32f1415",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langfuse.openai import openai\n",
    "\n",
    "# Set PROXY_URL to the url of your lite_llm_proxy (by default: http://0.0.0.0:4000)\n",
    "PROXY_URL=\"http://0.0.0.0:4000\"\n",
    "\n",
    "system_prompt = \"You are a very accurate calculator. You output only the result of the calculation.\"\n",
    "\n",
    "client = openai.OpenAI(\n",
    "    base_url=PROXY_URL,\n",
    ")\n",
    "gpt_completion = client.chat.completions.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "      {\"role\": \"system\", \"content\": system_prompt},\n",
    "      {\"role\": \"user\", \"content\": \"1 + 1 = \"}],\n",
    ")\n",
    "print(gpt_completion.choices[0].message.content)\n",
    "\n",
    "llama_completion = client.chat.completions.create(\n",
    "  model=\"ollama/llama3\",\n",
    "  messages=[\n",
    "      {\"role\": \"system\", \"content\": system_prompt},\n",
    "      {\"role\": \"user\", \"content\": \"3 + 3 = \"}],\n",
    ")\n",
    "print(llama_completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31fe883d-88eb-4ebf-8870-99e9c60104f7",
   "metadata": {},
   "source": [
    "[GPT-Completion](https://cloud.langfuse.com/project/clqz0knc900024jay4kgx7nja/traces/58c384f6-37e9-4628-bd2b-e6b9709804a5) <br />\n",
    "[LLama-Completion](https://cloud.langfuse.com/project/clqz0knc900024jay4kgx7nja/traces/c9aa13cc-b601-4d3d-8243-eac72e244dfc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4144dcf-3478-4e3f-bed2-a394e8ccc253",
   "metadata": {},
   "source": [
    "### Example using @observe() decorator\n",
    "\n",
    "The @observe() decorator integrates tracing directly into your Python applications, automatically capturing and logging execution details such as inputs, outputs, timings, and more. The decorator simplifies achieving in-depth observability in your applications with minimal code.\n",
    "\n",
    "For more details on how to utilize this decorator and customize your tracing, refer to our [documentation](https://langfuse.com/docs/sdk/python/decorators).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aeaa015-6b6b-4ac8-8e07-55e1783a398e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langfuse.decorators import observe\n",
    "from langfuse.openai import openai\n",
    "\n",
    "@observe()\n",
    "def rap_battle():\n",
    "    client = openai.OpenAI(\n",
    "        base_url=PROXY_URL,\n",
    "    )\n",
    "\n",
    "    system_prompt = \"You are a rap artist. Drop a fresh line.\"\n",
    "\n",
    "    # First model starts the rap\n",
    "    gpt_completion = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": \"Kick it off, here's the mic...\"}\n",
    "        ],\n",
    "    )\n",
    "    first_rap = gpt_completion.choices[0].message.content\n",
    "    print(\"Rap 1:\", first_rap)\n",
    "\n",
    "    # Second model responds\n",
    "    llama_completion = client.chat.completions.create(\n",
    "        model=\"ollama/llama3\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": first_rap}\n",
    "        ],\n",
    "    )\n",
    "    second_rap = llama_completion.choices[0].message.content\n",
    "    print(\"Rap 2:\", second_rap)\n",
    "\n",
    "    # Third model continues\n",
    "    mistral_completion = client.chat.completions.create(\n",
    "        model=\"ollama/mistral\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": second_rap}\n",
    "        ],\n",
    "    )\n",
    "    third_rap = mistral_completion.choices[0].message.content\n",
    "    print(\"Rap 3:\", third_rap)\n",
    "\n",
    "# Call the function\n",
    "rap_battle()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e4d553-5efc-4b94-a1c2-0637133f376b",
   "metadata": {},
   "source": [
    "[Rap Battle](https://cloud.langfuse.com/project/clqz0knc900024jay4kgx7nja/traces/549d1815-436c-4fa3-9a3b-857b5628a44f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

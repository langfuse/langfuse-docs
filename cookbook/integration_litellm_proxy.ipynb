{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6726658-864c-4d13-8fbb-2ada933c86b9",
   "metadata": {},
   "source": [
    "---\n",
    "description: The stack to use any of 100+ models in Python without having to change your code and with full observability.\n",
    "category: Integrations\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b26fc84-994d-4e93-9e57-6a427561ac86",
   "metadata": {},
   "source": [
    "# Cookbook: LiteLLM (Proxy) + Langfuse OpenAI Integration + `@observe` Decorator\n",
    "\n",
    "We want to share a stack that's commonly used by the Langfuse community to quickly experiment with 100+ models from different providers without changing code. This stack includes:\n",
    "- [**LiteLLM Proxy**](https://docs.litellm.ai/docs/) ([GitHub](https://github.com/BerriAI/litellm)) which standardizes 100+ model provider APIs on the OpenAI API schema. It removes the complexity of direct API calls by centralizing interactions with these APIs through a single endpoint. You can also self-host the LiteLLM Proxy as it is open-source.\n",
    "- **Langfuse OpenAI SDK Wrapper** ([Python](https://langfuse.com/docs/integrations/openai/python/get-started/python/get-started), [JS](https://langfuse.com/docs/integrations/openai/python/get-started/js/get-started)) to natively instrument calls to all these 100+ models via the OpenAI SDK. This automatically captures token counts, latencies, streaming response times (time to first token), api errors, and more.\n",
    "- **Langfuse**: OSS LLM Observability, full overview [here](https://langfuse.com/docs).\n",
    "\n",
    "\n",
    "This cookbook is an end-to-end guide to set up and use this stack. As we'll use Python in this example, we will also use the `@observe` decorator to create nested traces. More on this below.\n",
    "\n",
    "Let's dive right in!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee92c44-cc7c-4bdb-9153-0ddc8c7c07b0",
   "metadata": {},
   "source": [
    "## Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e39484-8901-4c98-95ec-911ea9f170b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install \"litellm[proxy]\" langfuse openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc213d5-116e-4a99-95b0-423bbb6bfb75",
   "metadata": {},
   "source": [
    "## Setup environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69298dc-6c55-46e4-801f-dd74f47a8675",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langfuse.openai import openai\n",
    "\n",
    "# Get keys for your project from the project settings page\n",
    "# https://cloud.langfuse.com\n",
    "os.environ[\"LANGFUSE_PUBLIC_KEY\"] = \"\"\n",
    "os.environ[\"LANGFUSE_SECRET_KEY\"] = \"\"\n",
    "os.environ[\"LANGFUSE_HOST\"] = \"https://cloud.langfuse.com\" # ðŸ‡ªðŸ‡º EU region\n",
    "# os.environ[\"LANGFUSE_HOST\"] = \"https://us.cloud.langfuse.com\" # ðŸ‡ºðŸ‡¸ US region\n",
    "\n",
    "# Your openai key\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
    " \n",
    "# Test connection to Langfuse, not recommended for production as it is blocking\n",
    "openai.langfuse_auth_check()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d92f6fdd-48f1-4f40-bf58-17be90b8e3b5",
   "metadata": {},
   "source": [
    "## Setup Lite LLM Proxy\n",
    "\n",
    "In this example, we'll use GPT-3.5-turbo directly from OpenAI, and llama3 and mistral via the Ollama on our local machine.\n",
    "\n",
    "**Steps**\n",
    "\n",
    "1. Create a `litellm_config.yaml` to configure which models are available ([docs](https://litellm.vercel.app/docs/proxy/configs)). We'll use gpt-3.5-turbo, and llama3 and mistral via Ollama in this example. Make sure to replace `<openai_key>` with your OpenAI API key.\n",
    "    ```yaml\n",
    "    model_list:\n",
    "      - model_name: gpt-3.5-turbo\n",
    "        litellm_params:\n",
    "          model: gpt-3.5-turbo\n",
    "          api_key: <openai_key>\n",
    "      - model_name: ollama/llama3\n",
    "        litellm_params:\n",
    "          model: ollama/llama3\n",
    "      - model_name: ollama/mistral\n",
    "        litellm_params:\n",
    "          model: ollama/mistral\n",
    "    ```\n",
    "3. Ensure that you installed Ollama and have pulled the llama3 (8b) and mistral (7b) models: `ollama pull llama3 && ollama pull mistral`\n",
    "4. Run the following cli command to start the proxy: `litellm --config litellm_config.yaml`\n",
    "\n",
    "The Lite LLM Proxy should be now running on http://0.0.0.0:4000\n",
    "\n",
    "To verify the connection you can run `litellm --test`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "042146c5-679b-4259-a55c-56aaf51bdfd7",
   "metadata": {},
   "source": [
    "## Log single LLM Call via Langfuse OpenAI Wrapper\n",
    "The Langfuse SDK offers a wrapper function around the OpenAI SDK, automatically logging all OpenAI calls as generations to Langfuse.\n",
    "\n",
    "For more details, please refer to our [documentation](https://langfuse.com/docs/integrations/openai/python/get-started/python/get-started)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb4761a-7387-4375-aab0-da02c32f1415",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langfuse.openai import openai\n",
    "\n",
    "# Set PROXY_URL to the url of your lite_llm_proxy (by default: http://0.0.0.0:4000)\n",
    "PROXY_URL=\"http://0.0.0.0:4000\"\n",
    "\n",
    "system_prompt = \"You are a very accurate calculator. You output only the result of the calculation.\"\n",
    "\n",
    "# Configure the OpenAI client to use the LiteLLM proxy\n",
    "client = openai.OpenAI(base_url=PROXY_URL)\n",
    "\n",
    "gpt_completion = client.chat.completions.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  name=\"gpt-3.5\", # optional name of the generation in langfuse\n",
    "  messages=[\n",
    "      {\"role\": \"system\", \"content\": system_prompt},\n",
    "      {\"role\": \"user\", \"content\": \"1 + 1 = \"}],\n",
    ")\n",
    "print(gpt_completion.choices[0].message.content)\n",
    "\n",
    "llama_completion = client.chat.completions.create(\n",
    "  model=\"ollama/llama3\",\n",
    "  name=\"llama3\", # optional name of the generation in langfuse\n",
    "  messages=[\n",
    "      {\"role\": \"system\", \"content\": system_prompt},\n",
    "      {\"role\": \"user\", \"content\": \"3 + 3 = \"}],\n",
    ")\n",
    "print(llama_completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31fe883d-88eb-4ebf-8870-99e9c60104f7",
   "metadata": {},
   "source": [
    "Public trace links for the following examples:\n",
    "- [GPT-3.5-turbo](https://cloud.langfuse.com/project/cloramnkj0002jz088vzn1ja4/traces/a4e67d7d-d9cb-455b-9795-3ad41f39431e?observation=81006513-82b1-4ae4-bb98-7e1bc6c009a7)\n",
    "- [llama3](https://cloud.langfuse.com/project/cloramnkj0002jz088vzn1ja4/traces/22fdce4a-4d74-4af3-9746-7bafaa45247c?observation=b9b30b5d-7fbf-40b4-acd9-4fdc1776cc87)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4144dcf-3478-4e3f-bed2-a394e8ccc253",
   "metadata": {},
   "source": [
    "## Trace nested LLM Calls via Langfuse OpenAI Wrapper and `@observe` decorator\n",
    "\n",
    "Via the Langfuse `@observe()` decorator we can automatically capture execution details of any python function such as inputs, outputs, timings, and more. The decorator simplifies achieving in-depth observability in your applications with minimal code, especially when non-LLM calls are involved for knowledge retrieval (RAG) or api calls (agents).\n",
    "\n",
    "For more details on how to utilize this decorator and customize your tracing, refer to our [documentation](https://langfuse.com/docs/sdk/python/decorators).\n",
    "\n",
    "Let's have a look at a simple example which uses all three models we have set up in the LiteLLM Proxy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aeaa015-6b6b-4ac8-8e07-55e1783a398e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langfuse.decorators import observe\n",
    "from langfuse.openai import openai\n",
    "\n",
    "@observe()\n",
    "def rap_battle(topic: str):\n",
    "    client = openai.OpenAI(\n",
    "        base_url=PROXY_URL,\n",
    "    )\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a rap artist. Drop a fresh line.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Kick it off, today's topic is {topic}, here's the mic...\"}\n",
    "    ]\n",
    "\n",
    "    # First model (gpt-3.5-turbo) starts the rap\n",
    "    gpt_completion = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        name=\"rap-gpt-3.5-turbo\", # add custom name to Langfuse observation\n",
    "        messages=messages,\n",
    "    )\n",
    "    first_rap = gpt_completion.choices[0].message.content\n",
    "    messages.append({\"role\": \"assistant\", \"content\": first_rap})\n",
    "    print(\"Rap 1:\", first_rap)\n",
    "\n",
    "    # Second model (ollama/llama3) responds\n",
    "    llama_completion = client.chat.completions.create(\n",
    "        model=\"ollama/llama3\",\n",
    "        name=\"rap-llama3\",\n",
    "        messages=messages,\n",
    "    )\n",
    "    second_rap = llama_completion.choices[0].message.content\n",
    "    messages.append({\"role\": \"assistant\", \"content\": second_rap})\n",
    "    print(\"Rap 2:\", second_rap)\n",
    "\n",
    "    # Third model (ollama/mistral) adds the final touch\n",
    "    mistral_completion = client.chat.completions.create(\n",
    "        model=\"ollama/mistral\",\n",
    "        name=\"rap-mistral\",\n",
    "        messages=messages,\n",
    "    )\n",
    "    third_rap = mistral_completion.choices[0].message.content\n",
    "    messages.append({\"role\": \"assistant\", \"content\": third_rap})\n",
    "    print(\"Rap 3:\", third_rap)\n",
    "    \n",
    "    return messages\n",
    "\n",
    "# Call the function\n",
    "rap_battle(\"typography\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e4d553-5efc-4b94-a1c2-0637133f376b",
   "metadata": {},
   "source": [
    "[Public trace](https://cloud.langfuse.com/project/cloramnkj0002jz088vzn1ja4/traces/7b1af1be-8096-474e-a2fc-081538ca333c)\n",
    "\n",
    "![Public Trace](https://langfuse.com/images/cookbook/integration_litellm_proxy_trace.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51eb9581",
   "metadata": {},
   "source": [
    "## Learn more\n",
    "\n",
    "Check out the docs to learn more about all components of this stack:\n",
    "- [LiteLLM Proxy](https://docs.litellm.ai/docs/)\n",
    "- [Langfuse OpenAI SDK Wrapper](https://langfuse.com/docs/integrations/openai/python/get-started/python/get-started)\n",
    "- [Langfuse @observe() decorator](https://langfuse.com/docs/sdk/python/decorators)\n",
    "- [Langfuse](https://langfuse.com/docs)\n",
    "\n",
    "If you do not want to capture traces via the OpenAI SDK Wrapper, you can also directly log requests from the LiteLLM Proxy to Langfuse. For more details, refer to the [LiteLLM Docs](https://litellm.vercel.app/docs/proxy/logging#logging-proxy-inputoutput---langfuse)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

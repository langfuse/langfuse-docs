{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c02e4b23",
   "metadata": {},
   "source": [
    "<!-- NOTEBOOK_METADATA source: \"⚠️ Jupyter Notebook\" title: \"LiteLLM SDK Integration\" sidebarTitle: \"LiteLLM SDK\" logo: \"/images/integrations/litellm_icon.png\" description: \"In this guide, we will show you how to use the LiteLLM SDK to capture LLM calls and log them to Langfuse.\" category: \"Integrations\" -->\n",
    "\n",
    "# LiteLLM SDK Integration\n",
    "\n",
    "In this guide, we will show you how to use the LiteLLM SDK to capture LLM calls and log them to Langfuse.\n",
    "\n",
    "> **What is LiteLLM?** [LiteLLM](https://github.com/BerriAI/litellm) is an open-source proxy and SDK that provides a single unified API to call and manage hundreds of different LLM providers and models with OpenAI-compatible endpoints.\n",
    "\n",
    "> **What is Langfuse?** [Langfuse](https://langfuse.com) is an open-source LLM observability platform that helps you trace, monitor, and debug your LLM applications.\n",
    "\n",
    "There are three ways to integrate LiteLLM with Langfuse:\n",
    "\n",
    "1. Using the LiteLLM SDK to capture LLM calls directly.\n",
    "2. Sending logs via the LiteLLM Proxy to capture all LLM calls going through the proxy.\n",
    "3. Using any compatible framework (such as the OpenAI or LangChain SDK) to capture LLM calls.\n",
    "\n",
    "<Callout type=\"info\">\n",
    "  This integration is for the LiteLLM SDK. If you are looking for the LiteLLM\n",
    "  Proxy integration, see the [LiteLLM Proxy\n",
    "  Integration](/integrations/gateways/litellm) page.\n",
    "</Callout>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9afb95",
   "metadata": {},
   "source": [
    "## Features\n",
    "\n",
    "- Automatic trace collection for all LiteLLM requests\n",
    "- Support for Langfuse Cloud (EU and US regions)\n",
    "- Support for self-hosted Langfuse instances\n",
    "- Custom endpoint configuration\n",
    "- Secure authentication using Basic Auth\n",
    "- Consistent attribute mapping with other OTEL integrations\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "1. **Langfuse Account**: Sign up at [Langfuse Cloud](https://cloud.langfuse.com) or set up a self-hosted instance\n",
    "2. **API Keys**: Get your public and secret keys from your Langfuse project settings\n",
    "3. **Dependencies**: Install required packages:\n",
    "   ```bash\n",
    "   pip install litellm opentelemetry-api opentelemetry-sdk opentelemetry-exporter-otlp\n",
    "   ```\n",
    "\n",
    "## Configuration\n",
    "\n",
    "### Environment Variables\n",
    "\n",
    "| Variable | Required | Description | Example |\n",
    "|----------|----------|-------------|---------|\n",
    "| `LANGFUSE_PUBLIC_KEY` | Yes | Your Langfuse public key | `pk-lf-...` |\n",
    "| `LANGFUSE_SECRET_KEY` | Yes | Your Langfuse secret key | `sk-lf-...` |\n",
    "| `LANGFUSE_OTEL_HOST` | No | OTEL endpoint host | `https://otel.my-langfuse.com` |\n",
    "\n",
    "### Endpoint Resolution\n",
    "\n",
    "The integration automatically constructs the OTEL endpoint from `LANGFUSE_OTEL_HOST`\n",
    "- **Default (US)**: `https://us.cloud.langfuse.com/api/public/otel`\n",
    "- **EU Region**: `https://cloud.langfuse.com/api/public/otel`\n",
    "- **Self-hosted**: `{LANGFUSE_OTEL_HOST}/api/public/otel`\n",
    "\n",
    "## Usage\n",
    "\n",
    "### Basic Setup\n",
    "\n",
    "```python\n",
    "import os\n",
    "import litellm\n",
    "\n",
    "# Set your Langfuse credentials\n",
    "os.environ[\"LANGFUSE_PUBLIC_KEY\"] = \"pk-lf-...\"\n",
    "os.environ[\"LANGFUSE_SECRET_KEY\"] = \"sk-lf-...\"\n",
    "\n",
    "# Enable Langfuse OTEL integration\n",
    "litellm.callbacks = [\"langfuse_otel\"]\n",
    "\n",
    "# Make LLM requests as usual\n",
    "response = litellm.completion(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Hello!\"}]\n",
    ")\n",
    "```\n",
    "\n",
    "### Advanced Configuration\n",
    "\n",
    "```python\n",
    "import os\n",
    "import litellm\n",
    "\n",
    "# Set your Langfuse credentials\n",
    "os.environ[\"LANGFUSE_PUBLIC_KEY\"] = \"pk-lf-...\"\n",
    "os.environ[\"LANGFUSE_SECRET_KEY\"] = \"sk-lf-...\"\n",
    "\n",
    "# Use EU region\n",
    "os.environ[\"LANGFUSE_OTEL_HOST\"] = \"https://cloud.langfuse.com\"  # EU region\n",
    "# os.environ[\"LANGFUSE_OTEL_HOST\"] = \"https://otel.my-langfuse.company.com\"  # custom OTEL endpoint\n",
    "\n",
    "# Or use self-hosted instance\n",
    "# os.environ[\"LANGFUSE_OTEL_HOST\"] = \"https://my-langfuse.company.com\"\n",
    "\n",
    "litellm.callbacks = [\"langfuse_otel\"]\n",
    "```\n",
    "\n",
    "### Manual OTEL Configuration\n",
    "\n",
    "If you need direct control over the OpenTelemetry configuration:\n",
    "\n",
    "```python\n",
    "import os\n",
    "import base64\n",
    "import litellm\n",
    "\n",
    "# Get keys for your project from the project settings page: https://cloud.langfuse.com\n",
    "os.environ[\"LANGFUSE_PUBLIC_KEY\"] = \"pk-lf-...\" \n",
    "os.environ[\"LANGFUSE_SECRET_KEY\"] = \"sk-lf-...\" \n",
    "os.environ[\"LANGFUSE_OTEL_HOST\"] = \"https://cloud.langfuse.com\" # EU region\n",
    "# os.environ[\"LANGFUSE_OTEL_HOST\"] = \"https://us.cloud.langfuse.com\" # US region\n",
    "# os.environ[\"LANGFUSE_OTEL_HOST\"] = \"https://otel.my-langfuse.company.com\" # custom OTEL endpoint\n",
    "\n",
    "LANGFUSE_AUTH = base64.b64encode(\n",
    "    f\"{os.environ.get('LANGFUSE_PUBLIC_KEY')}:{os.environ.get('LANGFUSE_SECRET_KEY')}\".encode()\n",
    ").decode()\n",
    "\n",
    "host = os.environ.get(\"LANGFUSE_OTEL_HOST\")\n",
    "os.environ[\"OTEL_EXPORTER_OTLP_ENDPOINT\"] = host + \"/api/public/otel\"\n",
    "os.environ[\"OTEL_EXPORTER_OTLP_HEADERS\"] = f\"Authorization=Basic {LANGFUSE_AUTH}\"\n",
    "\n",
    "litellm.callbacks = [\"langfuse_otel\"]\n",
    "```\n",
    "\n",
    "### With LiteLLM Proxy\n",
    "\n",
    "Add the integration to your proxy configuration:\n",
    "\n",
    "1. Add the credentials to your environment variables\n",
    "\n",
    "```bash\n",
    "export LANGFUSE_PUBLIC_KEY=\"pk-lf-...\"\n",
    "export LANGFUSE_SECRET_KEY=\"sk-lf-...\"\n",
    "export LANGFUSE_OTEL_HOST=\"https://us.cloud.langfuse.com\"  # Default US region\n",
    "# export LANGFUSE_OTEL_HOST=\"https://otel.my-langfuse.company.com\"  # custom OTEL endpoint\n",
    "```\n",
    "\n",
    "2. Setup config.yaml\n",
    "\n",
    "```yaml\n",
    "# config.yaml\n",
    "litellm_settings:\n",
    "  callbacks: [\"langfuse_otel\"]\n",
    "```\n",
    "\n",
    "3. Run the proxy\n",
    "\n",
    "```bash\n",
    "litellm --config /path/to/config.yaml\n",
    "```\n",
    "\n",
    "## Data Collected\n",
    "\n",
    "The integration automatically collects the following data:\n",
    "\n",
    "- **Request Details**: Model, messages, parameters (temperature, max_tokens, etc.)\n",
    "- **Response Details**: Generated content, token usage, finish reason\n",
    "- **Timing Information**: Request duration, time to first token\n",
    "- **Metadata**: User ID, session ID, custom tags (if provided)\n",
    "- **Error Information**: Exception details and stack traces (if errors occur)\n",
    "\n",
    "## Metadata Support\n",
    "\n",
    "All metadata fields available in the vanilla Langfuse integration are now **fully supported** when you use the OTEL integration.\n",
    "\n",
    "- Any key you pass in the `metadata` dictionary (`generation_name`, `trace_id`, `session_id`, `tags`, and the rest) is exported as an OpenTelemetry span attribute.\n",
    "- Attribute names are prefixed with `langfuse.` so you can filter or search for them easily in your observability backend.\n",
    "  Examples: `langfuse.generation.name`, `langfuse.trace.id`, `langfuse.trace.session_id`.\n",
    "\n",
    "### Passing Metadata – Example\n",
    "\n",
    "```python\n",
    "response = litellm.completion(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Hello!\"}],\n",
    "    metadata={\n",
    "        \"generation_name\": \"welcome-message\",\n",
    "        \"trace_id\": \"trace-123\",\n",
    "        \"session_id\": \"sess-42\",\n",
    "        \"tags\": [\"prod\", \"beta-user\"]\n",
    "    }\n",
    ")\n",
    "```\n",
    "\n",
    "The resulting span will contain attributes similar to:\n",
    "\n",
    "```\n",
    "langfuse.generation.name   = \"welcome-message\"\n",
    "langfuse.trace.id          = \"trace-123\"\n",
    "langfuse.trace.session_id  = \"sess-42\"\n",
    "langfuse.trace.tags        = [\"prod\", \"beta-user\"]\n",
    "```\n",
    "\n",
    "Use the **Langfuse UI** (Traces tab) to search, filter and analyse spans that contain the `langfuse.*` attributes.\n",
    "The OTEL exporter in this integration sends data directly to Langfuse’s OTLP HTTP endpoint; it is **not** intended for Grafana, Honeycomb, Datadog, or other generic OTEL back-ends.\n",
    "\n",
    "## Authentication\n",
    "\n",
    "The integration uses HTTP Basic Authentication with your Langfuse public and secret keys:\n",
    "\n",
    "```\n",
    "Authorization: Basic <base64(public_key:secret_key)>\n",
    "```\n",
    "\n",
    "This is automatically handled by the integration - you just need to provide the keys via environment variables.\n",
    "\n",
    "## Troubleshooting\n",
    "\n",
    "### Common Issues\n",
    "\n",
    "1. **Missing Credentials Error**\n",
    "   ```\n",
    "   ValueError: LANGFUSE_PUBLIC_KEY and LANGFUSE_SECRET_KEY must be set\n",
    "   ```\n",
    "   **Solution**: Ensure both environment variables are set with valid keys.\n",
    "\n",
    "2. **Connection Issues**\n",
    "   - Check your internet connection\n",
    "   - Verify the endpoint URL is correct\n",
    "   - For self-hosted instances, ensure the `/api/public/otel` endpoint is accessible\n",
    "\n",
    "3. **Authentication Errors**\n",
    "   - Verify your public and secret keys are correct\n",
    "   - Check that the keys belong to the same Langfuse project\n",
    "   - Ensure the keys have the necessary permissions\n",
    "\n",
    "### Debug Mode\n",
    "\n",
    "Enable verbose logging to see detailed information:\n",
    "\n",
    "**SDK:**\n",
    "\n",
    "```python\n",
    "import litellm\n",
    "litellm._turn_on_debug()\n",
    "```\n",
    "\n",
    "**Proxy:**\n",
    "\n",
    "```bash\n",
    "export LITELLM_LOG=\"DEBUG\"\n",
    "```\n",
    "\n",
    "\n",
    "This will show:\n",
    "- Endpoint resolution logic\n",
    "- Authentication header creation\n",
    "- OTEL trace submission details\n",
    "\n",
    "## Related Links\n",
    "\n",
    "- [Langfuse Documentation](https://langfuse.com/docs)\n",
    "- [Langfuse OpenTelemetry Guide](https://langfuse.com/integrations/native/opentelemetry)\n",
    "- [OpenTelemetry Python SDK](https://opentelemetry.io/docs/languages/python/)\n",
    "- [LiteLLM Observability](https://docs.litellm.ai/docs/observability/) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

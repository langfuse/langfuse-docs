{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: Observability for LlamaIndex Workflows with Langfuse  \n",
    "description: Learn how to integrate Langfuse with LlamaIndex Workflows using OpenTelemetry. This cookbook shows you how to trace AI workflows, improve observability, and debug LLM applications.  \n",
    "---\n",
    "\n",
    "# Observability for LlamaIndex Workflows\n",
    "\n",
    "This cookbook demonstrates how to use [Langfuse](https://langfuse.com) to gain real-time observability for your [LlamaIndex Workflows](https://docs.llamaindex.ai/en/stable/module_guides/workflow/). You will learn how to leverage OpenTelemetry to trace each step within a workflow for improved monitoring, debugging, and performance optimization.\n",
    "\n",
    "> **What are LlamaIndex Workflows?** [LlamaIndex Workflows](https://docs.llamaindex.ai/en/stable/module_guides/workflow/) is a flexible, event-driven framework designed to build robust AI agents. In LlamaIndex, workflows are created by chaining together multiple stepsâ€”each defined and validated using the `@step` decorator. Every step processes specific event types, allowing you to orchestrate complex processes such as AI agent collaboration, RAG flows, data extraction, and more.\n",
    "\n",
    "> **What is Langfuse?** [Langfuse](https://langfuse.com) is the open source LLM engineering platform. It helps teams to collaboratively manage prompts, trace applications, debug problems, and evaluate their LLM system in production.\n",
    "\n",
    "## Get Started\n",
    "\n",
    "We'll walk through a simple example of using LlamaIndex Workflows and integrating it with Langfuse via OpenTelemetry.\n",
    "\n",
    "### Step 1: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install opentelemetry-sdk opentelemetry-exporter-otlp\n",
    "%pip install openai gcsfs nest-asyncio \"openinference-instrumentation-llama-index>=2.0.0\"\n",
    "%pip install llama-index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Set Up Environment Variables\n",
    "\n",
    "Configure your Langfuse API keys and OpenTelemetry export settings. Replace the placeholder values with your actual credentials. These settings ensure that OpenTelemetry traces are sent to Langfuse for real-time observability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import base64\n",
    "\n",
    "LANGFUSE_PUBLIC_KEY=\"pk-lf-...\"\n",
    "LANGFUSE_SECRET_KEY=\"sk-lf-...\"\n",
    "LANGFUSE_AUTH=base64.b64encode(f\"{LANGFUSE_PUBLIC_KEY}:{LANGFUSE_SECRET_KEY}\".encode()).decode()\n",
    "\n",
    "# your openai key\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\n",
    "\n",
    "os.environ[\"OTEL_EXPORTER_OTLP_ENDPOINT\"] = \"https://cloud.langfuse.com/api/public/otel\" # ðŸ‡ªðŸ‡º EU data region\n",
    "# os.environ[\"OTEL_EXPORTER_OTLP_ENDPOINT\"] = \"https://us.cloud.langfuse.com/api/public/otel\" # ðŸ‡ºðŸ‡¸ US data region\n",
    "os.environ[\"OTEL_EXPORTER_OTLP_HEADERS\"] = f\"Authorization=Basic {LANGFUSE_AUTH}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Initialize the `LlamaIndexInstrumentor`\n",
    "\n",
    "Initialize the `LlamaIndexInstrumentor` from the [openinference library](https://github.com/Arize-ai/openinference) before your application code. Configure `tracer_provider` and add a span processor to export traces to Langfuse. `OTLPSpanExporter()` (here imported as `HTTPSpanExporter`) uses the endpoint and headers from the environment variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from opentelemetry.sdk import trace as trace_sdk\n",
    "from opentelemetry.sdk.trace.export import SimpleSpanProcessor\n",
    "from opentelemetry.exporter.otlp.proto.http.trace_exporter import (\n",
    "    OTLPSpanExporter as HTTPSpanExporter,\n",
    ")\n",
    "from openinference.instrumentation.llama_index import LlamaIndexInstrumentor\n",
    "\n",
    "# Add Langfuse\n",
    "span_langfuse_processor = SimpleSpanProcessor(HTTPSpanExporter())\n",
    "\n",
    "# Add them to the tracer\n",
    "tracer_provider = trace_sdk.TracerProvider()\n",
    "tracer_provider.add_span_processor(span_processor=span_langfuse_processor)\n",
    "\n",
    "# Instrument the application\n",
    "LlamaIndexInstrumentor().instrument(tracer_provider=tracer_provider)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Create a Simple LlamaIndex Workflows Application\n",
    "\n",
    "In LlamaIndex Workflows, you build event-driven AI agents by defining steps with the `@step` decorator. Each step processes an event and, if appropriate, emits new events. In this example, we create a simple workflow with two steps: one that pre-processes an incoming event and another that generates a reply."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis:\n",
      "This joke plays on the pun of \"fish and ships\" sounding like \"fish and chips,\" a popular dish at seafood restaurants. The joke also incorporates the pirate theme by mentioning a pirate going to a seafood restaurant, which adds an element of humor.\n",
      "\n",
      "Critique:\n",
      "Overall, this joke is light-hearted and playful, making it suitable for a general audience. The pun is clever and well-executed, providing a humorous twist to a common phrase. However, the joke may be considered somewhat predictable or clichÃ©, as puns involving homophones are a common comedic device. Additionally, the joke relies heavily on wordplay and does not have a complex setup or punchline, which may limit its appeal to some audiences. Overall, while this joke is amusing and likely to elicit a chuckle, it may not be considered particularly original or innovative.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.workflow import (\n",
    "    Event,\n",
    "    StartEvent,\n",
    "    StopEvent,\n",
    "    Workflow,\n",
    "    step,\n",
    ")\n",
    "\n",
    "# `pip install llama-index-llms-openai` if you don't already have it\n",
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "class JokeEvent(Event):\n",
    "    joke: str\n",
    "\n",
    "class JokeFlow(Workflow):\n",
    "    llm = OpenAI()\n",
    "\n",
    "    @step\n",
    "    async def generate_joke(self, ev: StartEvent) -> JokeEvent:\n",
    "        topic = ev.topic\n",
    "\n",
    "        prompt = f\"Write your best joke about {topic}.\"\n",
    "        response = await self.llm.acomplete(prompt)\n",
    "        return JokeEvent(joke=str(response))\n",
    "\n",
    "    @step\n",
    "    async def critique_joke(self, ev: JokeEvent) -> StopEvent:\n",
    "        joke = ev.joke\n",
    "\n",
    "        prompt = f\"Give a thorough analysis and critique of the following joke: {joke}\"\n",
    "        response = await self.llm.acomplete(prompt)\n",
    "        return StopEvent(result=str(response))\n",
    "\n",
    "w = JokeFlow(timeout=60, verbose=False)\n",
    "result = await w.run(topic=\"pirates\")\n",
    "print(str(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "source": [
    "### Step 5: View Traces in Langfuse\n",
    "\n",
    "After running your workflow, log in to [Langfuse](https://cloud.langfuse.com) to explore the generated OpenTelemetry traces. You will see detailed logs for each workflow step along with metrics such as token counts, latencies, and execution paths. For example, the trace may show how the preprocessing and reply generation steps were executed in your LlamaIndex Workflows application.\n",
    "\n",
    "![Langfuse Trace Example](https://langfuse.com/images/cookbook/integration-llamaindex-workflows/llamaindex-workflows-example-trace.png)\n",
    "\n",
    "_[Public example trace in Langfuse](https://cloud.langfuse.com/project/cloramnkj0002jz088vzn1ja4/traces/9463f912ef8b9763a62d67445bcbc737?timestamp=2025-02-06T13%3A51%3A33.358Z&observation=7d9e694bfe0dd983)_\n",
    "\n",
    "## References\n",
    "\n",
    "- [Langfuse OpenTelemetry Docs](https://langfuse.com/docs/opentelemetry/get-started)  \n",
    "- [LlamaIndex Workflows Documentation](https://docs.llamaindex.ai/en/stable/module_guides/workflow/)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

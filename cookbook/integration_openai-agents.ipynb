{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: Observe OpenAI Agents with Langfuse\n",
        "description: Learn how to use Langfuse to monitor OpenAI Agents SDK to debug and evaluate your AI agents\n",
        "category: Integrations\n",
        "---\n",
        "\n",
        "# Observe OpenAI Agents with Langfuse\n",
        "\n",
        "This notebook demonstrates how to **integrate Langfuse** into your **OpenAI Agents** workflow.\n",
        "\n",
        "> **What are OpenAI Agents?**: [OpenAI Agents](https://platform.openai.com/docs/guides/agents) are systems that intelligently accomplish tasks, from simple steps to complex, open-ended objectives.\n",
        "\n",
        "> **What is Langfuse?**: [Langfuse](https://langfuse.com/) is an open-source observability platform for AI agents. It helps you visualize and monitor LLM calls, tool usage, cost, latency, and more."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Install Dependencies\n",
        "\n",
        "Below we install the `openai-agents` library (the OpenAI Agents SDK), and the `pydantic-ai[logfire]` OpenTelemetry instrumentation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install openai-agents\n",
        "%pip install nest_asyncio\n",
        "%pip install pydantic-ai[logfire]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Configure Environment & Langfuse Credentials\n",
        "\n",
        "Next, we'll set environment variables to connect to Langfuse and your OpenAI API key. \n",
        "- Replace `pk-lf-...` and `sk-lf-...` with your actual Langfuse keys.\n",
        "- Replace the `OPENAI_API_KEY` with your valid OpenAI API key.\n",
        "\n",
        "If you have multiple regions, use the correct `LANGFUSE_HOST` (EU or US)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import base64\n",
        "\n",
        "# Replace with your Langfuse keys.\n",
        "os.environ[\"LANGFUSE_PUBLIC_KEY\"] = \"pk-lf-...\"  \n",
        "os.environ[\"LANGFUSE_SECRET_KEY\"] = \"sk-lf-...\"  \n",
        "os.environ[\"LANGFUSE_HOST\"] = \"https://cloud.langfuse.com\"  # or 'https://us.cloud.langfuse.com'\n",
        "\n",
        "# Build Basic Auth header.\n",
        "LANGFUSE_AUTH = base64.b64encode(\n",
        "    f\"{os.environ.get('LANGFUSE_PUBLIC_KEY')}:{os.environ.get('LANGFUSE_SECRET_KEY')}\".encode()\n",
        ").decode()\n",
        "\n",
        "# Configure OpenTelemetry endpoint & headers\n",
        "os.environ[\"OTEL_EXPORTER_OTLP_ENDPOINT\"] = os.environ.get(\"LANGFUSE_HOST\") + \"/api/public/otel\"\n",
        "os.environ[\"OTEL_EXPORTER_OTLP_HEADERS\"] = f\"Authorization=Basic {LANGFUSE_AUTH}\"\n",
        "\n",
        "# OpenAI API Key\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"sk-proj-...\" "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Instrumenting the Agent\n",
        "\n",
        "Pydantic Logfire offers an instrumentation for the OpenAi Agent SDK. We use this to send traces to the [Langfuse OpenTelemetry Backend](https://langfuse.com/docs/opentelemetry/get-started).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "import nest_asyncio\n",
        "nest_asyncio.apply()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import logfire\n",
        "\n",
        "# Configure logfire instrumentation.\n",
        "logfire.configure(\n",
        "    service_name='my_agent_service',\n",
        "\n",
        "    send_to_logfire=False,\n",
        ")\n",
        "# This method automatically patches OpenAI Agents to send logs via OTLP to Langfuse.\n",
        "logfire.instrument_openai_agents()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Hello World Example\n",
        "\n",
        "Below we create an **OpenAI Agent** that always replies in **haiku** form. We run it with `Runner.run` and print the final output.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import asyncio\n",
        "from agents import Agent, Runner\n",
        "\n",
        "async def main():\n",
        "    agent = Agent(\n",
        "        name=\"Assistant\",\n",
        "        instructions=\"You only respond in haikus.\",\n",
        "    )\n",
        "\n",
        "    result = await Runner.run(agent, \"Tell me about recursion in programming.\")\n",
        "    print(result.final_output)\n",
        "\n",
        "loop = asyncio.get_running_loop()\n",
        "await loop.create_task(main())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![Example trace in Langfuse](https://langfuse.com/images/cookbook/integration_openai-agents/openai-agent-example-trace.png)\n",
        "\n",
        "**Example**: [Langfuse Trace](https://cloud.langfuse.com/project/cloramnkj0002jz088vzn1ja4/traces/019589d78d9fe296dcdc8975d7127c8f?timestamp=2025-03-12T10%3A12%3A39.967Z)\n",
        "\n",
        "Clicking the link above (or your own project link) lets you view all sub-spans, token usage, latencies, etc., for debugging or optimization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Multi-agent Handoff Example\n",
        "\n",
        "Here we create:\n",
        "- A **Spanish agent** that responds only in Spanish.\n",
        "- An **English agent** that responds only in English.\n",
        "- A **Triage agent** that routes the request to the correct agent based on the input language.\n",
        "\n",
        "Any calls or handoffs are captured as part of the trace. That way, you can see which sub-agent or tool was used, as well as the final result."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from agents import Agent, Runner\n",
        "import asyncio\n",
        "\n",
        "spanish_agent = Agent(\n",
        "    name=\"Spanish agent\",\n",
        "    instructions=\"You only speak Spanish.\",\n",
        ")\n",
        "\n",
        "english_agent = Agent(\n",
        "    name=\"English agent\",\n",
        "    instructions=\"You only speak English\",\n",
        ")\n",
        "\n",
        "triage_agent = Agent(\n",
        "    name=\"Triage agent\",\n",
        "    instructions=\"Handoff to the appropriate agent based on the language of the request.\",\n",
        "    handoffs=[spanish_agent, english_agent],\n",
        ")\n",
        "\n",
        "result = await Runner.run(triage_agent, input=\"Hola, ¿cómo estás?\")\n",
        "print(result.final_output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![Example trace in Langfuse](https://langfuse.com/images/cookbook/integration_openai-agents/openai-agent-example-trace-handoff.png)\n",
        "\n",
        "**Example**: [Langfuse Trace](https://cloud.langfuse.com/project/cloramnkj0002jz088vzn1ja4/traces/019589d7eb4d42b2cd24067ac4eb0a33?timestamp=2025-03-12T10%3A13%3A03.949Z)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Functions Example\n",
        "\n",
        "OpenAI Agents typically allow the agent to call Python functions. With Langfuse instrumentation, you can see which **functions** are called, their arguments, and the return values. Here we define a simple function `get_weather(city: str)` and add it as a tool.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import asyncio\n",
        "from agents import Agent, Runner, function_tool\n",
        "\n",
        "# Example function tool.\n",
        "@function_tool\n",
        "def get_weather(city: str) -> str:\n",
        "    return f\"The weather in {city} is sunny.\"\n",
        "\n",
        "agent = Agent(\n",
        "    name=\"Hello world\",\n",
        "    instructions=\"You are a helpful agent.\",\n",
        "    tools=[get_weather],\n",
        ")\n",
        "\n",
        "async def main():\n",
        "    result = await Runner.run(agent, input=\"What's the weather in Tokyo?\")\n",
        "    print(result.final_output)\n",
        "\n",
        "loop = asyncio.get_running_loop()\n",
        "await loop.create_task(main())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![Example trace in Langfuse](https://langfuse.com/images/cookbook/integration_openai-agents/openai-agent-example-trace-function.png)\n",
        "\n",
        "**Example**: [Langfuse Trace](https://cloud.langfuse.com/project/cloramnkj0002jz088vzn1ja4/traces/019589d809d7c869f1164b0d1bdab0f3?timestamp=2025-03-12T10%3A13%3A11.767Z)\n",
        "\n",
        "When viewing the trace, you’ll see a span capturing the function call `get_weather` and the arguments passed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Grouping Agent Runs\n",
        "\n",
        "In some workflows, you want to group multiple calls into a single trace—for instance, when building a small chain of prompts that all relate to the same user request. You can use a `trace(...)` context manager to nest multiple calls under one top-level trace.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from agents import Agent, Runner, trace\n",
        "import asyncio\n",
        "\n",
        "async def main():\n",
        "    agent = Agent(name=\"Joke generator\", instructions=\"Tell funny jokes.\")\n",
        "\n",
        "    with trace(\"Joke workflow\"):\n",
        "        first_result = await Runner.run(agent, \"Tell me a joke\")\n",
        "        second_result = await Runner.run(agent, f\"Rate this joke: {first_result.final_output}\")\n",
        "        print(f\"Joke: {first_result.final_output}\")\n",
        "        print(f\"Rating: {second_result.final_output}\")\n",
        "\n",
        "loop = asyncio.get_running_loop()\n",
        "await loop.create_task(main())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![Example trace in Langfuse](https://langfuse.com/images/cookbook/integration_openai-agents/openai-agent-example-trace-grouped.png)\n",
        "\n",
        "**Example**: [Langfuse Trace](https://cloud.langfuse.com/project/cloramnkj0002jz088vzn1ja4/traces/019589d88acdc96f860fdd904968b006?timestamp=2025-03-12T10%3A13%3A44.781Z)\n",
        "\n",
        "Each child call is represented as a sub-span under the top-level **Joke workflow** span, making it easy to see the entire conversation or sequence of calls."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}

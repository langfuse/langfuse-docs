{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<!-- NOTEBOOK_METADATA source: \"âš ï¸ Jupyter Notebook\" title: \"Trace the OpenAI Agents SDK with Langfuse\" sidebarTitle: \"OpenAI Agents\" logo: \"/images/integrations/openai_icon.svg\" description: \"Learn how to use Langfuse to monitor OpenAI Agents SDK to debug and evaluate your AI agents\" category: \"Integrations\" -->\n",
        "\n",
        "# Trace the OpenAI Agents SDK with Langfuse\n",
        "\n",
        "This notebook demonstrates how to **integrate Langfuse** into your **OpenAI Agents** workflow to monitor, debug and evaluate your AI agents.\n",
        "\n",
        "> **What is the OpenAI Agents SDK?**: The [OpenAI Agents SDK](https://openai.github.io/openai-agents-python/) is a lightweight, open-source framework that lets developers build AI agents and orchestrate multi-agent workflows. It provides building blocksâ€”such as tools, handoffs, and guardrails to configure large language models with custom instructions and integrated tools. Its Python-first design supports dynamic instructions and function tools for rapid prototyping and integration with external systems.\n",
        "\n",
        "> **What is Langfuse?**: [Langfuse](https://langfuse.com/) is an open-source observability platform for AI agents. It helps you visualize and monitor LLM calls, tool usage, cost, latency, and more."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Install Dependencies\n",
        "\n",
        "Below we install the `openai-agents` library (the OpenAI Agents SDK), and the [OpenInference OpenAI Agents instrumentation](https://github.com/Arize-ai/openinference/tree/main/python/instrumentation/openinference-instrumentation-openai-agents) library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install openai-agents langfuse nest_asyncio openinference-instrumentation-openai-agents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Configure Environment & Langfuse Credentials\n",
        "\n",
        "Next, set up your Langfuse API keys. You can get these keys by signing up for a free [Langfuse Cloud](https://cloud.langfuse.com/) account or by [self-hosting Langfuse](https://langfuse.com/self-hosting). These environment variables are essential for the Langfuse client to authenticate and send data to your Langfuse project."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Get keys for your project from the project settings page: https://cloud.langfuse.com\n",
        "os.environ[\"LANGFUSE_PUBLIC_KEY\"] = \"pk-lf-...\" \n",
        "os.environ[\"LANGFUSE_SECRET_KEY\"] = \"sk-lf-...\" \n",
        "os.environ[\"LANGFUSE_BASE_URL\"] = \"https://cloud.langfuse.com\" # ðŸ‡ªðŸ‡º EU region\n",
        "# os.environ[\"LANGFUSE_BASE_URL\"] = \"https://us.cloud.langfuse.com\" # ðŸ‡ºðŸ‡¸ US region\n",
        "\n",
        "# Your openai key\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"sk-proj-...\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Instrumenting the Agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "import nest_asyncio\n",
        "nest_asyncio.apply()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, we initialize the [OpenInference OpenAI Agents instrumentation](https://github.com/Arize-ai/openinference/tree/main/python/instrumentation/openinference-instrumentation-openai-agents). This third-party instrumentation automatically captures OpenAI Agents operations and exports OpenTelemetry (OTel) spans to Langfuse."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from openinference.instrumentation.openai_agents import OpenAIAgentsInstrumentor\n",
        "\n",
        "OpenAIAgentsInstrumentor().instrument()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now initialize the Langfuse client. `get_client()` initializes the Langfuse client using the credentials provided in the environment variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langfuse import get_client\n",
        " \n",
        "langfuse = get_client()\n",
        " \n",
        "# Verify connection\n",
        "if langfuse.auth_check():\n",
        "    print(\"Langfuse client is authenticated and ready!\")\n",
        "else:\n",
        "    print(\"Authentication failed. Please check your credentials and host.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Hello World Example\n",
        "\n",
        "Below we create an **OpenAI Agent** that always replies in **haiku** form. We run it with `Runner.run` and print the final output.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import asyncio\n",
        "from agents import Agent, Runner\n",
        "\n",
        "async def main():\n",
        "    agent = Agent(\n",
        "        name=\"Assistant\",\n",
        "        instructions=\"You only respond in haikus.\",\n",
        "    )\n",
        "\n",
        "    result = await Runner.run(agent, \"Tell me about recursion in programming.\")\n",
        "    print(result.final_output)\n",
        "\n",
        "loop = asyncio.get_running_loop()\n",
        "await loop.create_task(main())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![Example trace in Langfuse](https://langfuse.com/images/cookbook/integration_openai-agents/openai-agent-example-trace.png)\n",
        "\n",
        "**Example**: [Langfuse Trace](https://cloud.langfuse.com/project/cloramnkj0002jz088vzn1ja4/traces/483a2afb22b41ee49d7d1e5b41b4967a?timestamp=2025-09-30T09%3A01%3A29.194Z&observation=e3e3580728f3fec4)\n",
        "\n",
        "Clicking the link above (or your own project link) lets you view all sub-spans, token usage, latencies, etc., for debugging or optimization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Multi-agent Handoff Example\n",
        "\n",
        "Here we create:\n",
        "- A **Spanish agent** that responds only in Spanish.\n",
        "- An **English agent** that responds only in English.\n",
        "- A **Triage agent** that routes the request to the correct agent based on the input language.\n",
        "\n",
        "Any calls or handoffs are captured as part of the trace. That way, you can see which sub-agent or tool was used, as well as the final result."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from agents import Agent, Runner\n",
        "import asyncio\n",
        "\n",
        "spanish_agent = Agent(\n",
        "    name=\"Spanish agent\",\n",
        "    instructions=\"You only speak Spanish.\",\n",
        ")\n",
        "\n",
        "english_agent = Agent(\n",
        "    name=\"English agent\",\n",
        "    instructions=\"You only speak English\",\n",
        ")\n",
        "\n",
        "triage_agent = Agent(\n",
        "    name=\"Triage agent\",\n",
        "    instructions=\"Handoff to the appropriate agent based on the language of the request.\",\n",
        "    handoffs=[spanish_agent, english_agent],\n",
        ")\n",
        "\n",
        "result = await Runner.run(triage_agent, input=\"Hola, Â¿cÃ³mo estÃ¡s?\")\n",
        "print(result.final_output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![Example trace in Langfuse](https://langfuse.com/images/cookbook/integration_openai-agents/openai-agent-example-trace-handoff.png)\n",
        "\n",
        "**Example**: [Langfuse Trace](https://cloud.langfuse.com/project/cloramnkj0002jz088vzn1ja4/traces/c376e44920527b875add9d97b4ed9312?observation=94a1dc00d7067ae8&timestamp=2025-09-30T09%3A03%3A51.191Z)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Functions Example\n",
        "\n",
        "The OpenAI Agents SDK allows the agent to call Python functions. With Langfuse instrumentation, you can see which **functions** are called, their arguments, and the return values. Here we define a simple function `get_weather(city: str)` and add it as a tool.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import asyncio\n",
        "from agents import Agent, Runner, function_tool\n",
        "\n",
        "# Example function tool.\n",
        "@function_tool\n",
        "def get_weather(city: str) -> str:\n",
        "    return f\"The weather in {city} is sunny.\"\n",
        "\n",
        "agent = Agent(\n",
        "    name=\"Hello world\",\n",
        "    instructions=\"You are a helpful agent.\",\n",
        "    tools=[get_weather],\n",
        ")\n",
        "\n",
        "async def main():\n",
        "    result = await Runner.run(agent, input=\"What's the weather in Tokyo?\")\n",
        "    print(result.final_output)\n",
        "\n",
        "loop = asyncio.get_running_loop()\n",
        "await loop.create_task(main())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![Example trace in Langfuse](https://langfuse.com/images/cookbook/integration_openai-agents/openai-agent-example-trace-function.png)\n",
        "\n",
        "**Example**: [Langfuse Trace](https://cloud.langfuse.com/project/cloramnkj0002jz088vzn1ja4/traces/fee618f96dc31e0ca38b2f7b26eb8b29?timestamp=2025-09-30T09%3A03%3A59.962Z&observation=5b99c3e3411ed17b)\n",
        "\n",
        "When viewing the trace, youâ€™ll see a span capturing the function call `get_weather` and the arguments passed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Grouping Agent Runs\n",
        "\n",
        "In some workflows, you want to group multiple calls into a single traceâ€”for instance, when building a small chain of prompts that all relate to the same user request. You can use a `trace(...)` context manager to nest multiple calls under one top-level trace.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from agents import Agent, Runner, trace\n",
        "import asyncio\n",
        "\n",
        "async def main():\n",
        "    agent = Agent(name=\"Joke generator\", instructions=\"Tell funny jokes.\")\n",
        "\n",
        "    with trace(\"Joke workflow\"):\n",
        "        first_result = await Runner.run(agent, \"Tell me a joke\")\n",
        "        second_result = await Runner.run(agent, f\"Rate this joke: {first_result.final_output}\")\n",
        "        print(f\"Joke: {first_result.final_output}\")\n",
        "        print(f\"Rating: {second_result.final_output}\")\n",
        "\n",
        "loop = asyncio.get_running_loop()\n",
        "await loop.create_task(main())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![Example trace in Langfuse](https://langfuse.com/images/cookbook/integration_openai-agents/openai-agent-example-trace-grouped.png)\n",
        "\n",
        "**Example**: [Langfuse Trace](https://cloud.langfuse.com/project/cloramnkj0002jz088vzn1ja4/traces/afa1ae379b6c3a5333e2bab357f31cdc?timestamp=2025-09-30T09%3A04%3A06.678Z&observation=496efdc060a5399b)\n",
        "\n",
        "Each child call is represented as a sub-span under the top-level **Joke workflow** span, making it easy to see the entire conversation or sequence of calls."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Link Langfuse Prompt\n",
        "\n",
        "If you manage your prompt with [Langfuse Prompt Management](https://langfuse.com/docs/prompt-management/overview), you can link the used prompt to the trace by setting up an OTel Span processor.\n",
        "\n",
        "<!-- CALLOUT_START type: \"info\" emoji: \"âš ï¸\" -->\n",
        "**Limitation:** This method links the Langfuse Prompt to all generation spans in the trace that start with the defined string (see next cell).\n",
        "<!-- CALLOUT_END -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from contextvars import ContextVar\n",
        "from typing import Optional\n",
        "from opentelemetry import context as context_api, trace\n",
        "from opentelemetry.sdk.trace import TracerProvider\n",
        "from opentelemetry.sdk.trace.export import Span, SpanProcessor\n",
        "\n",
        "prompt_info_var = ContextVar(\"prompt_info\", default=None)\n",
        "\n",
        "# Make sure to set the name of the generation spans in your trace\n",
        "class LangfuseProcessor(SpanProcessor):\n",
        "    def on_start(self, span: \"Span\", parent_context: Optional[context_api.Context] = None) -> None:\n",
        "        if span.name.startswith(\"response\"): # The name of the generation spans in your trace \n",
        "            prompt_info = prompt_info_var.get()\n",
        "            if prompt_info:\n",
        "                span.set_attribute(\"langfuse.prompt.name\", prompt_info.get(\"name\"))\n",
        "                span.set_attribute(\"langfuse.prompt.version\", prompt_info.get(\"version\"))\n",
        "\n",
        "\n",
        "# 1) Register your processor BEFORE instantiating Langfuse\n",
        "trace.get_tracer_provider().add_span_processor(LangfuseProcessor())\n",
        "\n",
        "# 2) Now bring up Langfuse (it will attach its own span processor/exporter to the same provider)\n",
        "from langfuse import Langfuse, get_client\n",
        "langfuse = get_client()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from openinference.instrumentation.openai_agents import OpenAIAgentsInstrumentor\n",
        "\n",
        "OpenAIAgentsInstrumentor().instrument()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fetch the prompt from Langfuse Prompt Management\n",
        "langfuse_prompt = langfuse.get_prompt(\"movie-critic\")\n",
        "system_prompt = langfuse_prompt.compile(criticlevel = \"expert\", movie = \"Dune 2\")\n",
        "\n",
        "# Pass the prompt to the SpanProcessor\n",
        "prompt_info_var.set({\n",
        "    \"name\": langfuse_prompt.name,\n",
        "    \"version\": langfuse_prompt.version,\n",
        "})\n",
        "\n",
        "# Run the agent ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Resources\n",
        "\n",
        "- [Example notebook on evaluating agents](https://langfuse.com/guides/cookbook/example_evaluating_openai_agents) with Langfuse. \n",
        "\n",
        "<!-- MARKDOWN_COMPONENT name: \"LearnMore\" path: \"@/components-mdx/integration-learn-more.mdx\" -->"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}

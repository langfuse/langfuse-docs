{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ki7E44X5ViQB"
   },
   "source": [
    "---\n",
    "title: OSS Observability for OpenAI Assistants API\n",
    "description: Use of Langfuse decorator to trace calls made to openai assistants\n",
    "category: Integrations\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cookbook: Observability for OpenAI Assistants API with Langfuse\n",
    "\n",
    "This cookbook demonstrates how to use the Langfuse [`observe` decorator](https://langfuse.com/docs/sdk/python/decorators) to trace calls made to the [OpenAI Assistants API](https://platform.openai.com/docs/assistants/overview). It covers creating an assistant, running it on a thread, and observing the execution with [Langfuse tracing](https://langfuse.com/docs/tracing).\n",
    "\n",
    "Note: The native [OpenAI SDK wrapper](https://langfuse.com/docs/integrations/openai/python/get-started) does not support tracing of the OpenAI assistants API, you need to instrument it via the decorator as shown in this notebook.\n",
    "\n",
    "## What is the Assistants API?\n",
    "\n",
    "The Assistants API from OpenAI allows developers to build AI assistants that can utilize multiple tools and data sources in parallel, such as code interpreters, file search, and custom tools created by calling functions. These assistants can access OpenAI's language models like GPT-4 with specific prompts, maintain persistent conversation histories, and process various file formats like text, images, and spreadsheets. Developers can fine-tune the language models on their own data and control aspects like output randomness. The API provides a framework for creating AI applications that combine language understanding with external tools and data.\n",
    "\n",
    "## Example Trace Output\n",
    "\n",
    "![OpenAI Assistants Trace in Langfuse](https://langfuse.com/images/docs/openai-assistants-trace.png)\n",
    "\n",
    "## Setup\n",
    "\n",
    "Install the required packages:\n",
    "\n",
    "_**Note:** This guide uses our Python SDK v2. We have a new, improved SDK available based on OpenTelemetry. Please check out the [SDK v3](https://langfuse.com/docs/sdk/python/sdk-v3) for a more powerful and simpler to use SDK._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade openai \"langfuse<3.0.0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mfMAzJYcirtK"
   },
   "source": [
    "Set your environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mfMAzJYcirtK"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Get keys for your project from the project settings page\n",
    "# https://cloud.langfuse.com\n",
    "os.environ[\"LANGFUSE_PUBLIC_KEY\"] = \"\"\n",
    "os.environ[\"LANGFUSE_SECRET_KEY\"] = \"\"\n",
    "os.environ[\"LANGFUSE_HOST\"] = \"https://cloud.langfuse.com\" # ðŸ‡ªðŸ‡º EU region\n",
    "# os.environ[\"LANGFUSE_HOST\"] = \"https://us.cloud.langfuse.com\" # ðŸ‡ºðŸ‡¸ US region\n",
    "\n",
    "# Your openai key\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mfMAzJYcirtK"
   },
   "source": [
    "## Step by step\n",
    "\n",
    "### 1. Creating an Assistant\n",
    "\n",
    "Use the `client.beta.assistants.create` method to create a new assistant. Alternatively you can also create the assistant via the OpenAI console:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mfMAzJYcirtK"
   },
   "outputs": [],
   "source": [
    "from langfuse import observe\n",
    "from openai import OpenAI\n",
    "\n",
    "@observe()\n",
    "def create_assistant():\n",
    "    client = OpenAI()\n",
    "    \n",
    "    assistant = client.beta.assistants.create(\n",
    "        name=\"Math Tutor\",\n",
    "        instructions=\"You are a personal math tutor. Answer questions briefly, in a sentence or less.\",\n",
    "        model=\"gpt-4\"\n",
    "    )\n",
    "    \n",
    "    return assistant\n",
    "\n",
    "assistant = create_assistant()\n",
    "print(f\"Created assistant: {assistant.id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Public link of example trace](https://cloud.langfuse.com/project/cloramnkj0002jz088vzn1ja4/traces/e659e523-2957-4452-83c4-426f29783923) of assistant creation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mfMAzJYcirtK"
   },
   "source": [
    "### 2. Running the Assistant\n",
    "\n",
    "Create a thread and run the assistant on it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mfMAzJYcirtK"
   },
   "outputs": [],
   "source": [
    "@observe()\n",
    "def run_assistant(assistant_id, user_input):\n",
    "    client = OpenAI()\n",
    "    \n",
    "    thread = client.beta.threads.create()\n",
    "\n",
    "    client.beta.threads.messages.create(\n",
    "        thread_id=thread.id, role=\"assistant\", content=\"I am a math tutor that likes to help math students, how can I help?\"\n",
    "    )\n",
    "    client.beta.threads.messages.create(\n",
    "        thread_id=thread.id, role=\"user\", content=user_input\n",
    "    )\n",
    "    \n",
    "    run = client.beta.threads.runs.create(\n",
    "        thread_id=thread.id,\n",
    "        assistant_id=assistant_id,\n",
    "    )\n",
    "    \n",
    "    return run, thread\n",
    "\n",
    "user_input = \"I need to solve the equation `3x + 11 = 14`. Can you help me?\"\n",
    "run, thread = run_assistant(assistant.id, user_input)\n",
    "print(f\"Created run: {run.id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Public link of example trace](https://cloud.langfuse.com/project/cloramnkj0002jz088vzn1ja4/traces/e659e523-2957-4452-83c4-426f29783923) of message and trace creation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Getting the Response\n",
    "\n",
    "Retrieve the assistant's response from the thread:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mfMAzJYcirtK"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from langfuse import get_client\n",
    "langfuse = get_client()\n",
    "\n",
    "@observe()\n",
    "def get_response(thread_id, run_id):\n",
    "    client = OpenAI()\n",
    "    \n",
    "    messages = client.beta.threads.messages.list(thread_id=thread_id, order=\"asc\")\n",
    "    assistant_response = messages.data[-1].content[0].text.value\n",
    "\n",
    "    # get run for token counts\n",
    "    run_log = client.beta.threads.runs.retrieve(\n",
    "        thread_id=thread_id,\n",
    "        run_id=run_id\n",
    "    )\n",
    "\n",
    "    message_log = client.beta.threads.messages.list(\n",
    "        thread_id=thread_id,\n",
    "    )\n",
    "    input_messages = [{\"role\": message.role, \"content\": message.content[0].text.value} for message in message_log.data[::-1][:-1]]\n",
    "\n",
    "    # log internal generation within the openai assistant as a separate child generation to langfuse\n",
    "    # get langfuse client used by the decorator, uses the low-level Python SDK\n",
    "    langfuse_client = langfuse.client_instance\n",
    "    # pass trace_id and current observation ids to the newly created child generation\n",
    "    langfuse_client.generation(\n",
    "        trace_id=langfuse.get_current_trace_id(),\n",
    "        parent_observation_id=langfuse.get_current_observation_id(),\n",
    "        model=run.model,\n",
    "        usage_details=run.usage,\n",
    "        input=input_messages,\n",
    "        output=assistant_response\n",
    "    )\n",
    "    \n",
    "    return assistant_response, run\n",
    "\n",
    "response = get_response(thread.id, run.id)\n",
    "print(f\"Assistant response: {response[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Public link of example trace](https://cloud.langfuse.com/project/cloramnkj0002jz088vzn1ja4/traces/e0933ea5-6806-4eb7-aed8-a42d23c57096?observation=401fb816-22e5-45ac-a4c9-e437b120f2e7) of fetching the response**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All in one trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "@observe()\n",
    "def run_math_tutor(user_input):\n",
    "    assistant = create_assistant()\n",
    "    run, thread = run_assistant(assistant.id, user_input)\n",
    "\n",
    "    time.sleep(5) # notebook only, wait for the assistant to finish\n",
    "\n",
    "    response = get_response(thread.id, run.id)\n",
    "    \n",
    "    return response[0]\n",
    "\n",
    "user_input = \"I need to solve the equation `3x + 11 = 14`. Can you help me?\"\n",
    "response = run_math_tutor(user_input)\n",
    "print(f\"Assistant response: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Langfuse trace shows the flow of creating the assistant, running it on a thread with user input, and retrieving the response, along with the captured input/output data.\n",
    "\n",
    "**[Public link of example trace](https://cloud.langfuse.com/project/cloramnkj0002jz088vzn1ja4/traces/b3b7b128-5664-4f42-9fab-31999da9e2f1)**\n",
    "\n",
    "![OpenAI Assistants Trace in Langfuse](https://langfuse.com/images/docs/openai-assistants-trace.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learn more\n",
    "\n",
    "If you use non-Assistants API endpoints, you can use the OpenAI SDK wrapper for tracing. Check out the [Langfuse documentation](https://langfuse.com/docs/integrations/openai/python/get-started) for more details."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

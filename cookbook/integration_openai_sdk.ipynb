{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ki7E44X5ViQB"
   },
   "source": [
    "---\n",
    "description: Drop-in replacement of OpenAI SDK to get full observability in Langfuse by changing only the import\n",
    "---\n",
    "\n",
    "# OpenAI Integration (Python)\n",
    "\n",
    "If you use the OpenAI Python SDK, you can use the Langfuse **drop-in replacement** to get full logging by changing only the import.\n",
    "\n",
    "```diff\n",
    "- import openai\n",
    "+ from langfuse.openai import openai\n",
    "```\n",
    "\n",
    "_**Current limitations**_\n",
    "\n",
    "- Each generation creates a new trace in Langfuse\n",
    "- It is not possible to attach a score to the generation\n",
    "\n",
    "Use this integration if you want to get started with Langfuse super fast and mostly care about tracking costs and monitoring model inputs and outputs. This integration is work in progress and features will be added over time. Suggestions and PRs welcome!\n",
    "\n",
    "For full flexibility, consider using the fully-featured [Python SDK](https://langfuse.com/docs/integrations/sdk/python)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uq04G_FSWjF-"
   },
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hVOOiBtUPtOO",
    "outputId": "3f41cc29-6698-486c-d097-48cb53e64e88"
   },
   "outputs": [],
   "source": [
    "%pip install langfuse openai --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "7-s-hY3PPupC"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# get keys for your project from https://cloud.langfuse.com\n",
    "os.environ[\"LANGFUSE_PUBLIC_KEY\"] = \"\"\n",
    "os.environ[\"LANGFUSE_SECRET_KEY\"] = \"\"\n",
    "\n",
    "# your openai key\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
    "\n",
    "# if you do not use Langfuse Cloud\n",
    "# os.environ[\"LANGFUSE_HOST\"] = \"http://localhost:3000\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yxw57KFaWtfr"
   },
   "source": [
    "## 2. Replace import\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "ldSEJ0bAP4sj"
   },
   "outputs": [],
   "source": [
    "# instead of: import openai\n",
    "from langfuse.openai import openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IbQ7NlJuXGnx"
   },
   "source": [
    "## 3. Use SDK as usual\n",
    "\n",
    "_No changes required._\n",
    "\n",
    "Optionally:\n",
    "- Set `name` to identify a specific type of generation\n",
    "- Set `metadata` with additional information that you want to see in Langfuse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2ovnAAdbaLmD"
   },
   "source": [
    "### Chat completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "c8RhokKUP9I0"
   },
   "outputs": [],
   "source": [
    "completion = openai.ChatCompletion.create(\n",
    "  name=\"test-chat\",\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "      {\"role\": \"system\", \"content\": \"You are a very accurate calculator. You output only the result of the calculation.\"},\n",
    "      {\"role\": \"user\", \"content\": \"1 + 1 = \"}],\n",
    "  temperature=0,\n",
    "  metadata={\"someMetadataKey\": \"someValue\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ky7CtCNzaSrn"
   },
   "source": [
    "### Functions\n",
    "\n",
    "Simple example using Pydantic to generate the function schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jJfBdHowaRgs",
    "outputId": "1269ff45-e56c-4968-8fad-832a0b854381"
   },
   "outputs": [],
   "source": [
    "%pip install pydantic==1.* --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "2gA-zGk7VYYp"
   },
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from pydantic import BaseModel\n",
    "\n",
    "class StepByStepAIResponse(BaseModel):\n",
    "    title: str\n",
    "    steps: List[str]\n",
    "schema = StepByStepAIResponse.schema() # returns a dict like JSON schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "ORtNcN4-afDC"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "response = openai.ChatCompletion.create(\n",
    "    name=\"test-function\",\n",
    "    model=\"gpt-3.5-turbo-0613\",\n",
    "    messages=[\n",
    "       {\"role\": \"user\", \"content\": \"Explain how to assemble a PC\"}\n",
    "    ],\n",
    "    functions=[\n",
    "        {\n",
    "          \"name\": \"get_answer_for_user_query\",\n",
    "          \"description\": \"Get user answer in series of steps\",\n",
    "          \"parameters\": StepByStepAIResponse.schema()\n",
    "        }\n",
    "    ],\n",
    "    function_call={\"name\": \"get_answer_for_user_query\"}\n",
    ")\n",
    "\n",
    "output = json.loads(response.choices[0][\"message\"][\"function_call\"][\"arguments\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qurrm-Ntp24O"
   },
   "source": [
    "## 4. Debug & measure in Langfuse\n",
    "\n",
    "Go to https://cloud.langfuse.com or your own instance\n",
    "\n",
    "### Dashboard\n",
    "![Dashboard](https://langfuse.com/images/docs/openai-dashboard.png)\n",
    "\n",
    "### List of generations\n",
    "![List of generations](https://langfuse.com/images/docs/openai-generation-list.png)\n",
    "\n",
    "### Chat completion\n",
    "![Chat completion](https://langfuse.com/images/docs/openai-chat.png)\n",
    "\n",
    "### Function\n",
    "![Function](https://langfuse.com/images/docs/openai-function.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Group multiple generations into a single trace\n",
    "\n",
    "Many applications require more than one OpenAI call. By setting the `trace_id` you can group them into a single trace for improved debugging and reporting. The `trace_id` usually comes from your own application or you create a random one to group calls together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from uuid import uuid4\n",
    "trace_id = str(uuid4())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "country = openai.ChatCompletion.create(\n",
    "  name=\"random-country\",\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "      {\"role\": \"user\", \"content\": \"Pick a random country\"}],\n",
    "  temperature=1,\n",
    "  trace_id=trace_id\n",
    ")[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "capital = openai.ChatCompletion.create(\n",
    "  name=\"geography-teacher\",\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "      {\"role\": \"system\", \"content\": \"You are a Geography teacher helping students learn the capitals of countries. Output only the capital when being asked.\"},\n",
    "      {\"role\": \"user\", \"content\": country}],\n",
    "  temperature=0,\n",
    "  trace_id=trace_id\n",
    ")[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "poem = openai.ChatCompletion.create(\n",
    "  name=\"poet\",\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "      {\"role\": \"system\", \"content\": \"You are a poet. Create a poem about a city.\"},\n",
    "      {\"role\": \"user\", \"content\": capital}],\n",
    "  temperature=1,\n",
    "  max_tokens=200,\n",
    "  trace_id=trace_id\n",
    ")[\"choices\"][0][\"message\"][\"content\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Trace with multiple OpenAI calls](https://langfuse.com/images/docs/openai-trace-grouped.png)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ki7E44X5ViQB"
      },
      "source": [
        "---\n",
        "description: Drop-in replacement of OpenAI SDK to get full observability in Langfuse by changing only the import\n",
        "---\n",
        "\n",
        "# OpenAI Integration (Python)\n",
        "\n",
        "If you use the OpenAI Python SDK, you can use the Langfuse **drop-in replacement** to get full logging by changing only the import.\n",
        "\n",
        "```diff\n",
        "- import openai\n",
        "+ from langfuse.openai import openai\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uq04G_FSWjF-"
      },
      "source": [
        "## 1. Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hVOOiBtUPtOO"
      },
      "outputs": [],
      "source": [
        "# supports openai 1.x and 0.x\n",
        "%pip install langfuse openai --upgrade"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "7-s-hY3PPupC"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# get keys for your project from https://cloud.langfuse.com\n",
        "os.environ[\"LANGFUSE_PUBLIC_KEY\"] = \"\"\n",
        "os.environ[\"LANGFUSE_SECRET_KEY\"] = \"\"\n",
        "\n",
        "# your openai key\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
        "\n",
        "# if you do not use Langfuse Cloud\n",
        "# os.environ[\"LANGFUSE_HOST\"] = \"http://localhost:3000\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yxw57KFaWtfr"
      },
      "source": [
        "## 2. Replace import\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ldSEJ0bAP4sj"
      },
      "outputs": [],
      "source": [
        "# instead of: import openai\n",
        "from langfuse.openai import openai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XYoil3FcOIQt"
      },
      "source": [
        "The integration is compatible with OpenAI SDK versions `>=0.27.8`.\n",
        "\n",
        "Async functions and streaming are supported for OpenAI SDK versions `>=1.0.0`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aHlrUFA9GJsi"
      },
      "source": [
        "### Attributes\n",
        "\n",
        "Instead of setting the environment variables before importing the SDK, you can also use the following attributes after the import. This works for the async OpenAI client as well:\n",
        "\n",
        "| Attribute |Description   | Default value  \n",
        "| --- | --- | ---\n",
        "| `openai.langfuse_host` | BaseUrl of the Langfuse API | `LANGFUSE_HOST` environment variable, defaults to `\"https://cloud.langfuse.com\"`       \n",
        "| `openai.langfuse_public_key` | Public key of the Langfuse API | `LANGFUSE_PUBLIC_KEY` environment variable       \n",
        "| `openai.langfuse_secret_key` | Private key of the Langfuse API | `LANGFUSE_SECRET_KEY` environment variable       "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "oHWiOtBaJzMG"
      },
      "outputs": [],
      "source": [
        "# Instead of environment variables, use the module variables to configure Langfuse\n",
        "\n",
        "# openai.langfuse_host = '...'\n",
        "# openai.langfuse_public_key = '...'\n",
        "# openai.langfuse_secret_key = '...'\n",
        "\n",
        "# This works for the async client as well\n",
        "# from langfuse.openai import AsyncOpenAI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IbQ7NlJuXGnx"
      },
      "source": [
        "## 3. Use SDK as usual\n",
        "\n",
        "_No changes required._\n",
        "\n",
        "Optionally:\n",
        "- Set `name` to identify a specific type of generation\n",
        "- Set `metadata` with additional information that you want to see in Langfuse"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ovnAAdbaLmD"
      },
      "source": [
        "### Chat completion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "c8RhokKUP9I0"
      },
      "outputs": [],
      "source": [
        "completion = openai.chat.completions.create(\n",
        "  name=\"test-chat\",\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=[\n",
        "      {\"role\": \"system\", \"content\": \"You are a very accurate calculator. You output only the result of the calculation.\"},\n",
        "      {\"role\": \"user\", \"content\": \"1 + 1 = \"}],\n",
        "  temperature=0,\n",
        "  metadata={\"someMetadataKey\": \"someValue\"},\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SAqxBgOqKTzO"
      },
      "source": [
        "#### Streaming\n",
        "\n",
        "Simple example using the OpenAI streaming functionality."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b9gRlb2rKTaA"
      },
      "outputs": [],
      "source": [
        "completion = openai.chat.completions.create(\n",
        "  name=\"test-chat\",\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=[\n",
        "      {\"role\": \"system\", \"content\": \"You are a very accurate calculator. You output only the result of the calculation.\"},\n",
        "      {\"role\": \"user\", \"content\": \"1 + 1 = \"}],\n",
        "  temperature=0,\n",
        "  metadata={\"someMetadataKey\": \"someValue\"},\n",
        "  stream=True\n",
        ")\n",
        "\n",
        "for chunk in completion:\n",
        "  print(chunk)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F2pvm0qLKg7Q"
      },
      "source": [
        "#### Async support\n",
        "\n",
        "Simple example using the OpenAI async client. It takes the Langfuse configurations either from the environment variables or from the attributes on the `openai` module."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Hggwggv_MKpV"
      },
      "outputs": [],
      "source": [
        "from langfuse.openai import AsyncOpenAI\n",
        "\n",
        "async_client = AsyncOpenAI()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZIUKD8Z3KmvQ"
      },
      "outputs": [],
      "source": [
        "completion = await async_client.chat.completions.create(\n",
        "  name=\"test-chat\",\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=[\n",
        "      {\"role\": \"system\", \"content\": \"You are a very accurate calculator. You output only the result of the calculation.\"},\n",
        "      {\"role\": \"user\", \"content\": \"1 + 100 = \"}],\n",
        "  temperature=0,\n",
        "  metadata={\"someMetadataKey\": \"someValue\"},\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ky7CtCNzaSrn"
      },
      "source": [
        "### Functions\n",
        "\n",
        "Simple example using Pydantic to generate the function schema."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jJfBdHowaRgs"
      },
      "outputs": [],
      "source": [
        "%pip install pydantic --upgrade"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2gA-zGk7VYYp"
      },
      "outputs": [],
      "source": [
        "from typing import List\n",
        "from pydantic import BaseModel\n",
        "\n",
        "class StepByStepAIResponse(BaseModel):\n",
        "    title: str\n",
        "    steps: List[str]\n",
        "schema = StepByStepAIResponse.schema() # returns a dict like JSON schema"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ORtNcN4-afDC"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "response = openai.chat.completions.create(\n",
        "    name=\"test-function\",\n",
        "    model=\"gpt-3.5-turbo-0613\",\n",
        "    messages=[\n",
        "       {\"role\": \"user\", \"content\": \"Explain how to assemble a PC\"}\n",
        "    ],\n",
        "    functions=[\n",
        "        {\n",
        "          \"name\": \"get_answer_for_user_query\",\n",
        "          \"description\": \"Get user answer in series of steps\",\n",
        "          \"parameters\": StepByStepAIResponse.schema()\n",
        "        }\n",
        "    ],\n",
        "    function_call={\"name\": \"get_answer_for_user_query\"}\n",
        ")\n",
        "\n",
        "output = json.loads(response.choices[0].message.function_call.arguments)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qurrm-Ntp24O"
      },
      "source": [
        "## 4. Debug & measure in Langfuse\n",
        "\n",
        "Go to https://cloud.langfuse.com or your own instance\n",
        "\n",
        "### Dashboard\n",
        "![Dashboard](https://langfuse.com/images/docs/openai-dashboard.png)\n",
        "\n",
        "### List of generations\n",
        "![List of generations](https://langfuse.com/images/docs/openai-generation-list.png)\n",
        "\n",
        "### Chat completion\n",
        "![Chat completion](https://langfuse.com/images/docs/openai-chat.png)\n",
        "\n",
        "### Function\n",
        "![Function](https://langfuse.com/images/docs/openai-function.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RITvEOforPPh"
      },
      "source": [
        "## 5. Track OpenAI errors\n",
        "\n",
        "Langfuse automatically monitors OpenAI errors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F-gFsEVXrPPh"
      },
      "outputs": [],
      "source": [
        "# Cause an error by attempting to use a host that does not exist.\n",
        "openai.api_base = \"https://example.com\"\n",
        "\n",
        "country = openai.chat.completions.create(\n",
        "  name=\"will-error\",\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=[\n",
        "      {\"role\": \"user\", \"content\": \"How are you?\"}],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CvZ_4dwArPPh"
      },
      "source": [
        "Throws error ðŸ‘†\n",
        "\n",
        "![Openai error](https://langfuse.com/images/docs/openai-error.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JLmHuBCsrPPh"
      },
      "outputs": [],
      "source": [
        "# Reset\n",
        "openai.api_base = \"https://api.openai.com/v1\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Su1OaQq3rPPh"
      },
      "source": [
        "## 5. Group multiple generations into a single trace\n",
        "\n",
        "Many applications require more than one OpenAI call. By setting the `trace_id` you can group them into a single trace for improved debugging and reporting. The `trace_id` usually comes from your own application or you create a random one to group calls together."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QIkaOeworPPh"
      },
      "outputs": [],
      "source": [
        "from uuid import uuid4\n",
        "trace_id = str(uuid4())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ols9EL8mrPPh"
      },
      "outputs": [],
      "source": [
        "country = openai.chat.completions.create(\n",
        "  name=\"random-country\",\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=[\n",
        "      {\"role\": \"user\", \"content\": \"Pick a random country\"}],\n",
        "  temperature=1,\n",
        "  trace_id=trace_id\n",
        ").choices[0].message.content\n",
        "\n",
        "capital = openai.chat.completions.create(\n",
        "  name=\"geography-teacher\",\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=[\n",
        "      {\"role\": \"system\", \"content\": \"You are a Geography teacher helping students learn the capitals of countries. Output only the capital when being asked.\"},\n",
        "      {\"role\": \"user\", \"content\": country}],\n",
        "  temperature=0,\n",
        "  trace_id=trace_id\n",
        ").choices[0].message.content\n",
        "\n",
        "poem = openai.chat.completions.create(\n",
        "  name=\"poet\",\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=[\n",
        "      {\"role\": \"system\", \"content\": \"You are a poet. Create a poem about a city.\"},\n",
        "      {\"role\": \"user\", \"content\": capital}],\n",
        "  temperature=1,\n",
        "  max_tokens=200,\n",
        "  trace_id=trace_id\n",
        ").choices[0].message.content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ehx2NZuIrPPh"
      },
      "source": [
        "![Trace with multiple OpenAI calls](https://langfuse.com/images/docs/openai-trace-grouped.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O3jxed-VrPPi"
      },
      "source": [
        "## 6. Add scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMO6tn53rPPi"
      },
      "source": [
        "You can also add [scores](https://langfuse.com/docs/scores) to the trace, to e.g. record user feedback or some other evaluation. Scores are used throughout Langfuse to filter traces and on the dashboard. See the docs on scores for more details.\n",
        "\n",
        "The score is associated to the trace using the `trace_id` (see previous step)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J0argbJhrPPi"
      },
      "outputs": [],
      "source": [
        "from langfuse import Langfuse\n",
        "from langfuse.model import InitialScore\n",
        "\n",
        "langfuse = Langfuse()\n",
        "\n",
        "langfuse.score(InitialScore(\n",
        "    traceId=trace_id,\n",
        "    name=\"my-score-name\",\n",
        "    value=1\n",
        "));"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TC8s26KirPPi"
      },
      "source": [
        "![Trace with score](https://langfuse.com/images/docs/openai-trace-with-score.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xhaXYk9erPPi"
      },
      "source": [
        "## Troubleshooting\n",
        "\n",
        "### Shutdown behavior\n",
        "\n",
        "The Langfuse SDK executes network requests in the background on a separate thread for better performance of your application. This can lead to lost events in short lived environments like AWS Lambda functions when the Python process is terminated before the SDK sent all events to the Langfuse backend.\n",
        "\n",
        "To avoid this, ensure that the `openai.flush_langfuse()` function is called before termination. This method is blocking as it awaits all requests to be completed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JGKyb0EvrPPi"
      },
      "outputs": [],
      "source": [
        "openai.flush_langfuse()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

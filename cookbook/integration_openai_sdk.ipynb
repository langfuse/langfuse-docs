{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ki7E44X5ViQB"
      },
      "source": [
        "---\n",
        "description: Drop-in replacement of OpenAI SDK to get full observability in Langfuse by changing only the import\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Cookbook: OpenAI Integration (Python)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This is a cookbook with examples of the Langfuse Integration for OpenAI (Python).\n",
        "\n",
        "Follow the [integration guide](https://langfuse.com/docs/integrations/openai/get-started) to add this integration to your OpenAI project."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uq04G_FSWjF-"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XYoil3FcOIQt"
      },
      "source": [
        "The integration is compatible with OpenAI SDK versions `>=0.27.8`. It supports async functions and streaming for OpenAI SDK versions `>=1.0.0`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hVOOiBtUPtOO"
      },
      "outputs": [],
      "source": [
        "%pip install langfuse openai --upgrade"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7-s-hY3PPupC"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# get keys for your project from https://cloud.langfuse.com\n",
        "os.environ[\"LANGFUSE_PUBLIC_KEY\"] = \"\"\n",
        "os.environ[\"LANGFUSE_SECRET_KEY\"] = \"\"\n",
        "\n",
        "# your openai key\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
        "\n",
        "# Your host, defaults to https://cloud.langfuse.com\n",
        "# For US data region, set to \"https://us.cloud.langfuse.com\"\n",
        "# os.environ[\"LANGFUSE_HOST\"] = \"http://localhost:3000\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ldSEJ0bAP4sj"
      },
      "outputs": [],
      "source": [
        "# instead of: import openai\n",
        "from langfuse.openai import openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G8qkHd8oK_o9"
      },
      "outputs": [],
      "source": [
        "# For debugging, checks the SDK connection with the server. Do not use in production as it adds latency.\n",
        "from langfuse.openai import auth_check\n",
        "\n",
        "auth_check()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ovnAAdbaLmD"
      },
      "source": [
        "## Examples\n",
        "\n",
        "### Chat completion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c8RhokKUP9I0"
      },
      "outputs": [],
      "source": [
        "completion = openai.chat.completions.create(\n",
        "  name=\"test-chat\",\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=[\n",
        "      {\"role\": \"system\", \"content\": \"You are a very accurate calculator. You output only the result of the calculation.\"},\n",
        "      {\"role\": \"user\", \"content\": \"1 + 1 = \"}],\n",
        "  temperature=0,\n",
        "  metadata={\"someMetadataKey\": \"someValue\"},\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SAqxBgOqKTzO"
      },
      "source": [
        "### Chat completion (streaming)\n",
        "\n",
        "Simple example using the OpenAI streaming functionality."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b9gRlb2rKTaA",
        "outputId": "07ef0725-b8bf-4830-ef39-7656cb03c31e"
      },
      "outputs": [],
      "source": [
        "completion = openai.chat.completions.create(\n",
        "  name=\"test-chat\",\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=[\n",
        "      {\"role\": \"system\", \"content\": \"You are a professional comedian.\"},\n",
        "      {\"role\": \"user\", \"content\": \"Tell me a joke.\"}],\n",
        "  temperature=0,\n",
        "  metadata={\"someMetadataKey\": \"someValue\"},\n",
        "  stream=True\n",
        ")\n",
        "\n",
        "for chunk in completion:\n",
        "  print(chunk.choices[0].delta.content, end=\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F2pvm0qLKg7Q"
      },
      "source": [
        "### Chat completion (async)\n",
        "\n",
        "Simple example using the OpenAI async client. It takes the Langfuse configurations either from the environment variables or from the attributes on the `openai` module."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hggwggv_MKpV"
      },
      "outputs": [],
      "source": [
        "from langfuse.openai import AsyncOpenAI\n",
        "\n",
        "async_client = AsyncOpenAI()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZIUKD8Z3KmvQ"
      },
      "outputs": [],
      "source": [
        "completion = await async_client.chat.completions.create(\n",
        "  name=\"test-chat\",\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=[\n",
        "      {\"role\": \"system\", \"content\": \"You are a very accurate calculator. You output only the result of the calculation.\"},\n",
        "      {\"role\": \"user\", \"content\": \"1 + 100 = \"}],\n",
        "  temperature=0,\n",
        "  metadata={\"someMetadataKey\": \"someValue\"},\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Go to https://cloud.langfuse.com or your own instance to see your generation.\n",
        "\n",
        "![Chat completion](https://langfuse.com/images/docs/openai-chat.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ky7CtCNzaSrn"
      },
      "source": [
        "### Functions\n",
        "\n",
        "Simple example using Pydantic to generate the function schema."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jJfBdHowaRgs"
      },
      "outputs": [],
      "source": [
        "%pip install pydantic --upgrade"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2gA-zGk7VYYp"
      },
      "outputs": [],
      "source": [
        "from typing import List\n",
        "from pydantic import BaseModel\n",
        "\n",
        "class StepByStepAIResponse(BaseModel):\n",
        "    title: str\n",
        "    steps: List[str]\n",
        "schema = StepByStepAIResponse.schema() # returns a dict like JSON schema"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ORtNcN4-afDC"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "response = openai.chat.completions.create(\n",
        "    name=\"test-function\",\n",
        "    model=\"gpt-3.5-turbo-0613\",\n",
        "    messages=[\n",
        "       {\"role\": \"user\", \"content\": \"Explain how to assemble a PC\"}\n",
        "    ],\n",
        "    functions=[\n",
        "        {\n",
        "          \"name\": \"get_answer_for_user_query\",\n",
        "          \"description\": \"Get user answer in series of steps\",\n",
        "          \"parameters\": StepByStepAIResponse.schema()\n",
        "        }\n",
        "    ],\n",
        "    function_call={\"name\": \"get_answer_for_user_query\"}\n",
        ")\n",
        "\n",
        "output = json.loads(response.choices[0].message.function_call.arguments)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qurrm-Ntp24O"
      },
      "source": [
        "Go to https://cloud.langfuse.com or your own instance to see your generation.\n",
        "\n",
        "![Function](https://langfuse.com/images/docs/openai-function.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Su1OaQq3rPPh"
      },
      "source": [
        "### Group multiple generations into a single trace\n",
        "\n",
        "Many applications require more than one OpenAI call. In Langfuse, all LLM calls of a single API invocation can be grouped into the same `trace`.\n",
        "\n",
        "There are 2 options: (1) pass a `trace_id` (own or random string) or (2) create a trace with the Langfuse SDK."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Simple: `trace_id` as string\n",
        "\n",
        "To get started, you can just add an identifier from your own application (e.g., conversation-id) to the openai calls â€“ or create a random id."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ols9EL8mrPPh"
      },
      "outputs": [],
      "source": [
        "# create random trace_id\n",
        "# could also use existing id from your application, e.g. conversation id\n",
        "from uuid import uuid4\n",
        "trace_id = str(uuid4())\n",
        "\n",
        "# create multiple completions, pass trace_id to each\n",
        "\n",
        "country = \"Bulgaria\"\n",
        "\n",
        "capital = openai.chat.completions.create(\n",
        "  name=\"geography-teacher\",\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=[\n",
        "      {\"role\": \"system\", \"content\": \"You are a Geography teacher helping students learn the capitals of countries. Output only the capital when being asked.\"},\n",
        "      {\"role\": \"user\", \"content\": country}],\n",
        "  temperature=0,\n",
        "  trace_id=trace_id\n",
        ").choices[0].message.content\n",
        "\n",
        "poem = openai.chat.completions.create(\n",
        "  name=\"poet\",\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=[\n",
        "      {\"role\": \"system\", \"content\": \"You are a poet. Create a poem about a city.\"},\n",
        "      {\"role\": \"user\", \"content\": capital}],\n",
        "  temperature=1,\n",
        "  max_tokens=200,\n",
        "  trace_id=trace_id\n",
        ").choices[0].message.content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ehx2NZuIrPPh"
      },
      "source": [
        "Go to https://cloud.langfuse.com or your own instance to see your trace.\n",
        "\n",
        "![Trace with multiple OpenAI calls](https://langfuse.com/images/docs/openai-trace-grouped.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-HeMqTWgK4xL"
      },
      "source": [
        "#### Fully featured: interoperability with Langfuse SDK\n",
        "\n",
        "The `trace` is a core object in Langfuse and you can add rich metadata to it. See [Python SDK docs](https://langfuse.com/docs/sdk/python#traces-1) for full documentation on this.\n",
        "\n",
        "Some of the functionality enabled by custom traces:\n",
        "- custom name to identify a specific trace-type\n",
        "- user-level tracking\n",
        "- experiment tracking via versions and releases\n",
        "- custom metadata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "28to65wpK4xL"
      },
      "outputs": [],
      "source": [
        "from langfuse import Langfuse\n",
        "\n",
        "# initialize SDK\n",
        "langfuse = Langfuse()\n",
        "\n",
        "# create trace and add params\n",
        "trace = langfuse.trace(\n",
        "    # optional, if you want to use your own id\n",
        "    # id = \"my-trace-id\",\n",
        "\n",
        "    name = \"country-poems\",\n",
        "    user_id = \"user@example.com\",\n",
        "    metadata = {\n",
        "        \"env\": \"development\",\n",
        "    },\n",
        "    release = \"v0.0.21\"\n",
        ")\n",
        "\n",
        "# get traceid to pass to openai calls\n",
        "trace_id = trace.id\n",
        "\n",
        "# create multiple completions, pass trace_id to each\n",
        "\n",
        "country = \"Bulgaria\"\n",
        "\n",
        "capital = openai.chat.completions.create(\n",
        "  name=\"geography-teacher\",\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=[\n",
        "      {\"role\": \"system\", \"content\": \"You are a Geography teacher helping students learn the capitals of countries. Output only the capital when being asked.\"},\n",
        "      {\"role\": \"user\", \"content\": country}],\n",
        "  temperature=0,\n",
        "  trace_id=trace_id\n",
        ").choices[0].message.content\n",
        "\n",
        "poem = openai.chat.completions.create(\n",
        "  name=\"poet\",\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=[\n",
        "      {\"role\": \"system\", \"content\": \"You are a poet. Create a poem about a city.\"},\n",
        "      {\"role\": \"user\", \"content\": capital}],\n",
        "  temperature=1,\n",
        "  max_tokens=200,\n",
        "  trace_id=trace_id\n",
        ").choices[0].message.content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You can also use the nesting capabilities of Langfuse Tracing by providing a `parent_observation_id`. For example, this can be the id of a span that you created via the Langfuse Python SDK."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O3jxed-VrPPi"
      },
      "source": [
        "### Add scores to generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMO6tn53rPPi"
      },
      "source": [
        "You can also add [scores](https://langfuse.com/docs/scores) to the trace, to e.g. record user feedback or some other evaluation. Scores are used throughout Langfuse to filter traces and on the dashboard. See the docs on scores for more details.\n",
        "\n",
        "The score is associated to the trace using the `trace_id` (see previous step)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J0argbJhrPPi"
      },
      "outputs": [],
      "source": [
        "from langfuse import Langfuse\n",
        "\n",
        "langfuse = Langfuse()\n",
        "\n",
        "langfuse.score(\n",
        "    trace_id=trace_id,\n",
        "    name=\"my-score-name\",\n",
        "    value=1\n",
        ");"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TC8s26KirPPi"
      },
      "source": [
        "Go to https://cloud.langfuse.com or your own instance to see your trace with score.\n",
        "\n",
        "![Trace with score](https://langfuse.com/images/docs/openai-trace-with-score.png)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

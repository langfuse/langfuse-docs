{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ki7E44X5ViQB"
      },
      "source": [
        "---\n",
        "description: Drop-in replacement of OpenAI SDK to get full observability in Langfuse by changing only the import\n",
        "category: Integrations\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mfMAzJYcirtK"
      },
      "source": [
        "# Cookbook: OpenAI Integration (Python)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B0A389k2irtK"
      },
      "source": [
        "This is a cookbook with examples of the Langfuse Integration for OpenAI (Python).\n",
        "\n",
        "Follow the [integration guide](https://langfuse.com/docs/integrations/openai/get-started) to add this integration to your OpenAI project."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uq04G_FSWjF-"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XYoil3FcOIQt"
      },
      "source": [
        "The integration is compatible with OpenAI SDK versions `>=0.27.8`. It supports async functions and streaming for OpenAI SDK versions `>=1.0.0`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hVOOiBtUPtOO"
      },
      "outputs": [],
      "source": [
        "%pip install langfuse openai --upgrade"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "7-s-hY3PPupC"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# get keys for your project from https://cloud.langfuse.com\n",
        "os.environ[\"LANGFUSE_PUBLIC_KEY\"] = \"\"\n",
        "os.environ[\"LANGFUSE_SECRET_KEY\"] = \"\"\n",
        "\n",
        "# your openai key\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
        "\n",
        "# Your host, defaults to https://cloud.langfuse.com\n",
        "# For US data region, set to \"https://us.cloud.langfuse.com\"\n",
        "# os.environ[\"LANGFUSE_HOST\"] = \"http://localhost:3000\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ldSEJ0bAP4sj"
      },
      "outputs": [],
      "source": [
        "# instead of: import openai\n",
        "from langfuse.openai import openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G8qkHd8oK_o9",
        "outputId": "43591fb2-0b12-434b-8ed4-62a82f1bb377"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# For debugging, checks the SDK connection with the server. Do not use in production as it adds latency.\n",
        "from langfuse.openai import auth_check\n",
        "\n",
        "auth_check()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ovnAAdbaLmD"
      },
      "source": [
        "## Examples\n",
        "\n",
        "### Chat completion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "c8RhokKUP9I0"
      },
      "outputs": [],
      "source": [
        "completion = openai.chat.completions.create(\n",
        "  name=\"test-chat\",\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=[\n",
        "      {\"role\": \"system\", \"content\": \"You are a very accurate calculator. You output only the result of the calculation.\"},\n",
        "      {\"role\": \"user\", \"content\": \"1 + 1 = \"}],\n",
        "  temperature=0,\n",
        "  metadata={\"someMetadataKey\": \"someValue\"},\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SAqxBgOqKTzO"
      },
      "source": [
        "### Chat completion (streaming)\n",
        "\n",
        "Simple example using the OpenAI streaming functionality."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b9gRlb2rKTaA",
        "outputId": "c6c8830c-bd05-44ba-b562-a2d0341ab460"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sure thing! Why did the scarecrow win an award? Because he was outstanding in his field!None"
          ]
        }
      ],
      "source": [
        "completion = openai.chat.completions.create(\n",
        "  name=\"test-chat\",\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=[\n",
        "      {\"role\": \"system\", \"content\": \"You are a professional comedian.\"},\n",
        "      {\"role\": \"user\", \"content\": \"Tell me a joke.\"}],\n",
        "  temperature=0,\n",
        "  metadata={\"someMetadataKey\": \"someValue\"},\n",
        "  stream=True\n",
        ")\n",
        "\n",
        "for chunk in completion:\n",
        "  print(chunk.choices[0].delta.content, end=\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F2pvm0qLKg7Q"
      },
      "source": [
        "### Chat completion (async)\n",
        "\n",
        "Simple example using the OpenAI async client. It takes the Langfuse configurations either from the environment variables or from the attributes on the `openai` module."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Hggwggv_MKpV"
      },
      "outputs": [],
      "source": [
        "from langfuse.openai import AsyncOpenAI\n",
        "\n",
        "async_client = AsyncOpenAI()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "ZIUKD8Z3KmvQ"
      },
      "outputs": [],
      "source": [
        "completion = await async_client.chat.completions.create(\n",
        "  name=\"test-chat\",\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=[\n",
        "      {\"role\": \"system\", \"content\": \"You are a very accurate calculator. You output only the result of the calculation.\"},\n",
        "      {\"role\": \"user\", \"content\": \"1 + 100 = \"}],\n",
        "  temperature=0,\n",
        "  metadata={\"someMetadataKey\": \"someValue\"},\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M4iJpqYQirtM"
      },
      "source": [
        "Go to https://cloud.langfuse.com or your own instance to see your generation.\n",
        "\n",
        "![Chat completion](https://langfuse.com/images/docs/openai-chat.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ky7CtCNzaSrn"
      },
      "source": [
        "### Functions\n",
        "\n",
        "Simple example using Pydantic to generate the function schema."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jJfBdHowaRgs",
        "outputId": "17e3b59c-9b13-4c05-d9b3-913466358c7e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.10/dist-packages (2.6.3)\n",
            "Collecting pydantic\n",
            "  Downloading pydantic-2.6.4-py3-none-any.whl (394 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m394.9/394.9 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic) (2.16.3)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic) (4.10.0)\n",
            "Installing collected packages: pydantic\n",
            "  Attempting uninstall: pydantic\n",
            "    Found existing installation: pydantic 2.6.3\n",
            "    Uninstalling pydantic-2.6.3:\n",
            "      Successfully uninstalled pydantic-2.6.3\n",
            "Successfully installed pydantic-2.6.4\n"
          ]
        }
      ],
      "source": [
        "%pip install pydantic --upgrade"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "2gA-zGk7VYYp"
      },
      "outputs": [],
      "source": [
        "from typing import List\n",
        "from pydantic import BaseModel\n",
        "\n",
        "class StepByStepAIResponse(BaseModel):\n",
        "    title: str\n",
        "    steps: List[str]\n",
        "schema = StepByStepAIResponse.schema() # returns a dict like JSON schema"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "ORtNcN4-afDC"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "response = openai.chat.completions.create(\n",
        "    name=\"test-function\",\n",
        "    model=\"gpt-3.5-turbo-0613\",\n",
        "    messages=[\n",
        "       {\"role\": \"user\", \"content\": \"Explain how to assemble a PC\"}\n",
        "    ],\n",
        "    functions=[\n",
        "        {\n",
        "          \"name\": \"get_answer_for_user_query\",\n",
        "          \"description\": \"Get user answer in series of steps\",\n",
        "          \"parameters\": StepByStepAIResponse.schema()\n",
        "        }\n",
        "    ],\n",
        "    function_call={\"name\": \"get_answer_for_user_query\"}\n",
        ")\n",
        "\n",
        "output = json.loads(response.choices[0].message.function_call.arguments)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qurrm-Ntp24O"
      },
      "source": [
        "Go to https://cloud.langfuse.com or your own instance to see your generation.\n",
        "\n",
        "![Function](https://langfuse.com/images/docs/openai-function.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Su1OaQq3rPPh"
      },
      "source": [
        "### Group multiple generations into a single trace\n",
        "\n",
        "Many applications require more than one OpenAI call. Langfuse `decorators` allows to nest all LLM calls of a single API invocation into the same `trace` with just a few lines of codes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zMDVxzS1ltWU",
        "outputId": "f499cf8a-91b6-456f-ea1a-934e120bfe14"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "In the heart of mountains and dreams,\n",
            "There lies a city, where beauty gleams.\n",
            "Sofia, a place of ancient tales,\n",
            "Where history whispers in every gale.\n",
            "\n",
            "Streets lined with cobblestones old,\n",
            "Tales of conquerors and stories untold.\n",
            "Architecture grand, a sight to behold,\n",
            "Domes and spires reaching for the sky's fold.\n",
            "\n",
            "Markets bustling with colors and sounds,\n",
            "A melting pot of cultures, where harmony abounds.\n",
            "The aroma of spices fills the air,\n",
            "As locals and travelers alike share a care.\n",
            "\n",
            "Parks and gardens, oases of green,\n",
            "Amidst the urban jungle, a peaceful scene.\n",
            "Bridges spanning rivers that flow,\n",
            "Connecting past and present in a graceful glow.\n",
            "\n",
            "Sofia, a city of contrasts and charm,\n",
            "Where past and present intertwine arm in arm.\n",
            "A place where time stands still, yet moves on,\n",
            "A city to love, a city to be drawn.\n",
            "\n",
            "So let us wander through Sofia's\n"
          ]
        }
      ],
      "source": [
        "from langfuse.decorators import langfuse_context, observe\n",
        "\n",
        "@observe() # decorator to automatically create trace and nest generations\n",
        "def main(country: str, user_id: str, **kwargs) -> str:\n",
        "    # nested generation 1: use openai to get capital of country\n",
        "    capital = openai.chat.completions.create(\n",
        "      name=\"geography-teacher\",\n",
        "      model=\"gpt-3.5-turbo\",\n",
        "      messages=[\n",
        "          {\"role\": \"system\", \"content\": \"You are a Geography teacher helping students learn the capitals of countries. Output only the capital when being asked.\"},\n",
        "          {\"role\": \"user\", \"content\": country}],\n",
        "      temperature=0,\n",
        "    ).choices[0].message.content\n",
        "\n",
        "    # nested generation 2: use openai to write poem on capital\n",
        "    poem = openai.chat.completions.create(\n",
        "      name=\"poet\",\n",
        "      model=\"gpt-3.5-turbo\",\n",
        "      messages=[\n",
        "          {\"role\": \"system\", \"content\": \"You are a poet. Create a poem about a city.\"},\n",
        "          {\"role\": \"user\", \"content\": capital}],\n",
        "      temperature=1,\n",
        "      max_tokens=200,\n",
        "    ).choices[0].message.content\n",
        "\n",
        "    return poem\n",
        "\n",
        "# run main function and let Langfuse decorator do the rest\n",
        "print(main(\"Bulgaria\", \"admin\"))\n",
        "\n",
        "# Flush observations to Langfuse\n",
        "langfuse_context.flush()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NzoB3TsairtN"
      },
      "source": [
        "#### Optional: Set `trace_id` manually\n",
        "\n",
        "When creating a trace, Langfuse offers users 2 options: (1) create a `trace_id` with the Langfuse SDK (default) or (2) pass a `trace_id` (own or random string) to the Langfuse SDK. With Langfuse `decorators` users can add an identifier from your their own application (e.g., conversation-id) via the `keyword arguments`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ols9EL8mrPPh",
        "outputId": "2bbb4d24-143f-4575-8dfa-b86cfe3ad6d4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "In the heart of the Balkan land,\n",
            "Where ancient stories make their stand,\n",
            "Lies a city full of grace and charm,\n",
            "Sofia, oh Sofia, where histories swarm.\n",
            "\n",
            "With domes and towers that reach the sky,\n",
            "Whispers of the past floating by,\n",
            "A place where East meets West in glee,\n",
            "In Sofia, where cultures dance free.\n",
            "\n",
            "Streets bustling with life and sound,\n",
            "Mosaics of colors all around,\n",
            "Each corner a new tale to tell,\n",
            "In Sofia, where secrets dwell.\n",
            "\n",
            "A city of contrasts, old and new,\n",
            "Where dreams are born, and skies are blue,\n",
            "In the cradle of mountains tall,\n",
            "Sofia stands proud, a city for all.\n",
            "\n",
            "So, let us wander, let us roam,\n",
            "Through streets of cobblestone,\n",
            "Among the ruins and the art,\n",
            "In Sofia, where memories start.\n",
            "\n",
            "For in this city, vibrant and true,\n",
            "There's a spirit that will guide you through,\n",
            "Sofia, oh Sofia,\n"
          ]
        }
      ],
      "source": [
        "from langfuse.decorators import langfuse_context, observe\n",
        "from uuid import uuid4\n",
        "\n",
        "# create random trace_id, could also use existing id from your application, e.g. conversation id\n",
        "trace_id = str(uuid4())\n",
        "\n",
        "# run main function and let Langfuse decorator do the rest\n",
        "print(main(\"Bulgaria\", \"admin\", langfuse_observation_id=trace_id))\n",
        "\n",
        "# Flush observations to Langfuse\n",
        "langfuse_context.flush()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ehx2NZuIrPPh"
      },
      "source": [
        "Go to https://cloud.langfuse.com or your own instance to see your trace.\n",
        "\n",
        "TODO: update screenshot https://cloud.langfuse.com/project/clr4qu8qv0000yu4ja339x02u/traces/158b5d9a-68ee-493f-b26d-2347a2333bac\n",
        "\n",
        "![Trace with multiple OpenAI calls](https://langfuse.com/images/docs/openai-trace-grouped.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-HeMqTWgK4xL"
      },
      "source": [
        "#### Fully featured: Interoperability with Langfuse SDK\n",
        "\n",
        "The `trace` is a core object in Langfuse and you can add rich metadata to it. See [Python SDK docs](https://langfuse.com/docs/sdk/python#traces-1) for full documentation on this.\n",
        "\n",
        "Some of the functionality enabled by custom traces:\n",
        "- custom name to identify a specific trace-type\n",
        "- user-level tracking\n",
        "- experiment tracking via versions and releases\n",
        "- custom metadata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "28to65wpK4xL",
        "outputId": "d0c5d712-6df7-47bc-baaa-30d16df56225"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "In the heart of Bulgaria lies a city fair,\n",
            "Sofia, with its charm beyond compare.\n",
            "Nestled among mountains, a sight to see,\n",
            "A tapestry of history and modernity.\n",
            "\n",
            "Streets lined with cafes, bustling and alive,\n",
            "Where stories are shared, dreams thrive.\n",
            "The ancient churches, with domes so grand,\n",
            "Whisper tales of a city that will forever stand.\n",
            "\n",
            "The people of Sofia, a vibrant blend,\n",
            "From all walks of life, they do extend\n",
            "A warm embrace to all who pass through,\n",
            "In this city where dreams can come true.\n",
            "\n",
            "Underneath the stars, Sofia shines bright,\n",
            "A beacon of hope in the darkest night.\n",
            "A city of contrasts, old and new,\n",
            "So rich in culture, so full of hue.\n",
            "\n",
            "So raise a glass to Sofia, proud and free,\n",
            "A city that holds the key\n",
            "To memories made and stories told,\n",
            "In a city of wonder, a sight to behold.\n"
          ]
        }
      ],
      "source": [
        "from langfuse.decorators import langfuse_context, observe\n",
        "\n",
        "@observe() # decorator to automatically create trace and nest generations\n",
        "def main(country: str, user_id: str, **kwargs) -> str:\n",
        "    # nested generation 1: use openai to get capital of country\n",
        "    capital = openai.chat.completions.create(\n",
        "      name=\"geography-teacher\",\n",
        "      model=\"gpt-3.5-turbo\",\n",
        "      messages=[\n",
        "          {\"role\": \"system\", \"content\": \"You are a Geography teacher helping students learn the capitals of countries. Output only the capital when being asked.\"},\n",
        "          {\"role\": \"user\", \"content\": country}],\n",
        "      temperature=0,\n",
        "    ).choices[0].message.content\n",
        "\n",
        "    # nested generation 2: use openai to write poem on capital\n",
        "    poem = openai.chat.completions.create(\n",
        "      name=\"poet\",\n",
        "      model=\"gpt-3.5-turbo\",\n",
        "      messages=[\n",
        "          {\"role\": \"system\", \"content\": \"You are a poet. Create a poem about a city.\"},\n",
        "          {\"role\": \"user\", \"content\": capital}],\n",
        "      temperature=1,\n",
        "      max_tokens=200,\n",
        "    ).choices[0].message.content\n",
        "\n",
        "    # rename trace and set attributes (e.g., medatata) as needed\n",
        "    langfuse_context.update_current_trace(\n",
        "        name=\"City poem generator\",\n",
        "        session_id=\"1234\",\n",
        "        user_id=user_id,\n",
        "        tags=[\"tag1\", \"tag2\"],\n",
        "        public=True,\n",
        "        metadata = {\n",
        "        \"env\": \"development\",\n",
        "        },\n",
        "        release = \"v0.0.21\"\n",
        "    )\n",
        "\n",
        "    return poem\n",
        "\n",
        "# create random trace_id, could also use existing id from your application, e.g. conversation id\n",
        "trace_id = str(uuid4())\n",
        "\n",
        "# run main function and let Langfuse decorator do the rest\n",
        "print(main(\"Bulgaria\", \"admin\", langfuse_observation_id=trace_id))\n",
        "\n",
        "# Flush observations to Langfuse\n",
        "langfuse_context.flush()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7OMmwle7yw-e"
      },
      "source": [
        "Screenshot? Trace: https://cloud.langfuse.com/project/clr4qu8qv0000yu4ja339x02u/traces/1835e03e-8dbc-4a2b-ba56-df4c18d023df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O3jxed-VrPPi"
      },
      "source": [
        "### Add scores to generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMO6tn53rPPi"
      },
      "source": [
        "You can also add [scores](https://langfuse.com/docs/scores) to the trace, to e.g. record user feedback or some other evaluation. Scores are used throughout Langfuse to filter traces and on the dashboard. See the docs on scores for more details.\n",
        "\n",
        "The score is associated to the trace using the `trace_id` (see previous step)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J0argbJhrPPi"
      },
      "outputs": [],
      "source": [
        "from langfuse import Langfuse\n",
        "\n",
        "langfuse = Langfuse()\n",
        "\n",
        "langfuse.score(\n",
        "    trace_id=trace_id,\n",
        "    name=\"my-score-name\",\n",
        "    value=1\n",
        ");"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TC8s26KirPPi"
      },
      "source": [
        "Go to https://cloud.langfuse.com or your own instance to see your trace with score.\n",
        "\n",
        "![Trace with score](https://langfuse.com/images/docs/openai-trace-with-score.png)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- NOTEBOOK_METADATA source: \"âš ï¸ Jupyter Notebook\" title: \"Monitor Parallel Tasks with Langfuse\" sidebarTitle: \"Parallel\" logo: \"/images/integrations/parallel_icon.svg\" description: \"Learn how to trace Parallel task execution using Langfuse to capture detailed observability data for your AI workflow operations.\" category: \"Integrations\" -->\n",
    "\n",
    "# Parallel Integration\n",
    "\n",
    "In this guide, we'll show you how to integrate [Langfuse](https://langfuse.com) with [Parallel](https://parallel.ai/) to trace your AI task operations. By leveraging Langfuse's tracing capabilities, you can automatically capture details such as inputs, outputs, and execution times of your Parallel tasks.\n",
    "\n",
    "> **What is Parallel?** [Parallel](https://parallel.ai/) develops a suite of web search and web agent APIs that connect AI agents, applications, and workflows to the open internet, enabling programmable tasks from simple searches to complex knowledge work.\n",
    "\n",
    "> **What is Langfuse?** [Langfuse](https://langfuse.com) is an open source LLM engineering platform that helps teams trace API calls, monitor performance, and debug issues in their AI applications.\n",
    "\n",
    "<!-- STEPS_START -->\n",
    "## Get Started\n",
    "\n",
    "First, install the necessary Python packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install langfuse parallel-web openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, configure your environment with your Parallel and Langfuse API keys. You can get these keys by signing up for a free [Langfuse Cloud](https://cloud.langfuse.com/) account or by [self-hosting Langfuse](https://langfuse.com/self-hosting) and from the [Parallel dashboard](https://parallel.ai/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Get keys for your project from the project settings page: https://cloud.langfuse.com\n",
    "os.environ[\"LANGFUSE_PUBLIC_KEY\"] = \"pk-lf-...\" \n",
    "os.environ[\"LANGFUSE_SECRET_KEY\"] = \"sk-lf-...\" \n",
    "os.environ[\"LANGFUSE_HOST\"] = \"https://cloud.langfuse.com\" # ðŸ‡ªðŸ‡º EU region\n",
    "# os.environ[\"LANGFUSE_HOST\"] = \"https://us.cloud.langfuse.com\" # ðŸ‡ºðŸ‡¸ US region\n",
    "\n",
    "# Your Parallel key\n",
    "os.environ[\"PARALLEL_API_KEY\"] = \"...\"\n",
    "\n",
    "# Your openai key\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-proj-...\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Tracing the Parallel Task API\n",
    "\n",
    "To monitor the Task API requests, we use the [Langfuse `@observe()` decorator](https://langfuse.com/docs/sdk/python/decorators). In this example, the `@observe()` decorator captures the inputs, outputs, and execution time of the `parallel_task()` function. For more control over the data you are sending to Langfuse, you can use the [Context Manager or create manual observations](https://langfuse.com/docs/observability/sdk/python/instrumentation#custom-instrumentation) using the Python SDK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from parallel import Parallel\n",
    "from parallel.types import TaskSpecParam\n",
    "from langfuse import observe\n",
    "\n",
    "client = Parallel(api_key=os.environ[\"PARALLEL_API_KEY\"])\n",
    "\n",
    "@observe(as_type=\"retriever\")\n",
    "def parallel_task(input: str):\n",
    "    task_run = client.task_run.create(\n",
    "        input=input,\n",
    "        task_spec=TaskSpecParam(\n",
    "            output_schema=\"The founding date of the company in the format MM-YYYY\"\n",
    "            ),\n",
    "        processor=\"base\"\n",
    "    )\n",
    "    print(f\"Run ID: {task_run.run_id}\")\n",
    "\n",
    "    run_result = client.task_run.result(task_run.run_id, api_timeout=3600)\n",
    "    print(run_result.output)\n",
    "\n",
    "    return run_result.output\n",
    "\n",
    "parallel_task(\"Langfuse\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: Tracing the Parallel Chat API\n",
    "\n",
    "You can trace the interactions with the Parallel Chat API by using the [Langfuse OpenAI wrapper](https://langfuse.com/integrations/model-providers/openai-py): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langfuse.openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=os.environ[\"PARALLEL_API_KEY\"],  # Your Parallel API key\n",
    "    base_url=\"https://api.parallel.ai\"  # Parallel's API beta endpoint\n",
    ")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"speed\", # Parallel model name\n",
    "    name=\"Parallel Chat\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"What does Parallel Web Systems do?\"}\n",
    "    ],\n",
    "    response_format={\n",
    "        \"type\": \"json_schema\",\n",
    "        \"json_schema\": {\n",
    "            \"name\": \"reasoning_schema\",\n",
    "            \"schema\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"reasoning\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"Think step by step to arrive at the answer\",\n",
    "                    },\n",
    "                    \"answer\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The direct answer to the question\",\n",
    "                    },\n",
    "                    \"citations\": {\n",
    "                        \"type\": \"array\",\n",
    "                        \"items\": {\"type\": \"string\"},\n",
    "                        \"description\": \"Sources cited to support the answer\",\n",
    "                    },\n",
    "                },\n",
    "            },\n",
    "        },\n",
    "    },\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 3: Parallel Search API and OpenAI\n",
    "\n",
    "You can also trace more complex workflows that involve summarizing the search results with OpenAI. Here we use the [Langfuse `@observe()` decorator](https://langfuse.com/docs/sdk/python/decorators) to group both the Parallel search and the OpenAI generation into one trace. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from parallel import Parallel\n",
    "from langfuse.openai import OpenAI\n",
    "from langfuse import observe\n",
    "\n",
    "\n",
    "@observe()\n",
    "def search_and_summarize(objective, search_queries):\n",
    "    # 1. Parallel Search API\n",
    "    parallel_client = Parallel(api_key=os.environ[\"PARALLEL_API_KEY\"])\n",
    "\n",
    "    @observe(as_type=\"retriever\")\n",
    "    def search_with_parallel(objective, search_queries, num_results: int = 5):\n",
    "        \"\"\"Search the web using Parallel and return results.\"\"\"\n",
    "        search = parallel_client.beta.search(\n",
    "            objective=objective,\n",
    "            search_queries=search_queries,\n",
    "            processor=\"base\",\n",
    "            max_results=num_results,\n",
    "            max_chars_per_result=6000\n",
    "        )\n",
    "        return search.results\n",
    "\n",
    "    results = search_with_parallel(objective, search_queries)\n",
    "    results_text = \"\\n\\n\".join(str(r) for r in results) if results else \"No results.\"\n",
    "\n",
    "    # 2. Summarize with OpenAI\n",
    "    openai_client = OpenAI()\n",
    "    resp = openai_client.chat.completions.create(\n",
    "        model=\"gpt-5-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"Summarize the following search results clearly and concisely.\"},\n",
    "            {\"role\": \"user\", \"content\": results_text}\n",
    "        ]\n",
    "    )\n",
    "    return resp.choices[0].message.content\n",
    "\n",
    "# Example usage\n",
    "search_and_summarize(\n",
    "    objective=\"Explain what Langfuse is and highlight its main features for LLM application observability.\",\n",
    "    search_queries=[\n",
    "        \"Langfuse LLM observability\",\n",
    "        \"Langfuse features and documentation\",\n",
    "        \"Langfuse tracing evaluations dashboards\"\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## See Traces in Langfuse\n",
    "\n",
    "After executing the traced functions, log in to your [Langfuse Dashboard](https://cloud.langfuse.com) to view detailed trace logs. You'll be able to see:\n",
    "\n",
    "- Individual task creation and retrieval operations\n",
    "- Parallel execution patterns and timing\n",
    "- Input prompts and output results\n",
    "- Performance metrics for each task\n",
    "\n",
    "![Parallel Example trace in Langfuse UI](https://langfuse.com/images/cookbook/integration_parallel-ai/parallel-ai-example-trace.png)\n",
    "\n",
    "[Example trace in Langfuse](https://cloud.langfuse.com/project/cloramnkj0002jz088vzn1ja4/traces/a664e35ab40a75d04cf6a262d0399f05?timestamp=2025-10-28T15%3A16%3A26.617Z&observation=13fc56583aafeb06)\n",
    "\n",
    "<!-- STEPS_END -->\n",
    "\n",
    "<!-- MARKDOWN_COMPONENT name: \"LearnMore\" path: \"@/components-mdx/integration-learn-more.mdx\" -->"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

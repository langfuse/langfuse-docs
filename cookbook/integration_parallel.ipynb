{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- NOTEBOOK_METADATA source: \"âš ï¸ Jupyter Notebook\" title: \"Monitor Parallel AI Tasks with Langfuse\" sidebarTitle: \"Parallel AI\" logo: \"/images/integrations/parallel_icon.png\" description: \"Learn how to trace Parallel AI task execution using Langfuse to capture detailed observability data for your AI workflow operations.\" category: \"Integrations\" -->\n",
    "\n",
    "# Parallel AI Integration\n",
    "\n",
    "In this guide, we'll show you how to integrate [Langfuse](https://langfuse.com) with [Parallel AI](https://parallel.ai/) to trace your AI task operations. By leveraging Langfuse's tracing capabilities, you can automatically capture details such as inputs, outputs, and execution times of your Parallel AI tasks.\n",
    "\n",
    "> **What is Parallel AI?** [Parallel AI](https://parallel.ai/) is an API service that enables you to execute AI tasks in parallel, optimizing workflow efficiency. It provides a powerful task API that allows you to run multiple AI operations concurrently, making it ideal for building scalable AI applications.\n",
    "\n",
    "> **What is Langfuse?** [Langfuse](https://langfuse.com) is an open source LLM engineering platform that helps teams trace API calls, monitor performance, and debug issues in their AI applications.\n",
    "\n",
    "## Get Started\n",
    "\n",
    "First, install the necessary Python packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install langfuse requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, configure your environment with your Langfuse API keys. You can obtain your keys from your Langfuse dashboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Get keys for your project from the project settings page https://cloud.langfuse.com\n",
    "\n",
    "os.environ[\"LANGFUSE_PUBLIC_KEY\"] = \"pk-lf-...\"\n",
    "os.environ[\"LANGFUSE_SECRET_KEY\"] = \"sk-lf-...\" \n",
    "os.environ[\"LANGFUSE_BASE_URL\"] = \"https://cloud.langfuse.com\" # ðŸ‡ªðŸ‡º EU region\n",
    "# os.environ[\"LANGFUSE_BASE_URL\"] = \"https://us.cloud.langfuse.com\" # ðŸ‡ºðŸ‡¸ US region"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up your Parallel AI API key. You can obtain your API key from the [Parallel AI dashboard](https://parallel.ai/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARALLEL_API_KEY = \"your-parallel-api-key\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tracing Parallel AI Tasks\n",
    "\n",
    "To monitor your Parallel AI tasks, we use the [Langfuse `@observe()` decorator](https://langfuse.com/docs/sdk/python/decorators). In this example, the `@observe()` decorator captures the inputs, outputs, and execution time of the functions that interact with Parallel AI. All trace data is automatically sent to Langfuse, allowing you to monitor your AI task operations in real time.\n",
    "\n",
    "### Creating and Running a Single Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from langfuse import observe\n",
    "\n",
    "@observe()\n",
    "def create_parallel_task(prompt: str, api_key: str):\n",
    "    \"\"\"Create a task using Parallel AI's Task API.\"\"\"\n",
    "    url = \"https://api.parallel.ai/v1/tasks\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {api_key}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    payload = {\n",
    "        \"prompt\": prompt,\n",
    "        \"model\": \"gpt-4\"\n",
    "    }\n",
    "    \n",
    "    response = requests.post(url, json=payload, headers=headers)\n",
    "    response.raise_for_status()\n",
    "    return response.json()\n",
    "\n",
    "@observe()\n",
    "def get_task_result(task_id: str, api_key: str):\n",
    "    \"\"\"Retrieve the result of a task.\"\"\"\n",
    "    url = f\"https://api.parallel.ai/v1/tasks/{task_id}\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {api_key}\"\n",
    "    }\n",
    "    \n",
    "    response = requests.get(url, headers=headers)\n",
    "    response.raise_for_status()\n",
    "    return response.json()\n",
    "\n",
    "# Example usage\n",
    "task_result = create_parallel_task(\n",
    "    \"Explain the benefits of parallel processing in AI applications\",\n",
    "    PARALLEL_API_KEY\n",
    ")\n",
    "print(f\"Task created with ID: {task_result.get('task_id')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running Multiple Tasks in Parallel\n",
    "\n",
    "One of the key features of Parallel AI is the ability to run multiple tasks concurrently. Here's how to trace multiple parallel requests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "@observe()\n",
    "def run_parallel_tasks(prompts: list[str], api_key: str):\n",
    "    \"\"\"Execute multiple AI tasks in parallel and monitor with Langfuse.\"\"\"\n",
    "    task_ids = []\n",
    "    \n",
    "    # Create all tasks\n",
    "    with ThreadPoolExecutor(max_workers=len(prompts)) as executor:\n",
    "        futures = [executor.submit(create_parallel_task, prompt, api_key) for prompt in prompts]\n",
    "        \n",
    "        for future in as_completed(futures):\n",
    "            result = future.result()\n",
    "            task_ids.append(result.get('task_id'))\n",
    "    \n",
    "    # Wait for tasks to complete and retrieve results\n",
    "    results = []\n",
    "    for task_id in task_ids:\n",
    "        # Poll for completion (in production, use webhooks)\n",
    "        time.sleep(2)\n",
    "        result = get_task_result(task_id, api_key)\n",
    "        results.append(result)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Example: Run multiple tasks in parallel\n",
    "prompts = [\n",
    "    \"What are the key benefits of using Langfuse for LLM observability?\",\n",
    "    \"How does parallel processing improve AI application performance?\",\n",
    "    \"Explain the concept of distributed tracing in AI systems.\"\n",
    "]\n",
    "\n",
    "results = run_parallel_tasks(prompts, PARALLEL_API_KEY)\n",
    "print(f\"Completed {len(results)} tasks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## See Traces in Langfuse\n",
    "\n",
    "After executing the traced functions, log in to your [Langfuse Dashboard](https://cloud.langfuse.com) to view detailed trace logs. You'll be able to see:\n",
    "\n",
    "- Individual task creation and retrieval operations\n",
    "- Parallel execution patterns and timing\n",
    "- Input prompts and output results\n",
    "- Performance metrics for each task\n",
    "\n",
    "The nested trace structure will show how multiple tasks are executed in parallel, making it easy to identify bottlenecks and optimize your AI workflows.\n",
    "\n",
    "### Example Trace View\n",
    "\n",
    "When you run multiple tasks in parallel, Langfuse captures the entire workflow:\n",
    "\n",
    "- The parent `run_parallel_tasks` span shows the overall execution time\n",
    "- Child spans for each `create_parallel_task` call show individual task creation\n",
    "- Additional child spans for `get_task_result` show result retrieval\n",
    "\n",
    "This hierarchical view helps you understand the performance characteristics of your parallel AI operations and identify opportunities for optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced: Custom Metadata and Attributes\n",
    "\n",
    "You can enhance your traces with custom metadata to make debugging and analysis easier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langfuse.decorators import langfuse_context\n",
    "\n",
    "@observe()\n",
    "def create_task_with_metadata(prompt: str, api_key: str, task_type: str = \"general\"):\n",
    "    \"\"\"Create a task with additional metadata for better observability.\"\"\"\n",
    "    \n",
    "    # Add custom metadata to the trace\n",
    "    langfuse_context.update_current_observation(\n",
    "        metadata={\n",
    "            \"task_type\": task_type,\n",
    "            \"prompt_length\": len(prompt),\n",
    "            \"model\": \"gpt-4\"\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    url = \"https://api.parallel.ai/v1/tasks\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {api_key}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    payload = {\n",
    "        \"prompt\": prompt,\n",
    "        \"model\": \"gpt-4\"\n",
    "    }\n",
    "    \n",
    "    response = requests.post(url, json=payload, headers=headers)\n",
    "    response.raise_for_status()\n",
    "    \n",
    "    result = response.json()\n",
    "    \n",
    "    # Update observation with task ID\n",
    "    langfuse_context.update_current_observation(\n",
    "        metadata={\"task_id\": result.get('task_id')}\n",
    "    )\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Example with metadata\n",
    "task = create_task_with_metadata(\n",
    "    \"Summarize the benefits of observability in AI systems\",\n",
    "    PARALLEL_API_KEY,\n",
    "    task_type=\"summarization\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- MARKDOWN_COMPONENT name: \"LearnMore\" path: \"@/components-mdx/integration-learn-more.mdx\" -->"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

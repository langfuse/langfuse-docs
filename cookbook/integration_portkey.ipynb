{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- NOTEBOOK_METADATA source: \"âš ï¸ Jupyter Notebook\" title: \"Integrate Portkey LLM Gateway with Langfuse\" description: \"Guide on using Portkey's AI gateway to access 250+ LLM models with Langfuse via the OpenAI SDK.\" category: \"Integrations\" -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Observability for Portkey LLM Gateway with Langfuse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This guide shows you how to integrate Portkey's AI gateway with Langfuse. Portkey's API endpoints are fully [compatible](https://portkey.ai/docs/api-reference/inference-api/introduction) with the OpenAI SDK, allowing you to trace and monitor your AI applications seamlessly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **What is Portkey?** [Portkey](https://portkey.ai/) is an AI gateway that provides a unified interface to interact with 250+ AI models, offering advanced tools for control, visibility, and security in your Generative AI apps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **What is Langfuse?** [Langfuse](https://langfuse.com) is an open source LLM engineering platform that helps teams trace LLM calls, monitor performance, and debug issues in their AI applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- STEPS_START -->\n",
    "## Step 1: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install openai langfuse portkey_ai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Set Up Environment Variables\n",
    "\n",
    "Next, set up your Langfuse API keys. You can get these keys by signing up for a free [Langfuse Cloud](https://cloud.langfuse.com/) account or by [self-hosting Langfuse](https://langfuse.com/self-hosting). These environment variables are essential for the Langfuse client to authenticate and send data to your Langfuse project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Get keys for your project from the project settings page: https://cloud.langfuse.com\n",
    "os.environ[\"LANGFUSE_PUBLIC_KEY\"] = \"pk-lf-...\" \n",
    "os.environ[\"LANGFUSE_SECRET_KEY\"] = \"sk-lf-...\" \n",
    "os.environ[\"LANGFUSE_HOST\"] = \"https://cloud.langfuse.com\" # ðŸ‡ªðŸ‡º EU region\n",
    "# os.environ[\"LANGFUSE_HOST\"] = \"https://us.cloud.langfuse.com\" # ðŸ‡ºðŸ‡¸ US region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langfuse import get_client\n",
    " \n",
    "get_client().auth_check()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Use Langfuse OpenAI Drop-in Replacement\n",
    "\n",
    "Next, you can use Langfuseâ€™s OpenAI-compatible client (from langfuse.openai import OpenAI) to trace all requests sent through the Portkey gateway. For detailed setup instructions on the LLM gateway and virtual LLM keys, refer to the [Portkey documentation](https://portkey.ai/docs/product/ai-gateway)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langfuse.openai import OpenAI\n",
    "from portkey_ai import createHeaders, PORTKEY_GATEWAY_URL\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=\"xxx\", #Since we are using a virtual key we do not need this \n",
    "    base_url = PORTKEY_GATEWAY_URL, \n",
    "    default_headers = createHeaders(\n",
    "    api_key = \"***\",\n",
    "    virtual_key = \"***\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Run an Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "  model=\"gpt-4o\",  # Or any model supported by your chosen provider\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What are the benefits of using an AI gateway?\"},\n",
    "  ],\n",
    ")\n",
    "print(response.choices[0].message.content)\n",
    "\n",
    "# Flush via global client\n",
    "langfuse = get_client()\n",
    "langfuse.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: See Traces in Langfuse\n",
    "\n",
    "After running the example, log in to Langfuse to view the detailed traces, including:\n",
    "\n",
    "- Request parameters\n",
    "- Response content\n",
    "- Token usage and latency metrics\n",
    "- LLM model information through Portkey gateway\n",
    "\n",
    "![Langfuse Trace Example](https://langfuse.com/images/cookbook/integration-portkey/portkey-example-trace.png)\n",
    "\n",
    "_[Public example trace link in Langfuse](https://cloud.langfuse.com/project/cloramnkj0002jz088vzn1ja4/traces/4a2391624dcd7478e56d188f55379049?timestamp=2025-07-01T13:54:00.114Z&display=details)_\n",
    "<!-- STEPS_END -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- MARKDOWN_COMPONENT name: \"LearnMore\" path: \"@/components-mdx/integration-learn-more.mdx\" -->"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- NOTEBOOK_METADATA source: \"\u26a0\ufe0f Jupyter Notebook\" title: \"Integrate Portkey LLM Gateway with Langfuse\" description: \"Guide on using Portkey's AI gateway to access 250+ LLM models with Langfuse via the OpenAI SDK.\" category: \"Integrations\" -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Observability for Portkey LLM Gateway with Langfuse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This guide shows you how to integrate Portkey's AI gateway with Langfuse. Portkey's API endpoints are fully [compatible](https://portkey.ai/docs/api-reference/inference-api/introduction) with the OpenAI SDK, allowing you to trace and monitor your AI applications seamlessly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **What is Portkey?** [Portkey](https://portkey.ai/) is an AI gateway that provides a unified interface to interact with 250+ AI models, offering advanced tools for control, visibility, and security in your Generative AI apps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **What is Langfuse?** [Langfuse](https://langfuse.com) is an open source LLM engineering platform that helps teams trace API calls, monitor performance, and debug issues in their AI applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- STEPS_START -->\n## Step 1: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install openai langfuse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Set Up Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n\n# Get keys for your project from the project settings page\n# https://cloud.langfuse.com\n\nos.environ[\"LANGFUSE_PUBLIC_KEY\"] = \"pk-lf-...\" \nos.environ[\"LANGFUSE_SECRET_KEY\"] = \"sk-lf-...\"\nos.environ[\"LANGFUSE_HOST\"] = \"https://cloud.langfuse.com\" # \ud83c\uddea\ud83c\uddfa EU region\n# os.environ[\"LANGFUSE_HOST\"] = \"https://us.cloud.langfuse.com\" # \ud83c\uddfa\ud83c\uddf8 US region\n\n# Set your Portkey API details\nos.environ[\"PORTKEY_API_BASE\"] = \"https://api.portkey.ai/v1\"\nos.environ[\"PORTKEY_API_KEY\"] = \"portkey-...\"  # Your Portkey API key\nos.environ[\"PORTKEY_PROVIDER\"] = \"openai\"  # Or any other provider supported by Portkey\nos.environ[\"PROVIDER_API_KEY\"] = \"sk-...\"  # Your provider's API key (e.g., OpenAI API key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Use Langfuse OpenAI Drop-in Replacement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langfuse.openai import openai\n\nclient = openai.OpenAI(\n  api_key=os.environ.get(\"PROVIDER_API_KEY\"),\n  base_url=os.environ.get(\"PORTKEY_API_BASE\"),\n  default_headers={\n    \"x-portkey-api-key\": os.environ.get(\"PORTKEY_API_KEY\"),\n    \"x-portkey-provider\": os.environ.get(\"PORTKEY_PROVIDER\")\n  }\n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Run an Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n  model=\"gpt-3.5-turbo\",  # Or any model supported by your chosen provider\n  messages=[\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": \"What are the benefits of using an AI gateway?\"},\n  ],\n  name = \"Portkey-Gateway-Trace\" # name of the trace\n)\nprint(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: See Traces in Langfuse\n\nAfter running the example, log in to Langfuse to view the detailed traces, including:\n\n- Request parameters\n- Response content\n- Token usage and latency metrics\n- Provider information through Portkey gateway\n\n![Langfuse Trace Example](https://langfuse.com/images/cookbook/integration-portkey/portkey-example-trace.png)\n\n_[Public example trace link in Langfuse](https://cloud.langfuse.com/project/cloramnkj0002jz088vzn1ja4/traces/2c11b0e4-eb40-49de-aee9-2ed11bed2839?timestamp=2025-03-05T13%3A31%3A34.781Z&observation=e9668bb4-29d7-4239-87be-e3019480f71f)_\n<!-- STEPS_END -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- MARKDOWN_COMPONENT name: \"LearnMore\" path: \"@/components-mdx/integration-learn-more.mdx\" -->"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
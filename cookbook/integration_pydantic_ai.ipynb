{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "<!-- NOTEBOOK_METADATA source: \"Jupyter Notebook\" title: \"Observability for Pydantic AI with Langfuse Integration\" description: \"Discover how to integrate Langfuse with Pydantic AI for enhanced LLM application monitoring, debugging, and tracing. Improve your AI development workflow today.\" category: \"Integrations\" -->\n",
        "\n",
        "# Integrate Langfuse with Pydantic AI\n",
        "\n",
        "This notebook provides a step-by-step guide on integrating **Langfuse** with **Pydantic AI** to achieve observability and debugging for your LLM applications.\n",
        "\n",
        "> **About PydanticAI:** [PydanticAI](https://pydantic-ai.readthedocs.io/en/latest/) is a Python agent framework designed to simplify the development of production-grade generative AI applications. It brings the same type-safety, ergonomic API design, and developer experience found in FastAPI to the world of GenAI app development. \n",
        "\n",
        "> **What is Langfuse?** [Langfuse](https://langfuse.com) is an open-source LLM engineering platform. It offers tracing and monitoring capabilities for AI applications. Langfuse helps developers debug, analyze, and optimize their AI systems by providing detailed insights and integrating with a wide array of tools and Pydantic AIs through native integrations, OpenTelemetry, and dedicated SDKs.\n",
        "\n",
        "## Getting Started\n",
        "\n",
        "Let's walk through a practical example of using Pydantic AI and integrating it with Langfuse for comprehensive tracing.\n",
        "\n",
        "<!-- STEPS_START -->\n",
        "### Step 1: Install Dependencies\n",
        "\n",
        "<!-- CALLOUT_START type: \"info\" emoji: \"⚠️\" -->\n",
        "_**Note:** This notebook utilizes the Langfuse OTel Python SDK v3. For users of Python SDK v2, please refer to our [legacy Pydantic AI integration guide](https://github.com/langfuse/langfuse-docs/blob/fdea27dd5d3f4a110a4f79c0ec1b7b381b48d091/cookbook/integration_pydantic_ai.ipynb)._\n",
        "<!-- CALLOUT_END -->\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install langfuse pydantic-ai -U"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### Step 2: Configure Langfuse SDK\n",
        "\n",
        "Next, set up your Langfuse API keys. You can get these keys by signing up for a free [Langfuse Cloud](https://cloud.langfuse.com/) account or by [self-hosting Langfuse](https://langfuse.com/self-hosting). These environment variables are essential for the Langfuse client to authenticate and send data to your Langfuse project.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Get keys for your project from the project settings page: https://cloud.langfuse.com\n",
        "os.environ[\"LANGFUSE_PUBLIC_KEY\"] = \"pk-lf-...\" \n",
        "os.environ[\"LANGFUSE_SECRET_KEY\"] = \"sk-lf-...\" \n",
        "os.environ[\"LANGFUSE_HOST\"] = \"https://cloud.langfuse.com\" # 🇪🇺 EU region\n",
        "# os.environ[\"LANGFUSE_HOST\"] = \"https://us.cloud.langfuse.com\" # 🇺🇸 US region\n",
        "\n",
        "# Your OpenAI key\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"sk-proj-...\" "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "With the environment variables set, we can now initialize the Langfuse client. `get_client()` initializes the Langfuse client using the credentials provided in the environment variables.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langfuse import get_client\n",
        " \n",
        "langfuse = get_client()\n",
        " \n",
        "# Verify connection\n",
        "if langfuse.auth_check():\n",
        "    print(\"Langfuse client is authenticated and ready!\")\n",
        "else:\n",
        "    print(\"Authentication failed. Please check your credentials and host.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### Step 3: Initialize Pydantic AI Instrumentation\n",
        "\n",
        "Now, we initialize the [Pydantic AI Instrumentation](https://ai.pydantic.dev/logfire/#logfire-with-an-alternative-otel-backend). This  automatically captures Pydantic AI operations and exports OpenTelemetry (OTel) spans to Langfuse."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pydantic_ai.agent import Agent\n",
        "\n",
        "# Initialize Pydantic AI instrumentation\n",
        "Agent.instrument_all()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### Step 4: Basic Pydantic AI Application\n",
        "\n",
        "Finally, run your Pydantic AI agent and generate trace data that will be sent to Langfuse. In the example below, the agent is executed with a dependency value (the winning square) and natural language input. The output from the tool function is then printed.\n",
        "\n",
        "Make sure to pass `instrument=True` while configuring the `Agent`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pydantic_ai import Agent, RunContext\n",
        "\n",
        "roulette_agent = Agent(\n",
        "    'openai:gpt-4o',\n",
        "    deps_type=int,\n",
        "    result_type=bool,\n",
        "    system_prompt=(\n",
        "        'Use the `roulette_wheel` function to see if the '\n",
        "        'customer has won based on the number they provide.'\n",
        "    ),\n",
        "    instrument=True\n",
        ")\n",
        "\n",
        "@roulette_agent.tool\n",
        "async def roulette_wheel(ctx: RunContext[int], square: int) -> str:\n",
        "    \"\"\"check if the square is a winner\"\"\"\n",
        "    return 'winner' if square == ctx.deps else 'loser'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run the agent - using await since we're in a Jupyter notebook\n",
        "success_number = 18\n",
        "result = await roulette_agent.run('Put my money on square eighteen', deps=success_number)\n",
        "print(result.output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### Step 5: View Traces in Langfuse\n",
        "\n",
        "After executing the application, navigate to your [Langfuse Trace Table](https://cloud.langfuse.com). You will find detailed traces of the application's execution, providing insights into the LLM calls, retrieval operations, inputs, outputs, and performance metrics. \n",
        "\n",
        "![Example Trace in Langfuse](https://langfuse.com/images/cookbook/otel-integration-pydantic-ai/pydanticai-openai-trace-tree.png)\n",
        "\n",
        "[Example Trace in Langfuse](https://cloud.langfuse.com/project/cloramnkj0002jz088vzn1ja4/traces/25f4bdeebaab60e6e1bee7e8469554bc?display=details%3Ftimestamp%3D2025-06-06T14%3A39%3A55.786Z%3Ftimestamp%3D2025-06-06T14%3A37%3A30.656Z%3Ftimestamp%3D2025-06-06T14%3A39%3A55.786Z?timestamp=2025-06-06T14%3A40%3A28.562Z)\n",
        "<!-- STEPS_END -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Add Additional Attributes\n",
        "\n",
        "Langfuse allows you to pass additional attributes to your spans. These can include `user_id`, `tags`, `session_id`, and custom `metadata`. Enriching traces with these details is important for analysis, debugging, and monitoring of your application's behavior across different users or sessions.\n",
        "\n",
        "The following code demonstrates how to start a custom span with `langfuse.start_as_current_span` and then update the trace associated with this span using `span.update_trace()`. \n",
        "\n",
        "**→ Learn more about [Updating Trace and Span Attributes](https://langfuse.com/docs/sdk/python/sdk-v3#updating-observations).**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a simple question-answering agent\n",
        "qa_agent = Agent(\n",
        "    'openai:gpt-4o',\n",
        "    system_prompt='You are a helpful assistant that answers questions clearly and concisely.',\n",
        "    instrument=True\n",
        ")\n",
        "\n",
        "with langfuse.start_as_current_span(\n",
        "    name=\"pydantic-ai-qa-trace\",\n",
        "    ) as span:\n",
        "    \n",
        "    # Run your application here\n",
        "    question = \"What is Langfuse?\"\n",
        "    response = await qa_agent.run(question)\n",
        "    print(response.output)\n",
        "\n",
        "    # Pass additional attributes to the span\n",
        "    span.update_trace(\n",
        "        input=question,\n",
        "        output=response.output,\n",
        "        user_id=\"user_123\",\n",
        "        session_id=\"session_abc\",\n",
        "        tags=[\"dev\", \"pydantic-ai\"],\n",
        "        metadata={\"email\": \"user@langfuse.com\"},\n",
        "        version=\"1.0.0\"\n",
        "        )\n",
        "\n",
        "# Flush events in short-lived applications\n",
        "langfuse.flush()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Score Traces and Spans\n",
        "\n",
        "Langfuse lets you ingest custom scores for individual spans or entire traces. This scoring workflow enables you to implement custom quality checks at runtime or facilitate human-in-the-loop evaluation processes.\n",
        "\n",
        "In the example below, we demonstrate how to score a specific span for `relevance` (a numeric score) and the overall trace for `feedback` (a categorical score). This helps in systematically assessing and improving your application.\n",
        "\n",
        "**→ Learn more about [Custom Scores in Langfuse](https://langfuse.com/docs/scores/custom).**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with langfuse.start_as_current_span(\n",
        "    name=\"pydantic-ai-scored-trace\",\n",
        "    ) as span:\n",
        "    \n",
        "    # Run your application here\n",
        "    question = \"What is Langfuse?\"\n",
        "    response = await qa_agent.run(question)\n",
        "    print(response.output)\n",
        "    \n",
        "    # Score this specific span\n",
        "    span.score(name=\"relevance\", value=0.9, data_type=\"NUMERIC\")\n",
        "\n",
        "    # Score the overall trace\n",
        "    span.score_trace(name=\"feedback\", value=\"positive\", data_type=\"CATEGORICAL\")\n",
        "\n",
        "# Flush events in short-lived applications\n",
        "langfuse.flush()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Manage Prompts with Langfuse\n",
        "\n",
        "Langfuse Prompt Management allows you to collaboratively create, version, and deploy prompts. You can manage prompts either through the Langfuse SDK or directly within the Langfuse UI. These managed prompts can then be fetched into your application at runtime.\n",
        "\n",
        "The code below illustrates fetching a prompt named `answer-question` from Langfuse, compiling it with an input variable (`country`), and then using this compiled prompt in the application. \n",
        "\n",
        "**→ Get started with [Langfuse Prompt Management](https://langfuse.com/docs/prompts/get-started).**\n",
        "\n",
        "<!-- CALLOUT_START type: \"info\" emoji: \"🔗\" -->\n",
        "_**Note:** Linking the Langfuse Prompt and the Generation is currently not possible. This is on our roadmap and we are tracking this [here](https://github.com/orgs/langfuse/discussions/7180)._\n",
        "<!-- CALLOUT_END -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fetch the prompt from langfuse\n",
        "langfuse_prompt = langfuse.get_prompt(name=\"answer-question\")\n",
        "\n",
        "# Compile the prompt with the input\n",
        "compiled_prompt = langfuse_prompt.compile(country = \"France\")\n",
        "\n",
        "# Run your application\n",
        "with langfuse.start_as_current_span(\n",
        "    name=\"pydantic-ai-prompt-trace\",\n",
        "    ) as span:\n",
        "    \n",
        "    # Run your application here with the compiled prompt\n",
        "    response = await qa_agent.run(compiled_prompt)\n",
        "    print(response.output)\n",
        "\n",
        "# Flush events in short-lived applications\n",
        "langfuse.flush()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Dataset Experiments\n",
        "\n",
        "Offline evaluation using datasets is a critical part of the LLM development lifecycle. Langfuse supports this through Dataset Experiments. The typical workflow involves:\n",
        "\n",
        "1.  **Benchmark Dataset**: Defining a dataset with input prompts and their corresponding expected outputs.\n",
        "2.  **Application Run**: Running your LLM application against each item in the dataset.\n",
        "3.  **Evaluation**: Comparing the generated outputs against the expected results or using other scoring mechanisms (e.g., model-based evaluation) to assess performance.\n",
        "\n",
        "The following example demonstrates how to use a pre-existing dataset containing countries and their capitals to run an experiment.\n",
        "\n",
        "**→ Learn more about [Langfuse Dataset Experiments](https://langfuse.com/docs/datasets/overview).**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langfuse import get_client\n",
        " \n",
        "langfuse = get_client()\n",
        " \n",
        "# Fetch an existing dataset\n",
        "dataset = langfuse.get_dataset(name=\"capital_cities_11\")\n",
        "for item in dataset.items:\n",
        "    print(f\"Input: {item.input['country']}, Expected Output: {item.expected_output}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "Next, we iterate through each item in the dataset, run our Pydantic AI application (`your_application`) with the item's input, and log the results as a run associated with that dataset item in Langfuse. This allows for structured evaluation and comparison of different application versions or prompt configurations.\n",
        "\n",
        "The `item.run()` context manager is used to create a new trace for each dataset item processed in the experiment. Optionally you can score the dataset runs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langfuse import get_client\n",
        " \n",
        "langfuse = get_client()\n",
        "dataset_name = \"capital_cities_11\"\n",
        "current_run_name = \"capital_cities_run-pydantic-ai_01\" # Identifies this specific evaluation run\n",
        "current_run_metadata={\"model_provider\": \"OpenAI\", \"temperature_setting\": 0.7}\n",
        "current_run_description=\"Evaluation run for Q&A model on June 4th\"\n",
        " \n",
        "# Define your application function using Pydantic AI agent\n",
        "async def your_application(question):\n",
        "    with langfuse.start_as_current_span(name=\"pydantic-ai-trace\") as span:\n",
        "\n",
        "        response = await qa_agent.run(question)\n",
        "        print(response.output)\n",
        " \n",
        "        # Update the trace with the input and output\n",
        "        span.update_trace(\n",
        "            input=question,\n",
        "            output=response.output,\n",
        "        )\n",
        " \n",
        "        return response.output\n",
        " \n",
        "dataset = langfuse.get_dataset(name=dataset_name) # Fetch your pre-populated dataset\n",
        " \n",
        "for item in dataset.items:\n",
        "    print(f\"Running evaluation for item: {item.id} (Input: {item.input['country']})\")\n",
        " \n",
        "    # Use the item.run() context manager\n",
        "    with item.run(\n",
        "        run_name=current_run_name,\n",
        "        run_metadata=current_run_metadata,\n",
        "        run_description=current_run_description\n",
        "    ) as root_span: \n",
        "        # All subsequent langfuse operations within this block are part of this trace.\n",
        "        generated_answer = await your_application(\n",
        "            question=\"What is the capital of \" + item.input[\"country\"] + \"? Just answer with the name of the city.\",\n",
        "        )\n",
        " \n",
        "        # Optionally, score the result against the expected output\n",
        "        if item.expected_output and generated_answer == item.expected_output:\n",
        "            root_span.score_trace(name=\"exact_match\", value=1.0)\n",
        "        else:\n",
        "            root_span.score_trace(name=\"exact_match\", value=0.0)\n",
        " \n",
        "print(f\"\\nFinished processing dataset '{dataset_name}' for run '{current_run_name}'.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Explore More Langfuse Features\n",
        "\n",
        "Langfuse offers more features to enhance your LLM development and observability workflow:\n",
        "\n",
        "- [LLM-as-a-Judge Evaluators](https://langfuse.com/docs/scores/model-based-evals)\n",
        "- [Custom Dashboards](https://langfuse.com/docs/analytics/custom-dashboards)\n",
        "- [LLM Playground](https://langfuse.com/docs/playground)\n",
        "- [Prompt Management](https://langfuse.com/docs/prompts/get-started)\n",
        "- [Prompt Experiments](https://langfuse.com/docs/datasets/prompt-experiments)\n",
        "- [More Integrations](https://langfuse.com/docs/integrations/overview)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}

{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<!-- NOTEBOOK_METADATA source: \"⚠️ Jupyter Notebook\" title: \"Trace Temporal Workflows with Langfuse\" sidebarTitle: \"Temporal\" logo: \"/images/integrations/temporal_icon.svg\" description: \"Learn how to use Langfuse to monitor Temporal workflows and activities via OpenTelemetry\" category: \"Integrations\" -->\n",
        "\n",
        "# Trace Temporal Workflows with Langfuse\n",
        "\n",
        "This notebook demonstrates how to **integrate Langfuse** into your **Temporal workflows** to monitor, debug, and evaluate your AI agents and LLM-powered applications.\n",
        "\n",
        "> **What is Temporal?**: [Temporal](https://temporal.io/) is a durable execution platform that guarantees the execution of your application code, even in the presence of failures. It provides reliability, scalability, and visibility into long-running workflows and distributed applications.\n",
        "\n",
        "> **What is Langfuse?**: [Langfuse](https://langfuse.com/) is an open-source observability platform for AI agents and LLM applications. It helps you visualize and monitor LLM calls, tool usage, cost, latency, and more."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Use Case: Deep Research Agent with Temporal\n",
        "\n",
        "In this example, we'll build a **deep research agent** that:\n",
        "- Uses Temporal workflows to orchestrate long-running research tasks\n",
        "- Leverages OpenAI for research planning and content generation\n",
        "- Sends all observability data to Langfuse via OpenTelemetry\n",
        "\n",
        "This setup allows you to:\n",
        "- **Track workflow execution**: See all workflow runs, activities, and their status\n",
        "- **Monitor LLM calls**: View prompts, completions, token usage, and costs\n",
        "- **Debug failures**: Identify bottlenecks and errors in your research pipeline\n",
        "- **Evaluate quality**: Assess the quality of research outputs over time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Install Dependencies\n",
        "\n",
        "Install Temporal SDK, OpenTelemetry packages, and Langfuse:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install temporalio openai opentelemetry-api opentelemetry-sdk opentelemetry-exporter-otlp-proto-http langfuse"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Configure Environment & API Keys\n",
        "\n",
        "Set up your Langfuse, Temporal, and OpenAI credentials. You can get Langfuse keys by signing up for a free [Langfuse Cloud](https://cloud.langfuse.com/) account or by [self-hosting Langfuse](https://langfuse.com/self-hosting)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Langfuse configuration\n",
        "# Get keys for your project from: https://cloud.langfuse.com\n",
        "os.environ[\"LANGFUSE_PUBLIC_KEY\"] = \"pk-lf-...\" \n",
        "os.environ[\"LANGFUSE_SECRET_KEY\"] = \"sk-lf-...\" \n",
        "os.environ[\"LANGFUSE_HOST\"] = \"https://cloud.langfuse.com\" # 🇪🇺 EU region\n",
        "# os.environ[\"LANGFUSE_HOST\"] = \"https://us.cloud.langfuse.com\" # 🇺🇸 US region\n",
        "\n",
        "# OpenAI API key\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"sk-proj-...\"\n",
        "\n",
        "# Temporal server address (use Temporal Cloud or local dev server)\n",
        "TEMPORAL_HOST = \"localhost:7233\"  # or your Temporal Cloud endpoint"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Set Up OpenTelemetry Integration\n",
        "\n",
        "Configure OpenTelemetry to send traces from Temporal to Langfuse. This setup uses the Langfuse OTLP endpoint with Basic Authentication."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import base64\n",
        "from opentelemetry import trace\n",
        "from opentelemetry.sdk.trace import TracerProvider\n",
        "from opentelemetry.sdk.trace.export import BatchSpanProcessor\n",
        "from opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporter\n",
        "from opentelemetry.sdk.resources import Resource\n",
        "\n",
        "# Create authentication header for Langfuse\n",
        "public_key = os.environ.get(\"LANGFUSE_PUBLIC_KEY\")\n",
        "secret_key = os.environ.get(\"LANGFUSE_SECRET_KEY\")\n",
        "langfuse_host = os.environ.get(\"LANGFUSE_HOST\", \"https://cloud.langfuse.com\")\n",
        "\n",
        "auth_string = f\"{public_key}:{secret_key}\"\n",
        "auth_header = base64.b64encode(auth_string.encode()).decode()\n",
        "\n",
        "# Configure OTLP exporter for Langfuse\n",
        "otlp_endpoint = f\"{langfuse_host}/api/public/otel\"\n",
        "headers = {\"Authorization\": f\"Basic {auth_header}\"}\n",
        "\n",
        "# Set up OpenTelemetry with Langfuse exporter\n",
        "resource = Resource(attributes={\"service.name\": \"temporal-research-agent\"})\n",
        "tracer_provider = TracerProvider(resource=resource)\n",
        "span_processor = BatchSpanProcessor(\n",
        "    OTLPSpanExporter(endpoint=otlp_endpoint, headers=headers)\n",
        ")\n",
        "tracer_provider.add_span_processor(span_processor)\n",
        "trace.set_tracer_provider(tracer_provider)\n",
        "\n",
        "print(f\"✅ OpenTelemetry configured to send traces to Langfuse at {otlp_endpoint}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Initialize Langfuse Client\n",
        "\n",
        "Verify the Langfuse connection:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langfuse import get_client\n",
        " \n",
        "langfuse = get_client()\n",
        " \n",
        "# Verify connection\n",
        "if langfuse.auth_check():\n",
        "    print(\"✅ Langfuse client is authenticated and ready!\")\n",
        "else:\n",
        "    print(\"❌ Authentication failed. Please check your credentials and host.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Define Temporal Activities\n",
        "\n",
        "Create activities that will be executed as part of the research workflow. Each activity represents a discrete step in the research process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from temporalio import activity\n",
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI()\n",
        "\n",
        "@activity.defn\n",
        "async def plan_research(search_term: str) -> dict:\n",
        "    \"\"\"Generate a research plan for the given search term.\"\"\"\n",
        "    tracer = trace.get_tracer(__name__)\n",
        "    \n",
        "    with tracer.start_as_current_span(\"PlannerAgent\") as span:\n",
        "        span.set_attribute(\"search_term\", search_term)\n",
        "        \n",
        "        response = client.chat.completions.create(\n",
        "            model=\"gpt-4.1-2025-04-14\",\n",
        "            messages=[\n",
        "                {\n",
        "                    \"role\": \"system\",\n",
        "                    \"content\": \"You are a research assistant. Given a search term, you search the web for relevant information and provide a comprehensive research plan.\"\n",
        "                },\n",
        "                {\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": f\"Search term: {search_term}\\nReason for searching: Look for travel tips and recommendations for April vacations in the Caribbean.\"\n",
        "                }\n",
        "            ],\n",
        "            temperature=1,\n",
        "        )\n",
        "        \n",
        "        result = {\n",
        "            \"search_term\": search_term,\n",
        "            \"plan\": response.choices[0].message.content,\n",
        "            \"model\": response.model,\n",
        "            \"usage\": {\n",
        "                \"prompt_tokens\": response.usage.prompt_tokens,\n",
        "                \"completion_tokens\": response.usage.completion_tokens,\n",
        "                \"total_tokens\": response.usage.total_tokens,\n",
        "            }\n",
        "        }\n",
        "        \n",
        "        span.set_attribute(\"result.plan\", result[\"plan\"])\n",
        "        span.set_attribute(\"result.total_tokens\", result[\"usage\"][\"total_tokens\"])\n",
        "        \n",
        "        return result\n",
        "\n",
        "@activity.defn\n",
        "async def execute_research(plan: dict) -> dict:\n",
        "    \"\"\"Execute the research plan and generate findings.\"\"\"\n",
        "    tracer = trace.get_tracer(__name__)\n",
        "    \n",
        "    with tracer.start_as_current_span(\"ResearchAgent\") as span:\n",
        "        span.set_attribute(\"plan\", str(plan))\n",
        "        \n",
        "        response = client.chat.completions.create(\n",
        "            model=\"gpt-4.1-2025-04-14\",\n",
        "            messages=[\n",
        "                {\n",
        "                    \"role\": \"system\",\n",
        "                    \"content\": \"You are a research assistant. Execute the research plan and provide comprehensive findings.\"\n",
        "                },\n",
        "                {\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": f\"Research plan: {plan['plan']}\\nPlease execute this plan and provide detailed findings.\"\n",
        "                }\n",
        "            ],\n",
        "            temperature=1,\n",
        "        )\n",
        "        \n",
        "        result = {\n",
        "            \"findings\": response.choices[0].message.content,\n",
        "            \"model\": response.model,\n",
        "            \"usage\": {\n",
        "                \"prompt_tokens\": response.usage.prompt_tokens,\n",
        "                \"completion_tokens\": response.usage.completion_tokens,\n",
        "                \"total_tokens\": response.usage.total_tokens,\n",
        "            }\n",
        "        }\n",
        "        \n",
        "        span.set_attribute(\"result.total_tokens\", result[\"usage\"][\"total_tokens\"])\n",
        "        \n",
        "        return result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Define Temporal Workflow\n",
        "\n",
        "Create a workflow that orchestrates the research activities. Temporal ensures the workflow executes reliably, even if failures occur."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from temporalio import workflow\n",
        "from datetime import timedelta\n",
        "\n",
        "@workflow.defn\n",
        "class ResearchWorkflow:\n",
        "    @workflow.run\n",
        "    async def run(self, search_term: str) -> dict:\n",
        "        \"\"\"Execute a complete research workflow.\"\"\"\n",
        "        \n",
        "        # Step 1: Create research plan\n",
        "        plan = await workflow.execute_activity(\n",
        "            plan_research,\n",
        "            search_term,\n",
        "            start_to_close_timeout=timedelta(seconds=30),\n",
        "        )\n",
        "        \n",
        "        # Step 2: Execute research\n",
        "        findings = await workflow.execute_activity(\n",
        "            execute_research,\n",
        "            plan,\n",
        "            start_to_close_timeout=timedelta(seconds=60),\n",
        "        )\n",
        "        \n",
        "        return {\n",
        "            \"search_term\": search_term,\n",
        "            \"plan\": plan,\n",
        "            \"findings\": findings,\n",
        "            \"status\": \"completed\"\n",
        "        }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Run the Workflow\n",
        "\n",
        "Execute the research workflow. This will send all traces to Langfuse via OpenTelemetry.\n",
        "\n",
        "**Note**: This requires a running Temporal server. You can start a local dev server with:\n",
        "```bash\n",
        "temporal server start-dev\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from temporalio.client import Client\n",
        "from temporalio.worker import Worker\n",
        "import asyncio\n",
        "\n",
        "async def main():\n",
        "    # Connect to Temporal\n",
        "    client = await Client.connect(TEMPORAL_HOST)\n",
        "    \n",
        "    # Start a worker in the background\n",
        "    async with Worker(\n",
        "        client,\n",
        "        task_queue=\"research-tasks\",\n",
        "        workflows=[ResearchWorkflow],\n",
        "        activities=[plan_research, execute_research],\n",
        "    ):\n",
        "        # Execute the workflow\n",
        "        result = await client.execute_workflow(\n",
        "            ResearchWorkflow.run,\n",
        "            \"Caribbean travel tips April 2023\",\n",
        "            id=\"research-workflow-001\",\n",
        "            task_queue=\"research-tasks\",\n",
        "        )\n",
        "        \n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(\"WORKFLOW COMPLETED\")\n",
        "        print(\"=\"*50)\n",
        "        print(f\"\\nSearch Term: {result['search_term']}\")\n",
        "        print(f\"\\nStatus: {result['status']}\")\n",
        "        print(f\"\\nPlan Tokens: {result['plan']['usage']['total_tokens']}\")\n",
        "        print(f\"Findings Tokens: {result['findings']['usage']['total_tokens']}\")\n",
        "        print(f\"\\nFindings Preview: {result['findings']['findings'][:200]}...\")\n",
        "\n",
        "# Run the workflow\n",
        "await main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. View Traces in Langfuse\n",
        "\n",
        "After running the workflow, you can view the complete trace in Langfuse. The trace will show:\n",
        "\n",
        "- **Workflow execution**: The entire `ResearchWorkflow` with timing and status\n",
        "- **Activity spans**: Each activity (`plan_research`, `execute_research`) as nested spans\n",
        "- **LLM calls**: OpenAI API calls with prompts, completions, and token usage\n",
        "- **Cost tracking**: Estimated costs based on token usage\n",
        "- **Latency metrics**: Time spent in each component\n",
        "\n",
        "![Example trace in Langfuse](https://langfuse.com/images/cookbook/integration_temporal/temporal-research-workflow-trace.png)\n",
        "\n",
        "**Example Trace**: [View in Langfuse](https://cloud.langfuse.com/project/cloramnkj0002jz088vzn1ja4/traces/08d49878ef6fd5985bf82b84b2dd5383?timestamp=2025-10-13T12%3A50%3A09.574Z&display=preview&observation=a29ef94f77c13acf)\n",
        "\n",
        "The trace view helps you:\n",
        "- Debug workflow failures by seeing exactly where errors occurred\n",
        "- Optimize performance by identifying slow activities\n",
        "- Monitor costs by tracking token usage across all LLM calls\n",
        "- Evaluate output quality by reviewing prompts and completions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Advanced: Custom Trace Attributes\n",
        "\n",
        "You can add custom attributes to your spans for better filtering and analysis in Langfuse:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@activity.defn\n",
        "async def advanced_research(search_term: str, user_id: str, session_id: str) -> dict:\n",
        "    \"\"\"Research activity with custom trace attributes.\"\"\"\n",
        "    tracer = trace.get_tracer(__name__)\n",
        "    \n",
        "    with tracer.start_as_current_span(\"AdvancedResearch\") as span:\n",
        "        # Add custom Langfuse attributes\n",
        "        span.set_attribute(\"langfuse.trace.user_id\", user_id)\n",
        "        span.set_attribute(\"langfuse.trace.session_id\", session_id)\n",
        "        span.set_attribute(\"langfuse.trace.tags\", [\"research\", \"production\"])\n",
        "        span.set_attribute(\"langfuse.generation.name\", \"research-planner\")\n",
        "        \n",
        "        # Your activity logic here\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"gpt-4.1-2025-04-14\",\n",
        "            messages=[{\"role\": \"user\", \"content\": search_term}],\n",
        "        )\n",
        "        \n",
        "        return {\"result\": response.choices[0].message.content}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Resources\n",
        "\n",
        "- [Temporal Documentation](https://docs.temporal.io/)\n",
        "- [Langfuse OpenTelemetry Integration Guide](https://langfuse.com/docs/integrations/opentelemetry)\n",
        "- [OpenTelemetry Python SDK](https://opentelemetry.io/docs/languages/python/)\n",
        "\n",
        "<!-- MARKDOWN_COMPONENT name: \"LearnMore\" path: \"@/components-mdx/integration-learn-more.mdx\" -->"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}

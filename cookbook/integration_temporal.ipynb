{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<!-- NOTEBOOK_METADATA source: \"‚ö†Ô∏è Jupyter Notebook\" title: \"Trace Temporal Workflows with Langfuse\" sidebarTitle: \"Temporal\" logo: \"/images/integrations/temporal_icon.png\" description: \"Learn how to use Langfuse to monitor Temporal workflows and activities via OpenTelemetry\" category: \"Integrations\" -->\n",
        "\n",
        "# Trace Temporal Workflows with Langfuse\n",
        "\n",
        "This notebook demonstrates how to **integrate Langfuse** into your **Temporal workflows** to monitor, debug, and evaluate your AI agents and LLM-powered applications.\n",
        "\n",
        "> **What is Temporal?**: [Temporal](https://temporal.io/) is a durable execution platform that guarantees the execution of your application code, even in the presence of failures. It provides reliability, scalability, and visibility into long-running workflows and distributed applications.\n",
        "\n",
        "> **What is Langfuse?**: [Langfuse](https://langfuse.com/) is an open-source observability platform for AI agents and LLM applications. It helps you visualize and monitor LLM calls, tool usage, cost, latency, and more."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Use Case: Deep Research Agent with Temporal\n",
        "\n",
        "In this example, we'll build a **deep research agent** that:\n",
        "- Uses Temporal workflows to orchestrate long-running research tasks\n",
        "- Leverages the OpenAI Agents SDK for research planning and content generation\n",
        "- Sends all observability data to Langfuse via OpenTelemetry\n",
        "\n",
        "This setup allows you to:\n",
        "- **Track workflow execution**: See all workflow runs, activities, and their status\n",
        "- **Monitor LLM calls**: View prompts, completions, token usage, and costs\n",
        "- **Debug failures**: Identify bottlenecks and errors in your research pipeline\n",
        "- **Evaluate quality**: Assess the quality of research outputs over time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Install Dependencies\n",
        "\n",
        "Install Temporal SDK, OpenTelemetry packages, and Langfuse:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install temporalio openai openai-agents langfuse openinference-instrumentation-openai-agents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Configure Environment & API Keys\n",
        "\n",
        "Set up your Langfuse, Temporal, and OpenAI credentials. You can get Langfuse keys by signing up for a free [Langfuse Cloud](https://cloud.langfuse.com/) account or by [self-hosting Langfuse](https://langfuse.com/self-hosting)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'agents-task-queue'"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# Get keys for your project from the project settings page: https://cloud.langfuse.com\n",
        "os.environ[\"LANGFUSE_PUBLIC_KEY\"] = \"pk-lf-...\" \n",
        "os.environ[\"LANGFUSE_SECRET_KEY\"] = \"sk-lf-...\" \n",
        "os.environ[\"LANGFUSE_HOST\"] = \"https://cloud.langfuse.com\" # üá™üá∫ EU region\n",
        "# os.environ[\"LANGFUSE_HOST\"] = \"https://us.cloud.langfuse.com\" # üá∫üá∏ US region\n",
        "\n",
        "# Your openai key\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"sk-proj-...\"\n",
        "\n",
        "# Temporal server address (use Temporal Cloud or local dev server)\n",
        "os.environ[\"TEMPORAL_HOST\"] = \"localhost:7233\"\n",
        "os.environ.setdefault(\"TEMPORAL_NAMESPACE\", \"default\")\n",
        "os.environ.setdefault(\"TEMPORAL_TASK_QUEUE\", \"agents-task-queue\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. OpenTelemetry Tracing for OpenAI Agents\n",
        "\n",
        "Use the [`OpenAIAgentsInstrumentor`](https://github.com/Arize-ai/openinference/tree/main/python/instrumentation/openinference-instrumentation-openai-agents/examples) library to wrap the OpenaAI Agents SDK and send OpenTelemetry spans to Langfuse."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "from openinference.instrumentation.openai_agents import OpenAIAgentsInstrumentor\n",
        "\n",
        "OpenAIAgentsInstrumentor().instrument()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Initialize Langfuse Client\n",
        "\n",
        "Verify the Langfuse connection:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langfuse import get_client\n",
        " \n",
        "langfuse = get_client()\n",
        " \n",
        "# Verify connection\n",
        "if langfuse.auth_check():\n",
        "    print(\"‚úÖ Langfuse client is authenticated and ready!\")\n",
        "else:\n",
        "    print(\"‚ùå Authentication failed. Please check your credentials and host.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Define Temporal Activities\n",
        "\n",
        "Create activities that will be executed as part of the research workflow. Each activity represents a discrete step in the research process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "import asyncio\n",
        "from temporalio import workflow\n",
        "from agents import Agent, RunConfig, Runner, WebSearchTool, custom_span, gen_trace_id, trace\n",
        "from agents.model_settings import ModelSettings\n",
        "from pydantic import BaseModel\n",
        "\n",
        "# Planner Agent Models\n",
        "class WebSearchItem(BaseModel):\n",
        "    reason: str\n",
        "    \"Your reasoning for why this search is important to the query.\"\n",
        "\n",
        "    query: str\n",
        "    \"The search term to use for the web search.\"\n",
        "\n",
        "class WebSearchPlan(BaseModel):\n",
        "    searches: list[WebSearchItem]\n",
        "    \"\"\"A list of web searches to perform to best answer the query.\"\"\"\n",
        "\n",
        "# Writer Agent Models\n",
        "class ReportData(BaseModel):\n",
        "    short_summary: str\n",
        "    \"\"\"A short 2-3 sentence summary of the findings.\"\"\"\n",
        "\n",
        "    markdown_report: str\n",
        "    \"\"\"The final report\"\"\"\n",
        "\n",
        "    follow_up_questions: list[str]\n",
        "    \"\"\"Suggested topics to research further\"\"\"\n",
        "\n",
        "# Agent factory functions\n",
        "def new_planner_agent():\n",
        "    return Agent(\n",
        "        name=\"PlannerAgent\",\n",
        "        instructions=(\n",
        "            \"You are a helpful research assistant. Given a query, come up with a set of web searches \"\n",
        "            \"to perform to best answer the query. Output between 5 and 20 terms to query for.\"\n",
        "        ),\n",
        "        model=\"gpt-4o\",\n",
        "        output_type=WebSearchPlan,\n",
        "    )\n",
        "\n",
        "def new_search_agent():\n",
        "    return Agent(\n",
        "        name=\"Search agent\",\n",
        "        instructions=(\n",
        "            \"You are a research assistant. Given a search term, you search the web for that term and \"\n",
        "            \"produce a concise summary of the results. The summary must 2-3 paragraphs and less than 300 \"\n",
        "            \"words. Capture the main points. Write succinctly, no need to have complete sentences or good \"\n",
        "            \"grammar. This will be consumed by someone synthesizing a report, so its vital you capture the \"\n",
        "            \"essence and ignore any fluff. Do not include any additional commentary other than the summary \"\n",
        "            \"itself.\"\n",
        "        ),\n",
        "        tools=[WebSearchTool()],\n",
        "        model_settings=ModelSettings(tool_choice=\"required\"),\n",
        "    )\n",
        "\n",
        "def new_writer_agent():\n",
        "    return Agent(\n",
        "        name=\"WriterAgent\",\n",
        "        instructions=(\n",
        "            \"You are a senior researcher tasked with writing a cohesive report for a research query. \"\n",
        "            \"You will be provided with the original query, and some initial research done by a research \"\n",
        "            \"assistant.\\n\"\n",
        "            \"You should first come up with an outline for the report that describes the structure and \"\n",
        "            \"flow of the report. Then, generate the report and return that as your final output.\\n\"\n",
        "            \"The final output should be in markdown format, and it should be lengthy and detailed. Aim \"\n",
        "            \"for 5-10 pages of content, at least 1000 words.\"\n",
        "        ),\n",
        "        model=\"o3-mini\",\n",
        "        output_type=ReportData,\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Define Temporal Workflow\n",
        "\n",
        "Create a workflow that orchestrates the research activities. Temporal ensures the workflow executes reliably, even if failures occur."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ResearchManager:\n",
        "    def __init__(self):\n",
        "        self.run_config = RunConfig()\n",
        "        self.search_agent = new_search_agent()\n",
        "        self.planner_agent = new_planner_agent()\n",
        "        self.writer_agent = new_writer_agent()\n",
        "\n",
        "    async def run(self, query: str) -> str:\n",
        "        trace_id = gen_trace_id()\n",
        "        with trace(\"Research trace\", trace_id=trace_id):\n",
        "            search_plan = await self._plan_searches(query)\n",
        "            search_results = await self._perform_searches(search_plan)\n",
        "            report = await self._write_report(query, search_results)\n",
        "\n",
        "        return report.markdown_report\n",
        "\n",
        "    async def _plan_searches(self, query: str) -> WebSearchPlan:\n",
        "        result = await Runner.run(\n",
        "            self.planner_agent,\n",
        "            f\"Query: {query}\",\n",
        "            run_config=self.run_config,\n",
        "        )\n",
        "        return result.final_output_as(WebSearchPlan)\n",
        "\n",
        "    async def _perform_searches(self, search_plan: WebSearchPlan) -> list[str]:\n",
        "        with custom_span(\"Search the web\"):\n",
        "            num_completed = 0\n",
        "            tasks = [\n",
        "                asyncio.create_task(self._search(item)) for item in search_plan.searches\n",
        "            ]\n",
        "            results = []\n",
        "            for task in workflow.as_completed(tasks):\n",
        "                result = await task\n",
        "                if result is not None:\n",
        "                    results.append(result)\n",
        "                num_completed += 1\n",
        "            return results\n",
        "\n",
        "    async def _search(self, item: WebSearchItem) -> str | None:\n",
        "        input = f\"Search term: {item.query}\\nReason for searching: {item.reason}\"\n",
        "        try:\n",
        "            result = await Runner.run(\n",
        "                self.search_agent,\n",
        "                input,\n",
        "                run_config=self.run_config,\n",
        "            )\n",
        "            return str(result.final_output)\n",
        "        except Exception:\n",
        "            return None\n",
        "\n",
        "    async def _write_report(self, query: str, search_results: list[str]) -> ReportData:\n",
        "        input = f\"Original query: {query}\\nSummarized search results: {search_results}\"\n",
        "        result = await Runner.run(\n",
        "            self.writer_agent,\n",
        "            input,\n",
        "            run_config=self.run_config,\n",
        "        )\n",
        "\n",
        "        return result.final_output_as(ReportData)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "from temporalio import workflow\n",
        "\n",
        "@workflow.defn\n",
        "class ResearchWorkflow:\n",
        "    @workflow.run\n",
        "    async def run(self, query: str) -> str:\n",
        "        return await ResearchManager().run(query)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Run the Workflow\n",
        "\n",
        "When you execute the research workflow, both Temporal `OpenAIAgentsPlugin` and the OpenInference `OpenAIAgentsInstrumentor` will send OTel spans to Langfuse.\n",
        "\n",
        "**Note**: This requires a running Temporal server. You can start a local dev server with:\n",
        "```bash\n",
        "temporal server start-dev\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "from temporalio.client import Client\n",
        "from temporalio.worker import Worker\n",
        "from temporalio.contrib.openai_agents import OpenAIAgentsPlugin\n",
        "from temporalio.worker import UnsandboxedWorkflowRunner\n",
        "\n",
        "async def main():\n",
        "    tls = os.environ.get(\"TEMPORAL_TLS\", \"\").lower() in (\"1\", \"true\", \"yes\")\n",
        "    api_key = os.environ.get(\"TEMPORAL_API_KEY\")\n",
        "\n",
        "    plugin = OpenAIAgentsPlugin()\n",
        "\n",
        "    client = await Client.connect(\n",
        "        target_host=os.environ.get(\"TEMPORAL_ADDRESS\", \"localhost:7233\"),\n",
        "        namespace=os.environ.get(\"TEMPORAL_NAMESPACE\", \"default\"),\n",
        "        api_key=api_key or None,\n",
        "        tls=tls,\n",
        "        plugins=[plugin]\n",
        "    )\n",
        "\n",
        "    worker = Worker(\n",
        "        client,\n",
        "        task_queue=os.environ.get(\"TEMPORAL_TASK_QUEUE\", \"openai-agents-task-queue\"),\n",
        "        workflows=[ResearchWorkflow],\n",
        "        workflow_runner=UnsandboxedWorkflowRunner()\n",
        "    )\n",
        "\n",
        "    async with worker:\n",
        "        handle = await client.start_workflow(\n",
        "            ResearchWorkflow.run,\n",
        "            id=\"research-workflow-01\",\n",
        "            task_queue=os.environ.get(\"TEMPORAL_TASK_QUEUE\", \"openai-agents-task-queue\"),\n",
        "            args=[\"Caribbean vacation spots in April, optimizing for surfing, hiking and water sports\"],\n",
        "        )\n",
        "        result = await handle.result()\n",
        "        print(\"\\nWorkflow result:\\n\", result)\n",
        "\n",
        "await main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. View Traces in Langfuse\n",
        "\n",
        "After running the workflow, you can view the complete trace in Langfuse. The trace will show:\n",
        "\n",
        "- **Workflow execution**: The entire `ResearchWorkflow` with timing and status\n",
        "- **Activity spans**: Each activity (`plan_research`, `execute_research`) as nested spans\n",
        "- **LLM calls**: OpenAI API calls with prompts, completions, and token usage\n",
        "- **Cost tracking**: Estimated costs based on token usage\n",
        "- **Latency metrics**: Time spent in each component\n",
        "\n",
        "![Example trace in Langfuse](https://langfuse.com/images/cookbook/integration_temporal/temporal-research-workflow-trace.png)\n",
        "\n",
        "**Example Trace**: [View in Langfuse](https://cloud.langfuse.com/project/cloramnkj0002jz088vzn1ja4/traces/02c0ff139fa7df3142e4bd436d72c09b?timestamp=2025-10-22T09:05:59.516Z)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}

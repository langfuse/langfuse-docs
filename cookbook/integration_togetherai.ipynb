{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- NOTEBOOK_METADATA source: \"âš ï¸ Jupyter Notebook\" title: \"Monitor Together AI with Langfuse\" sidebarTitle: \"Together AI\" logo: \"/images/integrations/togetherai_icon.svg\" description: \"Learn how to integrate Together.ai with Langfuse using the OpenAI drop-in replacement.\" category: \"Integrations\" -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Observability for Together.ai with Langfuse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This guide shows you how to integrate Together.ai with Langfuse. Together's API endpoints for chat, language and code, images, and embeddings are fully compatible with OpenAI's API. This allows us to use the Langfuse OpenAI drop-in replacement to trace all parts of your application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **What is Together.ai?** [Together.ai](https://docs.together.ai/docs/openai-api-compatibility) empowers developers and researchers to train, fine-tune, and deploy generative AI models, offering access to over 100 open-source models on both serverless and dedicated instances. The platform emphasizes decentralized cloud services, enabling organizations of all sizes to customize AI solutions using their own data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **What is Langfuse?** [Langfuse](https://langfuse.com) is an open source LLM engineering platform that helps teams trace API calls, monitor performance, and debug issues in their AI applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- STEPS_START -->\n",
    "## Step 1: Install Dependencies\n",
    "\n",
    "Make sure you have installed the necessary Python packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install openai langfuse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Set Up Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Get keys for your project from the project settings page\n",
    "# https://cloud.langfuse.com\n",
    "\n",
    "os.environ[\"LANGFUSE_PUBLIC_KEY\"] = \"pk-lf-...\" \n",
    "os.environ[\"LANGFUSE_SECRET_KEY\"] = \"sk-lf-...\" \n",
    "os.environ[\"LANGFUSE_HOST\"] = \"https://cloud.langfuse.com\" # ðŸ‡ªðŸ‡º EU region\n",
    "# os.environ[\"LANGFUSE_HOST\"] = \"https://us.cloud.langfuse.com\" # ðŸ‡ºðŸ‡¸ US region\n",
    "\n",
    "\n",
    "# Get your Together.ai API key from the project settings page\n",
    "os.environ[\"TOGETHER_API_KEY\"] = \"...\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Langfuse OpenAI drop-in Replacement\n",
    "\n",
    "In this step we use the native [OpenAI drop-in replacement](https://langfuse.com/integrations/model-providers/openai-py) by importing `from langfuse.openai import openai`.\n",
    "\n",
    "To start using Together with OpenAI's client libraries, pass in your Together API key to the `api_key` option, and change the `base_url` to `https://api.together.xyz/v1`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instead of import openai:\n",
    "from langfuse.openai import openai\n",
    "\n",
    "client = openai.OpenAI(\n",
    "  api_key=os.environ.get(\"TOGETHER_API_KEY\"),\n",
    "  base_url=\"https://api.together.xyz/v1\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** The OpenAI drop-in replacement is fully compatible with the [Low-Level Langfuse Python SDKs](https://langfuse.com/docs/sdk/python/low-level-sdk) and [`@observe()` decorator](https://langfuse.com/docs/sdk/python/decorators) to trace all parts of your application.\n",
    "\n",
    "## Step 4: Run An Example\n",
    "\n",
    "The following cell demonstrates how to call Together.ai's chat model using the traced OpenAI client. All API calls will be automatically traced by Langfuse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = openai.OpenAI(\n",
    "  api_key=os.environ.get(\"TOGETHER_API_KEY\"),\n",
    "  base_url=\"https://api.together.xyz/v1\",\n",
    ")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "  model=\"meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a travel agent. Be descriptive and helpful.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Tell me the top 3 things to do in San Francisco\"},\n",
    "  ]\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: See Traces in Langfuse\n",
    "\n",
    "After running the example model call, you can see the traces in Langfuse. You will see detailed information about your Together.ai API calls, including:\n",
    "\n",
    "- Request parameters (model, messages, temperature, etc.)\n",
    "- Response content\n",
    "- Token usage statistics\n",
    "- Latency metrics\n",
    "\n",
    "![Langfuse Trace Example](https://langfuse.com/images/cookbook/integration-togetherai/togetherai-example-trace.png)\n",
    "\n",
    "_[Public example trace link in Langfuse](https://cloud.langfuse.com/project/cloramnkj0002jz088vzn1ja4/traces/d3c13a6f-00c5-4090-8f18-6ce18c794950?timestamp=2025-02-25T15%3A43%3A52.800Z&observation=9eeb3b33-49f0-4557-ac00-d3cbe6bc051e)_\n",
    "<!-- STEPS_END -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- MARKDOWN_COMPONENT name: \"LearnMore\" path: \"@/components-mdx/integration-learn-more.mdx\" -->"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

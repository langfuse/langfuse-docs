{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IVKtUonV3xSl"
      },
      "source": [
        "<!-- NOTEBOOK_METADATA source: \"âš ï¸ Jupyter Notebook\" title: \"TrueFoundry AI Gateway Integration\" sidebarTitle: \"Truefoundry\" logo: \"/images/integrations/truefoundry-logo.png\" description: \"Learn how to integrate Truefoundry with Langfuse using the OpenAI drop-in replacement.\" category: \"Integrations\" -->\n",
        "\n",
        "# TrueFoundry AI Gateway Integration\n",
        "\n",
        "> **What is Truefoundry?** [TrueFoundry](https://www.truefoundry.com/ai-gateway) is an enterprise-grade AI Gateway and control plane that lets you deploy, govern, and monitor any LLM or Gen-AI workload behind a single OpenAI-compatible APIâ€”bringing rate-limiting, cost controls, observability, and on-prem support to production AI applications."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PT7e2ers3xSl"
      },
      "source": [
        "## How Truefoundry Integrates with Langfuse\n",
        "\n",
        "Truefoundryâ€™s AI Gateway and Langfuse combine to give you enterprise-grade observability, governance, and cost control over every LLM requestâ€”set up in minutes.\n",
        "\n",
        "<details>\n",
        "<summary><strong>Unified OpenAI-Compatible Endpoint</strong></summary>\n",
        "\n",
        "Point the Langfuse OpenAI client at Truefoundryâ€™s gateway URL. Truefoundry routes to any supported model (OpenAI, Anthropic, self-hosted, etc.), while Langfuse transparently captures each callâ€”no code changes required.\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "<summary><strong>End-to-End Tracing & Metrics</strong></summary>\n",
        "\n",
        "Langfuse delivers:\n",
        "- **Full request/response logs** (including system messages)  \n",
        "- **Token usage** (prompt, completion, total)  \n",
        "- **Latency breakdowns** per call  \n",
        "- **Cost analytics** by model and environment  \n",
        "Drill into any trace in seconds to optimize performance or debug regressions.\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "<summary><strong>Production-Ready Controls</strong></summary>\n",
        "\n",
        "Truefoundry augments your LLM stack with:\n",
        "- **Rate limiting & quotas** per team or user  \n",
        "- **Budget alerts & spend caps** to prevent overruns  \n",
        "- **Scoped API keys** with RBAC for dev, staging, prod  \n",
        "- **On-prem/VPC deployment** for full data sovereignty  \n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kqu2TXK5s-DC"
      },
      "source": [
        "## Prerequisites\n",
        "\n",
        "Before integrating Langfuse with TrueFoundry, ensure you have:\n",
        "1. TrueFoundry Account: Create a [Truefoundry account](https://www.truefoundry.com/register) with atleast one model provider and generate a Personal Access Token by following the instructions in [quick start](https://docs.truefoundry.com/gateway/quick-start) and [generating tokens](https://docs.truefoundry.com/gateway/authentication)\n",
        "2. Langfuse Account: Sign up for a free [Langfuse Cloud account](https://cloud.langfuse.com/) or self-host Langfuse"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B3Jz0Xnd3xSl"
      },
      "source": [
        "<!-- STEPS_START -->\n",
        "## Step 1: Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pGOLCw303xSl"
      },
      "outputs": [],
      "source": [
        "%pip install openai langfuse"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EHm1jDq23xSm"
      },
      "source": [
        "## Step 2: Set Up Environment Variables\n",
        "\n",
        "Next, set up your Langfuse API keys. You can get these keys by signing up for a free [Langfuse Cloud](https://cloud.langfuse.com/) account or by [self-hosting Langfuse](https://langfuse.com/self-hosting). These environment variables are essential for the Langfuse client to authenticate and send data to your Langfuse project."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w5z-wci23xSm"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Langfuse Configuration\n",
        "os.environ[\"LANGFUSE_PUBLIC_KEY\"] = \"pk-lf-...\"\n",
        "os.environ[\"LANGFUSE_SECRET_KEY\"] = \"sk-lf-...\"\n",
        "os.environ[\"LANGFUSE_HOST\"] = \"https://cloud.langfuse.com\"  # ðŸ‡ªðŸ‡º EU region\n",
        "# os.environ[\"LANGFUSE_HOST\"] = \"https://us.cloud.langfuse.com\"  # ðŸ‡ºðŸ‡¸ US region\n",
        "\n",
        "# TrueFoundry Configuration\n",
        "os.environ[\"TRUEFOUNDRY_API_KEY\"] = \"your-truefoundry-token\"\n",
        "os.environ[\"TRUEFOUNDRY_BASE_URL\"] = \"https://your-control-plane.truefoundry.cloud/api/llm\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NJuNI3KB3xSm",
        "outputId": "c1b24c54-1ba3-46fb-f5e5-0658e016ecbb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langfuse import get_client\n",
        "\n",
        "# Test Langfuse authentication\n",
        "get_client().auth_check()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VT023zjp3xSn"
      },
      "source": [
        "## Step 3: Use Langfuse OpenAI Drop-in Replacement\n",
        "\n",
        "Use Langfuse's OpenAI-compatible client to capture and trace every request routed through the TrueFoundry AI Gateway. Detailed steps for configuring the gateway and generating virtual LLM keys are available in the [TrueFoundry documentation](https://docs.truefoundry.com/gateway/langfuse)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3mje3Ste3xSn"
      },
      "outputs": [],
      "source": [
        "from langfuse.openai import OpenAI\n",
        "import os\n",
        "\n",
        "# Initialize OpenAI client with TrueFoundry Gateway\n",
        "client = OpenAI(\n",
        "    api_key=os.environ[\"TRUEFOUNDRY_API_KEY\"],\n",
        "    base_url=os.environ[\"TRUEFOUNDRY_BASE_URL\"]  # Base URL from unified code snippet\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "snT4qt-e3xSn"
      },
      "source": [
        "## Step 4: Run an Example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YwTS3Z3_3xSn"
      },
      "outputs": [],
      "source": [
        "# Make a request through TrueFoundry Gateway with Langfuse tracing\n",
        "response = client.chat.completions.create(\n",
        "    model=\"openai-main/gpt-4o\",  # Paste the model ID you copied from TrueFoundry Gateway\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful AI assistant specialized in explaining AI concepts.\"},\n",
        "        {\"role\": \"user\", \"content\": \"Why does an AI gateway help enterprises?\"},\n",
        "    ],\n",
        "    max_tokens=500,\n",
        "    temperature=0.7\n",
        ")\n",
        "\n",
        "print(response.choices[0].message.content)\n",
        "\n",
        "# Ensure all traces are sent to Langfuse\n",
        "langfuse = get_client()\n",
        "langfuse.flush()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zwVNFN_u3xSn"
      },
      "source": [
        "## Step 5: See Traces in Langfuse\n",
        "\n",
        "After running the example, log in to Langfuse to view the detailed traces, including:\n",
        "\n",
        "- Request parameters\n",
        "- Response content\n",
        "- Token usage and latency metrics\n",
        "- LLM model information through Truefoundry gateway\n",
        "\n",
        "![Langfuse Trace Example](https://langfuse.com/images/cookbook/integration_troufoundry/truefoundry-trace.png)\n",
        "\n",
        "> **Note**: All other features of Langfuse will work as expected, including prompt management, evaluations, custom dashboards, and advanced observability features. The TrueFoundry integration seamlessly supports the full Langfuse feature set.\n",
        "<!-- STEPS_END -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Advanced Integration with Langfuse Python SDK\n",
        "\n",
        "Enhance your observability by combining the automatic tracing with additional Langfuse features.\n",
        "\n",
        "### Using the @observe Decorator\n",
        "\n",
        "The `@observe()` decorator automatically wraps your functions and adds custom attributes to traces:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langfuse import observe, get_client\n",
        "\n",
        "langfuse = get_client()\n",
        "\n",
        "@observe()\n",
        "def analyze_customer_query(query, customer_id):\n",
        "    \"\"\"Analyze customer query using TrueFoundry Gateway with full observability\"\"\"\n",
        "    \n",
        "    response = client.chat.completions.create(\n",
        "        model=\"openai-main/gpt-4o\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a customer service AI assistant.\"},\n",
        "            {\"role\": \"user\", \"content\": query},\n",
        "        ],\n",
        "        temperature=0.3\n",
        "    )\n",
        "    \n",
        "    result = response.choices[0].message.content\n",
        "    \n",
        "    # Add custom metadata to the trace\n",
        "    langfuse.update_current_trace(\n",
        "        input={\"query\": query, \"customer_id\": customer_id},\n",
        "        output={\"response\": result},\n",
        "        user_id=customer_id,\n",
        "        session_id=f\"session_{customer_id}\",\n",
        "        tags=[\"customer-service\", \"truefoundry-gateway\"],\n",
        "        metadata={\n",
        "            \"model_used\": \"openai-main/gpt-4o\",\n",
        "            \"gateway\": \"truefoundry\",\n",
        "            \"query_type\": \"customer_support\"\n",
        "        },\n",
        "        version=\"1.0.0\"\n",
        "    )\n",
        "    \n",
        "    return result\n",
        "\n",
        "# Usage\n",
        "result = analyze_customer_query(\"How do I reset my password?\", \"customer_123\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Debug Mode\n",
        "\n",
        "Enable debug logging for troubleshooting:\n",
        "\n",
        "```python\n",
        "import logging\n",
        "logging.basicConfig(level=logging.DEBUG)\n",
        "```\n",
        "\n",
        "> **Note**: All other features of Langfuse will work as expected, including prompt management, evaluations, custom dashboards, and advanced observability features. The TrueFoundry integration seamlessly supports the full Langfuse feature set.\n",
        "\n",
        "## Learn More\n",
        "\n",
        "- **TrueFoundry AI Gateway Introduction**: [https://docs.truefoundry.com/gateway/intro-to-llm-gateway](https://docs.truefoundry.com/gateway/intro-to-llm-gateway)\n",
        "- **TrueFoundry Authentication Guide**: [https://docs.truefoundry.com/gateway/authentication](https://docs.truefoundry.com/gateway/authentication)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l6yXrarr3xSn"
      },
      "source": [
        "<!-- MARKDOWN_COMPONENT name: \"LearnMore\" path: \"@/components-mdx/integration-learn-more.mdx\" -->"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python (venv)",
      "language": "python",
      "name": "venv"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

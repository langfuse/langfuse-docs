{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<!-- NOTEBOOK_METADATA source: \"âš ï¸ Jupyter Notebook\" title: \"Tracing vLLM with Langfuse via OpenTelemetry\" sidebarTitle: \"vLLM\" logo: \"/images/integrations/vllm_icon.svg\" description: \"Learn how to trace vLLM inference with Langfuse using OpenTelemetry for LLM observability.\" category: \"Integrations\" -->\n",
        "\n",
        "# vLLM Integration\n",
        "\n",
        "This cookbook shows how to trace [vLLM](https://github.com/vllm-project/vllm) inference with [Langfuse](https://langfuse.com) using OpenTelemetry. vLLM has [built-in OpenTelemetry support](https://docs.vllm.ai/en/latest/features/observability.html) that can be configured to send traces to Langfuse's [OpenTelemetry endpoint](/docs/opentelemetry/get-started).\n",
        "\n",
        "> **What is vLLM?** [vLLM](https://github.com/vllm-project/vllm) is a fast and easy-to-use library for LLM inference and serving. It features state-of-the-art throughput, efficient memory management with PagedAttention, continuous batching, and support for a wide range of open-source models.\n",
        "\n",
        "> **What is Langfuse?** [Langfuse](https://langfuse.com) is an open-source LLM engineering platform. It provides tracing, prompt management, and evaluation capabilities to help teams debug, analyze, and iterate on their LLM applications.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Get Started\n",
        "\n",
        "We'll walk through a simple example of using vLLM with Langfuse tracing via OpenTelemetry.\n",
        "\n",
        "<Steps>\n",
        "### Step 1: Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T83YuJOsOjDK"
      },
      "outputs": [],
      "source": [
        "%pip install vllm langfuse -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 2: Set Up Environment Variables\n",
        "\n",
        "Get your Langfuse API keys by signing up for [Langfuse Cloud](https://cloud.langfuse.com) or [self-hosting Langfuse](https://langfuse.com/self-hosting).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-N44brlZOpiB"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Get keys for your project from the project settings page: https://cloud.langfuse.com\n",
        "os.environ[\"LANGFUSE_PUBLIC_KEY\"] = \"pk-lf-...\"\n",
        "os.environ[\"LANGFUSE_SECRET_KEY\"] = \"sk-lf-...\"\n",
        "os.environ[\"LANGFUSE_BASE_URL\"] = \"https://cloud.langfuse.com\" # ðŸ‡ªðŸ‡º EU region\n",
        "# os.environ[\"LANGFUSE_BASE_URL\"] = \"https://us.cloud.langfuse.com\" # ðŸ‡ºðŸ‡¸ US region\n",
        "\n",
        "# Configure OpenTelemetry endpoint & headers\n",
        "os.environ[\"OTEL_EXPORTER_OTLP_TRACES_PROTOCOL\"] = \"http/protobuf\"\n",
        "os.environ[\"OTEL_SERVICE_NAME\"] = \"vllm\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kmDr3CfTOjDK"
      },
      "source": [
        "### Step 3: Initialize OpenTelemetry Tracing\n",
        "\n",
        "vLLM automatically exposes OpenTelemetry spans when configured. The Langfuse client set up in the next step captures these OTEL spans and sends them to Langfuse."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2m3bKMRRQnS5"
      },
      "outputs": [],
      "source": [
        "from vllm import LLM, SamplingParams\n",
        "\n",
        "langfuse_host = \"https://cloud.langfuse.com\"  # or https://us.cloud.langfuse.com\n",
        "otlp_traces_endpoint = f\"{langfuse_host}/api/public/otel/v1/traces\"\n",
        "\n",
        "# --- vLLM ---\n",
        "llm = LLM(\n",
        "    model=\"facebook/opt-125m\",\n",
        "    otlp_traces_endpoint=otlp_traces_endpoint,\n",
        "    disable_log_stats=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we initialize the Langfuse OTel client. `get_client()` initializes the Langfuse client using the credentials provided in the environment variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langfuse import get_client\n",
        "\n",
        "langfuse = get_client()\n",
        "\n",
        "# Verify connection\n",
        "if langfuse.auth_check():\n",
        "    print(\"Langfuse client is authenticated and ready!\")\n",
        "else:\n",
        "    print(\"Authentication failed. Please check your credentials and host.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V9p7vBhADczU"
      },
      "source": [
        "### Step 4: Load the Model with vLLM\n",
        "\n",
        "We load the model using vLLM's `LLM` class. In this example, we use a small model (`facebook/opt-125m`) for demonstration purposes. You can replace this with any model supported by vLLM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WmPq7bx6fb3m"
      },
      "outputs": [],
      "source": [
        "out = llm.generate(\n",
        "    [\"Write one sentence about Berlin.\"],\n",
        "    SamplingParams(max_tokens=32),\n",
        ")\n",
        "print(out[0].outputs[0].text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HdgRew25F0h7"
      },
      "source": [
        "### Step 5: See traces in Langfuse\n",
        "\n",
        "After running the model, you can see new spans in Langfuse. \n",
        "\n",
        "_**Note:** vLLM currently only exports the token counts and latency metrics to Langfuse. The LLM input and output need to be manually captured in a separate trace using the Langfuse SDK.  _\n",
        "\n",
        "[Example trace in Langfuse](https://cloud.langfuse.com/project/cloramnkj0002jz088vzn1ja4/traces/462b76c435b348aa31ab82351c8ae33b?observation=a95f1c8affd878e9&timestamp=2025-12-23T12:59:03.259Z)\n",
        "\n",
        "</Steps>"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

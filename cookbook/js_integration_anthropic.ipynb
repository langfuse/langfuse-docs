{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63f54fb0",
   "metadata": {},
   "source": [
    "<!-- NOTEBOOK_METADATA source: \"âš ï¸ Jupyter Notebook\" title: \"Observability for Anthropic JS/TS with Langfuse\" sidebarTitle: \"Anthropic (JS/TS)\" logo: \"/images/integrations/anthropic_icon.png\" description: \"Learn how to integrate Langfuse with the Anthropic JS/TS SDK for tracing and debugging of your AI applications.\" category: \"Integrations\" -->\n",
    "\n",
    "# Trace Anthropic JS/TS with Langfuse\n",
    "\n",
    "<a href=\"https://langfuse.com/integrations/model-providers/anthropic\"><img className=\"inline\" alt=\"Python\" src=\"https://img.shields.io/badge/Python-3776AB?style=flat&logo=python&logoColor=white\" /></a> <a href=\"https://langfuse.com/integrations/model-providers/anthropic-js\"><img className=\"inline\" alt=\"JS/TS\" src=\"https://img.shields.io/badge/JS/TS-d4d4d8?style=flat&logo=javascript&logoColor=white\" /></a>\n",
    "\n",
    "Anthropic provides advanced language models like Claude, known for their safety, helpfulness, and strong reasoning capabilities. By combining Anthropic's JS/TS SDK with **Langfuse**, you can trace, monitor, and analyze your AI workloads in development and production.\n",
    "\n",
    "This notebook demonstrates how to use the [`AnthropicInstrumentation`](https://github.com/Arize-ai/openinference/tree/main/js/packages/openinference-instrumentation-anthropic) library from [OpenInference](https://github.com/Arize-ai/openinference) to automatically instrument Anthropic SDK calls and send OpenTelemetry spans to Langfuse.\n",
    "\n",
    "> **What is Anthropic?**  \n",
    "Anthropic is an AI safety company that develops Claude, a family of large language models designed to be helpful, harmless, and honest. Claude models excel at complex reasoning, analysis, and creative tasks.\n",
    "\n",
    "> **What is Langfuse?**  \n",
    "[Langfuse](https://langfuse.com) is an open source platform for LLM observability and monitoring. It helps you trace and monitor your AI applications by capturing metadata, prompt details, token usage, latency, and more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- STEPS_START -->\n",
    "## Step 1: Install Dependencies\n",
    "\n",
    "Install the necessary packages:\n",
    "\n",
    "```bash\n",
    "npm install @anthropic-ai/sdk @arizeai/openinference-instrumentation-anthropic @langfuse/otel @opentelemetry/sdk-node\n",
    "```\n",
    "\n",
    "> **Note**: This cookbook uses **Deno.js** for execution, which requires different syntax for importing packages and setting environment variables. For Node.js applications, the setup process is similar but uses standard `npm` packages and `process.env`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Configure Environment\n",
    "\n",
    "Set up your Langfuse and Anthropic API keys. You can get Langfuse keys by signing up for a free [Langfuse Cloud](https://cloud.langfuse.com/) account or by [self-hosting Langfuse](https://langfuse.com/self-hosting). Get your Anthropic API key from the [Anthropic Console](https://console.anthropic.com/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Set environment variables using Deno-specific syntax\n",
    "Deno.env.set(\"ANTHROPIC_API_KEY\", \"sk-ant-...\");\n",
    "\n",
    "// Langfuse authentication keys\n",
    "Deno.env.set(\"LANGFUSE_PUBLIC_KEY\", \"pk-lf-...\");\n",
    "Deno.env.set(\"LANGFUSE_SECRET_KEY\", \"sk-lf-...\");\n",
    "\n",
    "// Langfuse host configuration\n",
    "Deno.env.set(\"LANGFUSE_BASE_URL\", \"https://cloud.langfuse.com\"); // ðŸ‡ªðŸ‡º EU region\n",
    "// Deno.env.set(\"LANGFUSE_BASE_URL\", \"https://us.cloud.langfuse.com\"); // ðŸ‡ºðŸ‡¸ US region"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Initialize OpenTelemetry with Langfuse\n",
    "\n",
    "Set up the OpenTelemetry SDK with the `LangfuseSpanProcessor` and the [`AnthropicInstrumentation`](https://github.com/Arize-ai/openinference/tree/main/js/packages/openinference-instrumentation-anthropic) from OpenInference. The instrumentation automatically captures Anthropic SDK calls and sends them as OpenTelemetry spans to Langfuse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { NodeSDK } from \"npm:@opentelemetry/sdk-node\";\n",
    "import { LangfuseSpanProcessor } from \"npm:@langfuse/otel\";\n",
    "import { AnthropicInstrumentation } from \"npm:@arizeai/openinference-instrumentation-anthropic\";\n",
    "\n",
    "import Anthropic from \"npm:@anthropic-ai/sdk\";\n",
    "\n",
    "// Configure the instrumentation for the Anthropic SDK\n",
    "const instrumentation = new AnthropicInstrumentation();\n",
    "instrumentation.manuallyInstrument(Anthropic);\n",
    "\n",
    "// Initialize the OpenTelemetry SDK with Langfuse as the span processor\n",
    "const sdk = new NodeSDK({\n",
    "  spanProcessors: [new LangfuseSpanProcessor()],\n",
    "  instrumentations: [instrumentation],\n",
    "});\n",
    "\n",
    "sdk.start();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Use the Anthropic SDK\n",
    "\n",
    "Now use the Anthropic SDK as you normally would. All calls are automatically traced and sent to Langfuse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "const anthropic = new Anthropic();\n",
    "\n",
    "const message = await anthropic.messages.create({\n",
    "  model: \"claude-haiku-4-5\",\n",
    "  max_tokens: 1000,\n",
    "  messages: [{ role: \"user\", content: \"Hello, Claude!\" }],\n",
    "});\n",
    "\n",
    "console.log(message.content);\n",
    "\n",
    "await sdk.shutdown();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Traces in Langfuse\n",
    "\n",
    "After running the application, navigate to your Langfuse Trace Table. You will find detailed traces of the application's execution, providing insights into the LLM calls, inputs, outputs, and performance metrics.\n",
    "\n",
    "![Langfuse Trace](https://langfuse.com/images/cookbook/integration_anthropic/anthropic-example-trace-js.png)\n",
    "\n",
    "[Example trace in the Langfuse UI](https://cloud.langfuse.com/project/cloramnkj0002jz088vzn1ja4/traces/561e86ac4ec484282b7ca46737a64ca4?observation=0c936de60ccf69b9&timestamp=2026-02-11T10:31:58.406Z)\n",
    "\n",
    "<!-- STEPS_END -->\n",
    "\n",
    "<!-- MARKDOWN_COMPONENT name: \"LearnMore\" path: \"@/components-mdx/integration-learn-more-js.mdx\" -->"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Deno",
   "language": "typescript",
   "name": "deno"
  },
  "language_info": {
   "codemirror_mode": "typescript",
   "file_extension": ".ts",
   "mimetype": "text/x.typescript",
   "name": "typescript",
   "nbconvert_exporter": "script",
   "pygments_lexer": "typescript",
   "version": "5.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

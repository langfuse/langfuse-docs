{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<!-- NOTEBOOK_METADATA source: \"⚠️ Jupyter Notebook\" title: \"Cookbook - LiteLLM (Proxy) + Langfuse OpenAI Integration (JS/TS)\" sidebarTitle: \"LiteLLM (Proxy) + Langfuse OpenAI Integration (JS/TS)\" description: \"The stack to use any of 100+ models in your JS/TS application without having to change your code and with full observability.\" category: \"Examples\" -->\n",
        "\n",
        "# Cookbook: LiteLLM (Proxy) + Langfuse OpenAI Integration (JS/TS)\n",
        "\n",
        "This notebook demonstrates how to use the following stack to experiment with 100+ LLMs from different providers without changing code:\n",
        "\n",
        "- [**LiteLLM Proxy**](https://docs.litellm.ai/docs/) ([GitHub](https://github.com/BerriAI/litellm)): Standardizes 100+ model provider APIs on the OpenAI API schema.\n",
        "- **Langfuse OpenAI SDK Wrapper** ([JS/TS](https://langfuse.com/integrations/model-providers/openai-js)): Natively instruments calls to 100+ models via the OpenAI SDK.\n",
        "- **Langfuse**: OSS LLM Observability, full overview [here](https://langfuse.com/docs).\n",
        "\n",
        "> **Note**: This cookbook uses **Deno.js** for execution, which requires different syntax for importing packages and setting environment variables. For Node.js applications, the setup process is similar but uses standard `npm` packages and `process.env`.\n",
        "\n",
        "<!-- STEPS_START -->\n",
        "## Set Up Environment\n",
        "\n",
        "Get your Langfuse API keys by signing up for [Langfuse Cloud](https://cloud.langfuse.com/) or [self-hosting Langfuse](https://langfuse.com/self-hosting). You’ll also need your OpenAI API key.\n",
        "\n",
        "> **Note**: This cookbook uses **Deno.js** for execution, which requires different syntax for importing packages and setting environment variables. For Node.js applications, the setup process is similar but uses standard `npm` packages and `process.env`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "// Langfuse authentication keys\n",
        "Deno.env.set(\"LANGFUSE_PUBLIC_KEY\", \"pk-lf-***\");\n",
        "Deno.env.set(\"LANGFUSE_SECRET_KEY\", \"sk-lf-***\");\n",
        "\n",
        "// Langfuse host configuration\n",
        "// For US data region, set this to \"https://us.cloud.langfuse.com\"\n",
        "Deno.env.set(\"LANGFUSE_HOST\", \"https://cloud.langfuse.com\")\n",
        "\n",
        "// Set environment variables using Deno-specific syntax\n",
        "Deno.env.set(\"OPENAI_API_KEY\", \"sk-proj-***\");"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "With the environment variables set, we can now initialize the `langfuseSpanProcessor` which is passed to the main OpenTelemetry SDK that orchestrates tracing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "// Import required dependencies\n",
        "import 'npm:dotenv/config';\n",
        "import { NodeSDK } from \"npm:@opentelemetry/sdk-node\";\n",
        "import { LangfuseSpanProcessor } from \"npm:@langfuse/otel\";\n",
        " \n",
        "// Export the processor to be able to flush it later\n",
        "// This is important for ensuring all spans are sent to Langfuse\n",
        "export const langfuseSpanProcessor = new LangfuseSpanProcessor({\n",
        "    publicKey: process.env.LANGFUSE_PUBLIC_KEY!,\n",
        "    secretKey: process.env.LANGFUSE_SECRET_KEY!,\n",
        "    baseUrl: process.env.LANGFUSE_HOST ?? 'https://cloud.langfuse.com', // Default to cloud if not specified\n",
        "    environment: process.env.NODE_ENV ?? 'development', // Default to development if not specified\n",
        "  });\n",
        " \n",
        "// Initialize the OpenTelemetry SDK with our Langfuse processor\n",
        "const sdk = new NodeSDK({\n",
        "  spanProcessors: [langfuseSpanProcessor],\n",
        "});\n",
        " \n",
        "// Start the SDK to begin collecting telemetry\n",
        "// The warning about crypto module is expected in Deno and doesn't affect basic tracing functionality. Media upload features will be disabled, but all core tracing works normally\n",
        "sdk.start();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup Lite LLM Proxy\n",
        "\n",
        "In this example, we'll use llama3 via the Ollama on our local machine.\n",
        "\n",
        "**Steps**\n",
        "\n",
        "1. Create a `litellm_config.yaml` to configure which models are available ([docs](https://litellm.vercel.app/docs/proxy/configs)). We'll use llama3 via Ollama in this example. \n",
        "1. Ensure that you installed Ollama and have pulled the llama3 (8b) model: `ollama pull llama3`\n",
        "2. Run the following cli command to start the proxy: `litellm --config litellm_config.yaml`\n",
        "\n",
        "The Lite LLM Proxy should be now running on http://0.0.0.0:4000\n",
        "\n",
        "To verify the connection you can run `litellm --test`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Log single LLM Call via Langfuse OpenAI Wrapper\n",
        "\n",
        "The Langfuse SDK offers a wrapper function around the OpenAI SDK, automatically logging all OpenAI calls as generations to Langfuse. We wrap the client for each call separately in order to be able to pass a name. For more details, please refer to our [documentation](https://langfuse.com/integrations/model-providers/openai-js)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "6\n"
          ]
        }
      ],
      "source": [
        "import { OpenAI } from \"npm:openai\";\n",
        "import { observeOpenAI } from \"npm:@langfuse/openai\";\n",
        "\n",
        "const PROXY_URL = \"http://0.0.0.0:4000\";\n",
        "const client = observeOpenAI(new OpenAI({baseURL: PROXY_URL}));\n",
        "\n",
        "const systemPrompt = \"You are a very accurate calculator. You output only the result of the calculation.\";\n",
        "\n",
        "const llamaCompletion = await client.chat.completions.create({\n",
        "  model: \"ollama/llama3\",\n",
        "  messages: [\n",
        "    {role: \"system\", content: systemPrompt},\n",
        "    {role: \"user\", content: \"3 + 3 = \"}\n",
        "  ],\n",
        "}); \n",
        "console.log(llamaCompletion.choices[0].message.content);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[Public trace in the Langfuse UI](https://cloud.langfuse.com/project/cloramnkj0002jz088vzn1ja4/traces/0b78e39b9a36195b7373a3c4536ad574?timestamp=2025-08-25T15%3A07%3A24.014Z&display=details&observation=fa33232a3cc957e1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Trace nested LLM Calls using Langfuse JS SDK\n",
        "\n",
        "Use the context manager of the Langfuse TypeScript SDK to group multiple LiteLLM generations together and update the top level span."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import { startActiveObservation, startObservation } from \"npm:@langfuse/tracing\";\n",
        "import { observeOpenAI } from \"npm:@langfuse/openai\";\n",
        "\n",
        "const client = observeOpenAI(new OpenAI({baseURL: PROXY_URL}));\n",
        "\n",
        "const systemPrompt = \"You are a very accurate calculator. You output only the result of the calculation.\";\n",
        " \n",
        "await startActiveObservation(\"user-request\", async (span) => {\n",
        "\n",
        "  await client.chat.completions.create({\n",
        "    model: \"ollama/llama3\",\n",
        "    messages: [\n",
        "      {role: \"system\", content: systemPrompt},\n",
        "      {role: \"user\", content: \"3 + 3 = \"}\n",
        "    ],\n",
        "  }); \n",
        "\n",
        "  await client.chat.completions.create({\n",
        "    model: \"ollama/llama3\",\n",
        "    messages: [\n",
        "      {role: \"system\", content: systemPrompt},\n",
        "      {role: \"user\", content: \"1 - 1 = \"}\n",
        "    ],\n",
        "  }); \n",
        "\n",
        "  await client.chat.completions.create({\n",
        "    model: \"ollama/llama3\",\n",
        "    messages: [\n",
        "      {role: \"system\", content: systemPrompt},\n",
        "      {role: \"user\", content: \"2 + 3 = \"}\n",
        "    ],\n",
        "  }); \n",
        "\n",
        "  // Update trace\n",
        "  span.updateTrace({\n",
        "    name:\"LLM Calculator\",\n",
        "    tags: [\"updated\"],\n",
        "    metadata: {\"env\": \"development\"},\n",
        "    release: \"v0.0.2\",\n",
        "    input: systemPrompt,\n",
        "  });\n",
        "\n",
        "});"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## View traces in Langfuse\n",
        "\n",
        "![Public Trace](https://langfuse.com/images/cookbook/example-js-sdk/integration_litellm_proxy_trace.png)\n",
        "\n",
        "[Public trace in the Langfuse UI](https://cloud.langfuse.com/project/cloramnkj0002jz088vzn1ja4/traces/1aab1c70319ce53e2604801a5e464293?timestamp=2025-08-25T15%3A52%3A19.900Z&display=details&observation=b0f019915d463967)\n",
        "<!-- STEPS_END -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Learn more\n",
        "\n",
        "Check out the docs to learn more about all components of this stack:\n",
        "- [LiteLLM Proxy](https://docs.litellm.ai/docs/)\n",
        "- [Langfuse OpenAI SDK Wrapper](https://langfuse.com/integrations/model-providers/openai-js)\n",
        "- [Langfuse](https://langfuse.com/docs)\n",
        "\n",
        "If you do not want to capture traces via the OpenAI SDK Wrapper, you can also directly log requests from the LiteLLM Proxy to Langfuse. For more details, refer to the [LiteLLM Docs](https://litellm.vercel.app/docs/proxy/logging#logging-proxy-inputoutput---langfuse)."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Deno",
      "language": "typescript",
      "name": "deno"
    },
    "language_info": {
      "codemirror_mode": "typescript",
      "file_extension": ".ts",
      "mimetype": "text/x.typescript",
      "name": "typescript",
      "nbconvert_exporter": "script",
      "pygments_lexer": "typescript",
      "version": "5.6.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}

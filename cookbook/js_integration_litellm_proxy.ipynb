{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "description: The stack to use any of 100+ models in your JS/TS application without having to change your code and with full observability.\n",
    "category: Integrations\n",
    "---\n",
    "\n",
    "# Cookbook: LiteLLM (Proxy) + Langfuse OpenAI Integration (JS/TS)\n",
    "\n",
    "This notebook demonstrates how to use the following stack to experiment with 100+ LLMs from different providers without changing code:\n",
    "\n",
    "- [**LiteLLM Proxy**](https://docs.litellm.ai/docs/) ([GitHub](https://github.com/BerriAI/litellm)): Standardizes 100+ model provider APIs on the OpenAI API schema.\n",
    "- **Langfuse OpenAI SDK Wrapper** ([JS/TS](https://langfuse.com/docs/integrations/openai/python/get-started/js/get-started)): Natively instruments calls to 100+ models via the OpenAI SDK.\n",
    "- **Langfuse**: OSS LLM Observability, full overview [here](https://langfuse.com/docs).\n",
    "\n",
    "Let's get started!\n",
    "\n",
    "## Install dependencies\n",
    "\n",
    "*Note: This cookbook uses Deno.js, which requires different syntax for importing packages and setting environment variables.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { OpenAI } from \"npm:openai@^4.0.0\";\n",
    "import { observeOpenAI } from \"npm:langfuse@^3.6.0\";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Set env variables, Deno-specific syntax\n",
    "Deno.env.set(\"OPENAI_API_KEY\", \"\");\n",
    "Deno.env.set(\"LANGFUSE_PUBLIC_KEY\", \"\");\n",
    "Deno.env.set(\"LANGFUSE_SECRET_KEY\", \"\");\n",
    "Deno.env.set(\"LANGFUSE_HOST\", \"https://cloud.langfuse.com\"); // ðŸ‡ªðŸ‡º EU region\n",
    "// Deno.env.set(\"LANGFUSE_HOST\", \"https://us.cloud.langfuse.com\"); // ðŸ‡ºðŸ‡¸ US region"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Lite LLM Proxy\n",
    "\n",
    "In this example, we'll use GPT-3.5-turbo directly from OpenAI, and llama3 and mistral via the Ollama on our local machine.\n",
    "\n",
    "**Steps**\n",
    "\n",
    "1. Create a `litellm_config.yaml` to configure which models are available ([docs](https://litellm.vercel.app/docs/proxy/configs)). We'll use gpt-3.5-turbo, and llama3 and mistral via Ollama in this example. Make sure to replace `<openai_key>` with your OpenAI API key.\n",
    "    ```yaml\n",
    "    model_list:\n",
    "      - model_name: gpt-3.5-turbo\n",
    "        litellm_params:\n",
    "          model: gpt-3.5-turbo\n",
    "          api_key: <openai_key>\n",
    "      - model_name: ollama/llama3\n",
    "        litellm_params:\n",
    "          model: ollama/llama3\n",
    "      - model_name: ollama/mistral\n",
    "        litellm_params:\n",
    "          model: ollama/mistral\n",
    "    ```\n",
    "3. Ensure that you installed Ollama and have pulled the llama3 (8b) and mistral (7b) models: `ollama pull llama3 && ollama pull mistral`\n",
    "4. Run the following cli command to start the proxy: `litellm --config litellm_config.yaml`\n",
    "\n",
    "The Lite LLM Proxy should be now running on http://0.0.0.0:4000\n",
    "\n",
    "To verify the connection you can run `litellm --test`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log single LLM Call via Langfuse OpenAI Wrapper\n",
    "The Langfuse SDK offers a wrapper function around the OpenAI SDK, automatically logging all OpenAI calls as generations to Langfuse.\n",
    "\n",
    "We wrap the client for each call separately in order to be able to pass a name.\n",
    "\n",
    "For more details, please refer to our [documentation](https://langfuse.com/docs/integrations/openai/python/get-started/js/get-started)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "const PROXY_URL = \"http://0.0.0.0:4000\";\n",
    "const client = observeOpenAI(new OpenAI({baseURL: PROXY_URL}));\n",
    "\n",
    "const systemPrompt = \"You are a very accurate calculator. You output only the result of the calculation.\";\n",
    "\n",
    "const gptCompletion = await client.chat.completions.create({\n",
    "  model: \"gpt-3.5-turbo\", \n",
    "  messages: [\n",
    "    {role: \"system\", content: systemPrompt},\n",
    "    {role: \"user\", content: \"1 + 1 = \"}\n",
    "  ],\n",
    "});\n",
    "console.log(gptCompletion.choices[0].message.content);\n",
    "\n",
    "const llamaCompletion = await client.chat.completions.create({\n",
    "  model: \"ollama/llama3\",\n",
    "  messages: [\n",
    "    {role: \"system\", content: systemPrompt},\n",
    "    {role: \"user\", content: \"3 + 3 = \"}\n",
    "  ],\n",
    "}); \n",
    "console.log(llamaCompletion.choices[0].message.content);\n",
    "\n",
    "// notebook only: await events being flushed to Langfuse\n",
    "await client.flushAsync();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Public trace links for the following examples:\n",
    "- [GPT-3.5-turbo](https://cloud.langfuse.com/project/cloramnkj0002jz088vzn1ja4/traces/5084c45a-4e73-45f6-97b2-ad134abc6af1?observation=20073c4e-749a-4289-ad78-6b48f6e61093)\n",
    "- [llama3](https://cloud.langfuse.com/project/cloramnkj0002jz088vzn1ja4/traces/05e0d142-20be-4e67-9baf-feb0d18271e6?observation=5bb6d269-8f3d-4c6e-8464-5103cbee4ada)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trace nested LLM Calls using Langfuse JS SDK\n",
    "\n",
    "To capture nested LLM calls, use `langfuse.trace` to create a parent trace and pass it to `observeOpenAI`. This allows you to group multiple generations into a single trace, providing a comprehensive view of the interactions. You can also add rich metadata to the trace, such as custom names, tags, and userid. For more details, refer to the [Langfuse JS/TS SDK documentation](https://langfuse.com/docs/sdk/typescript/guide).\n",
    "\n",
    "We'll use the trace to log a rap battle between GPT-3.5-turbo, llama3, and mistral.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { Langfuse } from \"npm:langfuse\";\n",
    "\n",
    "const langfuse = new Langfuse();\n",
    "\n",
    "async function rapBattle(topic: string) {\n",
    "  const trace = langfuse.trace({name: \"Rap Battle\", input: topic});\n",
    "  \n",
    "  let messages = [\n",
    "    {role: \"system\", content: \"You are a rap artist. Drop a fresh line.\"},\n",
    "    {role: \"user\", content: `Kick it off, today's topic is ${topic}, here's the mic...`}\n",
    "  ];\n",
    "\n",
    "  const gptCompletion = await observeOpenAI(new OpenAI({baseURL: PROXY_URL}), {\n",
    "      parent: trace, generationName: \"rap-gpt-3.5-turbo\"\n",
    "  }).chat.completions.create({\n",
    "    model: \"gpt-3.5-turbo\",\n",
    "    messages,\n",
    "  });\n",
    "  const firstRap = gptCompletion.choices[0].message.content;\n",
    "  messages.push({role: \"assistant\", content: firstRap});\n",
    "  console.log(\"Rap 1:\", firstRap);\n",
    "\n",
    "  const llamaCompletion = await observeOpenAI(new OpenAI({baseURL: PROXY_URL}), {\n",
    "      parent: trace, generationName: \"rap-llama3\"\n",
    "  }).chat.completions.create({\n",
    "    model: \"ollama/llama3\", \n",
    "    messages,\n",
    "  });\n",
    "  const secondRap = llamaCompletion.choices[0].message.content;\n",
    "  messages.push({role: \"assistant\", content: secondRap});\n",
    "  console.log(\"Rap 2:\", secondRap);\n",
    "\n",
    "  const mistralCompletion = await observeOpenAI(new OpenAI({baseURL: PROXY_URL}), {\n",
    "      parent: trace, generationName: \"rap-mistral\"\n",
    "  }).chat.completions.create({\n",
    "    model: \"ollama/mistral\",\n",
    "    messages,\n",
    "  });\n",
    "  const thirdRap = mistralCompletion.choices[0].message.content;\n",
    "  messages.push({role: \"assistant\", content: thirdRap});\n",
    "  console.log(\"Rap 3:\", thirdRap);\n",
    "\n",
    "  trace.update({output: messages})\n",
    "  return messages;\n",
    "}\n",
    "\n",
    "await rapBattle(\"typography\");\n",
    "await langfuse.flushAsync();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example Trace** ([public link](https://cloud.langfuse.com/project/cloramnkj0002jz088vzn1ja4/traces/f1eee836-994b-4476-9dd5-3e09662a68c4))\n",
    "\n",
    "![Public Trace](https://langfuse.com/images/cookbook/integration_litellm_proxy_trace.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learn more\n",
    "\n",
    "Check out the docs to learn more about all components of this stack:\n",
    "- [LiteLLM Proxy](https://docs.litellm.ai/docs/)\n",
    "- [Langfuse OpenAI SDK Wrapper](https://langfuse.com/docs/integrations/openai/python/get-started/js/get-started)\n",
    "- [Langfuse](https://langfuse.com/docs)\n",
    "\n",
    "If you do not want to capture traces via the OpenAI SDK Wrapper, you can also directly log requests from the LiteLLM Proxy to Langfuse. For more details, refer to the [LiteLLM Docs](https://litellm.vercel.app/docs/proxy/logging#logging-proxy-inputoutput---langfuse)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Deno",
   "language": "typescript",
   "name": "deno"
  },
  "language_info": {
   "file_extension": ".ts",
   "mimetype": "text/x.typescript",
   "name": "typescript",
   "nb_converter": "script",
   "pygments_lexer": "typescript",
   "version": "5.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

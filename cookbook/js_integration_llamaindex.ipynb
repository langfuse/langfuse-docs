{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"LlamaIndex.TS Integration\"\n",
    "description: \"This guide shows how you can instrument the LlamaIndex.TS package to trace your LLM interactions to Langfuse using the Traceloop OpenTelemetry SDK.\"\n",
    "category: Integrations\n",
    "---\n",
    "\n",
    "# LlamaIndex.TS Integration\n",
    "\n",
    "This guide shows how you can instrument the [LlamaIndex.TS package](https://ts.llamaindex.ai/) to trace your LLM interactions to Langfuse using the [Traceloop OpenTelemetry SDK](https://github.com/traceloop/openllmetry-js).\n",
    "\n",
    "_**Note**: This cookbook uses Deno.js, which requires different syntax for importing packages and setting environment variables._\n",
    "\n",
    "## Step 1: Set Up Your Environment\n",
    "\n",
    "Before you begin, ensure that you have all the required npm packages installed. In this guide, we use the following packages:\n",
    "\n",
    "- **dotenv**: Loads environment variables from a .env file.\n",
    "- **base-64**: Encodes your Langfuse keys into a Base64 token.\n",
    "- **@traceloop/node-server-sdk**: Exports the collected traces to Langfuse.\n",
    "- **openai**: (Optional) Provides an alternative method to trigger chat completions.\n",
    "\n",
    "The first code cell sets up the environment by importing the necessary modules, reading the keys from the environment, and encoding these credentials. It also defines the endpoints for the OTLP exporter and Langfuse headers. Adjust the region (EU or US) as needed for your deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import * as dotenv from 'npm:dotenv';\n",
    "import base64 from 'npm:base-64';  // Use default import instead of namespace import\n",
    "\n",
    "dotenv.config();\n",
    "\n",
    "const LANGFUSE_PUBLIC_KEY = \"pk-lf-...\";\n",
    "const LANGFUSE_SECRET_KEY = \"sk-lf-...\";\n",
    "const LANGFUSE_AUTH = base64.encode(`${LANGFUSE_PUBLIC_KEY}:${LANGFUSE_SECRET_KEY}`);\n",
    "\n",
    "process.env[\"TRACELOOP_BASE_URL\"] = \"https://cloud.langfuse.com/api/public/otel\"; // ðŸ‡ªðŸ‡º EU data region\n",
    "// process.env[\"OTEL_EXPORTER_OTLP_ENDPOINT\"] = \"https://us.cloud.langfuse.com/api/public/otel\"; // ðŸ‡ºðŸ‡¸ US data region\n",
    "process.env[\"TRACELOOP_HEADERS\"] = `Authorization=Basic ${LANGFUSE_AUTH}`;\n",
    "\n",
    "// your openai key\n",
    "process.env[\"OPENAI_API_KEY\"] = \"sk-...\";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Initialize the Traceloop Instrumentation\n",
    "\n",
    "Next, initialize the traceloop instrumentation to route trace data from your LLM interactions to Langfuse. By passing `disableBatch: true`, traces are sent immediately Langfuse. This is useful when running in a notebook environment where you want real-time trace output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceloop exporting traces to https://cloud.langfuse.com/api/public/otel\n"
     ]
    }
   ],
   "source": [
    "import * as traceloop from \"npm:@traceloop/node-server-sdk\";\n",
    "\n",
    "traceloop.initialize({ disableBatch: true });"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Make LLM Chat Requests\n",
    "\n",
    "Once the environment and instrumentation are initialized, you can make LLM chat requests.\n",
    "\n",
    "This example uses the `openai` package to create a chat completion request. Every model call automatically generates a trace in Langfuse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why don't skeletons fight each other?\n",
      "\n",
      "They don't have the guts!\n"
     ]
    }
   ],
   "source": [
    "import { OpenAI } from \"npm:llamaindex\";\n",
    "\n",
    "const llm = new OpenAI();\n",
    "const response = await llm.chat({\n",
    "  messages: [{ content: \"Tell me a joke.\", role: \"user\" }],\n",
    "});\n",
    "\n",
    "console.log(response.message.content);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: View Your Traces in Langfuse\n",
    "\n",
    "After executing the code, all traced operations are sent to Langfuseâ€™s OpenTelemetry backend. You can now see traces in Langfuse and analyze the performance and flow of your LLM application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Deno",
   "language": "typescript",
   "name": "deno"
  },
  "language_info": {
   "codemirror_mode": "typescript",
   "file_extension": ".ts",
   "mimetype": "text/x.typescript",
   "name": "typescript",
   "nbconvert_exporter": "script",
   "pygments_lexer": "typescript",
   "version": "5.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

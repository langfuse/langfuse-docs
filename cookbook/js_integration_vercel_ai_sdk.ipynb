{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e212b922",
   "metadata": {},
   "source": [
    "<!-- NOTEBOOK_METADATA source: \"âš ï¸ Jupyter Notebook\" title: \"Observability and Tracing for the Vercel AI SDK\" sidebarTitle: \"Vercel AI SDK\" logo: \"/images/integrations/vercel_ai_sdk_icon.png\" description: \"Open source observability for Vercel AI SDK applications with Langfuse - metrics, evaluations, prompt management, playground, datasets.\" category: \"Integrations\" -->\n",
    "\n",
    "# Observability and Tracing for the Vercel AI SDK\n",
    "\n",
    "This notebook demonstrates how to **integrate Langfuse** with the **Vercel AI SDK** to monitor, debug, and evaluate your LLM-powered applications and AI agents.\n",
    "\n",
    "> **What is the Vercel AI SDK?**: The [Vercel AI SDK](https://vercel.com/docs/ai-sdk) is a lightweight toolkit that lets developers call and stream responses from AI models (like OpenAI, Anthropic, or any compliant provider) directly in web apps with simple server/client functions.\n",
    "\n",
    "> **What is Langfuse?**: [Langfuse](https://langfuse.com/) is an open-source observability platform for AI agents and LLM applications. It helps you visualize and monitor LLM calls, tool usage, cost, latency, and more.\n",
    "\n",
    "> **How do they work together?** The Vercel AI SDK has built-in telemetry based on [OpenTelemetry](https://opentelemetry.io/docs/what-is-opentelemetry/). Langfuse also uses OpenTelemetry, which means they integrate seamlessly. When you enable telemetry in the Vercel AI SDK and add the Langfuse span processor, your AI calls automatically flow into Langfuse where you can analyze them.\n",
    "\n",
    "## Steps to integrate Langfuse with the Vercel AI SDK\n",
    "\n",
    "### TL;DR\n",
    "Here's the flow of how Langfuse and the Vercel AI SDK work together:\n",
    "\n",
    "1. **You enable telemetry** in the AI SDK (`experimental_telemetry: { isEnabled: true }`)\n",
    "2. **The AI SDK creates spans** for each operation (model calls, tool executions, etc.)\n",
    "3. **The LangfuseSpanProcessor intercepts** these spans and sends them to Langfuse\n",
    "4. **Langfuse stores and visualizes** the data in traces you can explore\n",
    "\n",
    "This integration uses [OpenTelemetry](https://opentelemetry.io/docs/what-is-opentelemetry/), an observability standard. The Vercel AI SDK's telemetry feature is documented in the [Vercel AI SDK documentation on Telemetry](https://ai-sdk.dev/docs/ai-sdk-core/telemetry).\n",
    "\n",
    "Let's walk through the steps in more detail.\n",
    "\n",
    "<Steps>\n",
    "\n",
    "## 1. Install Dependencies\n",
    "\n",
    "Install the Vercel AI SDK, OpenTelemetry, and Langfuse:\n",
    "\n",
    "```bash\n",
    "npm install ai\n",
    "npm install @ai-sdk/openai\n",
    "npm install @langfuse/tracing @langfuse/otel @opentelemetry/sdk-node\n",
    "```\n",
    "\n",
    "<Callout type=\"info\">\n",
    "  _**Note:** While this example uses `@ai-sdk/openai`, you can use any AI provider supported by the Vercel AI SDK (Anthropic, Google, Mistral, etc.). The integration works the same way._\n",
    "</Callout>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d37113",
   "metadata": {},
   "source": [
    "## 2. Configure Environment & API Keys\n",
    "\n",
    "Set up your Langfuse and LLM provider credentials (this example uses OpenAI). You can get Langfuse keys by signing up for a free [Langfuse Cloud](https://cloud.langfuse.com/) account or by [self-hosting Langfuse](https://langfuse.com/self-hosting).\n",
    "\n",
    "```bash filename=\".env\"\n",
    "LANGFUSE_SECRET_KEY = \"sk-lf-...\"\n",
    "LANGFUSE_PUBLIC_KEY = \"pk-lf-...\"\n",
    "LANGFUSE_BASE_URL = \"https://cloud.langfuse.com\" # ðŸ‡ªðŸ‡º EU region\n",
    "# LANGFUSE_BASE_URL = \"https://us.cloud.langfuse.com\" # ðŸ‡ºðŸ‡¸ US region\n",
    "\n",
    "OPENAI_API_KEY = \"sk-proj-\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7acd0df",
   "metadata": {},
   "source": [
    "## 3. Initialize Langfuse with OpenTelemetry\n",
    "\n",
    "Langfuse's tracing is built on **[OpenTelemetry](https://opentelemetry.io)**. To connect Langfuse with the Vercel AI SDK, you need to set up the OpenTelemetry SDK with the **LangfuseSpanProcessor**.\n",
    "\n",
    "**The LangfuseSpanProcessor** is the component that captures telemetry data (called \"spans\" in OpenTelemetry) and sends it to Langfuse for storage and visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec20f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import { NodeSDK } from \"@opentelemetry/sdk-node\";\n",
    "import { LangfuseSpanProcessor } from \"@langfuse/otel\";\n",
    " \n",
    "const sdk = new NodeSDK({\n",
    "  spanProcessors: [new LangfuseSpanProcessor()],\n",
    "});\n",
    " \n",
    "sdk.start();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7789031",
   "metadata": {},
   "source": [
    "## 4. Enable telemetry in your AI SDK calls\n",
    "\n",
    "Simply pass `{ experimental_telemetry: { isEnabled: true }}` to your AI SDK functions. The AI SDK will automatically create telemetry spans, which the `LangfuseSpanProcessor` (from step 3) captures and sends to Langfuse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4912f123",
   "metadata": {},
   "outputs": [],
   "source": [
    "import { generateText, tool } from 'ai';\n",
    "import { openai } from '@ai-sdk/openai';\n",
    "import { z } from 'zod';\n",
    " \n",
    "const { text } = await generateText({\n",
    "  model: openai(\"gpt-5.1\"),\n",
    "  prompt: 'What is the weather like today in San Francisco?',\n",
    "  tools: {\n",
    "    getWeather: tool({\n",
    "      description: 'Get the weather in a location',\n",
    "      inputSchema: z.object({\n",
    "        location: z.string().describe('The location to get the weather for'),\n",
    "      }),\n",
    "      execute: async ({ location }) => ({\n",
    "        location,\n",
    "        temperature: 72 + Math.floor(Math.random() * 21) - 10,\n",
    "      }),\n",
    "    }),\n",
    "  },\n",
    "  experimental_telemetry: { isEnabled: true },\n",
    "});"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce20960d",
   "metadata": {},
   "source": [
    "That's it! This works for all AI SDK functions including `generateText`, `streamText`, `generateObject`, and tool calls.\n",
    "\n",
    "## 5. See traces in Langfuse\n",
    "\n",
    "After running the workflow, you can view the complete trace in Langfuse:\n",
    "\n",
    "![Example trace in Langfuse](https://langfuse.com/images/cookbook/integration_vercel-ai-sdk/ai-sdk_example-trace.png)\n",
    "\n",
    "**Example Trace**: [View in Langfuse](https://cloud.langfuse.com/project/cloramnkj0002jz088vzn1ja4/traces/df41e597d0a85e0d7a6ae8ebfaa70aa0?observation=91a17a2274bfd23f&timestamp=2025-11-17T14%3A55%3A18.786Z)\n",
    "\n",
    "</Steps>\n",
    "\n",
    "## Combining with Prompt Management\n",
    "\n",
    "If you're using [Langfuse Prompt Management](/docs/prompt-management) to version and manage your prompts, it's recommended to link your prompts to traces. This allows you to see which prompt version was used for each generation, in a trace.\n",
    "To link a prompt to a trace, pass the prompt metadata in the `experimental_telemetry` field:\n",
    "\n",
    "```typescript\n",
    "import { generateText } from \"ai\";\n",
    "import { openai } from \"@ai-sdk/openai\";\n",
    "import { LangfuseClient } from \"@langfuse/client\"; // Add this import       \n",
    "\n",
    "const langfuse = new LangfuseClient();\n",
    "\n",
    "// Get current `production` version\n",
    "const prompt = await langfuse.prompt.get(\"movie-critic\");\n",
    " \n",
    "// Insert variables into prompt template\n",
    "const compiledPrompt = prompt.compile({\n",
    "  someVariable: \"example-variable\",\n",
    "});\n",
    "\n",
    "const { text } = await generateText({\n",
    "  model: openai(\"gpt-5\"),\n",
    "  prompt: compiledPrompt,\n",
    "  experimental_telemetry: { \n",
    "    isEnabled: true,\n",
    "    metadata: {\n",
    "      langfusePrompt: prompt.toJSON() // This links the Generation to your prompt in Langfuse\n",
    "    },\n",
    "  },\n",
    "});\n",
    "```\n",
    "\n",
    "Once linked, you'll see the prompt version displayed in the trace details in Langfuse.\n",
    "\n",
    "## Setup with Next.js\n",
    "\n",
    "For production Next.js applications, you'll need a few additional configurations to handle streaming responses properly and ensure traces are sent before serverless functions terminate.\n",
    "\n",
    "This example shows how to:\n",
    "- Set up OpenTelemetry instrumentation in Next.js (using the `instrumentation.ts` file)\n",
    "- Handle streaming responses with proper span lifecycle\n",
    "- Add session and user tracking to traces\n",
    "- Ensure traces are flushed in serverless environments\n",
    "\n",
    "### Setup instrumentation\n",
    "\n",
    "Create a new file [`instrumentation.ts`](https://nextjs.org/docs/app/api-reference/file-conventions/instrumentation) in your project root. This file runs when your Next.js app starts up, initializing the OpenTelemetry setup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f82004",
   "metadata": {
    "tags": [
     "instrumentation.ts"
    ]
   },
   "outputs": [],
   "source": [
    "// instrumentation.ts\n",
    "\n",
    "import { LangfuseSpanProcessor, ShouldExportSpan } from \"@langfuse/otel\";\n",
    "import { NodeTracerProvider } from \"@opentelemetry/sdk-trace-node\";\n",
    "\n",
    "// Optional: filter our NextJS infra spans\n",
    "const shouldExportSpan: ShouldExportSpan = (span) => {\n",
    "  return span.otelSpan.instrumentationScope.name !== \"next.js\";\n",
    "};\n",
    "\n",
    "export const langfuseSpanProcessor = new LangfuseSpanProcessor({\n",
    "  shouldExportSpan,\n",
    "});\n",
    "\n",
    "const tracerProvider = new NodeTracerProvider({\n",
    "  spanProcessors: [langfuseSpanProcessor],\n",
    "});\n",
    "\n",
    "tracerProvider.register();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d639c2",
   "metadata": {},
   "source": "<Callout>\n  If you are using Next.js, please use a manual OpenTelemetry setup via the\n  `NodeTracerProvider` rather than via `registerOTel` from `@vercel/otel`. This\n  is because [the `@vercel/otel` package does not yet support the OpenTelemetry\n  JS SDK v2](https://github.com/vercel/otel/issues/154) on which the\n  `@langfuse/tracing` and `@langfuse/otel` packages are based.\n</Callout>\n\n### Create API route with streaming\n\nThe example below shows a chat endpoint that uses the Langfuse `observe()` wrapper to create a trace, adds session and user metadata, and properly handles streaming responses. Concretely: \n- `observe()` creates a Langfuse trace around your handler\n- `propagateAttributes()` adds session and user metadata for better trace organization\n- `forceFlush()` ensures traces are sent before the serverless function terminates\n- `endOnExit: false` keeps the observation open until the streaming response completes"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308f5ee1",
   "metadata": {},
   "outputs": [],
   "source": "// app/api/chat/route.ts\n\nimport { streamText } from \"ai\";\nimport { after } from \"next/server\";\n\nimport { openai } from \"@ai-sdk/openai\";\nimport {\n  observe,\n  propagateAttributes,\n  setActiveTraceIO,\n} from \"@langfuse/tracing\";\nimport { trace } from \"@opentelemetry/api\";\n\nimport { langfuseSpanProcessor } from \"@/src/instrumentation\";\n\nconst handler = async (req: Request) => {\n  const {\n    messages,\n    chatId,\n    userId,\n  }: { messages: UIMessage[]; chatId: string; userId: string } =\n    await req.json();\n\n  // Set session id and user id on active trace\n  const inputText = messages[messages.length - 1].parts.find(\n    (part) => part.type === \"text\"\n  )?.text;\n\n  // Set input on the active trace\n  setActiveTraceIO({\n    input: inputText,\n  });\n\n  // Add session and user context to the trace\n  await propagateAttributes(\n    {\n      traceName: \"chat-message\",\n      sessionId: chatId,  // Groups related messages together\n      userId,             // Track which user made the request\n    },\n    async () => {\n      const result = streamText({\n        model: openai(\"gpt-5.1\"),\n        messages,\n        experimental_telemetry: {\n          isEnabled: true,\n        },\n        onFinish: async (result) => {\n          // Update trace with final output after stream completes\n          setActiveTraceIO({\n            output: result.content,\n          });\n\n          // End span manually after stream has finished\n          trace.getActiveSpan().end();\n        },\n        onError: async (error) => {\n          setActiveTraceIO({\n            output: error,\n          });\n\n          // Manually end the span since we're streaming\n          trace.getActiveSpan()?.end();\n        },\n      });\n\n      // Critical for serverless: flush traces before function terminates\n      after(async () => await langfuseSpanProcessor.forceFlush());\n\n      return result.toUIMessageStreamResponse();\n    }\n  );\n};\n\n// Wrap handler with observe() to create a Langfuse trace\nexport const POST = observe(handler, {\n  name: \"handle-chat-message\",\n  endOnExit: false, // Don't end observation until stream finishes\n});"
  },
  {
   "cell_type": "markdown",
   "id": "a60b80e2",
   "metadata": {},
   "source": [
    "## Usage together with other observability tools\n",
    "\n",
    "The Vercel AI SDK uses OpenTelemetry for tracing (instrumentation scope: `ai`). If you're also using Sentry, Datadog, or other OTEL-based tools, you may need additional configuration to avoid conflicts. See [Using Langfuse with an Existing OpenTelemetry Setup](/faq/all/existing-otel-setup).\n",
    "\n",
    "<!-- MARKDOWN_COMPONENT name: \"LearnMore\" path: \"@/components-mdx/integration-learn-more-js.mdx\" -->"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Deno",
   "language": "typescript",
   "name": "deno"
  },
  "language_info": {
   "codemirror_mode": "typescript",
   "file_extension": ".ts",
   "mimetype": "text/x.typescript",
   "name": "typescript",
   "nbconvert_exporter": "script",
   "pygments_lexer": "typescript",
   "version": "5.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e212b922",
   "metadata": {},
   "source": [
    "<!-- NOTEBOOK_METADATA source: \"âš ï¸ Jupyter Notebook\" title: \"Observability and Tracing for the Vercel AI SDK\" sidebarTitle: \"Vercel AI SDK\" logo: \"/images/integrations/vercel_ai_sdk_icon.png\" description: \"Open source observability for Vercel AI SDK applications with Langfuse - metrics, evaluations, prompt management, playground, datasets.\" category: \"Integrations\" -->\n",
    "\n",
    "# Observability and Tracing for the Vercel AI SDK\n",
    "\n",
    "This notebook demonstrates how to **integrate Langfuse** with the **Vercel AI SDK** to monitor, debug, and evaluate your LLM-powered applications and AI agents.\n",
    "\n",
    "> **What is the Vercel AI SDK?**: The [Vercel AI SDK](https://vercel.com/docs/ai-sdk) is a lightweight toolkit that lets developers call and stream responses from AI models (like OpenAI, Anthropic, or any compliant provider) directly in web apps with simple server/client functions.\n",
    "\n",
    "> **What is Langfuse?**: [Langfuse](https://langfuse.com/) is an open-source observability platform for AI agents and LLM applications. It helps you visualize and monitor LLM calls, tool usage, cost, latency, and more.\n",
    "\n",
    "\n",
    "<Steps>\n",
    "\n",
    "## 1. Install Dependencies\n",
    "\n",
    "Install the Vercel AI SDK, OpenTelemetry, and Langfuse:\n",
    "\n",
    "```bash\n",
    "npm install ai\n",
    "npm install @ai-sdk/openai\n",
    "npm install @langfuse/tracing @langfuse/otel @opentelemetry/sdk-node\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d37113",
   "metadata": {},
   "source": [
    "## 2. Configure Environment & API Keys\n",
    "\n",
    "Set up your Langfuse and OpenAI credentials. You can get Langfuse keys by signing up for a free [Langfuse Cloud](https://cloud.langfuse.com/) account or by [self-hosting Langfuse](https://langfuse.com/self-hosting).\n",
    "\n",
    "```bash filename=\".env\"\n",
    "LANGFUSE_SECRET_KEY = \"sk-lf-...\"\n",
    "LANGFUSE_PUBLIC_KEY = \"pk-lf-...\"\n",
    "LANGFUSE_BASE_URL = \"https://cloud.langfuse.com\" # ðŸ‡ªðŸ‡º EU region\n",
    "# LANGFUSE_BASE_URL = \"https://us.cloud.langfuse.com\" # ðŸ‡ºðŸ‡¸ US region\n",
    "\n",
    "OPENAI_API_KEY = \"sk-proj-\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7acd0df",
   "metadata": {},
   "source": [
    "## 3. Initialize Langfuse\n",
    "\n",
    "The Langfuse TypeScript SDKâ€™s tracing is built on top of OpenTelemetry, so you need to set up the OpenTelemetry SDK. The `LangfuseSpanProcessor` is the key component that sends traces to Langfuse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec20f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import { NodeSDK } from \"@opentelemetry/sdk-node\";\n",
    "import { LangfuseSpanProcessor } from \"@langfuse/otel\";\n",
    " \n",
    "const sdk = new NodeSDK({\n",
    "  spanProcessors: [new LangfuseSpanProcessor()],\n",
    "});\n",
    " \n",
    "sdk.start();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7789031",
   "metadata": {},
   "source": [
    "## 4: Instrument your application\n",
    "\n",
    "The Vercel AI SDK offers native instrumentation with OpenTelemetry. You can enable the Vercel AI SDK telemetry by passing `{ experimental_telemetry: { isEnabled: true }}` to your AI SDK function calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d12697",
   "metadata": {},
   "outputs": [],
   "source": [
    "import { generateText } from \"ai\";\n",
    "import { openai } from \"@ai-sdk/openai\";\n",
    "\n",
    "const { text } = await generateText({\n",
    "  model: openai(\"gpt-5\"),\n",
    "  prompt: \"What is Langfuse?\",\n",
    "  experimental_telemetry: { isEnabled: true },\n",
    "});"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ca60f6",
   "metadata": {},
   "source": [
    "Another example using tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4912f123",
   "metadata": {},
   "outputs": [],
   "source": [
    "import { generateText, tool } from 'ai';\n",
    "import { openai } from '@ai-sdk/openai';\n",
    "import { z } from 'zod';\n",
    " \n",
    "const { text } = await generateText({\n",
    "  model: openai(\"gpt-5\"),\n",
    "  prompt: 'What is the weather like today in San Francisco?',\n",
    "  tools: {\n",
    "    getWeather: tool({\n",
    "      description: 'Get the weather in a location',\n",
    "      inputSchema: z.object({\n",
    "        location: z.string().describe('The location to get the weather for'),\n",
    "      }),\n",
    "      execute: async ({ location }) => ({\n",
    "        location,\n",
    "        temperature: 72 + Math.floor(Math.random() * 21) - 10,\n",
    "      }),\n",
    "    }),\n",
    "  },\n",
    "  experimental_telemetry: { isEnabled: true },\n",
    "});"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce20960d",
   "metadata": {},
   "source": [
    "## 5. See traces in Langfuse. \n",
    "\n",
    "After running the workflow, you can view the complete trace in Langfuse:\n",
    "\n",
    "![Example trace in Langfuse](https://langfuse.com/images/cookbook/integration_vercel-ai-sdk/ai-sdk_example-trace.png)\n",
    "\n",
    "**Example Trace**: [View in Langfuse](https://cloud.langfuse.com/project/cloramnkj0002jz088vzn1ja4/traces/df41e597d0a85e0d7a6ae8ebfaa70aa0?observation=91a17a2274bfd23f&timestamp=2025-11-17T14%3A55%3A18.786Z)\n",
    "\n",
    "</Steps>\n",
    "\n",
    "Learn more about the AI SDK Telemetry in the [Vercel AI SDK documentation on Telemetry](https://ai-sdk.dev/docs/ai-sdk-core/telemetry).\n",
    "\n",
    "## Next.js Example\n",
    "\n",
    "Here is a full example on how to set up tracing with the\n",
    "\n",
    "- AI SDK v5\n",
    "- Next.js\n",
    "- Deployed on Vercel\n",
    "\n",
    "Create a new file [`instrumentation.ts`](https://nextjs.org/docs/app/api-reference/file-conventions/instrumentation) in your project root with the following content:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f82004",
   "metadata": {
    "tags": [
     "instrumentation.ts"
    ]
   },
   "outputs": [],
   "source": [
    "// instrumentation.ts\n",
    "\n",
    "import { LangfuseSpanProcessor, ShouldExportSpan } from \"@langfuse/otel\";\n",
    "import { NodeTracerProvider } from \"@opentelemetry/sdk-trace-node\";\n",
    "\n",
    "// Optional: filter our NextJS infra spans\n",
    "const shouldExportSpan: ShouldExportSpan = (span) => {\n",
    "  return span.otelSpan.instrumentationScope.name !== \"next.js\";\n",
    "};\n",
    "\n",
    "export const langfuseSpanProcessor = new LangfuseSpanProcessor({\n",
    "  shouldExportSpan,\n",
    "});\n",
    "\n",
    "const tracerProvider = new NodeTracerProvider({\n",
    "  spanProcessors: [langfuseSpanProcessor],\n",
    "});\n",
    "\n",
    "tracerProvider.register();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d639c2",
   "metadata": {},
   "source": [
    "<Callout>\n",
    "  If you are using Next.js, please use a manual OpenTelemetry setup via the\n",
    "  `NodeTracerProvider` rather than via `registerOTel` from `@vercel/otel`. This\n",
    "  is because [the `@vercel/otel` package does not yet support the OpenTelemetry\n",
    "  JS SDK v2](https://github.com/vercel/otel/issues/154) on which the\n",
    "  `@langfuse/tracing` and `@langfuse/otel` packages are based.\n",
    "</Callout>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308f5ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "// route.ts\n",
    "\n",
    "import { streamText } from \"ai\";\n",
    "import { after } from \"next/server\";\n",
    "\n",
    "import { openai } from \"@ai-sdk/openai\";\n",
    "import {\n",
    "  observe,\n",
    "  updateActiveObservation,\n",
    "  updateActiveTrace,\n",
    "} from \"@langfuse/tracing\";\n",
    "import { trace } from \"@opentelemetry/api\";\n",
    "\n",
    "import { langfuseSpanProcessor } from \"@/src/instrumentation\";\n",
    "\n",
    "const handler = async (req: Request) => {\n",
    "  const {\n",
    "    messages,\n",
    "    chatId,\n",
    "    userId,\n",
    "  }: { messages: UIMessage[]; chatId: string; userId: string } =\n",
    "    await req.json();\n",
    "\n",
    "  // Set session id and user id on active trace\n",
    "  const inputText = messages[messages.length - 1].parts.find(\n",
    "    (part) => part.type === \"text\"\n",
    "  )?.text;\n",
    "\n",
    "  updateActiveObservation({\n",
    "    input: inputText,\n",
    "  });\n",
    "\n",
    "  updateActiveTrace({\n",
    "    name: \"my-ai-sdk-trace\",\n",
    "    sessionId: chatId,\n",
    "    userId,\n",
    "    input: inputText,\n",
    "  });\n",
    "\n",
    "  const result = streamText({\n",
    "    // ... other streamText options ...\n",
    "    experimental_telemetry: {\n",
    "      isEnabled: true,\n",
    "    },\n",
    "    onFinish: async (result) => {\n",
    "      updateActiveObservation({\n",
    "        output: result.content,\n",
    "      });\n",
    "      updateActiveTrace({\n",
    "        output: result.content,\n",
    "      });\n",
    "\n",
    "      // End span manually after stream has finished\n",
    "      trace.getActiveSpan().end();\n",
    "    },\n",
    "    onError: async (error) => {\n",
    "      updateActiveObservation({\n",
    "        output: error,\n",
    "        level: \"ERROR\"\n",
    "      });\n",
    "      updateActiveTrace({\n",
    "        output: error,\n",
    "      });\n",
    "\n",
    "      // End span manually after stream has finished\n",
    "      trace.getActiveSpan()?.end();\n",
    "    },\n",
    "  });\n",
    "\n",
    "  // Important in serverless environments: schedule flush after request is finished\n",
    "  after(async () => await langfuseSpanProcessor.forceFlush());\n",
    "\n",
    "  return result.toUIMessageStreamResponse();\n",
    "};\n",
    "\n",
    "export const POST = observe(handler, {\n",
    "  name: \"handle-chat-message\",\n",
    "  endOnExit: false, // end observation _after_ stream has finished\n",
    "});"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97102093",
   "metadata": {},
   "source": [
    "## Using Prompt Management\n",
    "\n",
    "You can use [Langfuse Prompt Management](/docs/prompt-management) to manage and fetch prompts from Langfguse and link LLM generations to prompt versions so you can understand how your prompts are performing and run experiments. \n",
    "\n",
    "The key changes are:\n",
    "1. Import the Langfuse client\n",
    "2. Fetch your prompt using `langfuse.prompt.get()`\n",
    "3. Add the prompt to the observation metadata\n",
    "4. Pass `langfusePrompt: prompt.toJSON()` in the telemetry metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762e7410",
   "metadata": {},
   "outputs": [],
   "source": [
    "import { generateText } from \"ai\";\n",
    "import { openai } from \"@ai-sdk/openai\";\n",
    "import { LangfuseClient } from \"@langfuse/client\"; // Add this import       \n",
    "\n",
    "const langfuse = new LangfuseClient();\n",
    "\n",
    "// Get current `production` version\n",
    "const prompt = await langfuse.prompt.get(\"movie-critic\");\n",
    " \n",
    "// Insert variables into prompt template\n",
    "const compiledPrompt = prompt.compile({\n",
    "  someVariable: \"example-variable\",\n",
    "});\n",
    "\n",
    "const { text } = await generateText({\n",
    "  model: openai(\"gpt-5\"),\n",
    "  prompt: compiledPrompt,\n",
    "  experimental_telemetry: { \n",
    "    isEnabled: true,\n",
    "    metadata: {\n",
    "      langfusePrompt: prompt.toJSON() // This links the Generation to your prompt in Langfuse\n",
    "    },\n",
    "  },\n",
    "});"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a60b80e2",
   "metadata": {},
   "source": [
    "<!-- MARKDOWN_COMPONENT name: \"LearnMore\" path: \"@/components-mdx/integration-learn-more-js.mdx\" -->"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Deno",
   "language": "typescript",
   "name": "deno"
  },
  "language_info": {
   "codemirror_mode": "typescript",
   "file_extension": ".ts",
   "mimetype": "text/x.typescript",
   "name": "typescript",
   "nbconvert_exporter": "script",
   "pygments_lexer": "typescript",
   "version": "5.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

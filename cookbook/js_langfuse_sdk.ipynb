{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28e400f4",
   "metadata": {},
   "source": [
    "---\n",
    "description: Learn how to use the Langfuse JS/TS SDK to log any LLM.\n",
    "category: Integrations\n",
    "---\n",
    "\n",
    "# Cookbook: Use the Langfuse JS/TS SDK to log any LLM.\n",
    "\n",
    "JS/TS applications can either be traces via the [Langfuse SDK](https://langfuse.com/docs/sdk/typescript/guide) by wrapping any LLM model, or by using one of our native integrations such as [OpenAI](https://langfuse.com/docs/integrations/openai/js/get-started), [LangChain](https://langfuse.com/docs/integrations/langchain/example-javascript) or [Vercel AI SDK](https://langfuse.com/docs/integrations/vercel-ai-sdk). In this cookbook, we show you both methods to get you started."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849415ec",
   "metadata": {},
   "source": [
    "## Step 1: Environment Variables\n",
    "\n",
    "*Note: This cookbook uses Deno.js, which requires different syntax for importing packages and setting environment variables.*\n",
    "\n",
    "Set your Langfuse API keys, the Langfuse host name and keys for the used LLM providers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ec3e7874-e6db-44cd-9d55-7ffa72f630fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "// Set env variables, Deno-specific syntax\n",
    "Deno.env.set(\"OPENAI_API_KEY\", \"sk-...\");\n",
    "\n",
    "Deno.env.set(\"ANTHROPIC_API_KEY\", \"sk-...\");\n",
    "\n",
    "Deno.env.set(\"LANGFUSE_SECRET_KEY\", \"sk-...\");\n",
    "Deno.env.set(\"LANGFUSE_PUBLIC_KEY\", \"pk-...\");\n",
    "Deno.env.set(\"LANGFUSE_HOST\", \"https://cloud.langfuse.com\") // For US data region, set this to \"https://us.cloud.langfuse.com\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b0d924",
   "metadata": {},
   "source": [
    "Initialize the Langfuse client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e2b8bb18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import Langfuse from \"npm:langfuse\";\n",
    "\n",
    "// Init Langfuse SDK\n",
    "const langfuse = new Langfuse();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9802f80",
   "metadata": {},
   "source": [
    "## Step 2: Create a Trace\n",
    "\n",
    "Langfuse observability is structured around [traces](https://langfuse.com/docs/tracing#introduction-to-observability--traces-in-langfuse). Each trace can contain multiple observations to log the individual steps of the execution. Observation can be `Events`, the basic building blocks which are used to track discrete events in a trace, `Spans`, representing durations of units of work in a trace,  or `Generations`, used to log model calls. \n",
    "\n",
    "To log an LLM call, we will first create a trace. In this step, we can also assign the trace metadata such as the a user id or tags.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "a68812d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "// Creation of a unique trace id.\n",
    "import { v4 as uuidv4 } from \"npm:uuid\";\n",
    "\n",
    "const traceId = uuidv4();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6987413c",
   "metadata": {},
   "outputs": [],
   "source": [
    "// Creation of the trace and assignment of metadata\n",
    "const trace = langfuse.trace({\n",
    "  id: traceId,\n",
    "  name: \"anthropic-trace\",\n",
    "  userId: \"user_123456789\",\n",
    "  metadata: { user: \"user@langfuse.com\" },\n",
    "  tags: [\"production\"],\n",
    "});\n",
    " \n",
    "// Example update, same params as create, cannot change id\n",
    "trace.update({\n",
    "  metadata: {\n",
    "    tag: \"long-running\",\n",
    "  },\n",
    "});"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ad59cb",
   "metadata": {},
   "source": [
    "## Option 1: Log Any LLM\n",
    "\n",
    "This part shows how to log an LLM call by passing the model in and outputs via the [Langfuse SDK](https://langfuse.com/docs/sdk/typescript/guide).\n",
    "\n",
    "We first create a observation of the type `Generation` which will be assigned to the trace we created earlier. In the second step, we use the Anthropic SDK to call the Clause 3.5 Sonnet model. This step can be replaced with any other LLM SDK.\n",
    "\n",
    "Lastly, we pass the model output, the mode name and usage metrics to the generation. We can now see this trace in the Langfuse UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "88ae3efe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  id: \"msg_01JaTqrfP1V93nxUKrCRgJo1\",\n",
      "  type: \"message\",\n",
      "  role: \"assistant\",\n",
      "  model: \"claude-3-5-sonnet-20241022\",\n",
      "  content: [\n",
      "    { type: \"text\", text: \"Hi! I'm Claude. How can I help you today?\" }\n",
      "  ],\n",
      "  stop_reason: \"end_turn\",\n",
      "  stop_sequence: null,\n",
      "  usage: { input_tokens: 10, output_tokens: 16 }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "const msg = \"Hello, Claude\";\n",
    "\n",
    "// Example generation creation\n",
    "const generation = trace.generation({\n",
    "  name: \"anthropic-generation01\",\n",
    "  input: msg,\n",
    "});\n",
    " \n",
    "// Application code\n",
    "const anthropic = new Anthropic({ apiKey: process.env.ANTHROPIC_API_KEY });\n",
    "\n",
    "const chatCompletion = await anthropic.messages.create({\n",
    "  model: \"claude-3-5-sonnet-20241022\",\n",
    "  max_tokens: 1024,\n",
    "  messages: [{ role: \"user\", content: msg }],\n",
    "});\n",
    " \n",
    "\n",
    "// Example end - sets endTime, optionally pass a body\n",
    "generation.end({\n",
    "  output: chatCompletion.content[0].text,\n",
    "  usage: {\n",
    "    promptTokens: chatCompletion.usage.input_tokens,\n",
    "    completionTokens: chatCompletion.usage.output_tokens,\n",
    "  },\n",
    "  model: chatCompletion.model,\n",
    "\n",
    "});\n",
    "\n",
    "console.log(chatCompletion);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b33b331",
   "metadata": {},
   "source": [
    "![Example Trace](/images/docs/js-any-llm-cookbook.png)\n",
    "\n",
    "Example trace in the Langfuse UI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44200d4",
   "metadata": {},
   "source": [
    "## Option 2: Using LangChain\n",
    "\n",
    "This step shows how to trace Langchain applications using the [Langchain integration](https://langfuse.com/docs/integrations/langchain/example-javascript). Since this is a native integration, the model parameters and outputs are automatically captured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "1b0ce7b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why don't bears wear shoes?\n",
      "\n",
      "Because they have bear feet!\n"
     ]
    }
   ],
   "source": [
    "import { CallbackHandler } from \"npm:langfuse-langchain\"\n",
    "const langfuseLangchainHandler = new CallbackHandler({\n",
    "    publicKey: Deno.env.get(\"LANGFUSE_PUBLIC_KEY\"),\n",
    "    secretKey: Deno.env.get(\"LANGFUSE_SECRET_KEY\"),\n",
    "    baseUrl: Deno.env.get(\"LANGFUSE_HOST\"),\n",
    "    flushAt: 1 // cookbook-only: do not batch events, send them immediately\n",
    "})\n",
    "\n",
    "import { ChatOpenAI } from \"npm:@langchain/openai\"\n",
    "import { PromptTemplate } from \"npm:@langchain/core/prompts\"\n",
    " \n",
    "const model = new ChatOpenAI({});\n",
    "const promptTemplate = PromptTemplate.fromTemplate(\n",
    "  \"Tell me a joke about {topic}\"\n",
    ");\n",
    "\n",
    "import { RunnableSequence } from \"npm:@langchain/core/runnables\";\n",
    " \n",
    "const chain = RunnableSequence.from([promptTemplate, model]);\n",
    " \n",
    "const res = await chain.invoke(\n",
    "    { topic: \"bears\" },\n",
    "    { callbacks: [langfuseLangchainHandler] }\n",
    ");\n",
    " \n",
    "console.log(res.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e61e01",
   "metadata": {},
   "source": [
    "## Option 3: Using OpenAI\n",
    "\n",
    "This step shows how to trace OpenAI applications using the [OpenAI integration](https://langfuse.com/docs/integrations/openai/js/get-started). Since this is a native integration, the model parameters and outputs are automatically captured.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "59a87971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why couldn't the bicycle stand up by itself? Because it was two tired!\n"
     ]
    }
   ],
   "source": [
    "import OpenAI from \"npm:openai\";\n",
    "import { observeOpenAI } from \"npm:langfuse\";\n",
    " \n",
    "// Configured via environment variables, see above\n",
    "const openai = observeOpenAI(new OpenAI());\n",
    " \n",
    "const completion = await openai.chat.completions.create({\n",
    "  model: 'gpt-3.5-turbo',\n",
    "  messages: [{ role: \"system\", content: \"Tell me a joke.\" }],\n",
    "  max_tokens: 100,\n",
    "});\n",
    " \n",
    "// notebook only: await events being flushed to Langfuse\n",
    "await openai.flushAsync();\n",
    " \n",
    "console.log(completion.choices[0]?.message.content);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e44ef4",
   "metadata": {},
   "source": [
    "## Step 3: Score the Trace (Optional)\n",
    "\n",
    "After logging the trace, we can add [scores](https://langfuse.com/docs/scores/custom) to it. This can help in evaluating the quality of the interaction. Scores can be any metric that is important to your application. In this example, we are scoring the trace based on user feedback.\n",
    "\n",
    "Since the scoring usually happens after the generation is complete, we use our unique trace id to score the trace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d8220d",
   "metadata": {},
   "outputs": [],
   "source": [
    "langfuse.score({\n",
    "  id: traceId,\n",
    "  name: \"user-feedback\",\n",
    "  value: 3,\n",
    "  comment: \"This was a good interaction\",\n",
    "});"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Deno",
   "language": "typescript",
   "name": "deno"
  },
  "language_info": {
   "codemirror_mode": "typescript",
   "file_extension": ".ts",
   "mimetype": "text/x.typescript",
   "name": "typescript",
   "nbconvert_exporter": "script",
   "pygments_lexer": "typescript",
   "version": "5.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28e400f4",
   "metadata": {},
   "source": [
    "<!-- NOTEBOOK_METADATA source: \"⚠️ Jupyter Notebook\" title: \"Langfuse JS/TS SDK\" sidebarTitle: \"JS/TS SDK Example\" description: \"End-to-end example of how to use the Langfuse TypeScript SDK to trace LLM applications.\" category: \"Examples\" -->\n",
    "\n",
    "# Cookbook: Langfuse JS/TS SDK\n",
    "\n",
    "JS/TS applications can either be traced via the [Langfuse JS/TS SDK](https://langfuse.com/docs/sdk/typescript/overview), or by using one of the native integrations such as [OpenAI](https://langfuse.com/integrations/model-providers/openai-js), [LangChain](https://langfuse.com/integrations/frameworks/langchain) or [Vercel AI SDK](https://langfuse.com/integrations/frameworks/vercel-ai-sdk).\n",
    "\n",
    "In this notebook, we will walk you through a **simple end-to-end example** that:\n",
    "\n",
    "- Shows how to log any LLM call via the low-level SDK methods\n",
    "- Uses integrations that are interoperable with low-level SDK\n",
    "    - LangChain integration \n",
    "    - OpenAI integration\n",
    "    - Vercel AI SDK\n",
    "\n",
    "For this guide, we assume that you are already familiar with the Langfuse data model (traces, spans, generations, etc.). If not, please read the [conceptual introduction](https://langfuse.com/docs/tracing) to tracing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849415ec",
   "metadata": {},
   "source": [
    "## Set Up Environment\n",
    "\n",
    "Get your Langfuse API keys by signing up for [Langfuse Cloud](https://cloud.langfuse.com/) or [self-hosting Langfuse](https://langfuse.com/self-hosting). You’ll also need your OpenAI API key.\n",
    "\n",
    "> **Note**: This cookbook uses **Deno.js** for execution, which requires different syntax for importing packages and setting environment variables. For Node.js applications, the setup process is similar but uses standard `npm` packages and `process.env`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3e7874-e6db-44cd-9d55-7ffa72f630fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "// Langfuse authentication keys\n",
    "Deno.env.set(\"LANGFUSE_PUBLIC_KEY\", \"pk-lf-***\");\n",
    "Deno.env.set(\"LANGFUSE_SECRET_KEY\", \"sk-lf-***\");\n",
    "\n",
    "// Langfuse host configuration\n",
    "// For US data region, set this to \"https://us.cloud.langfuse.com\"\n",
    "Deno.env.set(\"LANGFUSE_HOST\", \"https://cloud.langfuse.com\")\n",
    "\n",
    "// Set environment variables using Deno-specific syntax\n",
    "Deno.env.set(\"OPENAI_API_KEY\", \"sk-proj-***\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d95bd893",
   "metadata": {},
   "source": [
    "With the environment variables set, we can now initialize the `langfuseSpanProcessor` which is passed to the main OpenTelemetry SDK that orchestrates tracing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc4bedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "// Import required dependencies\n",
    "import 'npm:dotenv/config';\n",
    "import { NodeSDK } from \"npm:@opentelemetry/sdk-node\";\n",
    "import { LangfuseSpanProcessor } from \"npm:@langfuse/otel@^4.0.0-alpha\";\n",
    " \n",
    "// Export the processor to be able to flush it later\n",
    "// This is important for ensuring all spans are sent to Langfuse\n",
    "export const langfuseSpanProcessor = new LangfuseSpanProcessor({\n",
    "    publicKey: process.env.LANGFUSE_PUBLIC_KEY!,\n",
    "    secretKey: process.env.LANGFUSE_SECRET_KEY!,\n",
    "    baseUrl: process.env.LANGFUSE_HOST ?? 'https://cloud.langfuse.com', // Default to cloud if not specified\n",
    "    environment: process.env.NODE_ENV ?? 'development', // Default to development if not specified\n",
    "  });\n",
    " \n",
    "// Initialize the OpenTelemetry SDK with our Langfuse processor\n",
    "const sdk = new NodeSDK({\n",
    "  spanProcessors: [langfuseSpanProcessor],\n",
    "});\n",
    " \n",
    "// Start the SDK to begin collecting telemetry\n",
    "// The warning about crypto module is expected in Deno and doesn't affect basic tracing functionality. Media upload features will be disabled, but all core tracing works normally\n",
    "sdk.start();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b0d924",
   "metadata": {},
   "source": [
    "The **LangfuseClient** provides additional functionality beyond OpenTelemetry tracing, such as scoring, prompt management, and data retrieval. It automatically uses the same environment variables we set earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2b8bb18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import { LangfuseClient } from \"npm:@langfuse/client\";\n",
    " \n",
    "const langfuse = new LangfuseClient();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81e2004",
   "metadata": {},
   "source": [
    "## Log LLM Calls\n",
    "\n",
    "You can use the SDK to log any LLM call or any of the [integrations](https://langfuse.com/integrations) that are interoperable with it.\n",
    "\n",
    "In the following, we will demonstrate how to log LLM calls using the SDK, LangChain, Vercel AI SDK, and OpenAI integrations.\n",
    "\n",
    "### Option 1:  Context Manager\n",
    "\n",
    "To simplify nesting and context management, you can use startActiveSpan and startActiveGeneration. These functions take a callback and automatically manage the observation’s lifecycle and the OpenTelemetry context. Any observation created inside the callback will automatically be nested under the active observation, and the observation will be ended when the callback finishes.\n",
    "\n",
    "This is the recommended approach for most use cases as it prevents context leakage and ensures observations are properly ended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d0a56724",
   "metadata": {},
   "outputs": [],
   "source": [
    "// Import necessary functions from the tracing package\n",
    "import { startActiveSpan, startGeneration, updateActiveTrace, updateActiveSpan } from \"npm:@langfuse/tracing\";\n",
    " \n",
    "// Start a new span with automatic context management\n",
    "await startActiveSpan(\"context-manager\", async (span) => {\n",
    "  // Log the initial user query\n",
    "  span.update({\n",
    "    input: { query: \"What is the capital of France?\" }\n",
    "  });\n",
    " \n",
    "  // Create a new generation span that will automatically be a child of \"context-manager\"\n",
    "  const generation = startGeneration(\"llm-call\", {\n",
    "    model: \"gpt-4\",\n",
    "    input: [{ role: \"user\", content: \"What is the capital of France?\" }],\n",
    "  });\n",
    " \n",
    "  // ... LLM call logic would go here ...\n",
    "\n",
    "  // Update the generation with token usage statistics\n",
    "  generation.update({\n",
    "    usageDetails: {\n",
    "      input: 10, // Number of input tokens\n",
    "      output: 5, // Number of output tokens\n",
    "      cache_read_input_tokens: 2, // Tokens read from cache\n",
    "      some_other_token_count: 10, // Custom token metric\n",
    "      total: 17, // Optional: automatically calculated if not provided\n",
    "    },\n",
    "  });\n",
    " \n",
    "  // End the generation with the LLM response\n",
    "  generation.end({\n",
    "    output: { content: \"The capital of France is Paris.\" },\n",
    "  });\n",
    "\n",
    "  // Example user information\n",
    "  const user = { id: \"user-5678\", name: \"Jane Doe\", sessionId: \"123\" };\n",
    "\n",
    "  // Add a warning to the active span\n",
    "  updateActiveSpan({\n",
    "    level: \"WARNING\",\n",
    "    statusMessage: \"This is a warning\",\n",
    "  });\n",
    " \n",
    "  // Update the trace with user context\n",
    "  updateActiveTrace({\n",
    "    userId: user.id,\n",
    "    sessionId: user.sessionId,\n",
    "    metadata: { userName: user.name },\n",
    "  });\n",
    " \n",
    "  // Mark the span as complete with final output\n",
    "  span.update({ output: \"Successfully answered.\" });\n",
    "});\n",
    "\n",
    "// Ensure all spans are sent to Langfuse\n",
    "await langfuseSpanProcessor.forceFlush();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c785d37",
   "metadata": {},
   "source": [
    "[Public trace in the Langfuse UI](https://cloud.langfuse.com/project/cloramnkj0002jz088vzn1ja4/traces/7615670e8238b688cc020c70e17db75e?timestamp=2025-08-25T12%3A50%3A42.356Z&display=details)\n",
    "\n",
    "### Option 2: `observe` Decorator\n",
    "\n",
    "The `observe` wrapper is a powerful tool for tracing existing functions without modifying their internal logic. It acts as a decorator that automatically creates a span or generation around the function call. You can use the `updateActiveSpan` and `updateActiveGeneration` functions to add attributes to the observation from within the wrapped function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7c1440a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import { observe, updateActiveGeneration } from \"npm:@langfuse/tracing\";\n",
    " \n",
    "// An existing function\n",
    "async function fetchData(source: string) {\n",
    "  updateActiveGeneration({ usageDetails: {\n",
    "    // usage\n",
    "    input: 10,\n",
    "    output: 5,\n",
    "  }, \n",
    "})\n",
    "  \n",
    "  // ... logic to fetch data\n",
    "  return { data: `some data from ${source}` };\n",
    "}\n",
    " \n",
    "// Wrap the function to trace it\n",
    "const tracedFetchData = observe(fetchData, {\n",
    "  name: \"observe-wrapper\",\n",
    "  asType: \"generation\",\n",
    "});\n",
    " \n",
    "// Now, every time you call tracedFetchData, a span is created.\n",
    "// Its input and output are automatically populated with the\n",
    "// function's arguments and return value.\n",
    "const result = await tracedFetchData(\"API\");\n",
    "\n",
    "await langfuseSpanProcessor.forceFlush();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ecfe3ac",
   "metadata": {},
   "source": [
    "[Public trace in the Langfuse UI](https://cloud.langfuse.com/project/cloramnkj0002jz088vzn1ja4/traces/0c81d220d6f67922020e15b0a9160cfb?timestamp=2025-08-25T12:57:17.337Z&display=details)\n",
    "\n",
    "### Option 3: Manual Spans\n",
    "\n",
    "This part shows how to log any LLM call by passing the model in and outputs via the [Langfuse SDK](https://langfuse.com/docs/sdk/typescript/guide).\n",
    "\n",
    "Steps:\n",
    "1. Create span to contain this section within the trace\n",
    "2. Create generation, log input and model name as it is already known\n",
    "3. Call the LLM SDK and log the output\n",
    "4. End generation and span\n",
    "\n",
    "Teams typically wrap their LLM SDK calls in a helper function that manages tracing internally. This implementation occurs once and is then reused for all LLM calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3be09b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "// Import the startSpan function for manual span creation\n",
    "import { startSpan } from 'npm:@langfuse/tracing';\n",
    "\n",
    "// Create the root span for this operation\n",
    "const span = startSpan('manual-observation', {\n",
    "    input: { query: 'What is the capital of France?' },\n",
    "  });\n",
    "  \n",
    "// Create a child span for a tool call (e.g., weather API)\n",
    "const toolCall = span.startSpan('fetch-weather', { input: { city: 'Paris' } });\n",
    "// Simulate API call with timeout\n",
    "await new Promise((r) => setTimeout(r, 100));\n",
    "// End the tool call with its output\n",
    "toolCall.end({ output: { temperature: '15°C' } });\n",
    "\n",
    "// Create a generation span for the LLM call\n",
    "const generation = span.startGeneration('llm-call', {\n",
    "  model: 'gpt-4',\n",
    "  input: [{ role: 'user', content: 'What is the capital of France?' }],\n",
    "  output: { content: 'The capital of France is Paris.' },\n",
    "});\n",
    "\n",
    "// Update the generation with token usage details\n",
    "generation.update({\n",
    "  usageDetails: {\n",
    "    input: 10,              // Input token count\n",
    "    output: 5,              // Output token count\n",
    "    cache_read_input_tokens: 2,  // Cached tokens used\n",
    "    some_other_token_count: 10,  // Custom metric\n",
    "    total: 17,              // Total tokens (optional)\n",
    "  },\n",
    "});\n",
    "\n",
    "// End the generation with final output\n",
    "generation.end({\n",
    "  output: { content: 'The capital of France is Paris.' },\n",
    "});\n",
    "\n",
    "// End the root span with final status and session ID\n",
    "span.end({ \n",
    "  output: 'Successfully answered user request.', \n",
    "  sessionId: '123' \n",
    "});\n",
    "\n",
    "// Ensure all spans are flushed to Langfuse\n",
    "await langfuseSpanProcessor.forceFlush();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a20beb",
   "metadata": {},
   "source": [
    "[Public trace in the Langfuse UI](https://cloud.langfuse.com/project/cloramnkj0002jz088vzn1ja4/traces/27a5d45afd2eeda4a00828f5761b9256?timestamp=2025-08-25T12%3A49%3A39.750Z&display=details&observation=4687cb0a13949d75)\n",
    "\n",
    "## Native integrations\n",
    "\n",
    "Besides manual creation of spans using the SDK methods (decorator, context manager and manual creation), you can also use the native instrumentations for OpenAI or Langchain to automatically capture all generation details.\n",
    "\n",
    "### Option 1: Using OpenAI\n",
    "\n",
    "This step shows how to trace OpenAI applications using the [OpenAI integration](https://langfuse.com/integrations/model-providers/openai-js) which is interoperable with the Langfuse SDK. \n",
    "\n",
    "Since this is a native integration, the model parameters and outputs are automatically captured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Import required packages\n",
    "import OpenAI from \"npm:openai@^4\";\n",
    "import { observeOpenAI } from \"npm:@langfuse/openai@^4.0.0-alpha\";\n",
    " \n",
    "// Initialize the OpenAI client\n",
    "const openai = new OpenAI();\n",
    " \n",
    "// Wrap the OpenAI client with Langfuse tracing\n",
    "const tracedOpenAI = observeOpenAI(openai, {\n",
    "  // Configure trace-level attributes for all API calls\n",
    "  traceName: \"my-openai-trace\",      // Name for the trace\n",
    "  sessionId: \"user-session-123\",     // Track user session\n",
    "  userId: \"user-abc\",                // Track user identity\n",
    "  tags: [\"openai-integration\"],      // Add searchable tags\n",
    "});\n",
    " \n",
    "// Make an API call using the traced client\n",
    "// All parameters and responses will be automatically captured\n",
    "const completion = await tracedOpenAI.chat.completions.create({\n",
    "  model: \"gpt-4\",\n",
    "  messages: [{ role: \"user\", content: \"What is OpenTelemetry?\" }],\n",
    "});"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4445d304",
   "metadata": {},
   "source": [
    "[Public trace in the Langfuse UI](https://cloud.langfuse.com/project/cloramnkj0002jz088vzn1ja4/traces/0783f6aa102bd454c501d7a5ea7bee7e?timestamp=2025-08-25T12%3A57%3A57.781Z&display=details&observation=6f6eaf883dfce9f7)\n",
    "\n",
    "### Option 2: Using LangChain\n",
    "\n",
    "This step shows how to trace LangChain applications using the [LangChain integration](https://langfuse.com/integrations/frameworks/langchain) which is fully interoperable with the Langfuse SDK.\n",
    "\n",
    "Since this is a native integration, the model parameters and outputs are automatically captured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Import required LangChain and Langfuse packages\n",
    "import { ChatOpenAI } from \"npm:@langchain/openai@^0.3.1\";\n",
    "import { ChatPromptTemplate } from \"npm:@langchain/core/prompts\";\n",
    "import { CallbackHandler } from \"npm:@langfuse/langchain@^4.0.0-alpha\";\n",
    " \n",
    "// Initialize the Langfuse callback handler with tracing configuration\n",
    "const langfuseHandler = new CallbackHandler({\n",
    "  sessionId: \"user-session-123\",  // Track user session\n",
    "  userId: \"user-abc\",            // Track user identity\n",
    "  tags: [\"langchain-test\"],      // Add searchable tags\n",
    "});\n",
    " \n",
    "// Define the LangChain components\n",
    "const model = new ChatOpenAI({ model: \"gpt-4o\" });  // Initialize LLM\n",
    "const prompt = ChatPromptTemplate.fromTemplate(\"Tell me a joke about {topic}.\");  // Create prompt template\n",
    "const chain = prompt.pipe(model);  // Combine prompt and model into a chain\n",
    " \n",
    "// Execute the chain with Langfuse tracing\n",
    "const result = await chain.invoke(\n",
    "  { topic: \"developers\" },  // Input variables for the prompt\n",
    "  {\n",
    "    callbacks: [langfuseHandler],  // Enable Langfuse tracing\n",
    "    runName: \"joke-generator\",     // Name for the trace (if no active span)\n",
    "  }\n",
    ");\n",
    " \n",
    "// Output the result\n",
    "console.log(result.content);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e939f53f",
   "metadata": {},
   "source": [
    "[Public trace in the Langfuse UI](https://cloud.langfuse.com/project/cloramnkj0002jz088vzn1ja4/traces/e411f2f62f05a3db3dcadbed331cc443?timestamp=2025-08-25T12:58:54.900Z&display=details)\n",
    "\n",
    "### Option 3: Vercel AI SDK\n",
    "\n",
    "The Vercel AI SDK offers native instrumentation with OpenTelemetry. To send spans to your Langfuse instance, you need to set `experimental_telemetry: {isEnabled: true}`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e939f53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "// Import Vercel AI SDK components\n",
    "import { generateText } from \"npm:ai\"\n",
    "import { openai } from \"npm:@ai-sdk/openai\"\n",
    "\n",
    "// Generate text with OpenTelemetry tracing enabled\n",
    "const result_3 = await generateText({\n",
    "    model: openai('gpt-4.1'),                // Specify the OpenAI model\n",
    "    prompt: 'Write a short story about a cat.',  // The prompt for generation\n",
    "    experimental_telemetry: {\n",
    "      isEnabled: true,                       // Enable OpenTelemetry tracing\n",
    "      functionId: 'my-awesome-function',     // Identify the function being traced\n",
    "      metadata: {\n",
    "        something: 'custom',                 // Custom metadata fields\n",
    "        someOtherThing: 'other-value',\n",
    "        sessionId: '123',                    // Track user session\n",
    "        userId: '456',                       // Track user identity\n",
    "        tags: ['test', 'langfuse'],         // Add searchable tags\n",
    "      },\n",
    "    },\n",
    "  });"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1dbbd5",
   "metadata": {},
   "source": [
    "[Public trace in the Langfuse UI]([Public trace in the Langfuse UI](https://cloud.langfuse.com/project/cloramnkj0002jz088vzn1ja4/traces/27a5d45afd2eeda4a00828f5761b9256?timestamp=2025-08-25T12%3A49%3A39.750Z&display=details&observation=4687cb0a13949d75))\n",
    "\n",
    "## Step 5: View the Traces in Langfuse\n",
    "\n",
    "After ingesting your spans, you can view them in your Langfuse dashboard."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8185af",
   "metadata": {},
   "source": [
    "![Example trace of the OpenAI generation](https://langfuse.com/images/cookbook/example-js-sdk/example-trace-js-sdk-v4-openai.png)\n",
    "\n",
    "[Example trace](https://cloud.langfuse.com/project/cloramnkj0002jz088vzn1ja4/traces/79e78112fb548a9896f7582809e2dec8?timestamp=2025-08-22T14%3A02%3A47.988Z&display=details&observation=3e7091a6cea18243) in the Langfuse UI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5015c149",
   "metadata": {},
   "source": [
    "## Learn More\n",
    "\n",
    "- [Langfuse JS/TS SDK Documentation](https://langfuse.com/docs/observability/sdk/typescript/overview)\n",
    "- [Langfuse Integrations](https://langfuse.com/integrations)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Deno",
   "language": "typescript",
   "name": "deno"
  },
  "language_info": {
   "codemirror_mode": "typescript",
   "file_extension": ".ts",
   "mimetype": "text/x.typescript",
   "name": "typescript",
   "nbconvert_exporter": "script",
   "pygments_lexer": "typescript",
   "version": "5.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

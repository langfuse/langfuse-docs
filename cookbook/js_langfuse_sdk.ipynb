{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28e400f4",
   "metadata": {},
   "source": [
    "---\n",
    "title: Langfuse JS/TS SDK\n",
    "description: End-to-end example of how to use the Langfuse Open Source Tracing via the low-level Langfuse JS/TS SDK. Includes examples for Anthropic, OpenAI and LangChain.\n",
    "---\n",
    "\n",
    "# Cookbook: Langfuse JS/TS SDK\n",
    "\n",
    "JS/TS applications can either be traced via the [low-level Langfuse JS/TS SDK](https://langfuse.com/docs/sdk/typescript/guide), or by using one of the native integrations such as [OpenAI](https://langfuse.com/integrations/model-providers/openai-js), [LangChain](https://langfuse.com/integrations/frameworks/langchain) or [Vercel AI SDK](https://langfuse.com/integrations/frameworks/vercel-ai-sdk).\n",
    "\n",
    "In this notebook, we will walk you through a **simple end-to-end example** that:\n",
    "\n",
    "- Uses the core features of the Langfuse JS/TS SDK\n",
    "- Shows how to log any LLM call via the low-level SDK\n",
    "- Uses integrations that are interoperable with low-level SDK\n",
    "    - LangChain integration \n",
    "    - OpenAI integration\n",
    "\n",
    "For this guide, we assume that you are already familiar with the Langfuse data model (traces, spans, generations, etc.). If not, please read the [conceptual introduction](https://langfuse.com/docs/tracing) to tracing.\n",
    "\n",
    "[Example trace](https://cloud.langfuse.com/project/cloramnkj0002jz088vzn1ja4/traces/8d580443-519e-4713-9859-eff4a7193f87?timestamp=2024-12-03T17%3A45%3A16.787Z&observation=26ff69ed-8ba8-4bfe-9029-14a179828044&display=details) that we will create in this notebook:\n",
    "\n",
    "![Example trace with the three generations](https://static.langfuse.com/cookbooks/js-sdk-example/js-sdk-example.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849415ec",
   "metadata": {},
   "source": [
    "## Step 1: Setup\n",
    "\n",
    "*Note: This cookbook uses Deno.js, which requires different syntax for importing packages and setting environment variables.*\n",
    "\n",
    "Set your Langfuse API keys, the Langfuse host name and keys for the used LLM providers.\n",
    "\n",
    "See [SDK Guide](https://langfuse.com/docs/sdk/typescript/guide) for more details on how to initialize the SDK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ec3e7874-e6db-44cd-9d55-7ffa72f630fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "// Set env variables, Deno-specific syntax\n",
    "Deno.env.set(\"OPENAI_API_KEY\", \"sk-...\");\n",
    "\n",
    "Deno.env.set(\"ANTHROPIC_API_KEY\", \"sk-...\");\n",
    "\n",
    "Deno.env.set(\"LANGFUSE_SECRET_KEY\", \"sk-...\");\n",
    "Deno.env.set(\"LANGFUSE_PUBLIC_KEY\", \"pk-...\");\n",
    "Deno.env.set(\"LANGFUSE_HOST\", \"https://cloud.langfuse.com\") // For US data region, set this to \"https://us.cloud.langfuse.com\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b0d924",
   "metadata": {},
   "source": [
    "Initialize the Langfuse client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e2b8bb18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import Langfuse from \"npm:langfuse\";\n",
    "\n",
    "// Init Langfuse SDK\n",
    "const langfuse = new Langfuse();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9802f80",
   "metadata": {},
   "source": [
    "## Step 2: Create a Trace\n",
    "\n",
    "Langfuse observability is structured around [traces](https://langfuse.com/docs/tracing#introduction-to-observability--traces-in-langfuse). Each trace can contain multiple observations to log the individual steps of the execution. Observation can be `Events`, the basic building blocks which are used to track discrete events in a trace, `Spans`, representing durations of units of work in a trace,  or `Generations`, used to log model calls. \n",
    "\n",
    "To log an LLM call, we will first create a trace. In this step, we can also assign the trace metadata such as a user id or tags. The tracing documentation includes more details on all trace features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a68812d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "// Creation of a unique trace id.\n",
    "// It is optional, but this makes it easier for us to score the trace (add user feedback, etc.) afterwards. \n",
    "\n",
    "import { v4 as uuidv4 } from \"npm:uuid\";\n",
    "\n",
    "const traceId = uuidv4();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6987413c",
   "metadata": {},
   "outputs": [],
   "source": [
    "// Creation of the trace and assignment of metadata\n",
    "const trace = langfuse.trace({\n",
    "  id: traceId,\n",
    "  name: \"JS-SDK-Trace\",\n",
    "  userId: \"user_123456789\",\n",
    "  metadata: { user: \"user@langfuse.com\" },\n",
    "  tags: [\"production\"],\n",
    "});\n",
    " \n",
    "// Example update, same params as create, cannot change id\n",
    "trace.update({\n",
    "  metadata: {\n",
    "    foo: \"bar\",\n",
    "  },\n",
    "});"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ad59cb",
   "metadata": {},
   "source": [
    "## Step 3: Log LLM Calls\n",
    "\n",
    "You can use the low-level Langfuse SDK to log any LLM call or any of the [integrations](https://langfuse.com/integrations) that are interoperable with it.\n",
    "\n",
    "In the following, we will demonstrate how to log LLM calls using the low-level SDK, LangChain, and OpenAI integrations.\n",
    "\n",
    "### Option 1: Log Any LLM with low-level Langfuse SDK\n",
    "\n",
    "This part shows how to log any LLM call by passing the model in and outputs via the [Langfuse SDK](https://langfuse.com/docs/sdk/typescript/guide).\n",
    "\n",
    "Steps:\n",
    "\n",
    "1. Create span to contain this section within the trace\n",
    "2. Create generation, log input and model name as it is already known\n",
    "3. Call the LLM SDK and log the output\n",
    "4. End generation and span\n",
    "\n",
    "Teams typically wrap their LLM SDK calls in a helper function that manages tracing internally. This implementation occurs once and is then reused for all LLM calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "88ae3efe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! How can I help you today?\n"
     ]
    }
   ],
   "source": [
    "import Anthropic from \"npm:@anthropic-ai/sdk\";\n",
    "\n",
    "const msg = \"Hello, Claude\";\n",
    "\n",
    "// 1. Create wrapper span\n",
    "const span_name = \"Anthropic-Span\";\n",
    "const span = trace.span({ name: span_name });\n",
    "\n",
    "// 2. Create generation, log input and model name as it is already known\n",
    "const generation = span.generation({\n",
    "  name: \"anthropic-generation01\",\n",
    "  model: \"claude-3-5-sonnet-20241022\",\n",
    "  input: msg,\n",
    "});\n",
    " \n",
    "// 3. Call the LLM SDK and log the output\n",
    "const anthropic = new Anthropic({ apiKey: Deno.env.get(\"ANTHROPIC_API_KEY\") });\n",
    "const chatCompletion = await anthropic.messages.create({\n",
    "  model: \"claude-3-5-sonnet-20241022\",\n",
    "  max_tokens: 1024,\n",
    "  messages: [{ role: \"user\", content: msg }],\n",
    "});\n",
    " \n",
    "// 4. End generation and span\n",
    "// Example end - sets endTime, optionally pass a body\n",
    "generation.end({\n",
    "  output: chatCompletion.content[0].text,\n",
    "  usageDetails: {\n",
    "    input: chatCompletion.usage.input_tokens,\n",
    "    output: chatCompletion.usage.output_tokens,\n",
    "  },\n",
    "});\n",
    "// End span to get span-level latencies\n",
    "span.end();\n",
    "\n",
    "console.log(chatCompletion.content[0].text);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44200d4",
   "metadata": {},
   "source": [
    "### Option 2: Using LangChain\n",
    "\n",
    "This step shows how to trace LangChain applications using the [LangChain integration](https://langfuse.com/integrations/frameworks/langchain) which is fully interoperable with the Langfuse SDK.\n",
    "\n",
    "Since this is a native integration, the model parameters and outputs are automatically captured.\n",
    "\n",
    "Steps:\n",
    "\n",
    "1. Create wrapper span to contain this section within the trace\n",
    "2. Create LangChain handler scoped to this span by passing `root`\n",
    "3. Pass handler to LangChain to natively capture LangChain traces\n",
    "4. End wrapper span to get span-level latencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "1b0ce7b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the bear break up with his girlfriend?\n",
      "\n",
      "Because she was too grizzly for him!\n"
     ]
    }
   ],
   "source": [
    "import { CallbackHandler } from \"npm:langfuse-langchain\"\n",
    "import { ChatOpenAI } from \"npm:@langchain/openai\"\n",
    "import { PromptTemplate } from \"npm:@langchain/core/prompts\"\n",
    "import { RunnableSequence } from \"npm:@langchain/core/runnables\";\n",
    "\n",
    "// 1. Create wrapper span\n",
    "const span_name = \"Langchain-Span\";\n",
    "const span = trace.span({ name: span_name });\n",
    "\n",
    "// 2. Create Langchain handler scoped to this span\n",
    "const langfuseLangchainHandler = new CallbackHandler({root: span})\n",
    "\n",
    "// 3. Pass handler to Langchain to natively capture Langchain traces\n",
    "const model = new ChatOpenAI({});\n",
    "const promptTemplate = PromptTemplate.fromTemplate(\n",
    "  \"Tell me a joke about {topic}\"\n",
    ");\n",
    "const chain = RunnableSequence.from([promptTemplate, model]);\n",
    "const res = await chain.invoke(\n",
    "    { topic: \"bears\" },\n",
    "    { callbacks: [langfuseLangchainHandler] } // Pass handler to Langchain\n",
    ");\n",
    "\n",
    "// 4. End wrapper span to get span-level latencies\n",
    "span.end();\n",
    " \n",
    "console.log(res.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e61e01",
   "metadata": {},
   "source": [
    "### Option 3: Using OpenAI\n",
    "\n",
    "This step shows how to trace OpenAI applications using the [OpenAI integration](https://langfuse.com/integrations/model-providers/openai-js) which is interoperable with the Langfuse SDK. \n",
    "\n",
    "Since this is a native integration, the model parameters and outputs are automatically captured.\n",
    "\n",
    "Steps:\n",
    "1. Create wrapper span to contain this section within the trace\n",
    "2. Call OpenAI and pass `parent` to the `observeOpenAI` function\n",
    "3. End wrapper span to get span-level latencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a87971",
   "metadata": {},
   "outputs": [],
   "source": [
    "// Initialize SDKs\n",
    "const openai = new OpenAI();\n",
    " \n",
    "// 1. Create wrapper span\n",
    "const span_name = \"OpenAI-Span\";\n",
    "const span = trace.span({ name: span_name });\n",
    " \n",
    "// 2. Call OpenAI and pass `parent` to the `observeOpenAI` function to nest the generation within the span\n",
    "const joke = (\n",
    "  await observeOpenAI(openai, {\n",
    "    parent: span,\n",
    "    generationName: \"OpenAI-Generation\",\n",
    "  }).chat.completions.create({\n",
    "    model: \"gpt-4o\",\n",
    "    messages: [\n",
    "      { role: \"system\", content: \"Tell me a joke.\" },\n",
    "    ],\n",
    "  })\n",
    ").choices[0].message.content;\n",
    " \n",
    "// 3. End wrapper span to get span-level latencies\n",
    "span.end();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e44ef4",
   "metadata": {},
   "source": [
    "## Step 4: Score the Trace (Optional)\n",
    "\n",
    "After logging the trace, we can add [scores](https://langfuse.com/docs/scores/custom) to it. This can help in evaluating the quality of the interaction. Scores can be any metric that is important to your application. In this example, we are scoring the trace based on user feedback.\n",
    "\n",
    "Since the scoring usually happens after the generation is complete, we use the user-defined trace id to score the trace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d8220d",
   "metadata": {},
   "outputs": [],
   "source": [
    "langfuse.score({\n",
    "  id: traceId,\n",
    "  name: \"user-feedback\",\n",
    "  value: 3,\n",
    "  comment: \"This was a good interaction\",\n",
    "});"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1dbbd5",
   "metadata": {},
   "source": [
    "## Step 5: View the Trace in Langfuse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8185af",
   "metadata": {},
   "source": [
    "[Example trace](https://cloud.langfuse.com/project/cloramnkj0002jz088vzn1ja4/traces/8d580443-519e-4713-9859-eff4a7193f87?timestamp=2024-12-03T17%3A45%3A16.787Z&observation=26ff69ed-8ba8-4bfe-9029-14a179828044&display=details) in the Langfuse UI.\n",
    "\n",
    "![Example trace with the three generations](https://static.langfuse.com/cookbooks/js-sdk-example/js-sdk-example.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5015c149",
   "metadata": {},
   "source": [
    "## Learn More\n",
    "\n",
    "- [Langfuse JS/TS SDK Guide](https://langfuse.com/docs/sdk/typescript/guide)\n",
    "- [Langfuse Integrations](https://langfuse.com/integrations)\n",
    "- [Support](/support)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Deno",
   "language": "typescript",
   "name": "deno"
  },
  "language_info": {
   "codemirror_mode": "typescript",
   "file_extension": ".ts",
   "mimetype": "text/x.typescript",
   "name": "typescript",
   "nbconvert_exporter": "script",
   "pygments_lexer": "typescript",
   "version": "5.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

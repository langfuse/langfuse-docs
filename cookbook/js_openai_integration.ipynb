{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7cf83d5-7a20-4ae1-a5be-4a8f276a213d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why couldn't the bicycle stand up by itself? Because it was two-tired!\n"
     ]
    }
   ],
   "source": [
    "import OpenAI from \"npm:openai\";\n",
    "import { observeOpenAI } from \"npm:langfuse\";\n",
    "\n",
    "Deno.env.set(\"OPENAI_API_KEY\", \"\");\n",
    "Deno.env.set(\"LANGFUSE_PUBLIC_KEY\", \"\");\n",
    "Deno.env.set(\"LANGFUSE_SECRET_KEY\", \"\");\n",
    "\n",
    "\n",
    "const openai = observeOpenAI(new OpenAI(), { generationName: \"OpenAI.Chat.Trace\", tags: [\"simple\"]} );\n",
    "\n",
    "// simple\n",
    "\n",
    "const res = await openai.chat.completions.create({\n",
    "  model: 'gpt-3.5-turbo',\n",
    "  messages: [{ role: \"system\", content: \"Tell me a bad joke.\" }],\n",
    "  max_tokens: 100,\n",
    "});\n",
    "\n",
    "console.log(res.choices[0]?.message.content);\n",
    "\n",
    "// grouping into single trace\n",
    "\n",
    "// update trace\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80b727de-67bf-44c5-93bf-2a9c562794ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Why\n",
      " did\n",
      " the\n",
      " scare\n",
      "crow\n",
      " win\n",
      " an\n",
      " award\n",
      "?\n",
      "\n",
      "\n",
      "Because\n",
      " he\n",
      " was\n",
      " outstanding\n",
      " in\n",
      " his\n",
      " field\n",
      "!\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[\n",
       "  {\n",
       "    id: \u001b[32m\"da606dbe-e421-444c-882d-bf60346bc388\"\u001b[39m,\n",
       "    type: \u001b[32m\"trace-create\"\u001b[39m,\n",
       "    timestamp: \u001b[32m\"2024-04-16T16:38:43.543Z\"\u001b[39m,\n",
       "    body: {\n",
       "      id: \u001b[32m\"73d17fbd-b4f5-4a14-84c0-8fd34ca7d6ad\"\u001b[39m,\n",
       "      release: \u001b[90mundefined\u001b[39m,\n",
       "      generationName: \u001b[32m\"OpenAI.Chat.Trace\"\u001b[39m,\n",
       "      tags: [ \u001b[32m\"simple\"\u001b[39m ],\n",
       "      model: \u001b[32m\"gpt-3.5-turbo\"\u001b[39m,\n",
       "      input: { messages: [ \u001b[36m[Object]\u001b[39m ] },\n",
       "      modelParameters: {\n",
       "        frequency_penalty: \u001b[90mundefined\u001b[39m,\n",
       "        logit_bias: \u001b[90mundefined\u001b[39m,\n",
       "        logprobs: \u001b[90mundefined\u001b[39m,\n",
       "        max_tokens: \u001b[33m100\u001b[39m,\n",
       "        n: \u001b[90mundefined\u001b[39m,\n",
       "        presence_penalty: \u001b[90mundefined\u001b[39m,\n",
       "        seed: \u001b[90mundefined\u001b[39m,\n",
       "        stop: \u001b[90mundefined\u001b[39m,\n",
       "        stream: \u001b[90mundefined\u001b[39m,\n",
       "        temperature: \u001b[90mundefined\u001b[39m,\n",
       "        top_p: \u001b[90mundefined\u001b[39m,\n",
       "        user: \u001b[90mundefined\u001b[39m,\n",
       "        response_format: \u001b[90mundefined\u001b[39m,\n",
       "        top_logprobs: \u001b[90mundefined\u001b[39m\n",
       "      },\n",
       "      name: \u001b[32m\"OpenAI.Chat.Trace\"\u001b[39m,\n",
       "      startTime: \u001b[35m2024-04-16T16:38:43.543Z\u001b[39m,\n",
       "      timestamp: \u001b[35m2024-04-16T16:38:43.543Z\u001b[39m\n",
       "    },\n",
       "    metadata: \u001b[90mundefined\u001b[39m\n",
       "  },\n",
       "  {\n",
       "    id: \u001b[32m\"598ed629-6a84-4c76-8b32-95863706b342\"\u001b[39m,\n",
       "    type: \u001b[32m\"generation-create\"\u001b[39m,\n",
       "    timestamp: \u001b[32m\"2024-04-16T16:38:44.284Z\"\u001b[39m,\n",
       "    body: {\n",
       "      id: \u001b[32m\"0474f6e7-86db-426e-906f-0ece8933f4a3\"\u001b[39m,\n",
       "      startTime: \u001b[35m2024-04-16T16:38:43.543Z\u001b[39m,\n",
       "      model: \u001b[32m\"gpt-3.5-turbo\"\u001b[39m,\n",
       "      input: { messages: [ \u001b[36m[Object]\u001b[39m ] },\n",
       "      modelParameters: {\n",
       "        frequency_penalty: \u001b[90mundefined\u001b[39m,\n",
       "        logit_bias: \u001b[90mundefined\u001b[39m,\n",
       "        logprobs: \u001b[90mundefined\u001b[39m,\n",
       "        max_tokens: \u001b[33m100\u001b[39m,\n",
       "        n: \u001b[90mundefined\u001b[39m,\n",
       "        presence_penalty: \u001b[90mundefined\u001b[39m,\n",
       "        seed: \u001b[90mundefined\u001b[39m,\n",
       "        stop: \u001b[90mundefined\u001b[39m,\n",
       "        stream: \u001b[90mundefined\u001b[39m,\n",
       "        temperature: \u001b[90mundefined\u001b[39m,\n",
       "        top_p: \u001b[90mundefined\u001b[39m,\n",
       "        user: \u001b[90mundefined\u001b[39m,\n",
       "        response_format: \u001b[90mundefined\u001b[39m,\n",
       "        top_logprobs: \u001b[90mundefined\u001b[39m\n",
       "      },\n",
       "      name: \u001b[32m\"OpenAI.Chat.Trace\"\u001b[39m,\n",
       "      output: {\n",
       "        role: \u001b[32m\"assistant\"\u001b[39m,\n",
       "        content: \u001b[32m\"Why couldn't the bicycle stand up by itself? Because it was two-tired!\"\u001b[39m\n",
       "      },\n",
       "      endTime: \u001b[35m2024-04-16T16:38:44.283Z\u001b[39m,\n",
       "      usage: { promptTokens: \u001b[33m13\u001b[39m, completionTokens: \u001b[33m17\u001b[39m, totalTokens: \u001b[33m30\u001b[39m },\n",
       "      traceId: \u001b[32m\"73d17fbd-b4f5-4a14-84c0-8fd34ca7d6ad\"\u001b[39m,\n",
       "      parentObservationId: \u001b[1mnull\u001b[22m\n",
       "    },\n",
       "    metadata: \u001b[90mundefined\u001b[39m\n",
       "  },\n",
       "  {\n",
       "    id: \u001b[32m\"913c54ad-7159-41bb-8c8e-100db05f0bf2\"\u001b[39m,\n",
       "    type: \u001b[32m\"trace-create\"\u001b[39m,\n",
       "    timestamp: \u001b[32m\"2024-04-16T16:38:44.284Z\"\u001b[39m,\n",
       "    body: {\n",
       "      id: \u001b[32m\"73d17fbd-b4f5-4a14-84c0-8fd34ca7d6ad\"\u001b[39m,\n",
       "      release: \u001b[90mundefined\u001b[39m,\n",
       "      output: {\n",
       "        role: \u001b[32m\"assistant\"\u001b[39m,\n",
       "        content: \u001b[32m\"Why couldn't the bicycle stand up by itself? Because it was two-tired!\"\u001b[39m\n",
       "      }\n",
       "    },\n",
       "    metadata: \u001b[90mundefined\u001b[39m\n",
       "  },\n",
       "  {\n",
       "    id: \u001b[32m\"e2abffd0-84be-4657-bb40-3dff6d359645\"\u001b[39m,\n",
       "    type: \u001b[32m\"trace-create\"\u001b[39m,\n",
       "    timestamp: \u001b[32m\"2024-04-16T16:38:44.297Z\"\u001b[39m,\n",
       "    body: {\n",
       "      id: \u001b[32m\"adbc3808-cd87-45a3-a8f4-f8df3e043ec7\"\u001b[39m,\n",
       "      release: \u001b[90mundefined\u001b[39m,\n",
       "      generationName: \u001b[32m\"OpenAI.Stream.Trace\"\u001b[39m,\n",
       "      tags: [ \u001b[32m\"stream\"\u001b[39m ],\n",
       "      model: \u001b[32m\"gpt-3.5-turbo\"\u001b[39m,\n",
       "      input: { messages: [ \u001b[36m[Object]\u001b[39m ] },\n",
       "      modelParameters: {\n",
       "        frequency_penalty: \u001b[90mundefined\u001b[39m,\n",
       "        logit_bias: \u001b[90mundefined\u001b[39m,\n",
       "        logprobs: \u001b[90mundefined\u001b[39m,\n",
       "        max_tokens: \u001b[90mundefined\u001b[39m,\n",
       "        n: \u001b[90mundefined\u001b[39m,\n",
       "        presence_penalty: \u001b[90mundefined\u001b[39m,\n",
       "        seed: \u001b[90mundefined\u001b[39m,\n",
       "        stop: \u001b[90mundefined\u001b[39m,\n",
       "        stream: \u001b[33mtrue\u001b[39m,\n",
       "        temperature: \u001b[90mundefined\u001b[39m,\n",
       "        top_p: \u001b[90mundefined\u001b[39m,\n",
       "        user: \u001b[90mundefined\u001b[39m,\n",
       "        response_format: \u001b[90mundefined\u001b[39m,\n",
       "        top_logprobs: \u001b[90mundefined\u001b[39m\n",
       "      },\n",
       "      name: \u001b[32m\"OpenAI.Stream.Trace\"\u001b[39m,\n",
       "      startTime: \u001b[35m2024-04-16T16:38:44.297Z\u001b[39m,\n",
       "      timestamp: \u001b[35m2024-04-16T16:38:44.297Z\u001b[39m\n",
       "    },\n",
       "    metadata: \u001b[90mundefined\u001b[39m\n",
       "  },\n",
       "  {\n",
       "    id: \u001b[32m\"c6efa1aa-550e-40ad-bc20-3fe847e726fe\"\u001b[39m,\n",
       "    type: \u001b[32m\"generation-create\"\u001b[39m,\n",
       "    timestamp: \u001b[32m\"2024-04-16T16:38:44.808Z\"\u001b[39m,\n",
       "    body: {\n",
       "      id: \u001b[32m\"54b076cd-d4a9-4954-b547-42cd2d0cf3e0\"\u001b[39m,\n",
       "      startTime: \u001b[35m2024-04-16T16:38:44.297Z\u001b[39m,\n",
       "      model: \u001b[32m\"gpt-3.5-turbo\"\u001b[39m,\n",
       "      input: { messages: [ \u001b[36m[Object]\u001b[39m ] },\n",
       "      modelParameters: {\n",
       "        frequency_penalty: \u001b[90mundefined\u001b[39m,\n",
       "        logit_bias: \u001b[90mundefined\u001b[39m,\n",
       "        logprobs: \u001b[90mundefined\u001b[39m,\n",
       "        max_tokens: \u001b[90mundefined\u001b[39m,\n",
       "        n: \u001b[90mundefined\u001b[39m,\n",
       "        presence_penalty: \u001b[90mundefined\u001b[39m,\n",
       "        seed: \u001b[90mundefined\u001b[39m,\n",
       "        stop: \u001b[90mundefined\u001b[39m,\n",
       "        stream: \u001b[33mtrue\u001b[39m,\n",
       "        temperature: \u001b[90mundefined\u001b[39m,\n",
       "        top_p: \u001b[90mundefined\u001b[39m,\n",
       "        user: \u001b[90mundefined\u001b[39m,\n",
       "        response_format: \u001b[90mundefined\u001b[39m,\n",
       "        top_logprobs: \u001b[90mundefined\u001b[39m\n",
       "      },\n",
       "      name: \u001b[32m\"OpenAI.Stream.Trace\"\u001b[39m,\n",
       "      output: \u001b[32m\"Why did the scarecrow win an award?\\n\"\u001b[39m +\n",
       "        \u001b[32m\"\\n\"\u001b[39m +\n",
       "        \u001b[32m\"Because he was outstanding in his field!\"\u001b[39m,\n",
       "      endTime: \u001b[35m2024-04-16T16:38:44.808Z\u001b[39m,\n",
       "      completionStartTime: \u001b[35m2024-04-16T16:38:44.609Z\u001b[39m,\n",
       "      traceId: \u001b[32m\"adbc3808-cd87-45a3-a8f4-f8df3e043ec7\"\u001b[39m,\n",
       "      parentObservationId: \u001b[1mnull\u001b[22m\n",
       "    },\n",
       "    metadata: \u001b[90mundefined\u001b[39m\n",
       "  },\n",
       "  {\n",
       "    id: \u001b[32m\"662b9b85-46cd-48c6-9dfe-40b4000b6938\"\u001b[39m,\n",
       "    type: \u001b[32m\"trace-create\"\u001b[39m,\n",
       "    timestamp: \u001b[32m\"2024-04-16T16:38:44.809Z\"\u001b[39m,\n",
       "    body: {\n",
       "      id: \u001b[32m\"adbc3808-cd87-45a3-a8f4-f8df3e043ec7\"\u001b[39m,\n",
       "      release: \u001b[90mundefined\u001b[39m,\n",
       "      output: \u001b[32m\"Why did the scarecrow win an award?\\n\"\u001b[39m +\n",
       "        \u001b[32m\"\\n\"\u001b[39m +\n",
       "        \u001b[32m\"Because he was outstanding in his field!\"\u001b[39m\n",
       "    },\n",
       "    metadata: \u001b[90mundefined\u001b[39m\n",
       "  }\n",
       "]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// streaming\n",
    "\n",
    "const openaiWithLangfuse = observeOpenAI(new OpenAI(), { generationName: \"OpenAI.Stream.Trace\", tags: [\"stream\"]} )\n",
    "const stream = await openaiWithLangfuse.chat.completions.create({\n",
    "  model: 'gpt-3.5-turbo',\n",
    "  messages: [{ role: \"system\", content: \"Tell me a funny joke.\" }],\n",
    "  stream: true,\n",
    "});\n",
    "\n",
    "for await (const chunk of stream) {\n",
    "    const content = chunk.choices[0]?.delta?.content || '';\n",
    "    console.log(content);\n",
    "  }\n",
    "\n",
    "await openaiWithLangfuse.flushAsync();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f47eaa4f-e30d-40d4-a264-15796f3fd74c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\n",
       "  {\n",
       "    id: \u001b[32m\"c36ae10d-1bea-4a13-9d02-a07b644a42bb\"\u001b[39m,\n",
       "    type: \u001b[32m\"trace-create\"\u001b[39m,\n",
       "    timestamp: \u001b[32m\"2024-04-16T16:38:45.282Z\"\u001b[39m,\n",
       "    body: {\n",
       "      id: \u001b[32m\"88ecc5ea-43c9-48c6-8fc5-11273596d652\"\u001b[39m,\n",
       "      release: \u001b[90mundefined\u001b[39m,\n",
       "      generationName: \u001b[32m\"OpenAI.Function.Trace\"\u001b[39m,\n",
       "      tags: [ \u001b[32m\"function\"\u001b[39m ],\n",
       "      model: \u001b[32m\"gpt-3.5-turbo\"\u001b[39m,\n",
       "      input: {\n",
       "        messages: [ \u001b[36m[Object]\u001b[39m ],\n",
       "        tools: [ \u001b[36m[Object]\u001b[39m ],\n",
       "        tool_choice: \u001b[32m\"auto\"\u001b[39m\n",
       "      },\n",
       "      modelParameters: {\n",
       "        frequency_penalty: \u001b[90mundefined\u001b[39m,\n",
       "        logit_bias: \u001b[90mundefined\u001b[39m,\n",
       "        logprobs: \u001b[90mundefined\u001b[39m,\n",
       "        max_tokens: \u001b[90mundefined\u001b[39m,\n",
       "        n: \u001b[90mundefined\u001b[39m,\n",
       "        presence_penalty: \u001b[90mundefined\u001b[39m,\n",
       "        seed: \u001b[90mundefined\u001b[39m,\n",
       "        stop: \u001b[90mundefined\u001b[39m,\n",
       "        stream: \u001b[90mundefined\u001b[39m,\n",
       "        temperature: \u001b[90mundefined\u001b[39m,\n",
       "        top_p: \u001b[90mundefined\u001b[39m,\n",
       "        user: \u001b[90mundefined\u001b[39m,\n",
       "        response_format: \u001b[90mundefined\u001b[39m,\n",
       "        top_logprobs: \u001b[90mundefined\u001b[39m\n",
       "      },\n",
       "      name: \u001b[32m\"OpenAI.Function.Trace\"\u001b[39m,\n",
       "      startTime: \u001b[35m2024-04-16T16:38:45.282Z\u001b[39m,\n",
       "      timestamp: \u001b[35m2024-04-16T16:38:45.282Z\u001b[39m\n",
       "    },\n",
       "    metadata: \u001b[90mundefined\u001b[39m\n",
       "  }\n",
       "]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// function calling\n",
    "\n",
    "const openaiWithLangfuse = observeOpenAI(new OpenAI(), { generationName: \"OpenAI.Function.Trace\", tags: [\"function\"]} )\n",
    "\n",
    "async function getWeather(location: string) {\n",
    "  if (location === \"Berlin\") {return \"20degC\"} else {return \"unknown\"}\n",
    "}\n",
    "\n",
    "const functions = [{\n",
    "            type: \"function\",\n",
    "            function: {\n",
    "              name: \"getWeather\",\n",
    "              description: \"Get the current weather in a given location\",\n",
    "              parameters: {\n",
    "                type: \"object\",\n",
    "                properties: {\n",
    "                  location: {\n",
    "                    type: \"string\",\n",
    "                    description: \"The city, e.g. San Francisco\",\n",
    "                  },\n",
    "                },\n",
    "                required: [\"location\"],\n",
    "              },\n",
    "            },\n",
    "          }]\n",
    "\n",
    "// Main function to execute the OpenAI chat completions and auxiliary functions\n",
    "async function main() {\n",
    "      const messages =  [{\n",
    "              role: 'user',\n",
    "              content:\n",
    "                \"What's the weather like in Berlin today\",\n",
    "            },\n",
    "          ]\n",
    "      const res = await openaiWithLangfuse.chat.completions.create({\n",
    "          model: 'gpt-3.5-turbo',\n",
    "          messages: messages,\n",
    "          tool_choice: \"auto\",\n",
    "          tools: functions,\n",
    "        })\n",
    "        const content = res.choices[0].message.content;\n",
    "        const tool_call = res.choices[0].message.tool_calls;\n",
    "        if (tool_call[0].function.name === \"getWeather\") {\n",
    "            const argsStr = tool_call[0].function.arguments;\n",
    "            const args = JSON.parse(argsStr); \n",
    "            const answer = await getWeather(args[\"location\"]);\n",
    "        }\n",
    "}\n",
    "\n",
    "main();\n",
    "await openaiWithLangfuse.flushAsync();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6023dc91-7347-4d98-bf23-03b6da29de53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\n",
       "  {\n",
       "    id: \u001b[32m\"09de8052-6fe2-4ad8-afea-bc49a968fe6e\"\u001b[39m,\n",
       "    type: \u001b[32m\"trace-create\"\u001b[39m,\n",
       "    timestamp: \u001b[32m\"2024-04-16T16:38:45.464Z\"\u001b[39m,\n",
       "    body: {\n",
       "      id: \u001b[32m\"df08c671-d694-4345-ad5c-1b74316127e6\"\u001b[39m,\n",
       "      release: \u001b[90mundefined\u001b[39m,\n",
       "      name: \u001b[32m\"capital-poem-generator\"\u001b[39m\n",
       "    },\n",
       "    metadata: \u001b[90mundefined\u001b[39m\n",
       "  },\n",
       "  {\n",
       "    id: \u001b[32m\"b4deb7e1-92ac-4f98-bc84-26b5e4ab2058\"\u001b[39m,\n",
       "    type: \u001b[32m\"span-create\"\u001b[39m,\n",
       "    timestamp: \u001b[32m\"2024-04-16T16:38:45.465Z\"\u001b[39m,\n",
       "    body: {\n",
       "      id: \u001b[32m\"581aa84d-0ee3-47b4-b78d-6fecabd7206c\"\u001b[39m,\n",
       "      startTime: \u001b[35m2024-04-16T16:38:45.465Z\u001b[39m,\n",
       "      name: \u001b[32m\"Germany\"\u001b[39m,\n",
       "      traceId: \u001b[32m\"df08c671-d694-4345-ad5c-1b74316127e6\"\u001b[39m,\n",
       "      parentObservationId: \u001b[1mnull\u001b[22m\n",
       "    },\n",
       "    metadata: \u001b[90mundefined\u001b[39m\n",
       "  },\n",
       "  {\n",
       "    id: \u001b[32m\"0e5bd7fb-e794-428b-9a30-87f96e340caa\"\u001b[39m,\n",
       "    type: \u001b[32m\"generation-create\"\u001b[39m,\n",
       "    timestamp: \u001b[32m\"2024-04-16T16:38:45.937Z\"\u001b[39m,\n",
       "    body: {\n",
       "      id: \u001b[32m\"5656aeb5-9a1b-4c9f-bc56-ee2115abc855\"\u001b[39m,\n",
       "      startTime: \u001b[35m2024-04-16T16:38:45.465Z\u001b[39m,\n",
       "      parent: LangfuseSpanClient {\n",
       "        client: Langfuse {\n",
       "          debugMode: \u001b[33mfalse\u001b[39m,\n",
       "          pendingPromises: {},\n",
       "          _events: \u001b[36m[SimpleEventEmitter]\u001b[39m,\n",
       "          publicKey: \u001b[32m\"pk-lf-bbac702c-5fab-4660-b0eb-d326045051da\"\u001b[39m,\n",
       "          secretKey: \u001b[32m\"sk-lf-b115b6b1-d0c6-49f7-ad0a-28b545a53c91\"\u001b[39m,\n",
       "          baseUrl: \u001b[32m\"https://cloud.langfuse.com\"\u001b[39m,\n",
       "          flushAt: \u001b[33m15\u001b[39m,\n",
       "          flushInterval: \u001b[33m10000\u001b[39m,\n",
       "          release: \u001b[90mundefined\u001b[39m,\n",
       "          _retryOptions: \u001b[36m[Object]\u001b[39m,\n",
       "          requestTimeout: \u001b[33m10000\u001b[39m,\n",
       "          sdkIntegration: \u001b[32m\"DEFAULT\"\u001b[39m,\n",
       "          _promptCache: \u001b[36m[LangfusePromptCache]\u001b[39m,\n",
       "          _storageKey: \u001b[32m\"lf_pk-lf-bbac702c-5fab-4660-b0eb-d326045051da_langfuse\"\u001b[39m,\n",
       "          _storage: \u001b[36m[Object]\u001b[39m,\n",
       "          _storageCache: \u001b[36m[Object]\u001b[39m,\n",
       "          _flushTimer: \u001b[1mnull\u001b[22m\n",
       "        },\n",
       "        id: \u001b[32m\"581aa84d-0ee3-47b4-b78d-6fecabd7206c\"\u001b[39m,\n",
       "        traceId: \u001b[32m\"df08c671-d694-4345-ad5c-1b74316127e6\"\u001b[39m,\n",
       "        observationId: \u001b[32m\"581aa84d-0ee3-47b4-b78d-6fecabd7206c\"\u001b[39m\n",
       "      },\n",
       "      generationName: \u001b[32m\"get-capital\"\u001b[39m,\n",
       "      tag: [ \u001b[32m\"grouped\"\u001b[39m ],\n",
       "      model: \u001b[32m\"gpt-3.5-turbo\"\u001b[39m,\n",
       "      input: { messages: [ \u001b[36m[Object]\u001b[39m, \u001b[36m[Object]\u001b[39m ] },\n",
       "      modelParameters: {\n",
       "        frequency_penalty: \u001b[90mundefined\u001b[39m,\n",
       "        logit_bias: \u001b[90mundefined\u001b[39m,\n",
       "        logprobs: \u001b[90mundefined\u001b[39m,\n",
       "        max_tokens: \u001b[90mundefined\u001b[39m,\n",
       "        n: \u001b[90mundefined\u001b[39m,\n",
       "        presence_penalty: \u001b[90mundefined\u001b[39m,\n",
       "        seed: \u001b[90mundefined\u001b[39m,\n",
       "        stop: \u001b[90mundefined\u001b[39m,\n",
       "        stream: \u001b[90mundefined\u001b[39m,\n",
       "        temperature: \u001b[90mundefined\u001b[39m,\n",
       "        top_p: \u001b[90mundefined\u001b[39m,\n",
       "        user: \u001b[90mundefined\u001b[39m,\n",
       "        response_format: \u001b[90mundefined\u001b[39m,\n",
       "        top_logprobs: \u001b[90mundefined\u001b[39m\n",
       "      },\n",
       "      name: \u001b[32m\"get-capital\"\u001b[39m,\n",
       "      output: {\n",
       "        role: \u001b[32m\"assistant\"\u001b[39m,\n",
       "        content: \u001b[32m\"The capital of Germany is Berlin.\"\u001b[39m\n",
       "      },\n",
       "      endTime: \u001b[35m2024-04-16T16:38:45.937Z\u001b[39m,\n",
       "      usage: { promptTokens: \u001b[33m20\u001b[39m, completionTokens: \u001b[33m7\u001b[39m, totalTokens: \u001b[33m27\u001b[39m },\n",
       "      traceId: \u001b[32m\"df08c671-d694-4345-ad5c-1b74316127e6\"\u001b[39m,\n",
       "      parentObservationId: \u001b[32m\"581aa84d-0ee3-47b4-b78d-6fecabd7206c\"\u001b[39m\n",
       "    },\n",
       "    metadata: \u001b[90mundefined\u001b[39m\n",
       "  },\n",
       "  {\n",
       "    id: \u001b[32m\"b6a5228f-d281-4d1d-af8f-edcee0ae3da2\"\u001b[39m,\n",
       "    type: \u001b[32m\"generation-create\"\u001b[39m,\n",
       "    timestamp: \u001b[32m\"2024-04-16T16:38:48.380Z\"\u001b[39m,\n",
       "    body: {\n",
       "      id: \u001b[32m\"648d3210-9bc8-4036-90b9-e965940f24bc\"\u001b[39m,\n",
       "      startTime: \u001b[35m2024-04-16T16:38:45.938Z\u001b[39m,\n",
       "      parent: LangfuseSpanClient {\n",
       "        client: Langfuse {\n",
       "          debugMode: \u001b[33mfalse\u001b[39m,\n",
       "          pendingPromises: {},\n",
       "          _events: \u001b[36m[SimpleEventEmitter]\u001b[39m,\n",
       "          publicKey: \u001b[32m\"pk-lf-bbac702c-5fab-4660-b0eb-d326045051da\"\u001b[39m,\n",
       "          secretKey: \u001b[32m\"sk-lf-b115b6b1-d0c6-49f7-ad0a-28b545a53c91\"\u001b[39m,\n",
       "          baseUrl: \u001b[32m\"https://cloud.langfuse.com\"\u001b[39m,\n",
       "          flushAt: \u001b[33m15\u001b[39m,\n",
       "          flushInterval: \u001b[33m10000\u001b[39m,\n",
       "          release: \u001b[90mundefined\u001b[39m,\n",
       "          _retryOptions: \u001b[36m[Object]\u001b[39m,\n",
       "          requestTimeout: \u001b[33m10000\u001b[39m,\n",
       "          sdkIntegration: \u001b[32m\"DEFAULT\"\u001b[39m,\n",
       "          _promptCache: \u001b[36m[LangfusePromptCache]\u001b[39m,\n",
       "          _storageKey: \u001b[32m\"lf_pk-lf-bbac702c-5fab-4660-b0eb-d326045051da_langfuse\"\u001b[39m,\n",
       "          _storage: \u001b[36m[Object]\u001b[39m,\n",
       "          _storageCache: \u001b[36m[Object]\u001b[39m,\n",
       "          _flushTimer: \u001b[1mnull\u001b[22m\n",
       "        },\n",
       "        id: \u001b[32m\"581aa84d-0ee3-47b4-b78d-6fecabd7206c\"\u001b[39m,\n",
       "        traceId: \u001b[32m\"df08c671-d694-4345-ad5c-1b74316127e6\"\u001b[39m,\n",
       "        observationId: \u001b[32m\"581aa84d-0ee3-47b4-b78d-6fecabd7206c\"\u001b[39m\n",
       "      },\n",
       "      generationName: \u001b[32m\"generate-poem\"\u001b[39m,\n",
       "      model: \u001b[32m\"gpt-3.5-turbo\"\u001b[39m,\n",
       "      input: { messages: [ \u001b[36m[Object]\u001b[39m, \u001b[36m[Object]\u001b[39m ] },\n",
       "      modelParameters: {\n",
       "        frequency_penalty: \u001b[90mundefined\u001b[39m,\n",
       "        logit_bias: \u001b[90mundefined\u001b[39m,\n",
       "        logprobs: \u001b[90mundefined\u001b[39m,\n",
       "        max_tokens: \u001b[90mundefined\u001b[39m,\n",
       "        n: \u001b[90mundefined\u001b[39m,\n",
       "        presence_penalty: \u001b[90mundefined\u001b[39m,\n",
       "        seed: \u001b[90mundefined\u001b[39m,\n",
       "        stop: \u001b[90mundefined\u001b[39m,\n",
       "        stream: \u001b[90mundefined\u001b[39m,\n",
       "        temperature: \u001b[90mundefined\u001b[39m,\n",
       "        top_p: \u001b[90mundefined\u001b[39m,\n",
       "        user: \u001b[90mundefined\u001b[39m,\n",
       "        response_format: \u001b[90mundefined\u001b[39m,\n",
       "        top_logprobs: \u001b[90mundefined\u001b[39m\n",
       "      },\n",
       "      name: \u001b[32m\"generate-poem\"\u001b[39m,\n",
       "      output: {\n",
       "        role: \u001b[32m\"assistant\"\u001b[39m,\n",
       "        content: \u001b[32m\"In Berlin's heart beats history untold,\\n\"\u001b[39m +\n",
       "          \u001b[32m\"Where tales of power and struggle unfold.\\n\"\u001b[39m +\n",
       "          \u001b[32m\"A city scarred by \"\u001b[39m... 541 more characters\n",
       "      },\n",
       "      endTime: \u001b[35m2024-04-16T16:38:48.380Z\u001b[39m,\n",
       "      usage: { promptTokens: \u001b[33m30\u001b[39m, completionTokens: \u001b[33m143\u001b[39m, totalTokens: \u001b[33m173\u001b[39m },\n",
       "      traceId: \u001b[32m\"df08c671-d694-4345-ad5c-1b74316127e6\"\u001b[39m,\n",
       "      parentObservationId: \u001b[32m\"581aa84d-0ee3-47b4-b78d-6fecabd7206c\"\u001b[39m\n",
       "    },\n",
       "    metadata: \u001b[90mundefined\u001b[39m\n",
       "  },\n",
       "  {\n",
       "    id: \u001b[32m\"b0364b02-fa1e-48a9-b5db-9b9d5718803f\"\u001b[39m,\n",
       "    type: \u001b[32m\"span-update\"\u001b[39m,\n",
       "    timestamp: \u001b[32m\"2024-04-16T16:38:48.380Z\"\u001b[39m,\n",
       "    body: {\n",
       "      id: \u001b[32m\"581aa84d-0ee3-47b4-b78d-6fecabd7206c\"\u001b[39m,\n",
       "      traceId: \u001b[32m\"df08c671-d694-4345-ad5c-1b74316127e6\"\u001b[39m,\n",
       "      endTime: \u001b[35m2024-04-16T16:38:48.380Z\u001b[39m\n",
       "    },\n",
       "    metadata: \u001b[90mundefined\u001b[39m\n",
       "  }\n",
       "]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// grouped\n",
    "import Langfuse from \"npm:langfuse\";\n",
    "\n",
    "\n",
    "const langfuse = new Langfuse();\n",
    "const openai = new OpenAI();\n",
    " \n",
    "// Create trace and add params\n",
    "const trace = langfuse.trace({ name: \"capital-poem-generator\" });\n",
    " \n",
    "// Create span\n",
    "const country = \"Germany\";\n",
    "const span = trace.span({ name: country });\n",
    "\n",
    "const capital = (\n",
    "  await observeOpenAI(openai, {\n",
    "    parent: span,\n",
    "    generationName: \"get-capital\",\n",
    "    tag: [\"grouped\"],\n",
    "  }).chat.completions.create({\n",
    "    model: \"gpt-3.5-turbo\",\n",
    "    messages: [\n",
    "      { role: \"system\", content: \"What is the capital of the country?\" },\n",
    "      { role: \"user\", content: country },\n",
    "    ],\n",
    "  })\n",
    ").choices[0].message.content;\n",
    "\n",
    "const poem = (\n",
    "  await observeOpenAI(openai, {\n",
    "    parent: span,\n",
    "    generationName: \"generate-poem\",\n",
    "  }).chat.completions.create({\n",
    "    model: \"gpt-3.5-turbo\",\n",
    "    messages: [\n",
    "      {\n",
    "        role: \"system\",\n",
    "        content: \"You are a poet. Create a poem about this city.\",\n",
    "      },\n",
    "      { role: \"user\", content: capital },\n",
    "    ],\n",
    "  })\n",
    ").choices[0].message.content;\n",
    "\n",
    "span.end();\n",
    " \n",
    "// Flush the Langfuse client belonging to the parent span\n",
    "await langfuse.flushAsync();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "67d61c52-83c8-4799-990c-73242524f52a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\n",
       "  {\n",
       "    id: \u001b[32m\"780f3812-5595-4508-b667-653a2bb70dbe\"\u001b[39m,\n",
       "    type: \u001b[32m\"trace-create\"\u001b[39m,\n",
       "    timestamp: \u001b[32m\"2024-04-16T16:42:25.612Z\"\u001b[39m,\n",
       "    body: {\n",
       "      id: \u001b[32m\"97e90fab-94d5-4f50-b8ca-f4b63018d4eb\"\u001b[39m,\n",
       "      release: \u001b[90mundefined\u001b[39m,\n",
       "      name: \u001b[32m\"capital-poem-generator\"\u001b[39m\n",
       "    },\n",
       "    metadata: \u001b[90mundefined\u001b[39m\n",
       "  },\n",
       "  {\n",
       "    id: \u001b[32m\"eb715898-fe3e-4813-868d-ff14a0844bc1\"\u001b[39m,\n",
       "    type: \u001b[32m\"span-create\"\u001b[39m,\n",
       "    timestamp: \u001b[32m\"2024-04-16T16:42:25.613Z\"\u001b[39m,\n",
       "    body: {\n",
       "      id: \u001b[32m\"7c885f0b-76f1-45c8-9c53-38d09751c9ae\"\u001b[39m,\n",
       "      startTime: \u001b[35m2024-04-16T16:42:25.613Z\u001b[39m,\n",
       "      name: \u001b[32m\"France\"\u001b[39m,\n",
       "      traceId: \u001b[32m\"97e90fab-94d5-4f50-b8ca-f4b63018d4eb\"\u001b[39m,\n",
       "      parentObservationId: \u001b[1mnull\u001b[22m\n",
       "    },\n",
       "    metadata: \u001b[90mundefined\u001b[39m\n",
       "  },\n",
       "  {\n",
       "    id: \u001b[32m\"f2b7d771-c218-46e4-809d-a26c5787d346\"\u001b[39m,\n",
       "    type: \u001b[32m\"generation-create\"\u001b[39m,\n",
       "    timestamp: \u001b[32m\"2024-04-16T16:42:26.267Z\"\u001b[39m,\n",
       "    body: {\n",
       "      id: \u001b[32m\"68e2bf2a-4126-4cf4-bd80-1fbe05944137\"\u001b[39m,\n",
       "      startTime: \u001b[35m2024-04-16T16:42:25.613Z\u001b[39m,\n",
       "      parent: LangfuseSpanClient {\n",
       "        client: Langfuse {\n",
       "          debugMode: \u001b[33mfalse\u001b[39m,\n",
       "          pendingPromises: {},\n",
       "          _events: \u001b[36m[SimpleEventEmitter]\u001b[39m,\n",
       "          publicKey: \u001b[32m\"pk-lf-bbac702c-5fab-4660-b0eb-d326045051da\"\u001b[39m,\n",
       "          secretKey: \u001b[32m\"sk-lf-b115b6b1-d0c6-49f7-ad0a-28b545a53c91\"\u001b[39m,\n",
       "          baseUrl: \u001b[32m\"https://cloud.langfuse.com\"\u001b[39m,\n",
       "          flushAt: \u001b[33m15\u001b[39m,\n",
       "          flushInterval: \u001b[33m10000\u001b[39m,\n",
       "          release: \u001b[90mundefined\u001b[39m,\n",
       "          _retryOptions: \u001b[36m[Object]\u001b[39m,\n",
       "          requestTimeout: \u001b[33m10000\u001b[39m,\n",
       "          sdkIntegration: \u001b[32m\"DEFAULT\"\u001b[39m,\n",
       "          _promptCache: \u001b[36m[LangfusePromptCache]\u001b[39m,\n",
       "          _storageKey: \u001b[32m\"lf_pk-lf-bbac702c-5fab-4660-b0eb-d326045051da_langfuse\"\u001b[39m,\n",
       "          _storage: \u001b[36m[Object]\u001b[39m,\n",
       "          _storageCache: \u001b[36m[Object]\u001b[39m,\n",
       "          _flushTimer: \u001b[1mnull\u001b[22m\n",
       "        },\n",
       "        id: \u001b[32m\"7c885f0b-76f1-45c8-9c53-38d09751c9ae\"\u001b[39m,\n",
       "        traceId: \u001b[32m\"97e90fab-94d5-4f50-b8ca-f4b63018d4eb\"\u001b[39m,\n",
       "        observationId: \u001b[32m\"7c885f0b-76f1-45c8-9c53-38d09751c9ae\"\u001b[39m\n",
       "      },\n",
       "      generationName: \u001b[32m\"get-capital\"\u001b[39m,\n",
       "      tag: [ \u001b[32m\"update\"\u001b[39m ],\n",
       "      model: \u001b[32m\"gpt-3.5-turbo\"\u001b[39m,\n",
       "      input: { messages: [ \u001b[36m[Object]\u001b[39m, \u001b[36m[Object]\u001b[39m ] },\n",
       "      modelParameters: {\n",
       "        frequency_penalty: \u001b[90mundefined\u001b[39m,\n",
       "        logit_bias: \u001b[90mundefined\u001b[39m,\n",
       "        logprobs: \u001b[90mundefined\u001b[39m,\n",
       "        max_tokens: \u001b[90mundefined\u001b[39m,\n",
       "        n: \u001b[90mundefined\u001b[39m,\n",
       "        presence_penalty: \u001b[90mundefined\u001b[39m,\n",
       "        seed: \u001b[90mundefined\u001b[39m,\n",
       "        stop: \u001b[90mundefined\u001b[39m,\n",
       "        stream: \u001b[90mundefined\u001b[39m,\n",
       "        temperature: \u001b[90mundefined\u001b[39m,\n",
       "        top_p: \u001b[90mundefined\u001b[39m,\n",
       "        user: \u001b[90mundefined\u001b[39m,\n",
       "        response_format: \u001b[90mundefined\u001b[39m,\n",
       "        top_logprobs: \u001b[90mundefined\u001b[39m\n",
       "      },\n",
       "      name: \u001b[32m\"get-capital\"\u001b[39m,\n",
       "      output: { role: \u001b[32m\"assistant\"\u001b[39m, content: \u001b[32m\"The capital of France is Paris.\"\u001b[39m },\n",
       "      endTime: \u001b[35m2024-04-16T16:42:26.266Z\u001b[39m,\n",
       "      usage: { promptTokens: \u001b[33m20\u001b[39m, completionTokens: \u001b[33m7\u001b[39m, totalTokens: \u001b[33m27\u001b[39m },\n",
       "      traceId: \u001b[32m\"97e90fab-94d5-4f50-b8ca-f4b63018d4eb\"\u001b[39m,\n",
       "      parentObservationId: \u001b[32m\"7c885f0b-76f1-45c8-9c53-38d09751c9ae\"\u001b[39m\n",
       "    },\n",
       "    metadata: \u001b[90mundefined\u001b[39m\n",
       "  },\n",
       "  {\n",
       "    id: \u001b[32m\"b7f640a9-e356-4932-8b4b-80f17034fc94\"\u001b[39m,\n",
       "    type: \u001b[32m\"generation-create\"\u001b[39m,\n",
       "    timestamp: \u001b[32m\"2024-04-16T16:42:29.130Z\"\u001b[39m,\n",
       "    body: {\n",
       "      id: \u001b[32m\"fa21f0bf-d4b7-4390-9442-4340a2171ee8\"\u001b[39m,\n",
       "      startTime: \u001b[35m2024-04-16T16:42:26.267Z\u001b[39m,\n",
       "      parent: LangfuseSpanClient {\n",
       "        client: Langfuse {\n",
       "          debugMode: \u001b[33mfalse\u001b[39m,\n",
       "          pendingPromises: {},\n",
       "          _events: \u001b[36m[SimpleEventEmitter]\u001b[39m,\n",
       "          publicKey: \u001b[32m\"pk-lf-bbac702c-5fab-4660-b0eb-d326045051da\"\u001b[39m,\n",
       "          secretKey: \u001b[32m\"sk-lf-b115b6b1-d0c6-49f7-ad0a-28b545a53c91\"\u001b[39m,\n",
       "          baseUrl: \u001b[32m\"https://cloud.langfuse.com\"\u001b[39m,\n",
       "          flushAt: \u001b[33m15\u001b[39m,\n",
       "          flushInterval: \u001b[33m10000\u001b[39m,\n",
       "          release: \u001b[90mundefined\u001b[39m,\n",
       "          _retryOptions: \u001b[36m[Object]\u001b[39m,\n",
       "          requestTimeout: \u001b[33m10000\u001b[39m,\n",
       "          sdkIntegration: \u001b[32m\"DEFAULT\"\u001b[39m,\n",
       "          _promptCache: \u001b[36m[LangfusePromptCache]\u001b[39m,\n",
       "          _storageKey: \u001b[32m\"lf_pk-lf-bbac702c-5fab-4660-b0eb-d326045051da_langfuse\"\u001b[39m,\n",
       "          _storage: \u001b[36m[Object]\u001b[39m,\n",
       "          _storageCache: \u001b[36m[Object]\u001b[39m,\n",
       "          _flushTimer: \u001b[1mnull\u001b[22m\n",
       "        },\n",
       "        id: \u001b[32m\"7c885f0b-76f1-45c8-9c53-38d09751c9ae\"\u001b[39m,\n",
       "        traceId: \u001b[32m\"97e90fab-94d5-4f50-b8ca-f4b63018d4eb\"\u001b[39m,\n",
       "        observationId: \u001b[32m\"7c885f0b-76f1-45c8-9c53-38d09751c9ae\"\u001b[39m\n",
       "      },\n",
       "      generationName: \u001b[32m\"generate-poem\"\u001b[39m,\n",
       "      tag: [ \u001b[32m\"update\"\u001b[39m ],\n",
       "      model: \u001b[32m\"gpt-3.5-turbo\"\u001b[39m,\n",
       "      input: { messages: [ \u001b[36m[Object]\u001b[39m, \u001b[36m[Object]\u001b[39m ] },\n",
       "      modelParameters: {\n",
       "        frequency_penalty: \u001b[90mundefined\u001b[39m,\n",
       "        logit_bias: \u001b[90mundefined\u001b[39m,\n",
       "        logprobs: \u001b[90mundefined\u001b[39m,\n",
       "        max_tokens: \u001b[90mundefined\u001b[39m,\n",
       "        n: \u001b[90mundefined\u001b[39m,\n",
       "        presence_penalty: \u001b[90mundefined\u001b[39m,\n",
       "        seed: \u001b[90mundefined\u001b[39m,\n",
       "        stop: \u001b[90mundefined\u001b[39m,\n",
       "        stream: \u001b[90mundefined\u001b[39m,\n",
       "        temperature: \u001b[90mundefined\u001b[39m,\n",
       "        top_p: \u001b[90mundefined\u001b[39m,\n",
       "        user: \u001b[90mundefined\u001b[39m,\n",
       "        response_format: \u001b[90mundefined\u001b[39m,\n",
       "        top_logprobs: \u001b[90mundefined\u001b[39m\n",
       "      },\n",
       "      name: \u001b[32m\"generate-poem\"\u001b[39m,\n",
       "      output: {\n",
       "        role: \u001b[32m\"assistant\"\u001b[39m,\n",
       "        content: \u001b[32m\"In the heart of France, a city so grand,\\n\"\u001b[39m +\n",
       "          \u001b[32m\"Stands lovely Paris, a sight so grand.\\n\"\u001b[39m +\n",
       "          \u001b[32m\"Bustling streets and\"\u001b[39m... 476 more characters\n",
       "      },\n",
       "      endTime: \u001b[35m2024-04-16T16:42:29.130Z\u001b[39m,\n",
       "      usage: { promptTokens: \u001b[33m30\u001b[39m, completionTokens: \u001b[33m136\u001b[39m, totalTokens: \u001b[33m166\u001b[39m },\n",
       "      traceId: \u001b[32m\"97e90fab-94d5-4f50-b8ca-f4b63018d4eb\"\u001b[39m,\n",
       "      parentObservationId: \u001b[32m\"7c885f0b-76f1-45c8-9c53-38d09751c9ae\"\u001b[39m\n",
       "    },\n",
       "    metadata: \u001b[90mundefined\u001b[39m\n",
       "  },\n",
       "  {\n",
       "    id: \u001b[32m\"e9652784-adb0-460d-a967-5060d96bf5ee\"\u001b[39m,\n",
       "    type: \u001b[32m\"span-update\"\u001b[39m,\n",
       "    timestamp: \u001b[32m\"2024-04-16T16:42:29.131Z\"\u001b[39m,\n",
       "    body: {\n",
       "      input: \u001b[32m\"The capital of France is Paris.\"\u001b[39m,\n",
       "      output: \u001b[32m\"In the heart of France, a city so grand,\\n\"\u001b[39m +\n",
       "        \u001b[32m\"Stands lovely Paris, a sight so grand.\\n\"\u001b[39m +\n",
       "        \u001b[32m\"Bustling streets and\"\u001b[39m... 476 more characters,\n",
       "      id: \u001b[32m\"7c885f0b-76f1-45c8-9c53-38d09751c9ae\"\u001b[39m,\n",
       "      traceId: \u001b[32m\"97e90fab-94d5-4f50-b8ca-f4b63018d4eb\"\u001b[39m\n",
       "    },\n",
       "    metadata: \u001b[90mundefined\u001b[39m\n",
       "  },\n",
       "  {\n",
       "    id: \u001b[32m\"45a42d8c-bb9d-4831-b446-a900ec48b433\"\u001b[39m,\n",
       "    type: \u001b[32m\"span-update\"\u001b[39m,\n",
       "    timestamp: \u001b[32m\"2024-04-16T16:42:29.131Z\"\u001b[39m,\n",
       "    body: {\n",
       "      id: \u001b[32m\"7c885f0b-76f1-45c8-9c53-38d09751c9ae\"\u001b[39m,\n",
       "      traceId: \u001b[32m\"97e90fab-94d5-4f50-b8ca-f4b63018d4eb\"\u001b[39m,\n",
       "      endTime: \u001b[35m2024-04-16T16:42:29.131Z\u001b[39m\n",
       "    },\n",
       "    metadata: \u001b[90mundefined\u001b[39m\n",
       "  },\n",
       "  {\n",
       "    id: \u001b[32m\"b7334fcf-6e77-4bb8-9d94-f4723094604d\"\u001b[39m,\n",
       "    type: \u001b[32m\"trace-create\"\u001b[39m,\n",
       "    timestamp: \u001b[32m\"2024-04-16T16:42:29.131Z\"\u001b[39m,\n",
       "    body: {\n",
       "      id: \u001b[32m\"97e90fab-94d5-4f50-b8ca-f4b63018d4eb\"\u001b[39m,\n",
       "      release: \u001b[32m\"v0.0.21\"\u001b[39m,\n",
       "      name: \u001b[32m\"City poem generator\"\u001b[39m,\n",
       "      tags: [ \u001b[32m\"updated\"\u001b[39m ],\n",
       "      metadata: { env: \u001b[32m\"development\"\u001b[39m }\n",
       "    },\n",
       "    metadata: \u001b[90mundefined\u001b[39m\n",
       "  }\n",
       "]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Update trace example\n",
    "\n",
    "const trace = langfuse.trace({ name: \"capital-poem-generator\" });\n",
    "\n",
    "const span = trace.span({ name: \"France\" });\n",
    "\n",
    "const capital = (\n",
    "  await observeOpenAI(openai, {\n",
    "    parent: span,\n",
    "    generationName: \"get-capital\",\n",
    "    tag: [\"update\"],\n",
    "  }).chat.completions.create({\n",
    "    model: \"gpt-3.5-turbo\",\n",
    "    messages: [\n",
    "      { role: \"system\", content: \"What is the capital of the country?\" },\n",
    "      { role: \"user\", content: \"France\" },\n",
    "    ],\n",
    "  })\n",
    ").choices[0].message.content;\n",
    "\n",
    "const poem = (\n",
    "  await observeOpenAI(openai, {\n",
    "    parent: span,\n",
    "    generationName: \"generate-poem\",\n",
    "    tag: [\"update\"],\n",
    "  }).chat.completions.create({\n",
    "    model: \"gpt-3.5-turbo\",\n",
    "    messages: [\n",
    "      {\n",
    "        role: \"system\",\n",
    "        content: \"You are a poet. Create a poem about this city.\",\n",
    "      },\n",
    "      { role: \"user\", content: capital },\n",
    "    ],\n",
    "  })\n",
    ").choices[0].message.content;\n",
    "\n",
    "span.update({input: capital, output: poem});\n",
    "span.end();\n",
    "\n",
    "trace.update({name:\"City poem generator\", tags: [\"updated\"], metadata: {\"env\": \"development\"}, release: \"v0.0.21\"});\n",
    "\n",
    "await langfuse.flushAsync();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694f704c-ab53-47b8-8a1c-72ebc62bcc0d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Deno",
   "language": "typescript",
   "name": "deno"
  },
  "language_info": {
   "file_extension": ".ts",
   "mimetype": "text/x.typescript",
   "name": "typescript",
   "nb_converter": "script",
   "pygments_lexer": "typescript",
   "version": "5.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

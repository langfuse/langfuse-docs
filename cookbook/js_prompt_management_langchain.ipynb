{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- NOTEBOOK_METADATA source: \"⚠️ Jupyter Notebook\" title: \"Example - Langfuse Prompt Management with Langchain (JS)\" sidebarTitle: \"Prompt Management with Langchain (JS)\" description: \"Example how to version control and manage prompts with Langfuse Prompt Management and Langchain JS.\" category: \"Examples\" -->\n",
    "\n",
    "# Example: Langfuse Prompt Management with Langchain (JS)\n",
    "\n",
    "Langfuse [Prompt Management](https://langfuse.com/docs/prompts) helps to version control and manage prompts collaboratively in one place.\n",
    "\n",
    "This example demonstrates how to use Langfuse Prompt Management together with Langchain JS.\n",
    "\n",
    "## Set Up Environment\n",
    "\n",
    "Get your Langfuse API keys by signing up for [Langfuse Cloud](https://cloud.langfuse.com/) or [self-hosting Langfuse](https://langfuse.com/self-hosting). You’ll also need your OpenAI API key.\n",
    "\n",
    "> **Note**: This cookbook uses **Deno.js** for execution, which requires different syntax for importing packages and setting environment variables. For Node.js applications, the setup process is similar but uses standard `npm` packages and `process.env`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Langfuse authentication keys\n",
    "Deno.env.set(\"LANGFUSE_PUBLIC_KEY\", \"pk-lf-***\");\n",
    "Deno.env.set(\"LANGFUSE_SECRET_KEY\", \"sk-lf-***\");\n",
    "\n",
    "// Langfuse host configuration\n",
    "// For US data region, set this to \"https://us.cloud.langfuse.com\"\n",
    "Deno.env.set(\"LANGFUSE_HOST\", \"https://cloud.langfuse.com\")\n",
    "\n",
    "// Set environment variables using Deno-specific syntax\n",
    "Deno.env.set(\"OPENAI_API_KEY\", \"sk-proj-***\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the environment variables set, we can now initialize the `langfuseSpanProcessor` which is passed to the main OpenTelemetry SDK that orchestrates tracing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Import required dependencies\n",
    "import 'npm:dotenv/config';\n",
    "import { NodeSDK } from \"npm:@opentelemetry/sdk-node\";\n",
    "import { LangfuseSpanProcessor } from \"npm:@langfuse/otel\";\n",
    " \n",
    "// Export the processor to be able to flush it later\n",
    "// This is important for ensuring all spans are sent to Langfuse\n",
    "export const langfuseSpanProcessor = new LangfuseSpanProcessor({\n",
    "    publicKey: process.env.LANGFUSE_PUBLIC_KEY!,\n",
    "    secretKey: process.env.LANGFUSE_SECRET_KEY!,\n",
    "    baseUrl: process.env.LANGFUSE_HOST ?? 'https://cloud.langfuse.com', // Default to cloud if not specified\n",
    "    environment: process.env.NODE_ENV ?? 'development', // Default to development if not specified\n",
    "  });\n",
    " \n",
    "// Initialize the OpenTelemetry SDK with our Langfuse processor\n",
    "const sdk = new NodeSDK({\n",
    "  spanProcessors: [langfuseSpanProcessor],\n",
    "});\n",
    " \n",
    "// Start the SDK to begin collecting telemetry\n",
    "// The warning about crypto module is expected in Deno and doesn't affect basic tracing functionality. Media upload features will be disabled, but all core tracing works normally\n",
    "sdk.start();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **LangfuseClient** provides additional functionality beyond OpenTelemetry tracing, such as scoring, prompt management, and data retrieval. It automatically uses the same environment variables we set earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { LangfuseClient } from \"npm:@langfuse/client\";\n",
    " \n",
    "const langfuse = new LangfuseClient();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Text Prompt\n",
    "\n",
    "### Add new prompt\n",
    "\n",
    "We add the prompt used in this example via the SDK. Alternatively, you can also edit and version the prompt in the Langfuse UI.\n",
    "\n",
    "- `Name` that identifies the prompt in Langfuse Prompt Management\n",
    "- Prompt with `topic` variable\n",
    "- Config including `modelName`, `temperature`\n",
    "- `labels` to include `production` to immediately use prompt as the default\n",
    "\n",
    "For the sake of this notebook, we will add the prompt in Langfuse and use it right away. Usually, you'd update the prompt from time to time in Langfuse and your application fetches the current production version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Create a text prompt\n",
    "await langfuse.prompt.create({\n",
    "    name: \"jokes\",\n",
    "    type: \"text\",\n",
    "    prompt: \"Tell me a joke about {{topic}}\",\n",
    "    labels: [\"production\"], // directly promote to production\n",
    "    config: {\n",
    "      model: \"gpt-4o\",\n",
    "      temperature: 0.7,\n",
    "      supported_languages: [\"en\", \"fr\"],\n",
    "    }, // optionally, add configs (e.g. model parameters or model tools) or tags\n",
    "  });"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prompt in Langfuse\n",
    "\n",
    "![Prompt in Langfuse](https://langfuse.com/images/cookbook/example-js-sdk/js_prompt_management_langchain_simple_prompt.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run example\n",
    "\n",
    "#### Get current prompt version from Langfuse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Get current `production` version\n",
    "const prompt = await langfuse.prompt.get(\"jokes\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prompt includes the prompt string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt.prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and the config object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt.config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transform prompt into Langchain PromptTemplate\n",
    "\n",
    "Use the utility method `.getLangchainPrompt()` to transform the Langfuse prompt into a string that can be used in Langchain.\n",
    "\n",
    "Context: Langfuse declares input variables in prompt templates using double brackets (`{{input variable}}`). Langchain uses single brackets for declaring input variables in PromptTemplates (`{input variable}`). The utility method `.getLangchainPrompt()` replaces the double brackets with single brackets.\n",
    "\n",
    "Also, pass the Langfuse prompt as metadata to the PromptTemplate to automatically link generations that use the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { PromptTemplate } from \"npm:@langchain/core/prompts\"\n",
    "\n",
    "const langfuseTextPrompt = await langfuse.prompt.get(\"jokes\"); // Fetch a previously created text prompt\n",
    " \n",
    "// Pass the langfuseTextPrompt to the PromptTemplate as metadata to link it to generations that use it\n",
    "const langchainTextPrompt = PromptTemplate.fromTemplate(\n",
    "  langfuseTextPrompt.getLangchainPrompt()\n",
    ").withConfig({\n",
    "  metadata: { langfusePrompt: langfuseTextPrompt },\n",
    "});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup Langfuse Tracing for Langchain JS\n",
    "\n",
    "We'll use the native [Langfuse Tracing for Langchain JS](https://langfuse.com/integrations/frameworks/langchain) when executing this chain. This is fully optional and can be used independently from Prompt Management."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { CallbackHandler } from \"npm:@langfuse/langchain\";\n",
    " \n",
    "// 1. Initialize the Langfuse callback handler\n",
    "const langfuseHandler = new CallbackHandler({\n",
    "  sessionId: \"user-session-123\",\n",
    "  userId: \"user-abc\",\n",
    "  tags: [\"langchain-test\"],\n",
    "});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create chain\n",
    "\n",
    "We use the `modelName` and `temperature` stored in `prompt.config`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { ChatOpenAI } from \"npm:@langchain/openai\"\n",
    "import { RunnableSequence } from \"npm:@langchain/core/runnables\";\n",
    "\n",
    "const model = new ChatOpenAI({\n",
    "    modelName: prompt.config.model,\n",
    "    temperature: prompt.config.temperature\n",
    "});\n",
    "const chain = RunnableSequence.from([promptTemplate, model]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Invoke chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "const res = await chain.invoke(\n",
    "    { topic: \"developers\" },\n",
    "    { callbacks: [langfuseHandler] }\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View trace in Langfuse\n",
    "\n",
    "As we passed the langfuse callback handler, we can explore the execution trace in Langfuse.\n",
    "\n",
    "![Trace in Langfuse](https://langfuse.com/images/cookbook/example-js-sdk/js_prompt_management_langchain_simple_trace.png)\n",
    "\n",
    "[Public trace in the Langfuse UI](https://cloud.langfuse.com/project/cloramnkj0002jz088vzn1ja4/traces/4c35a5f52ca0588d022d2ac55d7322e7?timestamp=2025-08-25T13%3A59%3A30.900Z&display=details&observation=40be66bf70a82583)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: OpenAI functions and JsonOutputFunctionsParser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add prompt to Langfuse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await langfuse.prompt.create({\n",
    "    name: \"extractor\",\n",
    "    prompt: \"Extracts fields from the input.\",\n",
    "    config: {\n",
    "      modelName: \"gpt-4o\",\n",
    "      temperature: 0,\n",
    "      schema: {\n",
    "        type: \"object\",\n",
    "        properties: {\n",
    "          tone: {\n",
    "            type: \"string\",\n",
    "            enum: [\"positive\", \"negative\"],\n",
    "            description: \"The overall tone of the input\",\n",
    "          },\n",
    "          word_count: {\n",
    "            type: \"number\",\n",
    "            description: \"The number of words in the input\",\n",
    "          },\n",
    "          chat_response: {\n",
    "            type: \"string\",\n",
    "            description: \"A response to the human's input\",\n",
    "          },\n",
    "        },\n",
    "        required: [\"tone\", \"word_count\", \"chat_response\"],\n",
    "      }\n",
    "    }, // optionally, add configs (e.g. model parameters or model tools)\n",
    "    labels: [\"production\"] // directly promote to production\n",
    "});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prompt in Langfuse\n",
    "\n",
    "![Prompt in Langfuse](https://langfuse.com/images/cookbook/example-js-sdk/js_prompt_management_langchain_json_extraction_prompt.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "const extractorPrompt = await langfuse.prompt.get(\"extractor\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform into schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "const extractionFunctionSchema = {\n",
    "    name: \"extractor\",\n",
    "    description: extractorPrompt.prompt,\n",
    "    parameters: extractorPrompt.config.schema,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { ChatOpenAI } from \"npm:@langchain/openai\";\n",
    "import { JsonOutputFunctionsParser } from \"npm:langchain/output_parsers\";\n",
    "\n",
    "// Instantiate the parser\n",
    "const parser = new JsonOutputFunctionsParser();\n",
    "\n",
    "// Instantiate the ChatOpenAI class\n",
    "const model = new ChatOpenAI({ \n",
    "    modelName: extractorPrompt.config.modelName,\n",
    "    temperature: extractorPrompt.config.temperature\n",
    "});\n",
    "\n",
    "// Create a new runnable, bind the function to the model, and pipe the output through the parser\n",
    "const runnable = model\n",
    "  .bind({\n",
    "    functions: [extractionFunctionSchema],\n",
    "    function_call: { name: \"extractor\" },\n",
    "  })\n",
    "  .pipe(parser);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Invoke chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { HumanMessage } from \"npm:@langchain/core/messages\";\n",
    "\n",
    "// Invoke the runnable with an input\n",
    "const result = await runnable.invoke(\n",
    "    [new HumanMessage(\"What a beautiful day!\")],\n",
    "    { callbacks: [langfuseHandler] }\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View trace in Langfuse\n",
    "\n",
    "![Trace in Langfuse](https://langfuse.com/images/cookbook/example-js-sdk/js_prompt_management_langchain_json_extraction_trace.png)\n",
    "\n",
    "[Public trace in the Langfuse UI](https://cloud.langfuse.com/project/cloramnkj0002jz088vzn1ja4/traces/a74e403b5aa6dba68fecb0e46746d564?timestamp=2025-08-25T14:06:07.727Z&display=details)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Deno",
   "language": "typescript",
   "name": "deno"
  },
  "language_info": {
   "codemirror_mode": "typescript",
   "file_extension": ".ts",
   "mimetype": "text/x.typescript",
   "name": "typescript",
   "nbconvert_exporter": "script",
   "pygments_lexer": "typescript",
   "version": "5.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

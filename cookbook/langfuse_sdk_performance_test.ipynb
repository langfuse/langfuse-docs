{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HVkDob_6AJAS"
      },
      "source": [
        "# Langfuse SDK Performance Test\n",
        "\n",
        "Langfuse shall have a minimal impact on latency. This is achieved by running almost entirely in the background and by batching all requests to the Langfuse API.\n",
        "\n",
        "Coverage of this performance test:\n",
        "- Langfuse SDK: trace(), generation(), span()\n",
        "- Langchain Integration\n",
        "- OpenAI Integration\n",
        "- LlamaIndex Integration\n",
        "\n",
        "Limitation: We test integrations using OpenAI's hosted models, making the experiment less controlled but actual latency of the integrations impact more realistic."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x6gHthm3L7Pz"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W31LuRS51mqo",
        "outputId": "a15df2e4-ffc4-438a-baad-03679001199b"
      },
      "outputs": [],
      "source": [
        "%pip install langfuse --upgrade"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "tKNQTE9z1udw"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"LANGFUSE_PUBLIC_KEY\"] = \"\"\n",
        "os.environ[\"LANGFUSE_SECRET_KEY\"] = \"\"\n",
        "os.environ[\"LANGFUSE_HOST\"] = \"\"\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "B8wpAMmo8r5A"
      },
      "outputs": [],
      "source": [
        "from langfuse import Langfuse\n",
        "\n",
        "langfuse = Langfuse()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "cYI4T0Ow347l"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import timeit\n",
        "\n",
        "def time_func(func, runs=100):\n",
        "    durations = []\n",
        "    for _ in range(runs):\n",
        "        start = timeit.default_timer()\n",
        "        func()\n",
        "        stop = timeit.default_timer()\n",
        "        durations.append(stop - start)\n",
        "\n",
        "    desc = pd.Series(durations).describe()\n",
        "    desc.index = [f'{name} (sec)' if name != 'count' else name for name in desc.index]\n",
        "    return desc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1jhCkg8l3VU8"
      },
      "source": [
        "## Python SDK\n",
        "\n",
        "`trace()`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ayv14-jI2nCf",
        "outputId": "efe87727-d640-41e3-dda9-d5e3a0108e16"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "count         100.000000\n",
              "mean (sec)      0.000266\n",
              "std (sec)       0.000381\n",
              "min (sec)       0.000154\n",
              "25% (sec)       0.000191\n",
              "50% (sec)       0.000197\n",
              "75% (sec)       0.000211\n",
              "max (sec)       0.003784\n",
              "dtype: float64"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "time_func(lambda: langfuse.trace(name=\"perf-trace\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sHK6bfPN4MvG"
      },
      "source": [
        "`span()`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1nFFGsUC11N2",
        "outputId": "ddc98201-fd63-4fd3-c595-c4bba29f535e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "count         100.000000\n",
              "mean (sec)      0.000162\n",
              "std (sec)       0.000199\n",
              "min (sec)       0.000096\n",
              "25% (sec)       0.000099\n",
              "50% (sec)       0.000106\n",
              "75% (sec)       0.000130\n",
              "max (sec)       0.001635\n",
              "dtype: float64"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trace = langfuse.trace(name=\"perf-trace\")\n",
        "\n",
        "time_func(lambda: trace.span(name=\"perf-span\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xeJXtzgt5Sbz"
      },
      "source": [
        "`generation()`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sgaOuYR_4u2l",
        "outputId": "426e1c8a-ccce-4403-8243-4ffb3dc8ce9a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "count         100.000000\n",
              "mean (sec)      0.000196\n",
              "std (sec)       0.000165\n",
              "min (sec)       0.000132\n",
              "25% (sec)       0.000137\n",
              "50% (sec)       0.000148\n",
              "75% (sec)       0.000173\n",
              "max (sec)       0.001238\n",
              "dtype: float64"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trace = langfuse.trace(name=\"perf-trace\")\n",
        "\n",
        "time_func(lambda: trace.generation(name=\"perf-generation\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HaS_iKy85YH-"
      },
      "source": [
        "`event()`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wn4jVUKp5YH_",
        "outputId": "478b8788-ab3e-42f4-9e2b-f23c3580fdb8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "count         100.000000\n",
              "mean (sec)      0.000236\n",
              "std (sec)       0.000300\n",
              "min (sec)       0.000152\n",
              "25% (sec)       0.000177\n",
              "50% (sec)       0.000189\n",
              "75% (sec)       0.000219\n",
              "max (sec)       0.003144\n",
              "dtype: float64"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trace = langfuse.trace(name=\"perf-trace\")\n",
        "\n",
        "time_func(lambda: trace.event(name=\"perf-generation\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "otL2qdfo6nGo"
      },
      "source": [
        "## Langchain Integration\n",
        "\n",
        "Docs: https://langfuse.com/docs/integrations/langchain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "laH-rPO-6q6H",
        "outputId": "76a38fa0-6c21-4299-f7e7-005e7f3c6dd9"
      },
      "outputs": [],
      "source": [
        "%pip install langchain langchain-openai --upgrade"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X0peBWr461Nt"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.schema import StrOutputParser\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(\"what is the city {person} is from?\")\n",
        "model = ChatOpenAI(max_tokens=10)\n",
        "chain = prompt | model | StrOutputParser()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I3VmzcT38bDx"
      },
      "outputs": [],
      "source": [
        "from langfuse.callback import CallbackHandler\n",
        "langfuse_handler = CallbackHandler()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "upudubl9C8Cp"
      },
      "source": [
        "### Bechmark without Langfuse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AV_AKX6Q7W9H",
        "outputId": "c4eaa72b-e844-4db5-d790-5bea9a2f93e2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "count         100.000000\n",
              "mean (sec)      0.529463\n",
              "std (sec)       0.685193\n",
              "min (sec)       0.306092\n",
              "25% (sec)       0.373373\n",
              "50% (sec)       0.407278\n",
              "75% (sec)       0.530427\n",
              "max (sec)       7.107237\n",
              "dtype: float64"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "langchain_stats_no_langfuse = time_func(lambda: chain.invoke({\"person\":\"Paul Graham\"}))\n",
        "langchain_stats_no_langfuse"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zBSELTR0DBa6"
      },
      "source": [
        "### With Langfuse Tracing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vgt0ZlnU79Mt",
        "outputId": "6ea28ae9-22fb-40e8-cd0b-1e2c94022233"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "count         100.000000\n",
              "mean (sec)      0.618286\n",
              "std (sec)       0.165149\n",
              "min (sec)       0.464992\n",
              "25% (sec)       0.518323\n",
              "50% (sec)       0.598474\n",
              "75% (sec)       0.675420\n",
              "max (sec)       1.838614\n",
              "dtype: float64"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "langchain_stats_with_langfuse = time_func(lambda: chain.invoke({\"person\":\"Paul Graham\"}, {\"callbacks\":[langfuse_handler]}))\n",
        "langchain_stats_with_langfuse"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f_N-3iAX-lLz"
      },
      "source": [
        "## OpenAI Integration\n",
        "\n",
        "Docs: https://langfuse.com/docs/integrations/openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dRP_gpDO-nkj"
      },
      "outputs": [],
      "source": [
        "%pip install langfuse openai --upgrade --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Jx7mJNl-pSs"
      },
      "outputs": [],
      "source": [
        "import openai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XH4BveqiDXxa"
      },
      "source": [
        "### Benchmark without Langfuse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EH56ioru-tic",
        "outputId": "af0e4655-a896-4183-cf14-fa4dc2be6150"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "count         100.000000\n",
              "mean (sec)      0.524097\n",
              "std (sec)       0.220446\n",
              "min (sec)       0.288002\n",
              "25% (sec)       0.395479\n",
              "50% (sec)       0.507395\n",
              "75% (sec)       0.571789\n",
              "max (sec)       1.789671\n",
              "dtype: float64"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "time_func(lambda: openai.chat.completions.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=[\n",
        "      {\"role\": \"user\", \"content\": \"what is the city Paul Graham is from?\"}],\n",
        "  temperature=0,\n",
        "  max_tokens=10,\n",
        "))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ToKz-Cg8DcBZ"
      },
      "source": [
        "### With Langfuse Tracing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FU0aFfqx_bmg"
      },
      "outputs": [],
      "source": [
        "from langfuse.openai import openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RjOPRK1z_iMT",
        "outputId": "09fd0fc5-146d-44cd-ed86-8c8964b5231a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "count         100.000000\n",
              "mean (sec)      0.515243\n",
              "std (sec)       0.286902\n",
              "min (sec)       0.283431\n",
              "25% (sec)       0.378736\n",
              "50% (sec)       0.435775\n",
              "75% (sec)       0.558746\n",
              "max (sec)       2.613779\n",
              "dtype: float64"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "time_func(lambda: openai.chat.completions.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=[\n",
        "      {\"role\": \"user\", \"content\": \"what is the city Paul Graham is from?\"}],\n",
        "  temperature=0,\n",
        "  max_tokens=10,\n",
        "))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g4HjWleJMJq4"
      },
      "source": [
        "## LlamaIndex Integration\n",
        "\n",
        "Docs: https://langfuse.com/docs/integrations/llama-index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_YGf5MXEMLm3",
        "outputId": "613dc3fc-be5f-4fef-9862-e41785b978f2"
      },
      "outputs": [],
      "source": [
        "%pip install llama-index --upgrade --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QFgh4swCMs5V"
      },
      "source": [
        "Sample documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "J-EOqIb_MO3S"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import Document\n",
        "\n",
        "doc1 = Document(text=\"\"\"\n",
        "Maxwell \"Max\" Silverstein, a lauded movie director, screenwriter, and producer, was born on October 25, 1978, in Boston, Massachusetts. A film enthusiast from a young age, his journey began with home movies shot on a Super 8 camera. His passion led him to the University of Southern California (USC), majoring in Film Production. Eventually, he started his career as an assistant director at Paramount Pictures. Silverstein's directorial debut, “Doors Unseen,” a psychological thriller, earned him recognition at the Sundance Film Festival and marked the beginning of a successful directing career.\n",
        "\"\"\")\n",
        "doc2 = Document(text=\"\"\"\n",
        "Throughout his career, Silverstein has been celebrated for his diverse range of filmography and unique narrative technique. He masterfully blends suspense, human emotion, and subtle humor in his storylines. Among his notable works are \"Fleeting Echoes,\" \"Halcyon Dusk,\" and the Academy Award-winning sci-fi epic, \"Event Horizon's Brink.\" His contribution to cinema revolves around examining human nature, the complexity of relationships, and probing reality and perception. Off-camera, he is a dedicated philanthropist living in Los Angeles with his wife and two children.\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7HwT5bHgMvPv"
      },
      "source": [
        "### Bechmark without Langfuse\n",
        "\n",
        "Index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ii0oNLKoMn86",
        "outputId": "beb3656a-6319-4c4d-c43e-4bfbee10dd5a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "count         100.000000\n",
              "mean (sec)      0.171673\n",
              "std (sec)       0.058332\n",
              "min (sec)       0.112696\n",
              "25% (sec)       0.136361\n",
              "50% (sec)       0.157330\n",
              "75% (sec)       0.178455\n",
              "max (sec)       0.459417\n",
              "dtype: float64"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Example index construction + LLM query\n",
        "from llama_index.core import VectorStoreIndex\n",
        "\n",
        "time_func(lambda: VectorStoreIndex.from_documents([doc1,doc2]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96kB-hH7NOUk"
      },
      "source": [
        "Query"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QmiKPJD1NR3a",
        "outputId": "4b6bc744-0842-4b3d-e3bf-68924000e9d7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "count         100.000000\n",
              "mean (sec)      0.795817\n",
              "std (sec)       0.338263\n",
              "min (sec)       0.445060\n",
              "25% (sec)       0.614282\n",
              "50% (sec)       0.756573\n",
              "75% (sec)       0.908411\n",
              "max (sec)       3.495263\n",
              "dtype: float64"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "index = VectorStoreIndex.from_documents([doc1,doc2])\n",
        "time_func(lambda: index.as_query_engine().query(\"What did he do growing up?\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l7hHbmn7OWdm"
      },
      "source": [
        "### With Langfuse Tracing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "9sWR12LtNce2"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import Settings\n",
        "from llama_index.core.callbacks import CallbackManager\n",
        "from langfuse.llama_index import LlamaIndexCallbackHandler\n",
        "\n",
        "langfuse_callback_handler = LlamaIndexCallbackHandler()\n",
        "Settings.callback_manager = CallbackManager([langfuse_callback_handler])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "12a_7m-iNiWV"
      },
      "source": [
        "Index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eNoLmuYcNhlc",
        "outputId": "d4fc65a1-e99d-4741-80f4-21969ca31071"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "count         100.000000\n",
              "mean (sec)      0.178796\n",
              "std (sec)       0.101976\n",
              "min (sec)       0.112530\n",
              "25% (sec)       0.138217\n",
              "50% (sec)       0.163698\n",
              "75% (sec)       0.179563\n",
              "max (sec)       0.992403\n",
              "dtype: float64"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "time_func(lambda: VectorStoreIndex.from_documents([doc1,doc2]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VuiIWLlyNnnT"
      },
      "source": [
        "Query"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VJ80P7kaNmYb",
        "outputId": "bb3e67e0-4696-491f-f8e9-59e7427d9810"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "count         100.000000\n",
              "mean (sec)      0.802315\n",
              "std (sec)       0.230386\n",
              "min (sec)       0.423413\n",
              "25% (sec)       0.639373\n",
              "50% (sec)       0.784945\n",
              "75% (sec)       0.945300\n",
              "max (sec)       2.164593\n",
              "dtype: float64"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "index = VectorStoreIndex.from_documents([doc1,doc2])\n",
        "time_func(lambda: index.as_query_engine().query(\"What did he do growing up?\"))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

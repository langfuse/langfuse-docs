{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HVkDob_6AJAS"
      },
      "source": [
        "# Langfuse SDK Performance Test\n",
        "\n",
        "Langfuse shall have a minimal impact on latency. This is achieved by running almost entirely in the background and by batching all requests to the Langfuse API.\n",
        "\n",
        "Coverage of this performance test:\n",
        "- Langfuse SDK: trace(), generation(), span()\n",
        "- Langchain Integration (with control)\n",
        "- OpenAI Integration (with control)\n",
        "\n",
        "Limitation: We test integrations using OpenAI's hosted models, making the experiment less controlled but actual latency of the integrations impact more realistic."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W31LuRS51mqo",
        "outputId": "a15df2e4-ffc4-438a-baad-03679001199b"
      },
      "outputs": [],
      "source": [
        "%pip install langfuse --upgrade"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "tKNQTE9z1udw"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"LANGFUSE_PUBLIC_KEY\"] = \"\"\n",
        "os.environ[\"LANGFUSE_SECRET_KEY\"] = \"\"\n",
        "os.environ[\"LANGFUSE_HOST\"] = \"\"\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "B8wpAMmo8r5A"
      },
      "outputs": [],
      "source": [
        "from langfuse import Langfuse\n",
        "\n",
        "langfuse = Langfuse()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "cYI4T0Ow347l"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import timeit\n",
        "\n",
        "def time_func(func, runs=100):\n",
        "    durations = []\n",
        "    for _ in range(runs):\n",
        "        start = timeit.default_timer()\n",
        "        func()\n",
        "        stop = timeit.default_timer()\n",
        "        durations.append(stop - start)\n",
        "\n",
        "    desc = pd.Series(durations).describe()\n",
        "    desc.index = [f'{name} (sec)' if name != 'count' else name for name in desc.index]\n",
        "    return desc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1jhCkg8l3VU8"
      },
      "source": [
        "## Python SDK\n",
        "\n",
        "`trace()`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ayv14-jI2nCf",
        "outputId": "efe87727-d640-41e3-dda9-d5e3a0108e16"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "count         100.000000\n",
              "mean (sec)      0.000266\n",
              "std (sec)       0.000381\n",
              "min (sec)       0.000154\n",
              "25% (sec)       0.000191\n",
              "50% (sec)       0.000197\n",
              "75% (sec)       0.000211\n",
              "max (sec)       0.003784\n",
              "dtype: float64"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "time_func(lambda: langfuse.trace(name=\"perf-trace\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sHK6bfPN4MvG"
      },
      "source": [
        "`span()`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1nFFGsUC11N2",
        "outputId": "ddc98201-fd63-4fd3-c595-c4bba29f535e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "count         100.000000\n",
              "mean (sec)      0.000162\n",
              "std (sec)       0.000199\n",
              "min (sec)       0.000096\n",
              "25% (sec)       0.000099\n",
              "50% (sec)       0.000106\n",
              "75% (sec)       0.000130\n",
              "max (sec)       0.001635\n",
              "dtype: float64"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trace = langfuse.trace(name=\"perf-trace\")\n",
        "\n",
        "time_func(lambda: trace.span(name=\"perf-span\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xeJXtzgt5Sbz"
      },
      "source": [
        "`generation()`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sgaOuYR_4u2l",
        "outputId": "426e1c8a-ccce-4403-8243-4ffb3dc8ce9a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "count         100.000000\n",
              "mean (sec)      0.000196\n",
              "std (sec)       0.000165\n",
              "min (sec)       0.000132\n",
              "25% (sec)       0.000137\n",
              "50% (sec)       0.000148\n",
              "75% (sec)       0.000173\n",
              "max (sec)       0.001238\n",
              "dtype: float64"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trace = langfuse.trace(name=\"perf-trace\")\n",
        "\n",
        "time_func(lambda: trace.generation(name=\"perf-generation\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HaS_iKy85YH-"
      },
      "source": [
        "`event()`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wn4jVUKp5YH_",
        "outputId": "478b8788-ab3e-42f4-9e2b-f23c3580fdb8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "count         100.000000\n",
              "mean (sec)      0.000236\n",
              "std (sec)       0.000300\n",
              "min (sec)       0.000152\n",
              "25% (sec)       0.000177\n",
              "50% (sec)       0.000189\n",
              "75% (sec)       0.000219\n",
              "max (sec)       0.003144\n",
              "dtype: float64"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trace = langfuse.trace(name=\"perf-trace\")\n",
        "\n",
        "time_func(lambda: trace.event(name=\"perf-generation\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "otL2qdfo6nGo"
      },
      "source": [
        "## Langchain Integration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "laH-rPO-6q6H",
        "outputId": "76a38fa0-6c21-4299-f7e7-005e7f3c6dd9"
      },
      "outputs": [],
      "source": [
        "%pip install langchain langchain-openai --upgrade"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "X0peBWr461Nt"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.schema import StrOutputParser\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(\"what is the city {person} is from?\")\n",
        "model = ChatOpenAI(max_tokens=10)\n",
        "chain = prompt | model | StrOutputParser()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "I3VmzcT38bDx"
      },
      "outputs": [],
      "source": [
        "from langfuse.callback import CallbackHandler\n",
        "langfuse_handler = CallbackHandler()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "upudubl9C8Cp"
      },
      "source": [
        "Control: Invoking Langchain chain without Langfuse tracing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AV_AKX6Q7W9H",
        "outputId": "c4eaa72b-e844-4db5-d790-5bea9a2f93e2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "count         100.000000\n",
              "mean (sec)      0.529463\n",
              "std (sec)       0.685193\n",
              "min (sec)       0.306092\n",
              "25% (sec)       0.373373\n",
              "50% (sec)       0.407278\n",
              "75% (sec)       0.530427\n",
              "max (sec)       7.107237\n",
              "dtype: float64"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "langchain_stats_no_langfuse = time_func(lambda: chain.invoke({\"person\":\"Paul Graham\"}))\n",
        "langchain_stats_no_langfuse"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zBSELTR0DBa6"
      },
      "source": [
        "Invoking Langchain chain with Langfuse tracing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vgt0ZlnU79Mt",
        "outputId": "6ea28ae9-22fb-40e8-cd0b-1e2c94022233"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "count         100.000000\n",
              "mean (sec)      0.618286\n",
              "std (sec)       0.165149\n",
              "min (sec)       0.464992\n",
              "25% (sec)       0.518323\n",
              "50% (sec)       0.598474\n",
              "75% (sec)       0.675420\n",
              "max (sec)       1.838614\n",
              "dtype: float64"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "langchain_stats_with_langfuse = time_func(lambda: chain.invoke({\"person\":\"Paul Graham\"}, {\"callbacks\":[langfuse_handler]}))\n",
        "langchain_stats_with_langfuse"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f_N-3iAX-lLz"
      },
      "source": [
        "## OpenAI Integration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "dRP_gpDO-nkj"
      },
      "outputs": [],
      "source": [
        "%pip install langfuse openai --upgrade --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "3Jx7mJNl-pSs"
      },
      "outputs": [],
      "source": [
        "import openai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XH4BveqiDXxa"
      },
      "source": [
        "Control: OpenAI SDK without Langfuse Tracing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EH56ioru-tic",
        "outputId": "af0e4655-a896-4183-cf14-fa4dc2be6150"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "count         100.000000\n",
              "mean (sec)      0.524097\n",
              "std (sec)       0.220446\n",
              "min (sec)       0.288002\n",
              "25% (sec)       0.395479\n",
              "50% (sec)       0.507395\n",
              "75% (sec)       0.571789\n",
              "max (sec)       1.789671\n",
              "dtype: float64"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "openai_stats_no_langfuse = time_func(lambda: openai.chat.completions.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=[\n",
        "      {\"role\": \"user\", \"content\": \"what is the city Paul Graham is from?\"}],\n",
        "  temperature=0,\n",
        "  max_tokens=10,\n",
        "))\n",
        "openai_stats_no_langfuse\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ToKz-Cg8DcBZ"
      },
      "source": [
        "OpenAI SDK with Langfuse Tracing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "FU0aFfqx_bmg"
      },
      "outputs": [],
      "source": [
        "from langfuse.openai import openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RjOPRK1z_iMT",
        "outputId": "09fd0fc5-146d-44cd-ed86-8c8964b5231a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "count         100.000000\n",
              "mean (sec)      0.515243\n",
              "std (sec)       0.286902\n",
              "min (sec)       0.283431\n",
              "25% (sec)       0.378736\n",
              "50% (sec)       0.435775\n",
              "75% (sec)       0.558746\n",
              "max (sec)       2.613779\n",
              "dtype: float64"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "openai_stats_with_langfuse = time_func(lambda: openai.chat.completions.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=[\n",
        "      {\"role\": \"user\", \"content\": \"what is the city Paul Graham is from?\"}],\n",
        "  temperature=0,\n",
        "  max_tokens=10,\n",
        "))\n",
        "openai_stats_with_langfuse"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

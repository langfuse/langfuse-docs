{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: Tracing using the Arize SDK\n",
    "description: Example cookbook on using the Arize AI SDK to trace your application to Langfuse.\n",
    "---\n",
    "\n",
    "# Tracing using the Arize SDK\n",
    "\n",
    "Langfuse offers an [OpenTelemetry backend](https://langfuse.com/docs/opentelemetry/get-started) to ingest trace data from your LLM applications. With the Arize SDK and OpenTelemetry, you can log traces from multiple other frameworks to Langfuse. Below is an example of tracing OpenAI to Langfuse, you can find a full list of supported frameworks [here](https://docs.arize.com/phoenix/tracing/integrations-tracing). To make this example work with other frameworks, you just need to change the instrumentor to match the framework. \n",
    "\n",
    "> **Arize AI SDK:** Arize AI provides [Openinference](https://github.com/Arize-ai/openinference), a library that is complimentary to OpenTelemetry to enable tracing of AI applications. OpenInference can be used with any OpenTelemetry-compatible backend. \n",
    "\n",
    "## Step 1: Install Dependencies\n",
    "\n",
    "Install the necessary Python packages to enable OpenTelemetry tracing, openinference instrumentation, and the OpenAI SDK for making LLM requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install arize-phoenix-otel openai openinference-instrumentation-openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Configure Environment Variables\n",
    "\n",
    "Set your Langfuse API keys for the basic auth header. Get your Langfuse API keys by signing up for [Langfuse Cloud](https://cloud.langfuse.com) or [self-hosting Langfuse](https://langfuse.com/self-hosting).\n",
    "\n",
    "Also, add your `OPENAI_API_KEY` as an environment variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import base64\n",
    "\n",
    "LANGFUSE_PUBLIC_KEY = \"pk-lf-...\"\n",
    "LANGFUSE_SECRET_KEY = \"sk-lf-...\"\n",
    "LANGFUSE_AUTH = base64.b64encode(f\"{LANGFUSE_PUBLIC_KEY}:{LANGFUSE_SECRET_KEY}\".encode()).decode()\n",
    "\n",
    "os.environ[\"OTEL_EXPORTER_OTLP_ENDPOINT\"] = \"https://cloud.langfuse.com/api/public/otel\" # ðŸ‡ªðŸ‡º EU data region\n",
    "# os.environ[\"OTEL_EXPORTER_OTLP_ENDPOINT\"] = \"https://us.cloud.langfuse.com/api/public/otel\" # ðŸ‡ºðŸ‡¸ US data region\n",
    "\n",
    "os.environ[\"OTEL_EXPORTER_OTLP_HEADERS\"] = f\"Authorization=Basic {LANGFUSE_AUTH}\"\n",
    "\n",
    "# Set your OpenAI API key.\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-proj-...\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configure `tracer_provider` and add a span processor to export traces to Langfuse. `OTLPSpanExporter()` uses the endpoint and headers from the environment variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from opentelemetry.sdk.trace import TracerProvider\n",
    "from opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporter\n",
    "from opentelemetry.sdk.trace.export import SimpleSpanProcessor\n",
    "\n",
    "trace_provider = TracerProvider()\n",
    "trace_provider.add_span_processor(SimpleSpanProcessor(OTLPSpanExporter()))\n",
    "\n",
    "# Sets the global default tracer provider\n",
    "from opentelemetry import trace\n",
    "trace.set_tracer_provider(trace_provider)\n",
    "\n",
    "# Creates a tracer from the global tracer provider\n",
    "tracer = trace.get_tracer(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Initialize Instrumentation\n",
    "\n",
    "Initialize the Arize Phoenix module [`register()`](https://docs.arize.com/phoenix/tracing/how-to-tracing/setup-tracing-python). By setting `set_global_tracer_provider = False`, we can use the OpenTelemetry tracer provider we created in the previous step. Then, we can use the `OpenAIInstrumentor` to instrument the OpenAI SDK. You can replace this with any of the frameworks supported [here](https://docs.arize.com/phoenix/tracing/integrations-tracing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”­ OpenTelemetry Tracing Details ðŸ”­\n",
      "|  Phoenix Project: default\n",
      "|  Span Processor: SimpleSpanProcessor\n",
      "|  Collector Endpoint: localhost:4317\n",
      "|  Transport: gRPC\n",
      "|  Transport Headers: {'authorization': '****', 'user-agent': '****'}\n",
      "|  \n",
      "|  Using a default SpanProcessor. `add_span_processor` will overwrite this default.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# from phoenix.otel import register\n",
    "from phoenix.otel import register\n",
    "from openinference.instrumentation.openai import OpenAIInstrumentor\n",
    "\n",
    "# configure the Phoenix tracer\n",
    "register(set_global_tracer_provider = False,)\n",
    "\n",
    "OpenAIInstrumentor().instrument()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Execute a Sample LLM Request\n",
    "\n",
    "With instrumentation enabled, every OpenAI API call will now be traced. The following example sends a chat completion request to illustrate the integration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "response = openai.OpenAI().chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"How does enhanced LLM observability improve AI debugging?\",\n",
    "        }\n",
    "    ],\n",
    "    model=\"gpt-4o-mini\",\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Pass Additional Attributes (Optional)\n",
    "\n",
    "Opentelemetry lets you attach a set of attributes to all spans by setting [`set_attribute`](https://opentelemetry.io/docs/languages/python/instrumentation/#add-attributes-to-a-span). This allows you to set properties like a Langfuse Session ID, to group traces into Langfuse Sessions or a User ID, to assign traces to a specific user. You can find a list of all supported attributes in the [here](/docs/opentelemetry/get-started#property-mapping)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "with tracer.start_as_current_span(\"OpenAI-Trace\") as span:\n",
    "    span.set_attribute(\"langfuse.user.id\", \"user-123\")\n",
    "    span.set_attribute(\"langfuse.session.id\", \"123456789\")\n",
    "    span.set_attribute(\"langfuse.tags\", [\"staging\", \"demo\"])\n",
    "    span.set_attribute(\"langfuse.prompt.name\", \"test-1\")\n",
    "\n",
    "    # You application code below:\n",
    "\n",
    "    response = openai.OpenAI().chat.completions.create(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"How does enhanced LLM observability improve AI debugging?\",\n",
    "            }\n",
    "        ],\n",
    "        model=\"gpt-4o-mini\",\n",
    "    )\n",
    "    print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: View the Traces in Langfuse\n",
    "\n",
    "After running the above code, you can inspect the generated traces on your Langfuse dashboard:\n",
    "\n",
    "![Example trace in Langfuse](https://langfuse.com/images/cookbook/otel-integration-arize/arize-ai-instrumentation-example-trace.png)\n",
    "\n",
    "_[Public example trace in Langfuse](https://cloud.langfuse.com/project/cloramnkj0002jz088vzn1ja4/traces/76e520bd3ec1f70356cde4f6d369fd2e?timestamp=2025-02-28T12%3A57%3A01.513Z&observation=cc20bc20cebf9361)_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: OpenLIT Integration via OpenTelemetry\n",
        "description: Example cookbook for the OpenLIT Langfuse integration using OpenTelemetry.\n",
        "category: Integrations\n",
        "---\n",
        "\n",
        "# OpenLIT Integration via OpenTelemetry\n",
        "\n",
        "Langfuse is an [OpenTelemetry backend](https://langfuse.com/docs/opentelemetry/get-started), allowing trace ingestion from various OpenTelemetry instrumentation libraries. This guide demonstrates how to use the [OpenLit](https://docs.openlit.io/latest/features/tracing) instrumentation library to instrument a compatible framework or LLM provider.\n",
        "\n",
        "## Step 1: Install Dependencies\n",
        "\n",
        "Install the necessary Python packages: `openai`, `langfuse`, and `openlit`. These will allow you to interact with OpenAI as well as setup the instrumentation for tracing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "8xR_verv0yp8",
        "outputId": "1bf27d03-bc6a-4fda-e486-a049534cdc09"
      },
      "outputs": [],
      "source": [
        "%pip install openai langfuse openlit\n",
        "%pip install opentelemetry-sdk opentelemetry-exporter-otlp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Configure Environment Variables\n",
        "\n",
        "Before sending any requests, you need to configure your credentials and endpoints. First, set up the Langfuse authentication by providing your public and secret keys. Then, configure the OpenTelemetry exporter endpoint and headers to point to Langfuse's backend. You should also specify your OpenAI API key."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "1-vH4lFi6t-D"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import base64\n",
        "\n",
        "LANGFUSE_PUBLIC_KEY = \"pk-lf-...\"\n",
        "LANGFUSE_SECRET_KEY = \"sk-lf-...\"\n",
        "LANGFUSE_AUTH = base64.b64encode(f\"{LANGFUSE_PUBLIC_KEY}:{LANGFUSE_SECRET_KEY}\".encode()).decode()\n",
        "\n",
        "os.environ[\"OTEL_EXPORTER_OTLP_ENDPOINT\"] = \"https://cloud.langfuse.com/api/public/otel\" # ðŸ‡ªðŸ‡º EU data region\n",
        "# os.environ[\"OTEL_EXPORTER_OTLP_ENDPOINT\"] = \"https://us.cloud.langfuse.com/api/public/otel\" # ðŸ‡ºðŸ‡¸ US data region\n",
        "\n",
        "os.environ[\"OTEL_EXPORTER_OTLP_HEADERS\"] = f\"Authorization=Basic {LANGFUSE_AUTH}\"\n",
        "\n",
        "# Set your OpenAI API key.\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"sk-proj-...\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Configure `tracer_provider` and add a span processor to export traces to Langfuse. `OTLPSpanExporter()` uses the endpoint and headers from the environment variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "from opentelemetry.sdk.trace import TracerProvider\n",
        "from opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporter\n",
        "from opentelemetry.sdk.trace.export import SimpleSpanProcessor\n",
        "\n",
        "trace_provider = TracerProvider()\n",
        "trace_provider.add_span_processor(SimpleSpanProcessor(OTLPSpanExporter()))\n",
        "\n",
        "# Sets the global default tracer provider\n",
        "from opentelemetry import trace\n",
        "trace.set_tracer_provider(trace_provider)\n",
        "\n",
        "# Creates a tracer from the global tracer provider\n",
        "tracer = trace.get_tracer(__name__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*Explanation:* This block configures the necessary environment variables. The Langfuse keys are combined and base64 encoded to form an authentication token that is then set in the OTLP headers. Additionally, the OpenTelemetry endpoint is provided to direct trace data to Langfuse's backend.\n",
        "\n",
        "## Step 3: Initialize Instrumentation\n",
        "\n",
        "With the environment set up, import the needed libraries and initialize OpenLIT instrumentation. We set `tracer=tracer` to use the tracer we created in the previous step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import openlit\n",
        "\n",
        "# Initialize OpenLIT instrumentation. The disable_batch flag is set to true to process traces immediately.\n",
        "openlit.init(tracer=tracer, disable_batch=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Make a Chat Completion Request\n",
        "\n",
        "For this example, we will make a simple chat completion request to the OpenAI Chat API. This will generate trace data that you can later view in the Langfuse dashboard."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xMaAjHXSi0Kr",
        "outputId": "277ddaa5-8de1-47f8-b5d1-d4df5dbd448a"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "# Create an instance of the OpenAI client.\n",
        "openai_client = OpenAI()\n",
        "\n",
        "# Make a sample chat completion request. This request will be traced by OpenLIT and sent to Langfuse.\n",
        "chat_completion = openai_client.chat.completions.create(\n",
        "    messages=[\n",
        "        {\n",
        "          \"role\": \"user\",\n",
        "          \"content\": \"What is LLM Observability?\",\n",
        "        }\n",
        "    ],\n",
        "    model=\"gpt-4o\",\n",
        ")\n",
        "\n",
        "print(chat_completion)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Pass Additional Attributes (Optional)\n",
        "\n",
        "Opentelemetry lets you attach a set of attributes to all spans by setting [`set_attribute`](https://opentelemetry.io/docs/languages/python/instrumentation/#add-attributes-to-a-span). This allows you to set properties like a Langfuse Session ID, to group traces into Langfuse Sessions or a User ID, to assign traces to a specific user. You can find a list of all supported attributes in the [here](/docs/opentelemetry/get-started#property-mapping)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import openai\n",
        "\n",
        "input = \"How does enhanced LLM observability improve AI debugging?\"\n",
        "\n",
        "with tracer.start_as_current_span(\"OpenAI-Trace\") as span:\n",
        "    span.set_attribute(\"langfuse.user.id\", \"user-123\")\n",
        "    span.set_attribute(\"langfuse.session.id\", \"123456789\")\n",
        "    span.set_attribute(\"langfuse.tags\", [\"staging\", \"demo\"])\n",
        "    span.set_attribute(\"langfuse.prompt.name\", \"test-1\")\n",
        "\n",
        "    # You application code below:\n",
        "    response = openai.OpenAI().chat.completions.create(\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": input,\n",
        "            }\n",
        "        ],\n",
        "        model=\"gpt-4o-mini\",\n",
        "    )\n",
        "    print(response.choices[0].message.content)\n",
        "\n",
        "    # Add input and output values to the new parent span\n",
        "    span.set_attribute(\"input.value\", input)\n",
        "    span.set_attribute(\"output.value\", response.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: See Traces in Langfuse\n",
        "\n",
        "You can view the generated trace data in Langfuse. You can view this [example trace](https://cloud.langfuse.com/project/cloramnkj0002jz088vzn1ja4/traces/64902f6a5b4f27738be939b7ad38eab3?timestamp=2025-02-02T22%3A09%3A53.053Z) in the Langfuse UI."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EWrLi7RltrrK"
      },
      "source": [
        "![OpenLIT OpenAI Trace](https://langfuse.com/images/cookbook/otel-integration-openlit/openlit-openai-trace.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Using Dataset Experiments with the OpenLit Instrumentation\n",
        "\n",
        "With [Dataset Experiments](https://langfuse.com/docs/datasets/overview), you can test your application on a dataset before deploying it to production. \n",
        "\n",
        "First, set up the helper function (`otel_helper_function`) that will be used to run the application. This function returns the application output as well as the Langfuse trace to link to dataset run with the trace."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from opentelemetry.trace import format_trace_id\n",
        "\n",
        "def otel_helper_function(input):\n",
        "    with tracer.start_as_current_span(\"Otel-Trace\") as span:\n",
        "\n",
        "        # Your gen ai application logic here: (make sure this function is sending traces to Langfuse)\n",
        "        response = openai.OpenAI().chat.completions.create(\n",
        "            messages=[{\"role\": \"user\", \"content\": input}],\n",
        "            model=\"gpt-4o-mini\",\n",
        "        )\n",
        "        print(response.choices[0].message.content)\n",
        "\n",
        "        # Fetch the current span and trace id\n",
        "        current_span = trace.get_current_span()\n",
        "        span_context = current_span.get_span_context()\n",
        "        trace_id = span_context.trace_id\n",
        "        formatted_trace_id = format_trace_id(trace_id)\n",
        "\n",
        "        langfuse_trace = langfuse.trace(\n",
        "            id=formatted_trace_id, \n",
        "            input=input, \n",
        "            output=response.choices[0].message.content\n",
        "        )\n",
        "    return langfuse_trace, response.choices[0].message.content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then loop over the dataset items and run the application."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langfuse import Langfuse\n",
        "langfuse = Langfuse()\n",
        "\n",
        "dataset = langfuse.get_dataset(\"<langfuse_dataset_name>\")\n",
        "\n",
        "# Run our application against each dataset item\n",
        "for item in dataset.items:\n",
        "    langfuse_trace, output = otel_helper_function(item.input[\"text\"])\n",
        "\n",
        "    # Link the trace to the dataset item for analysis\n",
        "    item.link(\n",
        "        langfuse_trace,\n",
        "        run_name=\"run-01\",\n",
        "        run_metadata={ \"model\": \"gpt-4o-mini\" }\n",
        "    )\n",
        "\n",
        "    # Optionally, store a quick evaluation score for demonstration\n",
        "    langfuse_trace.score(\n",
        "        name=\"<example_eval>\",\n",
        "        value= your_evaluation_function(output),\n",
        "        comment=\"This is a comment\"\n",
        "    )\n",
        "\n",
        "# Flush data to ensure all telemetry is sent\n",
        "langfuse.flush()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You can repeat this process with different:\n",
        "- Models (OpenAI GPT, local LLM, etc.)\n",
        "- Prompts (different system messages)\n",
        "\n",
        "Then compare them side-by-side in Langfuse:\n",
        "\n",
        "![Dataset run overview](https://langfuse.com/images/cookbook/huggingface-agent-course/dataset_runs.png)\n",
        "![Dataset run comparison](https://langfuse.com/images/cookbook/huggingface-agent-course/dataset-run-comparison.png)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

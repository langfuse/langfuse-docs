{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24b5jqb5sXOL"
      },
      "source": [
        "---\n",
        "description: Example cookbook for the OpenLLMetry Langfuse integration using OpenTelemetry.\n",
        "category: Integrations\n",
        "---\n",
        "\n",
        "# OpenLLMetry Integration via OpenTelemetry\n",
        "\n",
        "Langfuse provides a backend built on OpenTelemetry for ingesting trace data, and you can use different instrumentation libraries to export traces from your applications. In this guide, we showcase how to instrument your LLM application using the [OpenLLMetry instrumentation library](https://github.com/traceloop/openllmetry) by Traceloop.\n",
        "\n",
        "> **About OpenLLMetry:** [OpenLLMetry](https://www.traceloop.com/docs/openllmetry/introduction) is an open source project that simplifies monitoring and debugging of your LLM application. It leverages OpenTelemetry to collect trace data in a non-intrusive manner.\n",
        "\n",
        "## Step 1: Install Dependencies\n",
        "\n",
        "Begin by installing the necessary Python packages. In this example, we need the `openai` library to interact with OpenAIâ€™s API and `traceloop-sdk` for enabling OpenLLMetry instrumentation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "tsziF7Z7-9ck",
        "outputId": "9d99f0ab-2a41-478d-90fb-1cf9121de0d8"
      },
      "outputs": [],
      "source": [
        "%pip install openai traceloop-sdk langfuse"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Configure Environment Variables\n",
        "\n",
        "Before sending any requests, configure your environment with the necessary credentials and endpoints. Here, we set up Langfuse authentication by combining your public and secret keys into a Base64-encoded token. We also specify the Langfuse endpoint based on your desired geographical region (EU or US) and provide your OpenAI API key."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "-u8TzbYhAMzs"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import base64\n",
        "\n",
        "# Get keys for your project from the project settings page: https://cloud.langfuse.com\n",
        "os.environ[\"LANGFUSE_PUBLIC_KEY\"] = \"pk-lf-...\" \n",
        "os.environ[\"LANGFUSE_SECRET_KEY\"] = \"sk-lf-...\" \n",
        "os.environ[\"LANGFUSE_HOST\"] = \"https://cloud.langfuse.com\" # ðŸ‡ªðŸ‡º EU region\n",
        "# os.environ[\"LANGFUSE_HOST\"] = \"https://us.cloud.langfuse.com\" # ðŸ‡ºðŸ‡¸ US region\n",
        "\n",
        "# Build Basic Auth header.\n",
        "LANGFUSE_AUTH = base64.b64encode(\n",
        "    f\"{os.environ.get('LANGFUSE_PUBLIC_KEY')}:{os.environ.get('LANGFUSE_SECRET_KEY')}\".encode()\n",
        ").decode()\n",
        "\n",
        "# Configure OpenTelemetry endpoint & headers\n",
        "os.environ[\"OTEL_EXPORTER_OTLP_ENDPOINT\"] = os.environ.get(\"LANGFUSE_HOST\") + \"/api/public/otel\"\n",
        "os.environ[\"OTEL_EXPORTER_OTLP_HEADERS\"] = f\"Authorization=Basic {LANGFUSE_AUTH}\"\n",
        "\n",
        "# Your openai key\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"sk-proj-...\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "With the environment variables set, we can now initialize the Langfuse client. `get_client()` initializes the Langfuse client using the credentials provided in the environment variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Langfuse client is authenticated and ready!\n"
          ]
        }
      ],
      "source": [
        "from langfuse import get_client\n",
        " \n",
        "langfuse = get_client()\n",
        " \n",
        "# Verify connection\n",
        "if langfuse.auth_check():\n",
        "    print(\"Langfuse client is authenticated and ready!\")\n",
        "else:\n",
        "    print(\"Authentication failed. Please check your credentials and host.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uHYYPKKGsTmt"
      },
      "source": [
        "## Step 3: Initialize Instrumentation\n",
        "\n",
        "Next, initialize the OpenLLMetry instrumentation using the `traceloop-sdk`. Using `disable_batch=True` is recommended if you run this code in a notebook as traces are sent immediately without waiting for batching. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XS9RbpCb_CU4",
        "outputId": "e70ee013-39b8-45e1-abc9-c99e0ff34ba3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[39m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR:root:Error initializing redis instrumentor: No module named 'opentelemetry.instrumentation.redis'\n"
          ]
        }
      ],
      "source": [
        "from traceloop.sdk import Traceloop\n",
        "\n",
        "Traceloop.init(disable_batch=True,\n",
        "               api_endpoint=os.environ.get(\"OTEL_EXPORTER_OTLP_ENDPOINT\"),\n",
        "               headers=os.environ.get(f\"Authorization=Basic {LANGFUSE_AUTH}\"),)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Execute a Sample LLM Request\n",
        "\n",
        "With instrumentation enabled, every OpenAI API call will now be traced. The following example sends a chat completion request to illustrate the integration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ChatCompletion(id='chatcmpl-BjRWj0Gn9A1PdPYslJ9rDNW730I97', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"LLM observability refers to the practices, tools, and methodologies used to monitor, analyze, and understand the behavior and performance of Large Language Models (LLMs) in real time. As organizations increasingly rely on LLMs for various applicationsâ€”such as natural language processing, chatbots, content generation, and moreâ€”ensuring their reliability, accuracy, and ethical alignment has become critical.\\n\\nKey components of LLM observability include:\\n\\n1. **Monitoring Performance**: Tracking metrics such as response time, resource utilization, and throughput to ensure that the model operates efficiently under load.\\n\\n2. **Quality Analysis**: Evaluating the quality of the model's outputs through various means, including user feedback, automated evaluation metrics, and comparison to ground truth data.\\n\\n3. **Behavior Analysis**: Analyzing the model's behavior in different contexts to identify biases, unintentional outputs, or other anomalies. This includes examining edge cases where the model might fail or produce unexpected results.\\n\\n4. **Debugging Tools**: Implementing tools that help trace issues or problems back to specific inputs, configurations, or model parameters that may be causing suboptimal performance.\\n\\n5. **Data Drift Detection**: Monitoring the input data for changes over time that could affect the model's performance, such as shifts in language use, terminology, or user behavior.\\n\\n6. **Feedback Loops**: Establishing mechanisms for continuous feedback from users and incorporating that information back into the model development lifecycle for fine-tuning and improvements.\\n\\n7. **Compliance and Safety**: Ensuring that the model adheres to ethical standards and legal requirements, especially regarding data usage and content generation.\\n\\nEffective observability can help organizations better manage their LLM deployments, minimize risks, and enhance the overall user experience by ensuring that models perform accurately and reliably in a wide range of scenarios.\", refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1750170273, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier='default', system_fingerprint='fp_34a54ae93c', usage=CompletionUsage(completion_tokens=368, prompt_tokens=14, total_tokens=382, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n"
          ]
        }
      ],
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "openai_client = OpenAI()\n",
        "\n",
        "chat_completion = openai_client.chat.completions.create(\n",
        "    messages=[\n",
        "        {\n",
        "          \"role\": \"user\",\n",
        "          \"content\": \"What is LLM Observability?\",\n",
        "        }\n",
        "    ],\n",
        "    model=\"gpt-4o-mini\",\n",
        ")\n",
        "\n",
        "print(chat_completion)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: View the Trace in Langfuse\n",
        "\n",
        "After running the above code, you can review the generated trace in your Langfuse dashboard:\n",
        "\n",
        "[Example Trace in Langfuse](https://cloud.langfuse.com/project/cloramnkj0002jz088vzn1ja4/traces/e417c49b4044725e48aa0e089534fa12?timestamp=2025-02-02T22%3A04%3A04.487Z)\n",
        "\n",
        "![OpenLLMetry OpenAI Trace Link](https://langfuse.com/images/cookbook/otel-integration-openllmetry/openllmetry-openai-trace.png)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24b5jqb5sXOL"
      },
      "source": [
        "---\n",
        "description: Example cookbook for the OpenLLMetry Langfuse integration using OpenTelemetry.\n",
        "category: Integrations\n",
        "---\n",
        "\n",
        "# OpenLLMetry Integration via OpenTelemetry\n",
        "\n",
        "Langfuse provides a backend built on OpenTelemetry for ingesting trace data, and you can use different instrumentation libraries to export traces from your applications. In this guide, we showcase how to instrument your LLM application using the [OpenLLMetry instrumentation library](https://github.com/traceloop/openllmetry) by Traceloop.\n",
        "\n",
        "> **About OpenLLMetry:** [OpenLLMetry](https://www.traceloop.com/docs/openllmetry/introduction) is an open source project that simplifies monitoring and debugging of your LLM application. It leverages OpenTelemetry to collect trace data in a non-intrusive manner.\n",
        "\n",
        "## Step 1: Install Dependencies\n",
        "\n",
        "Begin by installing the necessary Python packages. In this example, we need the `openai` library to interact with OpenAIâ€™s API and `traceloop-sdk` for enabling OpenLLMetry instrumentation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "tsziF7Z7-9ck",
        "outputId": "9d99f0ab-2a41-478d-90fb-1cf9121de0d8"
      },
      "outputs": [],
      "source": [
        "%pip install openai traceloop-sdk"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Configure Environment Variables\n",
        "\n",
        "Before sending any requests, configure your environment with the necessary credentials and endpoints. Here, we set up Langfuse authentication by combining your public and secret keys into a Base64-encoded token. We also specify the Langfuse endpoint based on your desired geographical region (EU or US) and provide your OpenAI API key."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "-u8TzbYhAMzs"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import base64\n",
        "\n",
        "LANGFUSE_PUBLIC_KEY = \"pk-lf-...\"\n",
        "LANGFUSE_SECRET_KEY = \"sk-lf-...\"\n",
        "LANGFUSE_AUTH = base64.b64encode(f\"{LANGFUSE_PUBLIC_KEY}:{LANGFUSE_SECRET_KEY}\".encode()).decode()\n",
        "\n",
        "os.environ[\"OTEL_EXPORTER_OTLP_ENDPOINT\"] = \"https://cloud.langfuse.com/api/public/otel\" # ðŸ‡ªðŸ‡º EU data region\n",
        "# os.environ[\"OTEL_EXPORTER_OTLP_ENDPOINT\"] = \"https://us.cloud.langfuse.com/api/public/otel\" # ðŸ‡ºðŸ‡¸ US data region\n",
        "\n",
        "os.environ[\"OTEL_EXPORTER_OTLP_HEADERS\"] = f\"Authorization=Basic {LANGFUSE_AUTH}\"\n",
        "\n",
        "# Set your OpenAI API key.\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"sk-proj-...\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Configure `tracer_provider` and add a span processor to export traces to Langfuse. `OTLPSpanExporter()` uses the endpoint and headers from the environment variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "from opentelemetry.sdk.trace import TracerProvider\n",
        "from opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporter\n",
        "from opentelemetry.sdk.trace.export import SimpleSpanProcessor\n",
        "\n",
        "trace_provider = TracerProvider()\n",
        "trace_provider.add_span_processor(SimpleSpanProcessor(OTLPSpanExporter()))\n",
        "\n",
        "# Sets the global default tracer provider\n",
        "from opentelemetry import trace\n",
        "trace.set_tracer_provider(trace_provider)\n",
        "\n",
        "# Creates a tracer from the global tracer provider\n",
        "tracer = trace.get_tracer(__name__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uHYYPKKGsTmt"
      },
      "source": [
        "## Step 3: Initialize Instrumentation\n",
        "\n",
        "Next, initialize the OpenLLMetry instrumentation using the `traceloop-sdk`. Using `disable_batch=True` is recommended if you run this code in a notebook as traces are sent immediately without waiting for batching. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XS9RbpCb_CU4",
        "outputId": "e70ee013-39b8-45e1-abc9-c99e0ff34ba3"
      },
      "outputs": [],
      "source": [
        "from traceloop.sdk import Traceloop\n",
        "\n",
        "Traceloop.init(disable_batch=True,\n",
        "               api_endpoint=os.environ.get(\"OTEL_EXPORTER_OTLP_ENDPOINT\"),\n",
        "               headers=os.environ.get(f\"Authorization=Basic {LANGFUSE_AUTH}\"),)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Execute a Sample LLM Request\n",
        "\n",
        "With instrumentation enabled, every OpenAI API call will now be traced. The following example sends a chat completion request to illustrate the integration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "openai_client = OpenAI()\n",
        "\n",
        "chat_completion = openai_client.chat.completions.create(\n",
        "    messages=[\n",
        "        {\n",
        "          \"role\": \"user\",\n",
        "          \"content\": \"What is LLM Observability?\",\n",
        "        }\n",
        "    ],\n",
        "    model=\"gpt-4o-mini\",\n",
        ")\n",
        "\n",
        "print(chat_completion)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Pass Additional Attributes (Optional)\n",
        "\n",
        "Opentelemetry lets you attach a set of attributes to all spans by setting [`set_attribute`](https://opentelemetry.io/docs/languages/python/instrumentation/#add-attributes-to-a-span). This allows you to set properties like a Langfuse Session ID, to group traces into Langfuse Sessions or a User ID, to assign traces to a specific user. You can find a list of all supported attributes in the [here](/docs/opentelemetry/get-started#property-mapping)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "with tracer.start_as_current_span(\"OpenAI-Trace\") as span:\n",
        "    span.set_attribute(\"langfuse.user.id\", \"user-123\")\n",
        "    span.set_attribute(\"langfuse.session.id\", \"123456789\")\n",
        "    span.set_attribute(\"langfuse.tags\", [\"smolagents\", \"demo\"])\n",
        "    span.set_attribute(\"langfuse.prompt.name\", \"test-1\")\n",
        "\n",
        "    # Create an instance of the OpenAI client.\n",
        "    openai_client = OpenAI()\n",
        "\n",
        "    # Make a sample chat completion request. This request will be traced by OpenLIT and sent to Langfuse.\n",
        "    chat_completion = openai_client.chat.completions.create(\n",
        "        messages=[\n",
        "            {\n",
        "              \"role\": \"user\",\n",
        "              \"content\": \"What is LLM Observability?\",\n",
        "            }\n",
        "        ],\n",
        "        model=\"gpt-4o-mini\",\n",
        "    )\n",
        "\n",
        "    print(chat_completion)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: View the Trace in Langfuse\n",
        "\n",
        "After running the above code, you can review the generated trace in your Langfuse dashboard:\n",
        "\n",
        "[Example Trace in Langfuse](https://cloud.langfuse.com/project/cloramnkj0002jz088vzn1ja4/traces/e417c49b4044725e48aa0e089534fa12?timestamp=2025-02-02T22%3A04%3A04.487Z)\n",
        "\n",
        "![OpenLLMetry OpenAI Trace Link](https://langfuse.com/images/cookbook/otel-integration-openllmetry/openllmetry-openai-trace.png)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

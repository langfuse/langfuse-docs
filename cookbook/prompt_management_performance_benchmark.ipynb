{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LMrp548tRBMR"
      },
      "source": [
        "# Langfuse Prompt Management Performance Test\n",
        "\n",
        "This notebook conducts a performance benchmark on Langfuse Prompt Management by measuring the latency of retrieving and compiling a prompt without caching (cache_ttl_seconds=0) over 1,000 sequential executions.\n",
        "\n",
        "In practice, this latency does not matter as the prompt is cached client-side in the SDKs. Learn more about caching in the [Langfuse Prompt Management documentation](https://langfuse.com/docs/prompt-management/features/caching).\n",
        "\n",
        "The test accounts for network latency, so absolute values may vary based on geography and load. Use the histogram and summary statistics to compare relative improvements, such as between SDK versions or caching settings.\n",
        "\n",
        "The test requires a prompt named `perf-test` to be set up in the authenticated project."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "r8StY__HRJn3"
      },
      "outputs": [],
      "source": [
        "%pip install langfuse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PThDEqJTRAOR"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Get keys for your project from the project settings page\n",
        "# https://cloud.langfuse.com\n",
        "os.environ[\"LANGFUSE_PUBLIC_KEY\"] = \"\"\n",
        "os.environ[\"LANGFUSE_SECRET_KEY\"] = \"\"\n",
        "os.environ[\"LANGFUSE_HOST\"] = \"https://cloud.langfuse.com\" # ðŸ‡ªðŸ‡º EU region\n",
        "# os.environ[\"LANGFUSE_HOST\"] = \"https://us.cloud.langfuse.com\" # ðŸ‡ºðŸ‡¸ US region"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jtRanXwuRHmJ"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "from langfuse import Langfuse\n",
        "\n",
        "# Initialize Langfuse client from environment variables\n",
        "langfuse = Langfuse()\n",
        "\n",
        "assert langfuse.auth_check(), \"Langfuse client not initialized â€“ check your environment variables.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zGVOHua4R-7u"
      },
      "outputs": [],
      "source": [
        "N_RUNS = 1_000\n",
        "prompt_name = \"perf-test\"\n",
        "\n",
        "durations = []\n",
        "for _ in tqdm(range(N_RUNS), desc=\"Benchmarking\"):\n",
        "    start = time.perf_counter()\n",
        "    prompt = langfuse.get_prompt(prompt_name, cache_ttl_seconds=0)\n",
        "    prompt.compile(input=\"test\")  # minimal compile to include serverâ€‘side processing\n",
        "    durations.append(time.perf_counter() - start)\n",
        "    time.sleep(0.05)\n",
        "\n",
        "durations_series = pd.Series(durations, name=\"seconds\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LJ6xglVeSQl5"
      },
      "outputs": [],
      "source": [
        "stats = durations_series.describe(percentiles=[0.25, 0.5, 0.75, 0.99])\n",
        "stats"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QTh5OHOUUcOc"
      },
      "source": [
        "Our last performance test\n",
        "\n",
        "```\n",
        "count    1000.000000\n",
        "mean        0.039335 sec\n",
        "std         0.014172 sec\n",
        "min         0.032702 sec\n",
        "25%         0.035387 sec\n",
        "50%         0.037030 sec\n",
        "75%         0.041111 sec\n",
        "99%         0.068914 sec\n",
        "max         0.409609 sec\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jl1prPP2SQ9U"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8,4))\n",
        "plt.hist(durations_series, bins=30)\n",
        "plt.xlabel(\"Execution time (sec)\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ibAvE5_bUgGi"
      },
      "source": [
        "Our last performance test\n",
        "\n",
        "![Chart](https://langfuse.com/images/docs/prompt-performance-chart.png)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

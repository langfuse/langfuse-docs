{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "description: Langfuse Python SDK - a decorators-based integration to give you powerful tracing, evals, and analytics for your LLM application\n",
        "category: SDKs\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Cookbook: Python Decorators\n",
        "\n",
        "The Langfuse Python SDK uses decorators for you to effortlessly integrate observability into your LLM applications. It supports both synchronous and asynchronous functions, automatically handling traces, spans, and generations, along with key execution details like inputs and outputs. This setup allows you to concentrate on developing high-quality applications while benefitting from observability insights with minimal code.\n",
        "\n",
        "This cookbook containes examples for all key functionalities of the decorator-based integration with Langfuse."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Installation & setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Install `langfuse`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install langfuse"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If you haven't done so yet, [sign up to Langfuse](https://cloud.langfuse.com/auth/sign-up) and obtain your API keys from the project settings. You can also [self-host](https://langfuse.com/docs/deployment/self-host) Langfuse."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Get keys for your project from the project settings page\n",
        "# https://cloud.langfuse.com\n",
        "os.environ[\"LANGFUSE_PUBLIC_KEY\"] = \"\"\n",
        "os.environ[\"LANGFUSE_SECRET_KEY\"] = \"\"\n",
        "os.environ[\"LANGFUSE_HOST\"] = \"https://cloud.langfuse.com\" # ðŸ‡ªðŸ‡º EU region\n",
        "# os.environ[\"LANGFUSE_HOST\"] = \"https://us.cloud.langfuse.com\" # ðŸ‡ºðŸ‡¸ US region\n",
        "\n",
        "# Your openai key\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Basic usage\n",
        "\n",
        "Langfuse simplifies observability in LLM-powered applications by organizing activities into traces. Each trace contains observations: spans for nested activities, events for distinct actions, or generations for LLM interactions. This setup mirrors your app's execution flow, offering insights into performance and behavior. See our [Tracing documentation](/docs/tracing/overview) for more details on Langfuse's telemetry model.\n",
        "\n",
        "`@observe()` decorator automatically and asynchronously logs nested traces to Langfuse. The outermost function becomes a `trace` in Langfuse, all children are `spans` by default.\n",
        "\n",
        "By default it captures:\n",
        "- nesting via context vars\n",
        "- timings/durations\n",
        "- args and kwargs as input dict\n",
        "- returned values as output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langfuse.decorators import langfuse_context, observe\n",
        "import time\n",
        "\n",
        "@observe()\n",
        "def wait():\n",
        "    time.sleep(1)\n",
        "\n",
        "@observe()\n",
        "def capitalize(input: str):\n",
        "    return input.upper()\n",
        "\n",
        "@observe()\n",
        "def main_fn(query: str):\n",
        "    wait()\n",
        "    capitalized = capitalize(query)\n",
        "    return f\"Q:{capitalized}; A: nice too meet you!\"\n",
        "\n",
        "main_fn(\"hi there\");"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "VoilÃ ! âœ¨ Langfuse will generate a trace with a nested span for you.\n",
        "\n",
        "> **Example trace**: https://cloud.langfuse.com/project/cloramnkj0002jz088vzn1ja4/traces/21128edc-27bf-4643-92f9-84d66c63de8d"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Add additional parameters to the trace\n",
        "\n",
        "In addition to the attributes automatically captured by the decorator, you can add others to use the full features of Langfuse.\n",
        "\n",
        "Two utility methods:\n",
        "- `langfuse_context.update_current_observation`: Update the trace/span of the current function scope\n",
        "- `langfuse_context.update_current_trace`: Update the trace itself, can also be called within any deeply nested span within the trace\n",
        "\n",
        "For details on available attributes, have a look at the [reference](https://python.reference.langfuse.com/langfuse/decorators#LangfuseDecorator.update_current_observation)\n",
        "\n",
        "Below is an example demonstrating how to enrich traces and observations with custom parameters:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langfuse.decorators import langfuse_context, observe\n",
        "\n",
        "@observe(as_type=\"generation\")\n",
        "def deeply_nested_llm_call():\n",
        "    # Enrich the current observation with a custom name, input, and output\n",
        "    langfuse_context.update_current_observation(\n",
        "        name=\"Deeply nested LLM call\", input=\"Ping?\", output=\"Pong!\"\n",
        "    )\n",
        "    # Set the parent trace's name from within a nested observation\n",
        "    langfuse_context.update_current_trace(\n",
        "        name=\"Trace name set from deeply_nested_llm_call\",\n",
        "        session_id=\"1234\",\n",
        "        user_id=\"5678\",\n",
        "        tags=[\"tag1\", \"tag2\"],\n",
        "        public=True\n",
        "    )\n",
        "\n",
        "@observe()\n",
        "def nested_span():\n",
        "    # Update the current span with a custom name and level\n",
        "    langfuse_context.update_current_observation(name=\"Nested Span\", level=\"WARNING\")\n",
        "    deeply_nested_llm_call()\n",
        "\n",
        "@observe()\n",
        "def main():\n",
        "    nested_span()\n",
        "\n",
        "# Execute the main function to generate the enriched trace\n",
        "main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "On the Langfuse platform the trace now shows with the updated name from the `deeply_nested_llm_call`, and the observations will be enriched with the appropriate data points.\n",
        "\n",
        "> **Example trace**: https://cloud.langfuse.com/project/cloramnkj0002jz088vzn1ja4/traces/f16e0151-cca8-4d90-bccf-1d9ea0958afb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Log an LLM Call using `as_type=\"generation\"`\n",
        "\n",
        "Model calls are represented by `generations` in Langfuse and allow you to add additional attributes. Use the `as_type=\"generation\"` flag to mark a function as a generation. Optionally, you can extract additional generation specific attributes ([reference](https://python.reference.langfuse.com/langfuse/decorators#LangfuseDecorator.update_current_observation)). \n",
        "\n",
        "This works with any LLM provider/SDK. In this example, we'll use Anthropic."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install anthropic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "os.environ[\"ANTHROPIC_API_KEY\"] = \"\"\n",
        "\n",
        "import anthropic\n",
        "anthopic_client = anthropic.Anthropic()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Wrap LLM function with decorator\n",
        "@observe(as_type=\"generation\")\n",
        "def anthropic_completion(**kwargs):\n",
        "  # extract some fields from kwargs\n",
        "  kwargs_clone = kwargs.copy()\n",
        "  input = kwargs_clone.pop('messages', None)\n",
        "  model = kwargs_clone.pop('model', None)\n",
        "  langfuse_context.update_current_observation(\n",
        "      input=input,\n",
        "      model=model,\n",
        "      metadata=kwargs_clone\n",
        "  )\n",
        "  \n",
        "  # return result\n",
        "  return anthopic_client.messages.create(**kwargs).content[0].text\n",
        "\n",
        "@observe()\n",
        "def main():\n",
        "  return anthropic_completion(\n",
        "      model=\"claude-3-opus-20240229\",\n",
        "      max_tokens=1024,\n",
        "      messages=[\n",
        "          {\"role\": \"user\", \"content\": \"Hello, Claude\"}\n",
        "      ]\n",
        "  )\n",
        "\n",
        "main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> **Example trace**: https://cloud.langfuse.com/project/cloramnkj0002jz088vzn1ja4/traces/ece9079d-e12c-4c0e-9dc3-8805d0bbe8ec?observation=723c04ff-cdca-4716-8143-e691129be315"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Customize input/output\n",
        "\n",
        "By default, input/ouput of a function are captured by `@observe()`.\n",
        "\n",
        "**You can disable capturing input/output** for a specific function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langfuse.decorators import observe\n",
        "\n",
        "@observe(capture_input=False, capture_output=False)\n",
        "def stealth_fn(input: str):\n",
        "    return input\n",
        "\n",
        "stealth_fn(\"Super secret content\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> **Example trace**: https://cloud.langfuse.com/project/cloramnkj0002jz088vzn1ja4/traces/6bdeb443-ef8c-41d8-a8a1-68fe75639428\n",
        "\n",
        "Alternatively, you can **override input and output** via `update_current_observation` (or `update_current_trace`):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langfuse.decorators import langfuse_context, observe\n",
        "\n",
        "@observe()\n",
        "def fn_2():\n",
        "    langfuse_context.update_current_observation(\n",
        "        input=\"Table?\", output=\"Tennis!\"\n",
        "    )\n",
        "    # Logic for a deeply nested LLM call\n",
        "    pass\n",
        "\n",
        "@observe()\n",
        "def main_fn():\n",
        "    langfuse_context.update_current_observation(\n",
        "        input=\"Ping?\", output=\"Pong!\"\n",
        "    )\n",
        "    fn_2()\n",
        "\n",
        "main_fn()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> **Example trace**: https://cloud.langfuse.com/project/cloramnkj0002jz088vzn1ja4/traces/d3c3ad92-d85d-4437-aaf3-7587d84f398c"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Interoperability with other Integrations\n",
        "\n",
        "Langfuse is tightly integrated with the OpenAI SDK, LangChain, and LlamaIndex. The integrations are seamlessly interoperable with each other within a decorated function. The following example demonstrates this interoperability by using all three integrations within a single trace.\n",
        "\n",
        "### 1. Initializing example applications"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install llama-index langchain langchain_openai --upgrade"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### OpenAI\n",
        "\n",
        "The [OpenAI integration](https://langfuse.com/docs/integrations/openai/get-started) automatically detects the context in which it is executed. Just use `from langfuse.openai import openai` and get native tracing of all OpenAI calls."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langfuse.openai import openai\n",
        "from langfuse.decorators import observe\n",
        "\n",
        "@observe()\n",
        "def openai_fn(calc: str):\n",
        "    res = openai.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[\n",
        "          {\"role\": \"system\", \"content\": \"You are a very accurate calculator. You output only the result of the calculation.\"},\n",
        "          {\"role\": \"user\", \"content\": calc}],\n",
        "    )\n",
        "    return res.choices[0].message.content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### LlamaIndex\n",
        "\n",
        "Via `Settings.callback_manager` you can configure the callback to use for tracing of the subsequent LlamaIndex executions. `langfuse_context.get_current_llama_index_handler()` exposes a callback handler scoped to the current trace context, in this case `llama_index_fn()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langfuse.decorators import langfuse_context, observe\n",
        "from llama_index.core import Document, VectorStoreIndex\n",
        "from llama_index.core import Settings\n",
        "from llama_index.core.callbacks import CallbackManager\n",
        "\n",
        "doc1 = Document(text=\"\"\"\n",
        "Maxwell \"Max\" Silverstein, a lauded movie director, screenwriter, and producer, was born on October 25, 1978, in Boston, Massachusetts. A film enthusiast from a young age, his journey began with home movies shot on a Super 8 camera. His passion led him to the University of Southern California (USC), majoring in Film Production. Eventually, he started his career as an assistant director at Paramount Pictures. Silverstein's directorial debut, â€œDoors Unseen,â€ a psychological thriller, earned him recognition at the Sundance Film Festival and marked the beginning of a successful directing career.\n",
        "\"\"\")\n",
        "doc2 = Document(text=\"\"\"\n",
        "Throughout his career, Silverstein has been celebrated for his diverse range of filmography and unique narrative technique. He masterfully blends suspense, human emotion, and subtle humor in his storylines. Among his notable works are \"Fleeting Echoes,\" \"Halcyon Dusk,\" and the Academy Award-winning sci-fi epic, \"Event Horizon's Brink.\" His contribution to cinema revolves around examining human nature, the complexity of relationships, and probing reality and perception. Off-camera, he is a dedicated philanthropist living in Los Angeles with his wife and two children.\n",
        "\"\"\")\n",
        "\n",
        "@observe()\n",
        "def llama_index_fn(question: str):\n",
        "    # Set callback manager for LlamaIndex, will apply to all LlamaIndex executions in this function\n",
        "    langfuse_handler = langfuse_context.get_current_llama_index_handler()\n",
        "    Settings.callback_manager = CallbackManager([langfuse_handler])\n",
        "\n",
        "    # Run application\n",
        "    index = VectorStoreIndex.from_documents([doc1,doc2])\n",
        "    response = index.as_query_engine().query(question)\n",
        "    return response"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### LangChain\n",
        "\n",
        "`langfuse_context.get_current_llama_index_handler()` exposes a callback handler scoped to the current trace context, in this case `langchain_fn()`. Pass it to subsequent runs to your LangChain application to get full tracing within the scope of the current trace."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from operator import itemgetter\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.schema import StrOutputParser\n",
        "from langfuse.decorators import observe\n",
        " \n",
        "prompt = ChatPromptTemplate.from_template(\"what is the city {person} is from?\")\n",
        "model = ChatOpenAI()\n",
        "chain = prompt | model | StrOutputParser()\n",
        "\n",
        "@observe()\n",
        "def langchain_fn(person: str):\n",
        "    # Get Langchain Callback Handler scoped to the current trace context\n",
        "    langfuse_handler = langfuse_context.get_current_langchain_handler()\n",
        "\n",
        "    # Pass handler to invoke\n",
        "    chain.invoke({\"person\": person}, config={\"callbacks\":[langfuse_handler]})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2. Run all in a single trace"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langfuse.decorators import observe\n",
        "\n",
        "@observe()\n",
        "def main():\n",
        "    output_openai = openai_fn(\"5+7\")\n",
        "    output_llamaindex = llama_index_fn(\"What did he do growing up?\")\n",
        "    output_langchain = langchain_fn(\"Feynman\")\n",
        "\n",
        "    return output_openai, output_llamaindex, output_langchain\n",
        "\n",
        "main();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "> **Example trace**: https://cloud.langfuse.com/project/cloramnkj0002jz088vzn1ja4/traces/4fcd93e3-79f2-474a-8e25-0e21c616249a"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "## Flush observations\n",
        "\n",
        "The Langfuse SDK executes network requests in the background on a separate thread for better performance of your application. This can lead to lost events in short lived environments such as AWS Lambda functions when the Python process is terminated before the SDK sent all events to the Langfuse API.\n",
        "\n",
        "Make sure to call `langfuse_context.flush()` before exiting to prevent this. This method waits for all tasks to finish."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Additional features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Scoring\n",
        "\n",
        "[Scores](https://langfuse.com/docs/scores/overview) are used to evaluate single observations or entire traces. You can create them manually in the Langfuse UI, run model-based evaluation or ingest via the SDK.\n",
        "\n",
        "| Parameter | Type   | Optional | Description\n",
        "| --- | --- | --- | ---\n",
        "| name | string | no | Identifier of the score.\n",
        "| value | number | no | The value of the score. Can be any number, often standardized to 0..1\n",
        "| comment | string | yes | Additional context/explanation of the score.\n",
        "\n",
        "\n",
        "#### Within the decorated function\n",
        "\n",
        "You can attach a score to the current observation context by calling `langfuse_context.score_current_observation`. You can also score the entire trace from anywhere inside the nesting hierarchy by calling `langfuse_context.score_current_trace`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langfuse.decorators import langfuse_context, observe\n",
        "\n",
        "@observe()\n",
        "def nested_span():\n",
        "    langfuse_context.score_current_observation(\n",
        "        name=\"feedback-on-span\",\n",
        "        value=1,\n",
        "        comment=\"I like how personalized the response is\",\n",
        "    )\n",
        "\n",
        "    langfuse_context.score_current_trace(\n",
        "        name=\"feedback-on-trace-from-nested-span\",\n",
        "        value=1,\n",
        "        comment=\"I like how personalized the response is\",\n",
        "    )\n",
        "\n",
        "\n",
        "# This will create a new trace\n",
        "@observe()\n",
        "def main():\n",
        "    langfuse_context.score_current_trace(\n",
        "        name=\"feedback-on-trace\",\n",
        "        value=1,\n",
        "        comment=\"I like how personalized the response is\",\n",
        "    )\n",
        "    nested_span()\n",
        "\n",
        "main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> **Example trace**: https://cloud.langfuse.com/project/cloramnkj0002jz088vzn1ja4/traces/1dfcff43-34c3-4888-b99a-bb9b9afd57c9"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Outside the decorated function\n",
        "\n",
        "Alternatively you may also score a trace or observation from outside its context as often scores are added async. For example, based on user feedback.\n",
        "\n",
        "The decorators expose the trace_id and observation_id which are necessary to add scores outside of the decorated functions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langfuse import Langfuse\n",
        "from langfuse.decorators import langfuse_context, observe\n",
        "\n",
        "# Initialize the Langfuse client\n",
        "langfuse_client = Langfuse()\n",
        "\n",
        "@observe()\n",
        "def nested_fn():\n",
        "    span_id = langfuse_context.get_current_observation_id()\n",
        "\n",
        "    # can also be accessed in main\n",
        "    trace_id = langfuse_context.get_current_trace_id()\n",
        "\n",
        "    return \"foo_bar\", trace_id, span_id\n",
        "\n",
        "# Create a new trace\n",
        "@observe()\n",
        "def main():\n",
        "\n",
        "    _, trace_id, span_id = nested_fn()\n",
        "\n",
        "    return \"main_result\", trace_id, span_id\n",
        "\n",
        "\n",
        "# Flush the trace to send it to the Langfuse platform\n",
        "langfuse_context.flush()\n",
        "\n",
        "# Execute the main function to generate a trace\n",
        "_, trace_id, span_id = main()\n",
        "\n",
        "# Score the trace from outside the trace context\n",
        "langfuse_client.score(\n",
        "    trace_id=trace_id,\n",
        "    name=\"trace-score\",\n",
        "    value=1,\n",
        "    comment=\"I like how personalized the response is\"\n",
        ")\n",
        "\n",
        "# Score the specific span/function from outside the trace context\n",
        "langfuse_client.score(\n",
        "    trace_id=trace_id,\n",
        "    observation_id=span_id,\n",
        "    name=\"span-score\",\n",
        "    value=1,\n",
        "    comment=\"I like how personalized the response is\"\n",
        ");"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> **Example trace**: https://cloud.langfuse.com/project/cloramnkj0002jz088vzn1ja4/traces/0090556d-015c-48cb-bc33-4af29b05af31"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Customize IDs\n",
        "\n",
        "By default, Langfuse assigns random ids to all logged events.\n",
        "\n",
        "If you have your own unique ID (e.g. messageId, traceId, correlationId), you can easily set those as trace or observation IDs for effective lookups in Langfuse.\n",
        "\n",
        "To dynamically set a custom ID for a trace or observation, simply pass a keyword argument `langfuse_observation_id` to the function decorated with `@observe()`. Thereby, the trace/observation in Langfuse will use this id. Note: ids in Langfuse are unique and traces/observations are upserted/merged on these ids."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langfuse.decorators import langfuse_context, observe\n",
        "import uuid\n",
        "\n",
        "@observe()\n",
        "def process_user_request(user_id, request_data, **kwargs):\n",
        "    # Function logic here\n",
        "    pass\n",
        "\n",
        "\n",
        "def main():\n",
        "    user_id = \"user123\"\n",
        "    request_data = {\"action\": \"login\"}\n",
        "\n",
        "    # Custom ID for tracking\n",
        "    custom_observation_id = \"custom-\" + str(uuid.uuid4())\n",
        "\n",
        "    # Pass id as kwarg\n",
        "    process_user_request(\n",
        "        user_id=user_id,\n",
        "        request_data=request_data,\n",
        "        # Pass the custom observation ID to the function\n",
        "        langfuse_observation_id=custom_observation_id,\n",
        "    )\n",
        "\n",
        "main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> **Example trace**: https://cloud.langfuse.com/project/cloramnkj0002jz088vzn1ja4/traces/custom-bbda815f-c61a-4cf5-a545-7fceeef1b635"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Debug mode\n",
        "Enable debug mode to get verbose logs. Set the debug mode via the environment variable `LANGFUSE_DEBUG=True`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Authentication check\n",
        "\n",
        "Use `langfuse_context.auth_check()` to verify that your host and API credentials are valid."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langfuse.decorators import langfuse_context\n",
        "\n",
        "assert langfuse_context.auth_check()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Learn more\n",
        "\n",
        "See Docs and [SDK reference](https://python.reference.langfuse.com/langfuse/decorators) for more details. Questions? Add them on [GitHub Discussions](https://github.com/orgs/langfuse/discussions/categories/support)."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}

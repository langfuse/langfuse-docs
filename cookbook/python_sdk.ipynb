{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nJXInxopjbLA"
   },
   "source": [
    "---\n",
    "description: Fully async and typed Python SDK. Uses Pydantic objects for data verification.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mqBspBzuRk9C"
   },
   "source": [
    "# Python SDK\n",
    "\n",
    "[![PyPI](https://img.shields.io/pypi/v/langfuse?style=flat-square)](https://pypi.org/project/langfuse/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qL7HhNyIYNwn"
   },
   "source": [
    "This is a Python SDK used to send LLM data to Langfuse in a convenient way. It uses a worker Thread and an internal queue to manage requests to the Langfuse backend asynchronously. Hence, the SDK does not impact your latencies and also does not impact your customers in case of exceptions.\n",
    "\n",
    "Using langchain? Use the [langchain integration](https://langfuse.com/docs/langchain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lc6Uxbl3R5El"
   },
   "source": [
    "## 1. Installation\n",
    "\n",
    "The Langfuse SDKs are hosted on the pypi index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F21wZSUyKLzb"
   },
   "outputs": [],
   "source": [
    "%pip install langfuse --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fAupsw1pR_6q"
   },
   "source": [
    "Initialize the client with api keys and optionally your environment. In the example we are using the cloud environment which is also the default. The Python client can modify all entities in the Langfuse API and therefore requires the secret key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iDfYwZf4KUnY"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# get keys for your project from https://cloud.langfuse.com\n",
    "os.environ[\"LANGFUSE_PUBLIC_KEY\"] = \"\"\n",
    "os.environ[\"LANGFUSE_SECRET_KEY\"] = \"\"\n",
    "\n",
    "# your openai key\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
    "\n",
    "# Your host, defaults to https://cloud.langfuse.com\n",
    "# For US data region, set to \"https://us.cloud.langfuse.com\"\n",
    "# os.environ[\"LANGFUSE_HOST\"] = \"http://localhost:3000\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PuPgkTU476y4"
   },
   "outputs": [],
   "source": [
    "from langfuse import Langfuse\n",
    "\n",
    "langfuse = Langfuse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hxlExyUilIPl"
   },
   "outputs": [],
   "source": [
    "# checks the SDK connection with the server.\n",
    "langfuse.auth_check()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ygl4DE2K2Jmz"
   },
   "source": [
    "### Options\n",
    "\n",
    "| Variable |Description   | Default value  \n",
    "| --- | --- | ---\n",
    "| host | Host of the Langfuse API, set to `\"https://us.cloud.langfuse.com\"` for US data region | `\"https://cloud.langfuse.com\"`       \n",
    "| release | The release number/hash of the application to provide analytics grouped by release.\t| `process.env.LANGFUSE_RELEASE` or [common system environment names](https://github.com/langfuse/langfuse-python/blob/main/langfuse/environment.py#L3)\n",
    "| debug | Prints debug logs to the console | `False`\n",
    "| number_of_consumers | Specifies the number of consumer threads to execute network requests to the Langfuse server. Helps scaling the SDK for high load. | 1\n",
    "\n",
    "\n",
    "\n",
    "At the bottom of the document are more detailed explanations for these."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hoybRnFrAi9x"
   },
   "source": [
    "## 2. Record a simple LLM call\n",
    "To record a single call to a LLM, you can use `langfuse.generations()` method from the SDK and provide it with the LLM configuration, prompt and completion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JtMxqNh0Azn-"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "generationStartTime = datetime.now()\n",
    "\n",
    "# call to an LLM API\n",
    "\n",
    "generation = langfuse.generation(\n",
    "    name=\"summary-generation\",\n",
    "    start_time=generationStartTime,\n",
    "    end_time=datetime.now(),\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    model_parameters={\"maxTokens\": \"1000\", \"temperature\": \"0.9\"},\n",
    "    prompt=[{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": \"Please generate a summary of the following documents \\nThe engineering department defined the following OKR goals...\\nThe marketing department defined the following OKR goals...\"}],\n",
    "    completion=\"The Q3 OKRs contain goals for multiple teams...\",\n",
    "    usage={\"input\": 50, \"output\": 49},\n",
    "    metadata={\"interface\": \"whatsapp\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CT4uaBm4SLvw"
   },
   "source": [
    "## 3. Record a more complex application\n",
    "```\n",
    "TRACE\n",
    "|\n",
    "|-- SPAN: Retrieval\n",
    "|   |\n",
    "|   |-- GENERATION: Vector DB Query Creation\n",
    "|   |\n",
    "|   |-- SPAN: Data Fetching\n",
    "|   |\n",
    "|   |-- EVENT: Data Summary Creation\n",
    "|\n",
    "|-- GENERATION: Output Generation\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N-1Ta3wVGGxq"
   },
   "outputs": [],
   "source": [
    "trace = langfuse.trace(name = \"llm-feature\")\n",
    "retrieval = trace.span(name = \"retrieval\")\n",
    "retrieval.generation(name = \"query-creation\")\n",
    "retrieval.span(name = \"vector-db-search\")\n",
    "retrieval.event(name = \"db-summary\")\n",
    "trace.generation(name = \"user-output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x9T9sbBFzm1I"
   },
   "source": [
    "The Langfuse SDK and UI are designed to support very complex LLM features which contain for example vector database searches and multiple LLM calls. For that, it is very convenient to nest or chain the SDK. Understanding a small number of terms makes it easy to integrate with Langfuse.\n",
    "\n",
    "#### Traces\n",
    "A `Trace` represents a single execution of a LLM feature. It is a container for all succeeding objects.\n",
    "#### Observations\n",
    "Each `Trace` can contain multiple `Observations` to record individual steps of an execution. There are different types of `Observations`.\n",
    "  - `Events` are the basic building block. They are used to track discrete events in a `Trace`.\n",
    "  - `Spans` can be used to record steps from a chain like fetching data from a vector databse. You are able to record inputs, outputs and more.\n",
    "  - `Generations` are a specific type of `Spans` which are used to record generations of an AI model. They contain additional metadata about the model and the prompt/completion and are specifically rendered in the langfuse UI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3GjVFk7N9jZr"
   },
   "source": [
    "### Traces\n",
    "\n",
    "Traces are the top-level entity in the Langfuse API. They represent an execution flow in a LLM application usually triggered by an external event.\n",
    "\n",
    "| Parameter | Type   | Optional | Description\n",
    "| --- | --- | --- | ---\n",
    "| id | string | yes | The id of the trace can be set, defaults to a random id. Set it to link traces to external systems or when grouping multiple runs into a single trace (e.g. messages in a chat thread).\n",
    "| name | string | yes | Identifier of the trace. Useful for sorting/filtering in the UI.\n",
    "| input | object | yes | The input of the trace. Can be any JSON object.\n",
    "| output | object | yes | The output of the trace. Can be any JSON object.\n",
    "| metadata | object | yes | Additional metadata of the trace. Can be any JSON object.\n",
    "| userId | string | yes | The id of the user that triggered the execution. Used to provide [user-level analytics](https://langfuse.com/docs/user-explorer).\n",
    "| version | string | yes | The version of the trace type. Used to understand how changes to the trace type affect metrics. Useful in debugging.\n",
    "| release | string | yes | The release identifier of the current deployment. Used to understand how changes of different deployments affect metrics. Useful in debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z9Kxxjp004WD"
   },
   "outputs": [],
   "source": [
    "trace = langfuse.trace(\n",
    "    name = \"docs-retrieval\",\n",
    "    user_id = \"user__935d7d1d-8625-4ef4-8651-544613e7bd22\",\n",
    "    metadata = {\n",
    "        \"env\": \"production\",\n",
    "        \"email\": \"user@langfuse.com\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UtWxwt3H90qF"
   },
   "source": [
    "### Span\n",
    "\n",
    "Spans represent durations of units of work in a trace. We generated convenient SDK functions for generic spans to support your use cases such as Agent tool usages.\n",
    "\n",
    "| Parameter | Type   | Optional | Description\n",
    "| --- | --- | --- | ---\n",
    "| id | string | yes | The id of the span can be set, otherwise a random id is generated.\n",
    "| startTime | datetime.datetime | yes | The time at which the span started, defaults to the current time.\n",
    "| endTime | datetime.datetime | yes | The time at which the span ended.\n",
    "| name | string | yes | Identifier of the span. Useful for sorting/filtering in the UI.\n",
    "| metadata | object | yes | Additional metadata of the span. Can be any JSON object.\n",
    "| level | string | yes | The level of the span. Can be `DEBUG`, `DEFAULT`, `WARNING` or `ERROR`. Used for sorting/filtering of traces with elevated error levels and for highlighting in the UI.\n",
    "| statusMessage | string | yes | The status message of the span. Additional field for context of the event. E.g. the error message of an error event.\n",
    "| input | object | yes | The input to the span. Can be any JSON object.\n",
    "| output | object | yes | The output to the span. Can be any JSON object.\n",
    "| version | string | yes | The version of the span type. Used to understand how changes to the span type affect metrics. Useful in debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "otJQPNC198Ti"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "retrievalStartTime = datetime.now()\n",
    "\n",
    "# retrieveDocs = retrieveDoc()\n",
    "# ...\n",
    "\n",
    "span = trace.span(\n",
    "    name=\"embedding-search\",\n",
    "    start_time=retrievalStartTime,\n",
    "    end_time=datetime.now(),\n",
    "    metadata={\"database\": \"pinecone\"},\n",
    "    input = {'query': 'This document entails the OKR goals for ACME'},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NZ7qCdGAOYvc"
   },
   "source": [
    "Spans can be updated once your function completes for example record outputs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r7I_JwbxOd82"
   },
   "outputs": [],
   "source": [
    "span = span.update(\n",
    "    output = {\"response\": \"[{'name': 'OKR Engineering', 'content': 'The engineering department defined the following OKR goals...'},{'name': 'OKR Marketing', 'content': 'The marketing department defined the following OKR goals...'}]\"}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uNPQH8Nz-duo"
   },
   "source": [
    "### Generation\n",
    "\n",
    "Generations are used to log generations of AI model. They contain additional metadata about the model and the prompt/completion and are specifically rendered in the langfuse UI.\n",
    "\n",
    "\n",
    "| Parameter | Type   | Optional | Description\n",
    "| --- | --- | --- | ---\n",
    "| id | string | yes | The id of the generation can be set, defaults to random id.\n",
    "| name | string | yes | Identifier of the generation. Useful for sorting/filtering in the UI.\n",
    "| startTime | datetime.datetime | yes | The time at which the generation started, defaults to the current time.\n",
    "| completionStartTime | datetime.datetime | yes | The time at which the completion started (streaming). Set it to get latency analytics broken down into time until completion started and completion duration.\n",
    "| endTime | datetime.datetime | yes | The time at which the generation ended.\n",
    "| model | string | yes | The name of the model used for the generation.\n",
    "| modelParameters | object | yes | The parameters of the model used for the generation; can be any key-value pairs.\n",
    "| prompt | object | yes | The prompt used for the generation; can be any string or JSON object (recommended for chat models or other models that use structured input).\n",
    "| completion | string | yes | The completion generated by the model.\n",
    "| usage | object | yes | The usage object supports the OpenAi structure with (`promptTokens`, `completionTokens`, `totalTokens`) and a more generalistic version (`input`, `output`, `total`, `unit`) where unit can be of value `TOKENS` (default) or `CHARACTERS`. For some models the token counts are [automatically calculated](https://langfuse.com/docs/token-usage) by Langfuse.\n",
    "| metadata | object | yes | Additional metadata of the generation. Can be any JSON object.\n",
    "| level | string | yes | The level of the generation. Can be `DEBUG`, `DEFAULT`, `WARNING` or `ERROR`. Used for sorting/filtering of traces with elevated error levels and for highlighting in the UI.\n",
    "| statusMessage | string | yes | The status message of the generation. Additional field for context of the event. E.g. the error message of an error event.\n",
    "| version | string | yes | The version of the generation type. Used to understand how changes to the span type affect metrics. Useful in debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nJfTbXvNQ6iD"
   },
   "outputs": [],
   "source": [
    "\n",
    "from datetime import datetime\n",
    "\n",
    "generationStartTime = datetime.now()\n",
    "\n",
    "# chat_completion = openai.ChatCompletion.create(model=\"gpt-3.5-turbo\", messages=[{\"role\": \"user\", \"content\": \"Hello world\"}])\n",
    "# ...\n",
    "\n",
    "generation = trace.generation(\n",
    "    name=\"summary-generation\",\n",
    "    start_time=generationStartTime,\n",
    "    end_time=datetime.now(),\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    model_parameters={\"maxTokens\": \"1000\", \"temperature\": \"0.9\"},\n",
    "    prompt=[{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": \"Please generate a summary of the following documents \\nThe engineering department defined the following OKR goals...\\nThe marketing department defined the following OKR goals...\"}],\n",
    "    metadata={\"interface\": \"whatsapp\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cYb2kvAMOqHg"
   },
   "source": [
    "Generations can be updated once your LLM function completes for example record outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WQccImMOOz8F"
   },
   "outputs": [],
   "source": [
    "generation.update(\n",
    "    completion=\"The Q3 OKRs contain goals for multiple teams...\",\n",
    "    usage= {\"input\": 50, \"output\": 49},\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wfzAYslz9Aks"
   },
   "source": [
    "### Events\n",
    "\n",
    "Events are used to track discrete events in a trace.\n",
    "\n",
    "| Parameter | Type   | Optional | Description\n",
    "| --- | --- | --- | ---\n",
    "| id | string | yes | The id of the event can be set, otherwise a random id is generated.\n",
    "| startTime | datetime.datetime | yes | The time at which the event started, defaults to the current time.\n",
    "| name | string | yes | Identifier of the event. Useful for sorting/filtering in the UI.\n",
    "| metadata | object | yes | Additional metadata of the event. Can be any JSON object.\n",
    "| level | string | yes | The level of the event. Can be `DEBUG`, `DEFAULT`, `WARNING` or `ERROR`. Used for sorting/filtering of traces with elevated error levels and for highlighting in the UI.\n",
    "| statusMessage | string | yes | The status message of the event. Additional field for context of the event. E.g. the error message of an error event.\n",
    "| input | object | yes | The input to the event. Can be any JSON object.\n",
    "| output | object | yes | The output to the event. Can be any JSON object.\n",
    "| version | string | yes | The version of the event type. Used to understand how changes to the event type affect metrics. Useful in debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tuSjykFW9Iw1"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "event = span.event(\n",
    "    name=\"chat-docs-retrieval\",\n",
    "    start_time=datetime.now(),\n",
    "    metadata={\"key\": \"value\"},\n",
    "    input = {\"key\": \"value\"},\n",
    "    output = {\"key\": \"value\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EABnZymiSej8"
   },
   "source": [
    "## 3. Scores\n",
    "\n",
    "[Scores](https://langfuse.com/docs/scores) are used to evaluate single executions/traces. They can created manually via the Langfuse UI or via the SDKs.\n",
    "\n",
    "If the score relates to a specific step of the trace, specify the `observationId`.\n",
    "\n",
    "| Parameter | Type   | Optional | Description\n",
    "| --- | --- | --- | ---\n",
    "| traceId | string | no | The id of the trace to which the score should be attached. Automatically set if you use `{trace,generation,span,event}.score({})`\n",
    "| observationId | string | yes | The id of the observation to which the score should be attached. Automatically set if you use `{generation,span,event}.score({})`\n",
    "| name | string | no | Identifier of the score.\n",
    "| value | number | no | The value of the score. Can be any number, often standardized to 0..1\n",
    "| comment | string | yes | Additional context/explanation of the score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mj19Zby3SfT9"
   },
   "outputs": [],
   "source": [
    "# via {trace, span, event, generation}.score\n",
    "trace.score(\n",
    "    name=\"user-explicit-feedback\",\n",
    "    value=1,\n",
    "    comment=\"I like how personalized the response is\"\n",
    ")\n",
    "\n",
    "# using the trace_id\n",
    "trace_id = trace.id\n",
    "langfuse.score(\n",
    "    trace_id=trace.id,\n",
    "    name=\"user-explicit-feedback\",\n",
    "    value=1,\n",
    "    comment=\"I like how personalized the response is\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9gHGMs9QINYG"
   },
   "source": [
    "## Additional configurations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-q5aljyIoU42"
   },
   "source": [
    "### Shutdown behavior\n",
    "\n",
    "The Langfuse SDK executes network requests in the background on a separate thread for better performance of your application. This can lead to lost events in short lived environments like NextJs cloud functions or AWS Lambda functions when the Python process is terminated before the SDK sent all events to our backend.\n",
    "\n",
    "To avoid this, ensure that the `langfuse.flush()` function is called before termination. This method is waiting for all tasks to have completed, hence it is blocking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5jpWEosnINa4"
   },
   "outputs": [],
   "source": [
    "langfuse.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kNI9WYgD0Cj8"
   },
   "source": [
    "### Releases and versions\n",
    "\n",
    "You might want to track releases in Langfuse to understand with which Software release a given Trace was generated. This can be done by either providing the environment variable `LANGFUSE_RELEASE` or instantiating the client with the release."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aqhmzp-V0e9k"
   },
   "outputs": [],
   "source": [
    "# The SDK will automatically include the env variable.\n",
    "os.environ[\"LANGFUSE_RELEASE\"] = \"ba7816b...\" # <- example, github sha\n",
    "\n",
    "# Alternatively, use the constructor of the SDK\n",
    "langfuse = Langfuse(release='ba7816b')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QWzxwJo91eBM"
   },
   "source": [
    "Apart from Software releases, users want to track versions of LLM apps (e.g. Prompt versions). For this, each `Generation`, `Span`, or `Event` has a version field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5LW8mFVV1cQq"
   },
   "outputs": [],
   "source": [
    "langfuse.span(name=\"retrieval\", version=\"<version>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1EmkTuP2ugkx"
   },
   "source": [
    "### Debug\n",
    "Enable debug mode to get verbose logs. Alternatively, set the debug mode via the environment variable `LANGFUSE_DEBUG`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Stj71r61uah5"
   },
   "outputs": [],
   "source": [
    "langfuse = Langfuse(debug=True)\n",
    "\n",
    "# Deactivating for the rest of the notebook\n",
    "langfuse = Langfuse()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upgrading from v1.x.x to v2.x.x\n",
    "\n",
    "### Removing Pydantic interfaces\n",
    "We want to make the SDK as easy to use as possible. Therefore, we removed the Pydantic interfaces and replaced them with simple function parameters. This makes it easier to integrate the SDK with your existing codebase without the need to import objects from our SDK.\n",
    "\n",
    "**Old**\n",
    "```python\n",
    "from langfuse.model import CreateTrace\n",
    "\n",
    "langfuse.trace(CreateTrace(name=\"My Trace\"))\n",
    "```\n",
    "\n",
    "**New**\n",
    "```python\n",
    "langfuse.trace(name=\"My Trace\")\n",
    "```\n",
    "\n",
    "Removing Pydantic objects means, we also removed Pydantic enums. Instead, we use strings.\n",
    "\n",
    "**Old**\n",
    "```python\n",
    "from langfuse.model import InitialGeneration\n",
    "from langfuse.api.resources.commons.types.observation_level import ObservationLevel\n",
    "\n",
    "langfuse.generation(InitialGeneration(level=ObservationLevel.ERROR))\n",
    "```\n",
    "\n",
    "**New**\n",
    "```python\n",
    "langfuse.generation(level=\"ERROR\")\n",
    "```\n",
    "\n",
    "All inserted parameters are validated on function call and print errors if the validation fails. No exceptions are thrown.\n",
    "\n",
    "### Renamings on Generations\n",
    "We decided to rename `prompt` and `completion` to `input` and `output` to be more consistent with the rest of the Langfuse API.\n",
    "\n",
    "### Snake case parameters\n",
    "\n",
    "As of now, all parameters are snake case. This means, that parameters such as `startTime` are now `start_time`. This is to ensure consistency with the rest of the Python ecosystem.\n",
    "\n",
    "### Flexible usage objects on Generations\n",
    "\n",
    "We wanted to make the SDK more flexible to allow you to ingest any kinds of usage while maintaining simplicity when using OpenAI. Therefore, we removed the `LlmUsage` Pydantic model and allow users to pass two different types of usage objects.\n",
    "\n",
    "**Old**\n",
    "```python\n",
    "\n",
    "from langfuse.model import InitialGeneration, Usage\n",
    "\n",
    " langfuse.generation(\n",
    "    InitialGeneration(\n",
    "        name=\"my-generation\",\n",
    "        usage=Usage(promptTokens=50, completionTokens=49),\n",
    "    )\n",
    ")\n",
    "```\n",
    "\n",
    "**New**\n",
    "\n",
    "Our new usage object supports the OpenAi structure with (`promptTokens`, `completionTokens`, `totalTokens`) and a more generalistic version (`input`, `output`, `total`, `unit`) where unit can be of value `TOKENS` (default) or `CHARACTERS`. This allows to ingest character based usage models as well. Rech out to us if you need more units.\n",
    "\n",
    "```python\n",
    "\n",
    "langfuse.generation(\n",
    "    name=\"my-openai-generation\",\n",
    "    usage={\"promptTokens\": 50, \"completionTokens\": 49, \"totalTokens\": 99}, # defaults to \"TOKENS\" unit\n",
    ")\n",
    "\n",
    "langfuse.generation(\n",
    "    name=\"my-claude-generation\",\n",
    "    usage={\"input\": 50, \"output\": 49, \"total\": 99, \"unit\": \"CHARACTERS\"}, # unit defaults to \"TOKENS\" unit if not set\n",
    ")\n",
    "\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-GcLUd6_JXWH"
   },
   "source": [
    "## FastAPI\n",
    "For engineers working with FastAPI, we have a short example, of how to use it there. [Here](https://github.com/langfuse/fastapi_demo) is a Git Repo with all the details.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V9WHoZAmKEgO"
   },
   "outputs": [],
   "source": [
    "%pip install fastapi --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "COd5Q_67KMqU"
   },
   "source": [
    "Here is an example of how to initialise FastAPI and register the `langfuse.flush()` method to run at shutdown.\n",
    "With this, your Python environment will only terminate once Langfuse received all the events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nu2ug2AoJ8d8"
   },
   "outputs": [],
   "source": [
    "from contextlib import asynccontextmanager\n",
    "from fastapi import FastAPI, Query, BackgroundTasks\n",
    "\n",
    "@asynccontextmanager\n",
    "async def lifespan(app: FastAPI):\n",
    "    # Operation on startup\n",
    "\n",
    "    yield  # wait until shutdown\n",
    "\n",
    "    # Flush all events to be sent to Langfuse on shutdown and terminate all Threads gracefully. This operation is blocking.\n",
    "    langfuse.flush()\n",
    "\n",
    "\n",
    "app = FastAPI(lifespan=lifespan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L91osyXKKZIn"
   },
   "outputs": [],
   "source": [
    "langfuse = Langfuse()\n",
    "\n",
    "@app.get(\"/generate/\",tags=[\"APIs\"])\n",
    "async def campaign(prompt: str = Query(..., max_length=20)):\n",
    "  # call to a LLM\n",
    "  generation = langfuse.generation(\n",
    "      name=\"llm-feature\", \n",
    "      metadata=\"test\", \n",
    "      prompt=prompt\n",
    "  )\n",
    "  return True"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nJXInxopjbLA"
      },
      "source": [
        "---\n",
        "title: Python SDK (Low-level)\n",
        "description: Fully async and typed Python SDK. Uses Pydantic objects for data verification.\n",
        "category: SDKs\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mqBspBzuRk9C"
      },
      "source": [
        "# Python SDK (Low-level)\n",
        "\n",
        "[![PyPI](https://img.shields.io/pypi/v/langfuse?style=flat-square)](https://pypi.org/project/langfuse/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qL7HhNyIYNwn"
      },
      "source": [
        "This is a Python SDK used to send LLM data to Langfuse in a convenient way. It uses a worker Thread and an internal queue to manage requests to the Langfuse backend asynchronously. Hence, the SDK adds only minimal latency to your application.\n",
        "\n",
        "For most use cases, you should check out the [decorator-based SDK](http://langfuse.com/docs/sdk/python/decorators), which is more convenient and easier to use. This SDK is more low-level and is only recommended if you need more control over the request process."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lc6Uxbl3R5El"
      },
      "source": [
        "## Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F21wZSUyKLzb"
      },
      "outputs": [],
      "source": [
        "%pip install langfuse --upgrade"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fAupsw1pR_6q"
      },
      "source": [
        "## Initialize Client\n",
        "\n",
        "To start, initialize the client by providing your credentials. You can set the credentials either as environment variables or constructor arguments.\n",
        "\n",
        "If you are self-hosting Langfuse or using the US data region, don't forget to configure `LANGFUSE_HOST`.\n",
        "\n",
        "To verify your credentials and host, use the `langfuse.auth_check()` function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "iDfYwZf4KUnY"
      },
      "outputs": [],
      "source": [
        " import os\n",
        "\n",
        "# get keys for your project from https://cloud.langfuse.com\n",
        "os.environ[\"LANGFUSE_PUBLIC_KEY\"] = \"\"\n",
        "os.environ[\"LANGFUSE_SECRET_KEY\"] = \"\"\n",
        "\n",
        "# your openai key\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
        "\n",
        "# Your host, defaults to https://cloud.langfuse.com\n",
        "# For US data region, set to \"https://us.cloud.langfuse.com\"\n",
        "# os.environ[\"LANGFUSE_HOST\"] = \"http://localhost:3000\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "PuPgkTU476y4"
      },
      "outputs": [],
      "source": [
        "from langfuse import Langfuse\n",
        "\n",
        "langfuse = Langfuse()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oj14lhVL1FU7"
      },
      "source": [
        "| Environment, Variable | Description   | Default value  \n",
        "| --- | --- | ---\n",
        "| `LANGFUSE_PUBLIC_KEY`, `public_key` | Public key, get in project settings |\n",
        "| `LANGFUSE_SECRET_KEY`, `secret_key` | Secret key, get in project settings |\n",
        "| `LANGFUSE_HOST`, `host` | Host of the Langfuse API | `\"https://cloud.langfuse.com\"`       \n",
        "| `LANGFUSE_RELEASE`, `release` | Optional. The release number/hash of the application to provide analytics grouped by release.\t| [common system environment names](https://github.com/langfuse/langfuse-python/blob/main/langfuse/environment.py#L3)\n",
        "| `LANGFUSE_DEBUG`, `debug` | Optional. Prints debug logs to the console | `False`\n",
        "| n/a, `threads` | Specifies the number of consumer threads to execute network requests to the Langfuse server. Helps scaling the SDK for high load. Only increase this if you run into scaling issues. | 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CT4uaBm4SLvw"
      },
      "source": [
        "## Tracing\n",
        "\n",
        "The Langfuse SDK and UI are designed to support complex LLM features which contain for example vector database searches and multiple LLM calls. For that, it is very convenient to nest or chain the SDK. Understanding a small number of terms makes it easy to integrate with Langfuse.\n",
        "\n",
        "**Traces**\n",
        "\n",
        "A `Trace` represents a single execution of a LLM feature. It is a container for all succeeding objects.\n",
        "\n",
        "**Observations**\n",
        "\n",
        "Each `Trace` can contain multiple `Observations` to record individual steps of an execution. There are different types of `Observations`:\n",
        "  - `Events` are the basic building block. They are used to track discrete events in a `Trace`.\n",
        "  - `Spans` track time periods and include an end_time.\n",
        "  - `Generations` are a specific type of `Spans` which are used to record generations of an AI model. They contain additional metadata about the model, LLM token and cost tracking, and the prompt/completions are specifically rendered in the langfuse UI.\n",
        "  \n",
        "\n",
        "_Example_\n",
        "  \n",
        "```\n",
        "TRACE\n",
        "|\n",
        "|-- SPAN: Retrieval\n",
        "|   |\n",
        "|   |-- GENERATION: Vector DB Query Creation\n",
        "|   |\n",
        "|   |-- SPAN: Data Fetching\n",
        "|   |\n",
        "|   |-- EVENT: Data Summary Creation\n",
        "|\n",
        "|-- GENERATION: Output Generation\n",
        "```\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "N-1Ta3wVGGxq"
      },
      "outputs": [],
      "source": [
        "trace = langfuse.trace(name = \"llm-feature\")\n",
        "retrieval = trace.span(name = \"retrieval\")\n",
        "retrieval.generation(name = \"query-creation\")\n",
        "retrieval.span(name = \"vector-db-search\")\n",
        "retrieval.event(name = \"db-summary\")\n",
        "trace.generation(name = \"user-output\");"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3GjVFk7N9jZr"
      },
      "source": [
        "### Traces\n",
        "\n",
        "Traces are the top-level entity in the Langfuse API. They represent an execution flow in a LLM application usually triggered by an external event.\n",
        "\n",
        "| Parameter | Type   | Optional | Description\n",
        "| --- | --- | --- | ---\n",
        "| id | string | yes | The id of the trace can be set, defaults to a random id. Set it to link traces to external systems or when creating a distributed trace. Traces are upserted on id.\n",
        "| name | string | yes | Identifier of the trace. Useful for sorting/filtering in the UI.\n",
        "| input | object | yes | The input of the trace. Can be any JSON object.\n",
        "| output | object | yes | The output of the trace. Can be any JSON object.\n",
        "| metadata | object | yes | Additional metadata of the trace. Can be any JSON object. Metadata is merged when being updated via the API.\n",
        "| user_id | string | yes | The id of the user that triggered the execution. Used to provide [user-level analytics](https://langfuse.com/docs/tracing/users).\n",
        "| session_id | string| yes | Used to group multiple traces into a [session](https://langfuse.com/docs/tracing/sessions) in Langfuse. Use your own session/thread identifier.\n",
        "| version | string | yes | The version of the trace type. Used to understand how changes to the trace type affect metrics. Useful in debugging.\n",
        "| release | string | yes | The release identifier of the current deployment. Used to understand how changes of different deployments affect metrics. Useful in debugging.\n",
        "| tags | string[] | yes | Tags are used to categorize or label traces. Traces can be filtered by tags in the UI and GET API. Tags can also be changed in the UI. Tags are merged and never deleted via the API. |\n",
        "| public | boolean | yes | You can make a trace `public` to share it via a [public link](https://langfuse.com/docs/tracing/). This allows others to view the trace without needing to log in or be members of your Langfuse project. |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Z9Kxxjp004WD"
      },
      "outputs": [],
      "source": [
        "trace = langfuse.trace(\n",
        "    name = \"docs-retrieval\",\n",
        "    user_id = \"user__935d7d1d-8625-4ef4-8651-544613e7bd22\",\n",
        "    metadata = {\n",
        "        \"email\": \"user@langfuse.com\",\n",
        "    },\n",
        "    tags = [\"production\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_HE5-a-v1FU7"
      },
      "source": [
        "Traces can be updated:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Mjz4YeN1FU7"
      },
      "outputs": [],
      "source": [
        "# option 1: using trace object\n",
        "trace.update(\n",
        "    input=\"Hi there\"\n",
        ")\n",
        "\n",
        "# option 2: via trace_id, trace is upserted on id\n",
        "langfuse.trace(id=trace.id, output=\"Hi ðŸ‘‹\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-G3dQZ81FU7"
      },
      "source": [
        "You can get the url of a trace in the Langfuse interface. Helpful in interactive use or when adding this url to your logs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xNvAliqD1FU7"
      },
      "outputs": [],
      "source": [
        "trace.get_trace_url()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UtWxwt3H90qF"
      },
      "source": [
        "### Span\n",
        "\n",
        "Spans represent durations of units of work in a trace.\n",
        "\n",
        "Parameters of `langfuse.span()`:\n",
        "\n",
        "| Parameter | Type   | Optional | Description\n",
        "| --- | --- | --- | ---\n",
        "| id | string | yes | The id of the span can be set, otherwise a random id is generated. Spans are upserted on id.\n",
        "| start_time | datetime.datetime | yes | The time at which the span started, defaults to the current time.\n",
        "| end_time | datetime.datetime | yes | The time at which the span ended. Automatically set by `span.end()`.\n",
        "| name | string | yes | Identifier of the span. Useful for sorting/filtering in the UI.\n",
        "| metadata | object | yes | Additional metadata of the span. Can be any JSON object. Metadata is merged when being updated via the API.\n",
        "| level | string | yes | The level of the span. Can be `DEBUG`, `DEFAULT`, `WARNING` or `ERROR`. Used for sorting/filtering of traces with elevated error levels and for highlighting in the UI.\n",
        "| status_message | string | yes | The status message of the span. Additional field for context of the event. E.g. the error message of an error event.\n",
        "| input | object | yes | The input to the span. Can be any JSON object.\n",
        "| output | object | yes | The output to the span. Can be any JSON object.\n",
        "| version | string | yes | The version of the span type. Used to understand how changes to the span type affect metrics. Useful in debugging."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MPIs1q5i1FU8"
      },
      "source": [
        "Use trace or observation objects to create child spans:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "otJQPNC198Ti"
      },
      "outputs": [],
      "source": [
        "# create span, sets start_time\n",
        "span = trace.span(\n",
        "    name=\"embedding-search\",\n",
        "    metadata={\"database\": \"pinecone\"},\n",
        "    input = {'query': 'This document entails the OKR goals for ACME'},\n",
        ")\n",
        "\n",
        "# function, mocked\n",
        "# retrieved_documents = retrieveDoc()\n",
        "retrieved_documents = {\"response\": \"[{'name': 'OKR Engineering', 'content': 'The engineering department defined the following OKR goals...'},{'name': 'OKR Marketing', 'content': 'The marketing department defined the following OKR goals...'}]\"}\n",
        "\n",
        "# update span and sets end_time\n",
        "span.end(\n",
        "    output=retrieved_documents\n",
        ");"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yQTAUDAM1FU8"
      },
      "source": [
        "Other span methods:\n",
        "- `span.update()`, does not change end_time if not explicitly set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CW68EXIE1FU8"
      },
      "source": [
        "Alternatively, if using the Langfuse objects is not convenient, you can use the `langfuse` client, `trace_id` and (optionally) `parent_observation_id` to create spans, and `id` to upsert a span."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1jyEim7w1FU8"
      },
      "outputs": [],
      "source": [
        "trace_id = trace.id\n",
        "\n",
        "# create span\n",
        "span = langfuse.span(\n",
        "    trace_id=trace_id,\n",
        "    name=\"initial name\"\n",
        ")\n",
        "\n",
        "# update span, upserts on id\n",
        "langfuse.span(\n",
        "    id=span.id,\n",
        "    name=\"updated name\"\n",
        ")\n",
        "\n",
        "# create new nested span\n",
        "langfuse.span(\n",
        "    trace_id=trace_id,\n",
        "    parent_observation_id=span.id,\n",
        "    name=\"nested span\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uNPQH8Nz-duo"
      },
      "source": [
        "### Generation\n",
        "\n",
        "Generations are used to log generations of AI models. They contain additional metadata about the model, the prompt/completion, the cost of executing the model and are specifically rendered in the langfuse UI.\n",
        "\n",
        "\n",
        "| Parameter | Type   | Optional | Description\n",
        "| --- | --- | --- | ---\n",
        "| id | string | yes | The id of the generation can be set, defaults to random id.\n",
        "| name | string | yes | Identifier of the generation. Useful for sorting/filtering in the UI.\n",
        "| start_time | datetime.datetime | yes | The time at which the generation started, defaults to the current time.\n",
        "| completion_start_time | datetime.datetime | yes | The time at which the completion started (streaming). Set it to get latency analytics broken down into time until completion started and completion duration.\n",
        "| end_time | datetime.datetime | yes | The time at which the generation ended. Automatically set by `generation.end()`.\n",
        "| model | string | yes | The name of the model used for the generation.\n",
        "| model_parameters | object | yes | The parameters of the model used for the generation; can be any key-value pairs.\n",
        "| input | object | yes | The prompt used for the generation. Can be any string or JSON object.\n",
        "| output | string | yes | The completion generated by the model. Can be any string or JSON object.\n",
        "| usage | object | yes | The usage object supports the OpenAi structure with {`promptTokens`, `completionTokens`, `totalTokens`} and a more generic version {`input`, `output`, `total`, `unit`, `inputCost`, `outputCost`, `totalCost`} where unit can be of value `\"TOKENS\"`, `\"CHARACTERS\"`, `\"MILLISECONDS\"`, `\"SECONDS\"`, or `\"IMAGES\"`. Refer to the docs on how to [automatically infer](https://langfuse.com/docs/model-usage-and-cost) token usage and costs in Langfuse.\n",
        "| metadata | object | yes | Additional metadata of the generation. Can be any JSON object. Metadata is merged when being updated via the API.\n",
        "| level | string | yes | The level of the generation. Can be `DEBUG`, `DEFAULT`, `WARNING` or `ERROR`. Used for sorting/filtering of traces with elevated error levels and for highlighting in the UI.\n",
        "| status_message | string | yes | The status message of the generation. Additional field for context of the event. E.g. the error message of an error event.\n",
        "| version | string | yes | The version of the generation type. Used to understand how changes to the span type affect metrics. Useful in debugging."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XfAgfVUh1FU8"
      },
      "source": [
        "Use trace or observation objects to create child generations:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "nJfTbXvNQ6iD"
      },
      "outputs": [],
      "source": [
        "# creates generation\n",
        "generation = trace.generation(\n",
        "    name=\"summary-generation\",\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    model_parameters={\"maxTokens\": \"1000\", \"temperature\": \"0.9\"},\n",
        "    input=[{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": \"Please generate a summary of the following documents \\nThe engineering department defined the following OKR goals...\\nThe marketing department defined the following OKR goals...\"}],\n",
        "    metadata={\"interface\": \"whatsapp\"}\n",
        ")\n",
        "\n",
        "# execute model, mocked here\n",
        "# chat_completion = openai.ChatCompletion.create(model=\"gpt-3.5-turbo\", messages=[{\"role\": \"user\", \"content\": \"Hello world\"}])\n",
        "chat_completion = {\n",
        "    \"completion\":\"The Q3 OKRs contain goals for multiple teams...\",\n",
        "    \"usage\":{\"input\": 50, \"output\": 49, \"unit\":\"TOKENS\"}\n",
        "}\n",
        "\n",
        "# update span and sets end_time\n",
        "generation.end(\n",
        "    output=chat_completion[\"completion\"],\n",
        "    usage=chat_completion[\"usage\"],\n",
        ");"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cYb2kvAMOqHg"
      },
      "source": [
        "Other generation methods:\n",
        "- `generation.update()`, does not change end_time if not explicitly set\n",
        "\n",
        "See documentation of spans above on how to use the langfuse client and ids if you cannot use the Langfuse objects to trace your application. This also fully applies to generations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wfzAYslz9Aks"
      },
      "source": [
        "### Events\n",
        "\n",
        "Events are used to track discrete events in a trace.\n",
        "\n",
        "| Parameter | Type   | Optional | Description\n",
        "| --- | --- | --- | ---\n",
        "| id | string | yes | The id of the event can be set, otherwise a random id is generated.\n",
        "| start_time | datetime.datetime | yes | The time at which the event started, defaults to the current time.\n",
        "| name | string | yes | Identifier of the event. Useful for sorting/filtering in the UI.\n",
        "| metadata | object | yes | Additional metadata of the event. Can be any JSON object. Metadata is merged when being updated via the API.\n",
        "| level | string | yes | The level of the event. Can be `DEBUG`, `DEFAULT`, `WARNING` or `ERROR`. Used for sorting/filtering of traces with elevated error levels and for highlighting in the UI.\n",
        "| status_message | string | yes | The status message of the event. Additional field for context of the event. E.g. the error message of an error event.\n",
        "| input | object | yes | The input to the event. Can be any JSON object.\n",
        "| output | object | yes | The output to the event. Can be any JSON object.\n",
        "| version | string | yes | The version of the event type. Used to understand how changes to the event type affect metrics. Useful in debugging."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pz_wX4Wm1FU8"
      },
      "source": [
        "Use trace or observation objects to create child generations:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tuSjykFW9Iw1"
      },
      "outputs": [],
      "source": [
        "event = span.event(\n",
        "    name=\"chat-docs-retrieval\",\n",
        "    metadata={\"key\": \"value\"},\n",
        "    input = {\"key\": \"value\"},\n",
        "    output = {\"key\": \"value\"}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_zhZL7Sw1FU8"
      },
      "source": [
        "See documentation of spans above on how to use the langfuse client and ids if you cannot use the Langfuse objects to trace your application. This also fully applies to events."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EABnZymiSej8"
      },
      "source": [
        "## Scores\n",
        "\n",
        "[Scores](https://langfuse.com/docs/scores/overview) are used to evaluate single executions/traces. They can created manually via the Langfuse UI or via the SDKs.\n",
        "\n",
        "If the score relates to a specific step of the trace, specify the `observation_id`.\n",
        "\n",
        "| Parameter | Type   | Optional | Description\n",
        "| --- | --- | --- | ---\n",
        "| trace_id | string | no | The id of the trace to which the score should be attached. Automatically set if you use `{trace,generation,span,event}.score({})`\n",
        "| observation_id | string | yes | The id of the observation to which the score should be attached. Automatically set if you use `{generation,span,event}.score({})`\n",
        "| name | string | no | Identifier of the score.\n",
        "| value | number | no | The value of the score. Can be any number, often standardized to 0..1\n",
        "| comment | string | yes | Additional context/explanation of the score."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mj19Zby3SfT9"
      },
      "outputs": [],
      "source": [
        "# via {trace, span, event, generation}.score\n",
        "trace.score(\n",
        "    name=\"user-explicit-feedback\",\n",
        "    value=1,\n",
        "    comment=\"I like how personalized the response is\",\n",
        ")\n",
        "\n",
        "# using the trace.id\n",
        "langfuse.score(\n",
        "    trace_id=trace.id,\n",
        "    name=\"user-explicit-feedback\",\n",
        "    value=1,\n",
        "    comment=\"I like how personalized the response is\"\n",
        ")\n",
        "\n",
        "# scoring a specific observation\n",
        "langfuse.score(\n",
        "    trace_id=trace.id,\n",
        "    observation_id=span.id,\n",
        "    name=\"user-explicit-feedback\",\n",
        "    value=1,\n",
        "    comment=\"I like how personalized the response is\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9gHGMs9QINYG"
      },
      "source": [
        "## Additional configurations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-q5aljyIoU42"
      },
      "source": [
        "### Shutdown behavior\n",
        "\n",
        "The Langfuse SDK executes network requests in the background on a separate thread for better performance of your application. This can lead to lost events in short lived environments like NextJs cloud functions or AWS Lambda functions when the Python process is terminated before the SDK sent all events to our backend.\n",
        "\n",
        "To avoid this, ensure that the `langfuse.flush()` function is called before termination. This method is waiting for all tasks to have completed, hence it is blocking."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5jpWEosnINa4"
      },
      "outputs": [],
      "source": [
        "langfuse.flush()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kNI9WYgD0Cj8",
        "tags": []
      },
      "source": [
        "### Releases and versions\n",
        "\n",
        "Track `releases` in Langfuse to relate traces in Langfuse with the versioning of your application. This can be done by either providing the environment variable `LANGFUSE_RELEASE`, instantiating the client with the release, or setting it as a trace parameter.\n",
        "\n",
        "If no release is set, this defaults to [common system environment names](https://github.com/langfuse/langfuse-python/blob/main/langfuse/environment.py#L3)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aqhmzp-V0e9k"
      },
      "outputs": [],
      "source": [
        "# The SDK will automatically include the env variable.\n",
        "os.environ[\"LANGFUSE_RELEASE\"] = \"ba7816b...\" # <- example, github sha\n",
        "\n",
        "# Alternatively, use the constructor of the SDK\n",
        "langfuse = Langfuse(release=\"ba7816b\")\n",
        "\n",
        "# Alternatively, set it when creating a trace\n",
        "langfuse.trace(release=\"ba7816b\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QWzxwJo91eBM"
      },
      "source": [
        "To track versions of individual pieces of you application apart from releases, use the `version` parameter on all observations. This is for example useful to track the effect of changed prompts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5LW8mFVV1cQq"
      },
      "outputs": [],
      "source": [
        "# works the same for spans, generations, events\n",
        "langfuse.span(name=\"retrieval\", version=\"<version>\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nIDWEDyj1FVA"
      },
      "source": [
        "## Troubleshooting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1EmkTuP2ugkx"
      },
      "source": [
        "### Debug mode\n",
        "Enable debug mode to get verbose logs.\n",
        "\n",
        "```python\n",
        "langfuse = Langfuse(debug=True)\n",
        "```\n",
        "\n",
        "Alternatively, set the debug mode via the environment variable `LANGFUSE_DEBUG=True`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h0jvkaIW1FVA"
      },
      "source": [
        "### Configuration/authentication problems\n",
        "\n",
        "Use auth_check() to verify that your host and api credentials are correct."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LLIJ7Wrh1FVA"
      },
      "outputs": [],
      "source": [
        "langfuse.auth_check()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LG-3FsBX1FVA"
      },
      "source": [
        "## Upgrading from v1.x.x to v2.x.x\n",
        "\n",
        "v2 is a major release with breaking changes to simplify the SDK and make it more consistent. We recommend to upgrade to v2 as soon as possible.\n",
        "\n",
        "You can automatically migrate your codebase using [grit](https://www.grit.io/), either [online](https://app.grit.io/migrations/new/langfuse_v2) or with the following CLI command:\n",
        "```\n",
        "npx -y @getgrit/launcher apply langfuse_v2\n",
        "```\n",
        "\n",
        "The grit binary executes entirely locally with AST-based transforms. Be sure to audit its changes: we suggest ensuring you have a clean working tree beforehand, and running `git add --patch` afterwards.\n",
        "\n",
        "If your Jupyter Notebooks are not in source control, it might be harder to track changes. You may want to copy each cell individually into grit's web interface, and paste the output back in.\n",
        "\n",
        "### Remove Pydantic interfaces\n",
        "\n",
        "We like Pydantic, but it made the Langfuse SDK interfaces messy. Therefore, we removed the objects from the function signatures and replaced them with named parameters.\n",
        "\n",
        "All parameters are still validated using Pydantic internally. If the validation fails, errors are logged instead of throwing exceptions.\n",
        "\n",
        "#### Pydantic objects\n",
        "\n",
        "**v1.x.x**\n",
        "```python\n",
        "from langfuse.model import CreateTrace\n",
        "\n",
        "langfuse.trace(CreateTrace(name=\"My Trace\"))\n",
        "```\n",
        "\n",
        "**v2.x.x**\n",
        "```python\n",
        "langfuse.trace(name=\"My Trace\")\n",
        "```\n",
        "\n",
        "#### Pydantic Enums\n",
        "\n",
        "**v1.x.x**\n",
        "```python\n",
        "from langfuse.model import InitialGeneration\n",
        "from langfuse.api.resources.commons.types.observation_level import ObservationLevel\n",
        "\n",
        "langfuse.generation(InitialGeneration(level=ObservationLevel.ERROR))\n",
        "```\n",
        "\n",
        "**v2.x.x**\n",
        "```python\n",
        "langfuse.generation(level=\"ERROR\")\n",
        "```\n",
        "\n",
        "### Rename `prompt` and `completion` to `input` and `output`\n",
        "To ensure consistency throughout Langfuse, we have renamed the `prompt` and `completion` parameters in the `generation` function to `input` and `output`, respectively. This change brings them in line with the rest of the Langfuse API.\n",
        "\n",
        "### Snake case parameters\n",
        "\n",
        "To increase consistency, all parameters are snake case in v2.\n",
        "- `trace_id` instead of `traceId`\n",
        "- `start_time` instead of `startTime`\n",
        "- `end_time` instead of `endTime`\n",
        "- `completion_start_time` instead of `completionStartTime`\n",
        "- `status_message` instead of `statusMessage`\n",
        "- `user_id` instead of `userId`\n",
        "- `session_id` instead of `sessionId`\n",
        "- `parent_observation_id` instead of `parentObservationId`\n",
        "- `model_parameters` instead of `modelParameters`\n",
        "\n",
        "\n",
        "### More generalized usage object\n",
        "\n",
        "We improved the flexibility of the SDK by allowing you to ingest any type of usage while still supporting the OpenAI-style usage object.\n",
        "\n",
        "**v1.x.x**\n",
        "```python\n",
        "\n",
        "from langfuse.model import InitialGeneration, Usage\n",
        "\n",
        " langfuse.generation(\n",
        "    InitialGeneration(\n",
        "        name=\"my-generation\",\n",
        "        usage=Usage(promptTokens=50, completionTokens=49),\n",
        "    )\n",
        ")\n",
        "```\n",
        "\n",
        "**v2.x.x**\n",
        "\n",
        "The usage object supports the OpenAi structure with {`promptTokens`, `completionTokens`, `totalTokens`} and a more generic version {`input`, `output`, `total`, `unit`} where unit can be of value `\"TOKENS\"` or `\"CHARACTERS\"`. For some models the token counts and costs are [automatically calculated](https://langfuse.com/docs/model-usage-and-cost) by Langfuse. Create an issue to request support for other units and models.\n",
        "\n",
        "```python\n",
        "# Generic style\n",
        "langfuse.generation(\n",
        "    name=\"my-claude-generation\",\n",
        "    usage={\n",
        "        \"input\": 50,\n",
        "        \"output\": 49,\n",
        "        \"total\": 99,\n",
        "        \"unit\": \"TOKENS\"\n",
        "    },\n",
        ")\n",
        "\n",
        "# OpenAI style\n",
        "langfuse.generation(\n",
        "    name=\"my-openai-generation\",\n",
        "    usage={\n",
        "        \"promptTokens\": 50,\n",
        "        \"completionTokens\": 49,\n",
        "        \"totalTokens\": 99\n",
        "    }, # defaults to \"TOKENS\" unit\n",
        ")\n",
        "\n",
        "# set ((input and/or output) or total), total is calculated automatically if not set\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-GcLUd6_JXWH"
      },
      "source": [
        "## FastAPI\n",
        "For engineers working with FastAPI, we have a short example, of how to use it there.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V9WHoZAmKEgO"
      },
      "outputs": [],
      "source": [
        "%pip install fastapi --upgrade"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "COd5Q_67KMqU"
      },
      "source": [
        "Here is an example of how to initialise FastAPI and register the `langfuse.flush()` method to run at shutdown.\n",
        "With this, your Python environment will only terminate once Langfuse received all the events."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nu2ug2AoJ8d8"
      },
      "outputs": [],
      "source": [
        "from contextlib import asynccontextmanager\n",
        "from fastapi import FastAPI, Query, BackgroundTasks\n",
        "\n",
        "@asynccontextmanager\n",
        "async def lifespan(app: FastAPI):\n",
        "    # Operation on startup\n",
        "\n",
        "    yield  # wait until shutdown\n",
        "\n",
        "    # Flush all events to be sent to Langfuse on shutdown and terminate all Threads gracefully. This operation is blocking.\n",
        "    langfuse.flush()\n",
        "\n",
        "\n",
        "app = FastAPI(lifespan=lifespan)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L91osyXKKZIn"
      },
      "outputs": [],
      "source": [
        "langfuse = Langfuse()\n",
        "\n",
        "@app.get(\"/generate/\",tags=[\"APIs\"])\n",
        "async def campaign(prompt: str = Query(..., max_length=20)):\n",
        "  # call to a LLM\n",
        "  generation = langfuse.generation(\n",
        "      name=\"llm-feature\",\n",
        "      metadata=\"test\",\n",
        "      input=prompt\n",
        "  )\n",
        "  return True"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

---
title: "Langfuse Launch Week #1"
date: 2024/04/19
description: "A week full of new features releases"
tag: launchweek
ogImage: /images/blog/launch-week-1/langfuse-launch-week.png
author: Clemens
---

<BlogHeader
  title="Langfuse Launch Week #1"
  description="A week full of new features releases"
  authors={["clemensrawert", "marcklingen", "maxdeichmann"]}
/>

import { BlogHeader } from "@/components/blog/BlogHeader";

<Frame fullWidth className="mb-12">
  ![Langfuse](/images/blog/launch-week-1/langfuse-launch-week.png)
</Frame>

## Joins us for Launch Week #1

We‚Äôre excited to announce Langfuse's first launch week. We're kicking it off on Monday April 22nd and will release a major upgrade to the Langfuse platform every day until Friday.

- ‚≠êÔ∏è [Star us](https://github.com/langfuse/langfuse/) on Github & see _all_ of our releases!
- [Twitter](https://x.com/langfuse) will be our main channel for all of Launch Week #1
- Join our [first town hall](#townhall) on Wednesday

import { Tweet } from "@/components/Tweet";

<Tweet id="1781324385774674407" />

## Launches

### Day 0: OpenAI JS SDK Integration

```ts /import { observeOpenAI } from "langfuse"/ /observeOpenAI/
import OpenAI from "openai";
import { observeOpenAI } from "langfuse";

// wrap the OpenAI SDK
const openai = observeOpenAI(new OpenAI());

// use the OpenAI SDK as you normally would
const res = await openai.chat.completions.create({
  messages: [{ role: "system", content: "Tell me a story." }],
});
```

We [launched](/changelog/2024-04-21-openai-integration-JS-SDK) a new wrapper for the OpenAI JS SDK. This integration, designed to enable easier monitoring of OpenAI API usage, features seamless observability with enhancements like automatic tracking of prompts, completions, and API errors, as well as insights into model usage and costs. After a soft launch that gathered user feedback for improvements, the integration is now fully available, complete with comprehensive [documentation](/docs/integrations/openai/js/get-started) and an [example notebook](/docs/integrations/openai/js/examples).

### Day 1: PostHog Integration

<CloudflareVideo
  videoId="24f7e625588a979f380effedda66a8ac"
  aspectRatio={16 / 9.53}
  title="PostHog Integration"
/>

We teamed up with [PostHog](https://posthog.com) (OSS product analytics) to integrate LLM-related product metrics into your existing PostHog dashboards. This integration is [now available](/changelog/2024-04-22-posthog-integration) in public beta on Langfuse cloud. You can configure it within your Langfuse project settings. When activated, Langfuse sends metrics related to traces, generations, and scores to PostHog. You can then build custom dashboards to visualize the data or use the _LLM Analytics_ dashboard template to get started quickly. See [docs](/docs/analytics/posthog) for more information.

### Day 2: LLM Playground

<CloudflareVideo
  videoId="3cfab665df39518f15fc18813cf82e3f"
  aspectRatio={16 / 10.47}
  title="LLM Playground"
/>

We're excited to introduce the [**LLM Playground**](/docs/playground) to Langfuse. By making prompt engineering possible directly in Langfuse, we take another step in our mission to build a feature-complete LLM engineering platform that helps you along the full live cycle of your LLM application. With the LLM playground, you can now test and iterate your prompts directly in Langfuse. Either start from scratch or jump into the playground from an existing prompt in your project. See the [docs](/docs/playground) for more details and let us know what you think in the [GitHub discussion](https://github.com/orgs/langfuse/discussions/1170).

### Day 3: Decorator-based integration for Python

<CloudflareVideo
  videoId="8a1f9282ef64c2bd0d932c92d6185668"
  aspectRatio={16 / 9.37}
  title="Decorator Integration"
/>

We're happy to share that the Decorator-based integration for Python now supports all Langfuse features and is the recommended way to use Langfuse in Python. The decorator makes integrating with Langfuse so much easier. Head over to the [Python Decorator docs](/docs/sdk/python/decorators) to learn more. All inputs, outputs, timings are captured automatically, and it works with all other [Langfuse integrations](/docs/integrations) (LangChain, LlamaIndex, OpenAI SDK, ...). To celebrate this milestone, we wrote a [blog post](/blog/2024-04-python-decorator) on the technical details and created the [example notebook](/guides/cookbook/example_decorator_openai_langchain) shown in the video as it demonstrates what's really cool about the decorator. Thanks again to [@lshalon](https://github.com/lshalon) and [@AshisGhosh](https://github.com/AshisGhosh) for your contributions to this!

### Day 4: Datasets v2

<CloudflareVideo
  videoId="bb521aa5ad9160ce723a6a77376f5651"
  aspectRatio={16 / 9.5}
  title="Datasets v2"
/>

We're thrilled to release Datasets v2, featuring significant enhancements to the dataset experience in Langfuse. Improvements include a new editor powered by Codemirror, metadata support on all objects, tables that render inputs/outputs side-by-side, the ability to link dataset runs to traces, and the option to create dataset items directly from traces. We've also extended the public API with new endpoints for programmatic management of datasets. Check out the [changelog](/changelog/2024-04-25-datasets-v2) which summarizes all the new features and improvements.

### Day 5: Model-based Evaluations

<CloudflareVideo
  videoId="c2debc8ad9e9df71d56f813510ffdf80"
  aspectRatio={16 / 9}
  title="Model-based Evaluations in Langfuse"
  className="mt-10"
  posterStartTime={137}
/>

On the final day of Launch Week 1, we're happy to release the biggest change to Langfuse yet: Model-based evaluations run right in Langfuse. So far, it was easy to measure LLM cost and latency in Langfuse. Quality is based on [scores](/docs/scores) which can be user feedback, manual labeling results, or be ingested by evaluation pipelines that you built yourself using the Langfuse SDKs/API. Model-based Evaluations in Langfuse make it way easier to continuously evalutate your application on the dimensions you care about. These can be: hallucinations, toxicity, relevance, correctness, conciseness, and so much more. We provide you with some battle-tested templates to get you started, but you can also write your own templates to cover any niche use case that might be exclusive to your application. Check out the [changelog](/changelog/2024-04-26-model-based-evaluation) or watch the video to learn more about all the details.

## Launch Week Events

### Wednesday: First virtual town hall [#townhall]

You're invited to our first virtual town hall. We (Max, Marc and Clemens) will be demoing new features in Langfuse, answering questions and talking about where we're taking the project. We're looking forward to hanging out!

- When: Wednesday, April 24th, noon PT, 9pm CET
- [Recording on YouTube](https://www.youtube.com/watch?v=WGERHcRnBYQ)

### Friday: Langfuse 2.0 on Product Hunt

We will end the week with the launch of **Langfuse 2.0** on Product Hunt on Friday, April 26th. After our initial launch last year ‚Äì which led to a [Golden Kitty Award](https://www.producthunt.com/golden-kitty-awards/hall-of-fame?year=2023#ai-infra) ‚Äì we are very excited to be back on Product Hunt.

[Launch post](https://www.producthunt.com/posts/langfuse-2-0) (_Spoiler: Langfuse became the #1 product of the day ü•á_)

## Learn More About Langfuse

import { FileCode, BookOpen, Video } from "lucide-react";

<Cards num={3}>
  <Card
    title="Docs"
    href="/docs/integrations/llama-index/get-started"
    icon={<BookOpen />}
  />
  <Card title="Quickstart" href="/docs/get-started" icon={<BookOpen />} />
  <Card
    title="Cookbooks"
    href="/docs/integrations/llama-index/example-python"
    icon={<FileCode />}
  />
</Cards>

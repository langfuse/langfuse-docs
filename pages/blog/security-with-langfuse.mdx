---
title: LLM Security in Langfuse
date: 2024/05/14
description: How to use Langfuse with security libraries to trace, prevent, and evaluate security issues.
ogImage: 
tag: integration
author: Lydia You
---

import { BlogHeader } from "@/components/blog/BlogHeader";

<BlogHeader
  title="Using LLM Security Libraries with Langfuse"
  description="How to use Langfuse to trace, prevent, and evaluate security risks common to LLM-based applications."
  date="2024/05/14"
  authors={["lydiayou"]}
/>

Protecting against security risks and attacks is becoming increasingly important for ensuring LLM apps are production ready. Not only do LLM applications need to be secure to protect users' private and sensitive information, they also need ensure a level of quality and safety of responses to maintain product standards.

This post offers an overview of how you can use security tools in conjunction with Langfuse to monitor and protect against common security risks. The [OWASP Top 10](https://owasp.org/www-project-top-10-for-large-language-model-applications/) list is a useful resource on the topic. It provides a consensus of the most critical security risks for LLM applications.

## Introduction

In the video below, we walk through an example of how to use the open-source security library LLM Guard, and how to integrate Langfuse to monitor and protect against common security risks.

{/* 
<CloudflareVideo
  videoId="8a1f9282ef64c2bd0d932c92d6185668"
  aspectRatio={16 / 9.37}
  title="Decorator Integration"
/> */}

WILL ATTACH VIDEO LATER

## How Does Langfuse Help?

Langfuse can be used to prevent, identify and mitigate security risks to your application. You can include measures at several points of your workflow:

- **Controlling LLM interactions**
  - **Input Controls** Flag potentially malicious or sensitive prompts and completion and decide whether to send them to the model.
  - **Output Controls**: Evaluate model outputs for risks before returning them to your users. Sanitize outputs and prevent data leakage.
  - **Security Libraries**: You can integrate Langfuse with most popular libraries such as LLM Guard, Nemo Guardrails or Prompt Armour.
- **Post-Ingestion - Evaluate residual risks**
  - You can use Langfuse's native functionality to scan your logs for residual risks and devise strategies to mitigate them
  - **Model-based evaluations**: Evaluate completed responses. Langfuse's [model-based evaluations](https://langfuse.com/docs/scores/model-based-evals) will run asynchronously on incoming traces and can scan and score for attributes such as toxicity or politeness. 
  - **Review and Mitigate**: Become aware of potential risks and mitigate through reviewing and iterating on your application to mitigate security risks
- **Balancing trade-offs**
  - **Latency**: Track how much latency is added to your traces through adding security steps.

The following code snippet shows how to use LLM Guard, an open-source security tool, with Langfuse decorators to observe the trace.

Exposing Personally Identifiable Information (PII) to models can pose security and privacy risks, such as violating contractual obligations or regulatory compliance requirements, or mitigating the risks of data leakage or a data breach.

The example below shows a simple application that summarizes a given court transcript. For privacy reasons, the application wants to anonymize PII before the information is fed into the model, and then un-redact the response to produce a coherent summary.

First, import the security packages and Langfuse tools.

```python
from llm_guard.input_scanners import Anonymize
from llm_guard.input_scanners.anonymize_helpers import BERT_LARGE_NER_CONF
from langfuse.openai import openai # OpenAI integration
from langfuse.decorators import observe, langfuse_context
from llm_guard.output_scanners import Deanonymize
```

Next, separate out each step of the security process into a separate wrapper function. This allows you to observe and trace each step individually to gain insights on latency and performance.

```python
@observe()
def anonymize(input: str):
  scanner = Anonymize(vault, preamble="Insert before prompt", allowed_names=["John Doe"], hidden_names=["Test LLC"],
                    recognizer_conf=BERT_LARGE_NER_CONF, language="en")
  sanitized_prompt, is_valid, risk_score = scanner.scan(prompt)
  return sanitized_prompt

@observe()
def deanonymize(sanitized_prompt: str, answer: str):
  scanner = Deanonymize(vault)
  sanitized_model_output, is_valid, risk_score = scanner.scan(sanitized_prompt, answer)

  return sanitized_model_output

@observe()
def summarize_transcript(prompt: str):
  sanitized_prompt = anonymize(prompt)

  answer = openai.chat.completions.create(
        model="gpt-3.5-turbo",
        max_tokens=100,
        messages=[
          {"role": "system", "content": "Summarize the given court transcript."},
          {"role": "user", "content": sanitized_prompt}
        ],
    ).choices[0].message.content

  sanitized_model_output = deanonymize(sanitized_prompt, answer)

  return sanitized_model_output
```

Run the function. In this example, we input a section of a court transcript. Applications that handle sensitive information will often need to use anonymize and deanonymize functionality to comply with data privacy policies such as HIPAA or GDPR.

```python
@observe()
def main():
    prompt = """
    Plaintiff, Jane Doe, by and through her attorneys, files this complaint 
    against Defendant, Big Corporation, and alleges upon information and belief, 
    except for those allegations pertaining to personal knowledge, that on or about 
    July 15, 2023, at the Defendant's manufacturing facility located at 123 Industrial Way, Springfield, Illinois, Defendant negligently failed to maintain safe working conditions, 
    leading to Plaintiff suffering severe and permanent injuries. As a direct and proximate 
    result of Defendant's negligence, Plaintiff has endured significant physical pain, emotional distress, and financial hardship due to medical expenses and loss of income. Plaintiff seeks compensatory damages, punitive damages, and any other relief the Court deems just and proper.
    """
    return summarize_transcript(prompt)

main()
```

## Get Started

Run the end-to-end [cookbook](/cookbook/security.ipynb) or check out our documentation.

import { FileCode, BookOpen } from "lucide-react";

<Cards num={2}>
  <Card
    title="Cookbook: Security in Langfuse"
    href="/guides/cookbook/security"
    icon={<FileCode />}
  />
  <Card
    title="Docs: Security with Langfuse"
    href="/docs/security"
    icon={<BookOpen />}
  />
</Cards>

## About Langfuse

Langfuse is the open source LLM engineering platform. It is used by teams to track and analyze their LLM app in production with regards to quality, cost, and latency across product releases and use cases.

## Questions / Feedback?

Langfuse is committed to being an open-source project. We welcome any feedback or questions on [GitHub Discussions](https://langfuse.com/ideas).

---
date: 2024-04-21
title: OpenAI SDK integration for JS/TS SDK
description: Langfuse now provides a simple to adopt wrapper for the OpenAI JS/TS SDK for you to seemlessly trace your OpenAI calls.
author: Hassieb
ogImage: /images/changelog/2024-04-21-openai-integration-JS-SDK.png
showOgInHeader: false
---

import { ChangelogHeader } from "@/components/changelog/ChangelogHeader";

<ChangelogHeader />

LLM observability just got a lot easier for users of the OpenAI JS SDK. Langfuse now provides a wrapper for the OpenAI SDK that gives you all the benefits of tracing your OpenAI calls with only minor code changes. Import the `observeOpenAI` function from `langfuse` and pass your OpenAI SDK instance to it. The wrapper will automatically track all your OpenAI calls and provide you with insights into your model usage, cost, and performance in the Langfuse UI.

### Quickstart

```typescript /import { observeOpenAI } from "langfuse"/ /observeOpenAI/
import OpenAI from "openai";
import { observeOpenAI } from "langfuse";

// wrap the OpenAI SDK
const openai = observeOpenAI(new OpenAI());

// use the OpenAI SDK as you normally would
const res = await openai.chat.completions.create({
  messages: [{ role: "system", content: "Tell me a story." }],
});
```

Langfuse will automatically capture the following information for you:

- All prompts/completions with support for streaming and function calling
- Total latencies and time-to-first-token
- OpenAI API Errors
- Model usage (tokens) and cost (USD) ([learn more](/docs/model-usage-and-cost))

### ðŸ“š Docs

[Get started with the OpenAI SDK integration](/docs/integrations/openai/js/get-started)

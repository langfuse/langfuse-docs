---
date: 2024-04-26
badge: Launch Week 1 ðŸš€
title: Model-based evals
description: Use Langfuse evaluations to gain confidence in the quality of your LLM application.
ogVideo: /images/changelog/2024-04-25-datasets-v2.mp4
showOgInHeader: false
author: Marc
---

import { ChangelogHeader } from "@/components/changelog/ChangelogHeader";

<ChangelogHeader />

On the final day of [Launch Week 1](/blog/launch-week-1), we're happy to release [model-based Evals](/docs/scores/model-based-evals/). So far, it was easy to measure LLM cost and latency. Evals will help our users to increase confidence in the quality of their LLM applications without human interaction.

If you want to judge feature quality of your LLM feature, you have to manually read all the lengthy LLM outputs  or ask your users to do that for you. This is very time consuming and error prone. Especially when you try to be a good engineer iterating fast on your LLM feature.
With model-based Evals, you can specify an LLM call that Langfuse will execute for you on each `trace`. You can sit back and ship new features, while Langfuse takes care of building up a large dataset of scored `traces`.

<CloudflareVideo
  videoId="bb521aa5ad9160ce723a6a77376f5651"
  aspectRatio={16 / 9.5}
  title="Datasets v2"
  className="mt-10"
/>

### Write your own templates

Our users build wildly different LLM applications. Therefore, they need to have full flexibility of the templates which will be sent to the LLM to make them work for their use cases. That's why we made sure that you can write your own prompts and specify the variables that Langfuse should inserted into the template when the eval runs.

<Frame border className="max-w-2xl">
  ![Metadata across dataset
  objects](/images/changelog/2024-04-26-evals-hallucination.png)
</Frame>



### Langfuse-managed battle tested templates

We did the research and found the most successful prompts for evals. While they are not a guarantee for the most accurate scores, they are a great starting point. We committ to keeping them updated to help you starting off successfully with evals. Of course, you can adjust these templates to your needs.

We found eval great templates for the following criteria:
- Hallucination
- Helpfulness
- Relevance
- Toxicity
- Correctness
- Contextrelevance
- Contextcorrectness
- Conciseness


### Full integration with Langfuse Tracing

Eval are most successful, if the template contains the right context. We made sure that you can easily access the Tracing data when applying the template. 
First, you can use rich filters to specify on which newly ingested `trace` the template should be applied to. 
Second, once Langfuse finds a `trace` that aligns with the filter, it will extract the variables from the `trace` objects and compile them into the template. Finally, we will make the LLM call and store the results in the `scores` object.

<Frame  border className="max-w-2xl">
  ![Metadata across dataset
  objects](/images/changelog/2024-04-26-evals-config.gif)
</Frame>



### Build up a large dataset of scored traces

While Langfuse evaluates your traces, it will generate `scores` and attach them to the `traces`. Scores are at the core of our platform and fully integrated. You can use them to get a quick overview of LLM quality over time in the dashboards or drill down by filtering by scores across the UI.

<Frame  border className="max-w-2xl">
  ![Metadata across dataset
  objects](/images/changelog/2024-04-26-evals-log.png)
</Frame>


### Runs all async in Langfuse

We did all the heavy lifting of running evals asynchronously for you. You just have to configure it, and we will do the job in the background while you sleep.


## Learn more

import { Book, FileCode } from "lucide-react";

<Cards num={2}>
  <Card title="Model based evals docs" href="/docs/scores/model-based-evals/" icon={<Book />} />
</Cards>
import {CloudflareVideo} from "@/components/Video"

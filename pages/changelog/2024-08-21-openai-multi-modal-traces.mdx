---
date: 2024-08-21
title: Support for Multi-Modal Traces in Langfuse UI
description: Add multiple modalities to a single trace, including text and images. 
author: Marlies
ogCloudflareVideo: 59d62219e3b4f9e107bd3a685e2ebff0
---

import { ChangelogHeader } from "@/components/changelog/ChangelogHeader";
import { FileCode, BookOpen } from "lucide-react";

<ChangelogHeader />

Langfuse now supports rendering multi-modal traces. In addition to text, you may now include images (passed via URL). We are following OpenAI's vision capability convention and expect images to be passed in their defined format. For more details, please refer to [Multi-Modal Tracing](https://langfuse.com/docs/tracing#multi-modal-traces). 

## How to trace vision input in Langfuse?

If you use the [OpenAI Python SDK](https://langfuse.com/docs/integrations/openai/python/get-started), you can follow our [OpenAI cookbook](https://langfuse.com/docs/integrations/openai/python/examples) which includes an example for tracing images in Langfuse.

<Cards num={3}>
  <Card
    title="Tracing: Multi-Modal Tracing"
    href="/docs/tracing#multi-modal-traces"
    icon={<FileCode />}
  />
  <Card
    title="Cookbook: OpenAI Integration (Python)"
    href="/docs/integrations/openai/python/examples"
    icon={<FileCode />}
  />
</Cards>
---
date: 2024-08-21
title: Support for Multi-Modal Traces in Langfuse UI
description: Add multiple modalities to a single trace, including text and images. 
author: Marlies
ogCloudflareVideo: 59d62219e3b4f9e107bd3a685e2ebff0
---

import { ChangelogHeader } from "@/components/changelog/ChangelogHeader";
import { FileCode, BookOpen } from "lucide-react";

<ChangelogHeader />

Langfuse now supports rendering multi-modal traces. In addition to text, you may now include images (passed via URL). We follow OpenAI's content format convention. For more details, please refer to [Multi-Modal Tracing](/docs/tracing-features/multi-modality). 

## How to trace vision input in Langfuse?

If you use the [OpenAI Python SDK](/docs/integrations/openai/python/get-started), you can follow our [OpenAI cookbook](/docs/integrations/openai/python/examples) which includes an example for tracing images in Langfuse.

<Cards num={3}>
  <Card
    title="Tracing: Multi-Modal Content"
    href="/docs/tracing-features/multi-modality"
    icon={<FileCode />}
  />
  <Card
    title="Cookbook: OpenAI Integration (Python)"
    href="/docs/integrations/openai/python/examples"
    icon={<FileCode />}
  />
</Cards>
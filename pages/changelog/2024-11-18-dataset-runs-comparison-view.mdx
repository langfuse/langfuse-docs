---
date: 2024-11-18
title: Dataset Experiment Runs Comparison View
badge: Launch Week 2 ðŸš€
badgeHref: /blog/2024-11-17-launch-week-2
description: Langfuse now offers a comparison view for dataset experiment runs. Compare your application runs on the same test dataset, view metrics, and peek into details of each dataset item across runs.
ogImage: /images/changelog/2024-11-18-dataset-runs-comparison-view/og.png
showOgInHeader: true
author: Marlies
---

import { ChangelogHeader } from "@/components/changelog/ChangelogHeader";

<ChangelogHeader />

And the fun continues, [Launch Week 2](/blog/2024-11-17-launch-week-2) day 1 is here. 

## Context

**What is a dataset in Langfuse?** In Machine Learning, one of our most essential tools are datasets to run tests on our applications. Datasets are collections of dataset items that are used to train, evaluate, and test machine learning models. In Langfuse, we provide a way to manage your datasets. 

**What is a dataset experiment run?** A dataset experiment run is a test of your application against the test dataset. Typically, you would run your application multiple times against the same test dataset for a given use case. Across runs, you might work with same prompt different model, different prompts same model, etc. Using these configurations, your application will run against all items in the dataset. Each item will yield a trace in Langfuse, which can be used to further analyze the dataset item.

## What's new?

[Langfuse Datasets](/docs/datasets/overview) now enables intuitive comparison of dataset experiment runs for technical and non-technical users. The view features an overview of each item in the dataset, and a summary of each selected experiment run. The latter includes metrics on latency, cost, scores and the application's output response for each dataset item.

## How to use the comparison view?

Prerequisites:

- Set up a dataset in Langfuse ([guide](/docs/datasets/overview#creating-a-dataset)).
- Populate the dataset with items, typically users combine sanitized prod data and synthetic data ([guide](/docs/datasets/overview#creating-dataset-items)).
- Chose a uniquely identifiable experiment run name, e.g. `llama3-8b-prod-vs-llama3-8b-synthetic`. Run your application multiple times against the test dataset, typically with varying prompts, models, or model parameters. Each experiment run should be executed against each item in the dataset([guide](/docs/datasets/overview#running-dataset-experiment-runs)) and be linked to a trace in Langfuse.

See cookbook for code snippets and step-by-step guides on how to run experiments on datasets ([link](/docs/datasets/python-cookbook)).

## Try it out

View the comparison view by navigating to the **Datasets** page and selecting a dataset. From there, use table multi-actions to select the **Experiment Runs** you'd like to compare and click **Compare** to view the comparison view in Langfuse UI.

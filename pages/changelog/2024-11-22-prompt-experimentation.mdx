---
date: 2024-11-22
title: Prompt Experiments on Datasets with LLM-as-a-Judge Evaluations
badge: Launch Week 2 ðŸš€
description: Iterate faster on prompts by running experiments on Datasets and directly comparing evaluation results side-by-side. Experimentation speeds up the feedback loop when working on prompts and prevents regressions when making rapid changes.
author: Marlies
---

import { ChangelogHeader } from "@/components/changelog/ChangelogHeader";

<ChangelogHeader />

---

On Day 5 of [Langfuse Launch Week 2](/blog/2024-11-17-launch-week-2), we're excited to announce **Prompt Experimentation** on Datasets with Evals.

**Prompt Experiments** are the final piece of the launch week theme of _"closing the development loop"_ and allow you to:

1. Test a prompt version from Langfuse [Prompt Management](/docs/prompts)
2. on a [Dataset](/docs/datasets) of test inputs and expected outputs
3. optionally, use [LLM-as-a-Judge Evaluators](/docs/scores/model-based-evals) to automatically evaluate the responses based on the expected outputs ([released](/changelog/2024-11-19-llm-as-a-judge-for-datasets) this launch week)
4. and finally, compare the results in the new side-by-side experiment comparison view ([released](/changelog/2024-11-18-dataset-runs-comparison-view) this launch week)

Watch the video below for a walkthrough of the new feature:

TODO: Add video

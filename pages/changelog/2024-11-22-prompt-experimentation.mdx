---
date: 2024-11-22
title: Prompt Experiments on Datasets with LLM-as-a-Judge Evaluations
badge: Launch Week 2 ðŸš€
description: Move fast on prompts without breaking things! Run experiments on Datasets and directly compare evaluation results side-by-side. Experimentation speeds up the feedback loop when working on prompts and prevents regressions when making rapid changes.
author: Marlies
---

import { ChangelogHeader } from "@/components/changelog/ChangelogHeader";

<ChangelogHeader />

<iframe
  width="100%"
  className="aspect-[16/9] rounded mt-10 w-full"
  src="https://www.youtube.com/embed/5c6k5ImCeIk?si=Ne58BBZ8XZXgKhTw"
  title="YouTube video player"
  frameborder="0"
  allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
  referrerpolicy="strict-origin-when-cross-origin"
  allowFullScreen
></iframe>

On Day 5 of [Langfuse Launch Week 2](/blog/2024-11-17-launch-week-2), we're excited to announce **Prompt Experimentation**. They are the final piece of the launch week theme of _"closing the development loop"_ and allow you to:

1. Test a prompt version from Langfuse [Prompt Management](/docs/prompts)
2. on a [Dataset](/docs/datasets) of test inputs and expected outputs
3. optionally, use [LLM-as-a-Judge Evaluators](/docs/scores/model-based-evals) to automatically evaluate the responses based on the expected outputs ([released](/changelog/2024-11-19-llm-as-a-judge-for-datasets) this launch week)
4. and finally, compare the results in the new side-by-side experiment comparison view ([released](/changelog/2024-11-18-dataset-runs-comparison-view) this launch week)

Watch the video below for a walkthrough of the new feature:

Check out the [docs](/docs/datasets/prompt-experiments) to learn more.

---
title: Langfuse Evaluator Library
description: Making LLM evaluation easier, more powerful, and production-ready is now possible with the new Langfuse Evaluator Library.
date: 2025-05-24
author: Marlies
badge: Launch Week 3 ðŸš€
ogImage: /images/changelog/lw3-6-evaluator-library.png
showOgInHeader: false
---

import { ChangelogHeader } from "@/components/changelog/ChangelogHeader";

<ChangelogHeader />

It includes Langfuse- and [RAGAS](https://docs.ragas.io/en/stable/)-managed evaluators to measure context relevance, SQL semantic equivalence, hallucinations, and other key dimensions. You can also define custom evaluators tailored to your needs. Stay ahead with evaluators kept up to date with the latest research, benefit from broad metric coverage, and enjoy a streamlined UX for setup and execution.

## Key Features:

- Stay current with evaluators that are regularly updated based on the latest research
- Broad metric coverage including faithfulness, correctness, topic adherence, and more
- A smoother UX for setting up and running evaluations in Langfuse

We've also improved the evaluator UX, making setup and configuration within Langfuse simpler and more intuitive than ever. We'd love your feedback!

## Getting Started

To get started, check out the [LLM-as-a-Judge Docs](/docs/scores/model-based-evals).
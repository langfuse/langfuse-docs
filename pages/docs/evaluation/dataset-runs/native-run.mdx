---
title: Native Dataset Runs (Prompt Experiments)
description: Experiment with different prompt versions and models on a dataset and compare the results side-by-side.
---

# Native Dataset Runs (Prompt Experiments)

You can execute Native Dataset Runs **(also called Prompt Experiments)** in the Langfuse UI to test different prompt versions from [Prompt Management](/docs/prompt-management) or language models and compare the results side-by-side.

Optionally, you can use [LLM-as-a-Judge Evaluators](/docs/evaluation/evaluation-methods/llm-as-a-judge) to automatically score the responses based on the expected outputs to further analyze the results on an aggregate level.

They are called "Native Dataset Runs" because they rely on Datasets, Prompts and optionally LLM-as-a-Judge Evaluators all being managed **natively** in Langfuse.

<CloudflareVideo
  videoId="0e2811d0dbcd59000837773aef814963"
  aspectRatio={16 / 9}
  title="Introduction to Dataset Runs"
/>

## Why use Native Dataset Runs (Prompt Experiments)?

- Quickly test different prompt versions or models
- Structure your prompt testing by using a dataset to test different prompt versions and models
- Quickly iterate on prompts through Dataset Runs
- Optionally use LLM-as-a-Judge Evaluators to score the responses based on the expected outputs from the dataset
- Prevent regressions by running tests when making prompt changes


## Prerequisites
<Steps>


### Create a usable prompt
Create a prompt that you want to test and evaluate. [How to create a prompt?](/docs/prompt-management/get-started)
<Callout type="info">
**A prompt is usable when:** your prompt has variables that match the dataset item keys in the dataset that will be used for the Dataset Run. See the example below.
</Callout>

<details>
<summary>Example: Prompt Variables & Dataset Item Keys Mapping</summary>
<div className="grid md:grid-cols-1 gap-4">
<div>

<br/>**Prompt:**
```bash
You are a Langfuse expert. Answer based on:
{{documentation}}

Question: {{question}}
```

</div>
<div>

<br/>**Dataset Item:**
```json
{
  "documentation": "Langfuse is an LLM Engineering Platform",
  "question": "What is Langfuse?"
}
```

</div>
</div>

In this example:
- The prompt variable `{{documentation}}` maps to the JSON key `"documentation"`
- The prompt variable `{{question}}` maps to the JSON key `"question"`
- Both keys must exist in the dataset item's input JSON for the experiment to run successfully

</details>

<details>
<summary>Example: Chat Message Placeholder Mapping</summary>

In addition to variables, you can also map placeholders in chat message prompts to dataset item keys.
This is useful when the dataset item also contains for example a chat message history to use.
Your chat prompt needs to contain a placeholder with a name. Variables within placeholders are not resolved.

**Chat Prompt:**
Placeholder named: `message_history`

**Dataset Item:**
```json
{
  "message_history": [
    {
      "role": "user",
      "content": "What is Langfuse?"
    },
    {
      "role": "assistant",
      "content": "Langfuse is a tool for tracking and analyzing the performance of language models."
    }
  ],
  "question": "What is Langfuse?"
}
```

In this example: 
- The chat prompt placeholder `message_history` maps to the JSON key `"message_history"`.
- The prompt variable `{{question}}` maps to the JSON key `"question"` in a variable not within a placeholder message.
- Both keys must exist in the dataset item's input JSON for the experiment to run successfully

</details>

### Create a usable dataset
Create a dataset with the inputs and expected outputs you want to use for your prompt experiments. [How to create a dataset?](/docs/evaluation/dataset-runs/datasets)

<Callout type="info">
**A dataset is usable when:** [1] the dataset items have JSON objects as input and [2] these objects have JSON keys that matches the prompt variables of the prompt(s) you will use. See the example below.
</Callout>

 <details>
<summary>Example: Prompt Variables & Dataset Item Keys Mapping</summary>
<div className="grid md:grid-cols-1 gap-4">
<div>

<br/>**Prompt:**
```bash
You are a Langfuse expert. Answer based on:
{{documentation}}

Question: {{question}}
```

</div>
<div>

<br/>**Dataset Item:**
```json
{
  "documentation": "Langfuse is an LLM Engineering Platform",
  "question": "What is Langfuse?"
}
```

</div>
</div>

In this example:
- The prompt variable `{{documentation}}` maps to the JSON key `"documentation"`
- The prompt variable `{{question}}` maps to the JSON key `"question"`
- Both keys must exist in the dataset item's input JSON for the experiment to run successfully

</details>

### Configure LLM connection

As your prompt will be executed for each dataset item, you need to configure an LLM connection in the project settings. [How to configure an LLM connection?](/faq/all/llm-connection)

### Optional: Set up LLM-as-a-judge

You can set up an LLM-as-a-judge evaluator to score the responses based on the expected outputs. Make sure to set the target of the LLM-as-a-Judge to "Experiment runs" and filter for the dataset you want to use. [How to set up LLM-as-a-judge?](/docs/evaluation/evaluation-methods/llm-as-a-judge)
</Steps>


## Trigger a Native Dataset Run (Prompt Experiment)

<Steps>
### Navigate to the dataset
Dataset Runs are currently started from the detail page of a dataset. 

- **Navigate to** `Your Project` > `Datasets`
- **Click on** the dataset you want to start a Dataset Run for

<Frame border fullWidth>
![New Experiment Button](/images/docs/navigate-to-dataset.png)
</Frame>

### Open the setup page

**Click on** `Start Experiment` to open the setup page

<Frame border fullWidth>
![New Experiment Button](/images/docs/trigger-process.png)
</Frame>

**Click on** `Create` below `prompt Experiment`

<Frame border>
![New Experiment Button](/images/docs/trigger-process-2.png)
</Frame>

### Configure the Dataset Run

1. **Set** a Dataset Run name
2. **Select** the prompt you want to use
3. **Set up or select** the LLM connection you want to use
4. **Select** the dataset you want to use
5. **Optionally select** the evaluator you want to use
6. **Click on** `Create` to trigger the Dataset Run

<Frame border>
![New Experiment Button](/images/docs/configure_dataset_run.png)
</Frame>

This will trigger the Dataset Run and you will be redirected to the Dataset Runs page. The run might take a few seconds or minutes to complete depending on the prompt complexity and dataset size.

### Compare runs

After each experiment run, you can check the aggregated score in the Dataset Runs table and compare results side-by-side.

<CloudflareVideo
  videoId="f8f2cf7ff86f2b54d1b90c0921d2c7e9"
  aspectRatio={16 / 9}
  gifStyle
/>

</Steps>

## GitHub Discussions

import { GhDiscussionsPreview } from "@/components/gh-discussions/GhDiscussionsPreview";

<GhDiscussionsPreview labels={["feat-prompt-experiments"]} />

---
description: Get started with Langfuse Datasets. Follow the step by step guide.
sidebarTitle: Offline
---

# Dataset Runs

Dataset Runs in Langfuse allow you to test your application on a dataset of inputs and expected outputs. You can then compare the results of different prompts, models, or other parameters side-by-side.

<Frame border fullWidth>
![Dataset Runs](/images/docs/dataset-runs-comparison.png)
</Frame>


## Why Dataset Runs?

- **Continuous improvement:** Create datasets from production edge cases to improve your application
- **Pre-deployment testing:** Benchmark new releases before deploying to production
- **Structured testing:** Run experiments on collections of inputs and expected outputs
- **Flexible evaluation:** Add custom evaluation metrics or use llm-as-a-judge
- **Integrates well:** Works with popular frameworks like LangChain and LlamaIndex

## Where to start

Follow the quickstart to add tracing to your LLM app.

import { Table, WandSparkles, CodeXml, Database} from "lucide-react";

<Cards num={2}>
  <Card
    icon={<Table size="24" />}
    title="Create a dataset"
    href="/docs/evaluation/dataset-runs/datasets"
    arrow
  />
  <Card
    icon={<Database size="24" />}
    title="Datamodel"
    href="/docs/evaluation/dataset-runs/data-model"
    arrow
  />
  <Card
    icon={<WandSparkles size="24" />}
    title="Dataset runs via the UI"
    href="/docs/evaluation/dataset-runs/run-via-ui"
    arrow
  />
  <Card
    icon={<CodeXml size="24" />}
    title="Dataset runs via the SDK"
    href="/docs/evaluation/dataset-runs/run-via-sdk"
    arrow
  />
</Cards>

## How to build a workflow around datasets

This is a high-level example workflow of using datasets to continuously improve an LLM application:

1. Create dataset items with inputs and expected outputs through:

   - Manual creation or import of test cases
   - Synthetic generation of questions/responses
   - Production app traces with issues that need attention

2. Make changes to your application that you want to test

3. Run your application (or parts of it) on all dataset items

4. Evaluate results:

   - Compare against baseline/expected outputs if available
   - Use custom evaluation metrics
   - Leverage LLM-based evaluation

5. Review aggregated results across the full dataset to:
   - Identify improvements
   - Catch regressions
   - Make data-driven decisions about releases

_Process diagram:_

<Frame fullWidth>
![Datasets](/images/docs/dataset-example-workflow.png)
</Frame>
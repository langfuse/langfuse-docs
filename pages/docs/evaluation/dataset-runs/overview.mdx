---
description: Get started with Langfuse Datasets. Follow the step by step guide.
sidebarTitle: Offline
---

# Dataset Runs

Dataset Runs in Langfuse allow you to test your application on a dataset of inputs and expected outputs. You can then compare the results of different prompts, models, or other parameters side-by-side.

<Frame border fullWidth>
![Dataset Runs](/images/docs/dataset-runs-comparison.png)
</Frame>


## Why Dataset Runs?

- **Continuous improvement:** Create datasets from production edge cases to improve your application
- **Pre-deployment testing:** Benchmark new releases before deploying to production
- **Structured testing:** Run experiments on collections of inputs and expected outputs
- **Flexible evaluation:** Add custom evaluation metrics or use llm-as-a-judge
- **Integrates well:** Works with popular frameworks like LangChain and LlamaIndex

## Where to start

Follow the quickstart to add tracing to your LLM app.

import { Table, WandSparkles, CodeXml, Database} from "lucide-react";

<Cards num={2}>
  <Card
    icon={<Table size="24" />}
    title="Create a dataset"
    href="/docs/evaluation/dataset-runs/datasets"
    arrow
  />
  <Card
    icon={<Database size="24" />}
    title="Datamodel"
    href="/docs/evaluation/dataset-runs/data-model"
    arrow
  />
  <Card
    icon={<WandSparkles size="24" />}
    title="Dataset runs via the UI"
    href="/docs/evaluation/dataset-runs/run-via-ui"
    arrow
  />
  <Card
    icon={<CodeXml size="24" />}
    title="Dataset runs via the SDK"
    href="/docs/evaluation/dataset-runs/run-via-sdk"
    arrow
  />
</Cards>
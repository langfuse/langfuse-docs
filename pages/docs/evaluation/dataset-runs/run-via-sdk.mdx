---
title: Dataset Runs via the SDK
description: Run dataset runs via the Langfuse SDK.
---

# Dataset Runs via the SDK

Once you [created a dataset](/docs/evaluation/dataset-runs/datasets), you can use the dataset to test how your application performs on different inputs.

## Get started
<Steps>

### Load the dataset

Use the Python or JS/TS SDK to load the dataset.

<Tabs items={["Python SDK", "JS/TS SDK"]}>
<Tab>

```python
from langfuse import get_client

dataset = get_client().get_dataset("<dataset_name>")
```

</Tab>
<Tab>

```ts
const dataset = await langfuse.getDataset("<dataset_name>");
```

</Tab>
</Tabs>

### Instrument your application

First we create our application runner helper function. This function will be call for every dataset item in the next step. 

<Callout type="info" emoji="ℹ️">
For a dataset run, it is important that your application creates Langfuse traces for each execution so they can be linked to the dataset item. Please refer to the [integrations](/docs/integrations/overview) page for details on how to instrument the framework you are using.
</Callout>

<Tabs items={["Python SDK v3", "JS/TS", "Langchain (Python)", "Langchain (JS/TS)", "Vercel AI SDK"]}>
<Tab>

Assume you already have a Langfuse-instrumented LLM-app:

```python filename="app.py"
from langfuse import get_client, observe
from langfuse.openai import OpenAI

@observe
def my_llm_function(question: str):
    response = OpenAI().chat.completions.create(
        model="gpt-4o", messages=[{"role": "user", "content": question}]
    )
    output = response.choices[0].message.content

    # Update trace input / output
    get_client().update_current_trace(input=question, output=output)

    return output
```

_See [Python SDK v3](/docs/sdk/python/sdk-v3) docs for more details._

</Tab>


<Tab>

```ts
// Work in progress

```

</Tab>
<Tab>

```python
# Work in progress
```

</Tab>

<Tab>

```typescript
// Work in progress
```

</Tab>

<Tab>

Please refer to the [Vercel AI SDK](/integrations/frameworks/vercel-ai-sdk) docs for details on how to use the Vercel AI SDK with Langfuse.

```typescript
// Work in progress
```

</Tab>
</Tabs>


### Loop over dataset items

When running an experiment on a dataset, the application that shall be tested is executed for each item in the dataset. The execution trace is then linked to the dataset item. This allows you to compare different runs of the same application on the same dataset. Each experiment is identified by a `run_name`.

<div className="mt-6" />

<Tabs items={["Python SDK v3",  "JS/TS", "Langchain (Python)", "Langchain (JS/TS)", "Vercel AI SDK"]}>
<Tab>

You may then execute that LLM-app for each dataset item to create a dataset run:

```python filename="execute_dataset.py" /for item in dataset.items:/
from langfuse import get_client
from .app import my_llm_application

dataset = get_client().get_dataset("<dataset_name>")

for item in dataset.items:
    # Use the item.run() context manager for automatic trace linking
    with item.run(
        run_name="<run_name>",
        run_description="My first run",
        run_metadata={"model": "llama3"},
    ) as root_span:
        # Execute your LLM-app against the dataset item input
        output = my_llm_application.run(item.input)

        # Optionally, score the result against the expected output
        root_span.score_trace(
            name="<example_eval>",
            value=my_eval_fn(item.input, output, item.expected_output),
            comment="This is a comment",  # optional, useful to add reasoning
        )

# Flush the langfuse client to ensure all data is sent to the server at the end of the experiment run
get_client().flush()
```

_See [Python SDK v3](/docs/sdk/python/sdk-v3) docs for details on the new OpenTelemetry-based SDK._

</Tab>

<Tab>

```ts /for (const item of dataset.items)/
const dataset = await langfuse.getDataset("<dataset_name>");

for (const item of dataset.items) {
  // execute application function and get langfuseObject (trace/span/generation/event)
  // output also returned as it is used to evaluate the run
  // you can also link using ids, see sdk reference for details
  const [langfuseObject, output] = await myLlmApplication.run(item.input);

  // link the execution trace to the dataset item and give it a run_name
  await item.link(langfuseObject, "<run_name>", {
    description: "My first run", // optional run description
    metadata: { model: "llama3" }, // optional run metadata
  });

  // optionally, evaluate the output to compare different runs more easily
  langfuseObject.score({
    name: "<score_name>",
    value: myEvalFunction(item.input, output, item.expectedOutput),
    comment: "This is a comment", // optional, useful to add reasoning
  });
}

// Flush the langfuse client to ensure all data is sent to the server at the end of the experiment run
await langfuse.flushAsync();
```

</Tab>
<Tab>

```python /for item in dataset.items:/
dataset = langfuse.get_dataset("<dataset_name>")

for item in dataset.items:
    # Langchain callback handler that automatically links the execution trace to the dataset item
    handler = item.get_langchain_handler(run_name="<run_name>")

    # Execute application and pass custom handler
    my_langchain_chain.run(item.input, callbacks=[handler])

    # Add a score to the linked trace depending on the chain output and expected output
    langfuse.score(trace_id=handler.get_trace_id(), name="my_score", value=1)

# Flush the langfuse client to ensure all data is sent to the server at the end of the experiment run
langfuse.flush()
```

</Tab>

<Tab>

```typescript /createDatasetItemHandler/ /for (const item of dataset.items)/
import { Langfuse } from "langfuse";
import { createDatasetItemHandler } from "langfuse-langchain";
...

const langfuse = new Langfuse()
const dataset = await langfuse.getDataset("my-dataset");
const runName = "my-dataset-run";

for (const item of dataset.items) {
  // Create langchain handler that automatically links the execution trace to the dataset item run
  const { handler, trace } = await createDatasetItemHandler({ item, runName, langfuseClient: langfuse });

  // Pass the callback handler when invoking the chain
  await chain.invoke({ country: item.input }, { callbacks: [handler] });

  // Add score to the linked trace depending on the chain output and expected output
  trace.score({
    name: "test-score",
    value: 0.5,
  });
}

await langfuse.flushAsync();
```

</Tab>
<Tab>

```typescript /for (const item of dataset.items)/
// fetch the dataset
const dataset = await langfuse.getDataset("vercel-ai-sdk-example");

// iterate over the dataset items
for (const item of dataset.items) {
  // create a trace manually in order to pass id to vercel ai sdk for later linking to the dataset run
  const trace = langfuse.trace({ name: "new experiment trace" });

  // run application on the dataset item input
  const output = await runMyLLMApplication(item.input, trace.id);

  // link the execution trace to the dataset item and give it a run_name
  await item.link(trace, "<run_name>", {
    description: "My first run", // optional run description
    metadata: { model: "gpt-4o" }, // optional run metadata
  });

  // optionally, evaluate the output to compare different runs more easily
  trace.score({
    name: "<score_name>",
    value: myEvalFunction(item.input, output, item.expectedOutput),
    comment: "This is a comment", // optional, useful to add reasoning
  });
}

// Flush the langfuse client to ensure all data is sent to the server at the end of the experiment run
await langfuse.flushAsync();
```

</Tab>
</Tabs>

### Optionally: Add an evaluation method

Optionally, the output of the application can be evaluated to compare different runs more easily. More details on scores/evals [here](/docs/scores/overview). Options:

- Use any evaluation function and directly add a score while running the experiment. See below for implementation details.
- Set up [LLM-as-a-judge](/docs/scores/model-based-evals) within Langfuse to automatically evaluate the outputs of these runs. This greatly simplifies the process of adding evaluations to your experiments. We have recorded a [10 min walkthrough](/guides/videos/llm-as-a-judge-eval-on-dataset-experiments) on how this works end-to-end.

### Compare dataset runs

After each experiment run on a dataset, you can check the aggregated score in the dataset runs table and compare results side-by-side.

<CloudflareVideo
  videoId="f8f2cf7ff86f2b54d1b90c0921d2c7e9"
  aspectRatio={16 / 9}
  gifStyle
/>
</Steps>
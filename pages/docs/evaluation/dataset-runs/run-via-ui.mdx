---
title: Dataset Runs via the UI
description: Experiment with different prompt versions and models on a dataset and compare the results side-by-side.
---

# Dataset Runs via the UI

You can execute Dataset Runs (formerly Prompt Experiments) in the Langfuse UI to test different prompt versions from [Prompt Management](/docs/prompt-management) or language models and compare the results side-by-side.

Optionally, you can use [LLM-as-a-Judge Evaluators](/docs/evaluation/evaluation-methods/llm-as-a-judge) to automatically score the responses based on the expected outputs to further analyze the results on an aggregate level.

<CloudflareVideo
  videoId="0e2811d0dbcd59000837773aef814963"
  aspectRatio={16 / 9}
  title="Introduction to Prompt Experiments"
/>

## Why run Dataset Runs via the UI?

- Quickly test different prompt versions or models
- Stucture your prompt testing by using a dataset to test different prompt versions and models
- Quickly iterate on prompts through Dataset Runs
- Optionally use LLM-as-a-Judge Evaluators to score the responses based on the expected outputs from the dataset
- Prevent regressions by running experiments when making prompt changes

## Prerequisites
<Steps>


### Create a usable prompt
Create a prompt that you want to test and evaluate. [How to create a prompt?](/docs/prompt-management/get-started)
<Callout type="info">
**A prompt is usable when:** your prompt has variables that match the dataset item keys in the dataset that will be used for the dataset run. See the example below.
</Callout>

<details>
<summary>Example: Prompt Variables & Dataset Item Keys Mapping</summary>
<div className="grid md:grid-cols-1 gap-4">
<div>

<br/>**Prompt:**
```bash
You are a Langfuse expert. Answer based on:
{{documentation}}

Question: {{question}}
```

</div>
<div>

<br/>**Dataset Item:**
```json
{
  "documentation": "Langfuse is an LLM Engineering Platform",
  "question": "What is Langfuse?"
}
```

</div>
</div>

In this example:
- The prompt variable `{{documentation}}` maps to the JSON key `"documentation"`
- The prompt variable `{{question}}` maps to the JSON key `"question"`
- Both keys must exist in the dataset item's input JSON for the experiment to run successfully

</details>

<details>
<summary>Example: Chat Message Placeholder Mapping</summary>

In addition to variables, you can also map placeholders in chat message prompts to dataset item keys.
This is useful when the dataset item also contains for example a chat message history to use.
Your chat prompt needs to contain a placeholder with a name. Variables within placeholders are not resolved.

**Chat Prompt:**
Placeholder named: `message_history`

**Dataset Item:**
```json
{
  "message_history": [
    {
      "role": "user",
      "content": "What is Langfuse?"
    },
    {
      "role": "assistant",
      "content": "Langfuse is a tool for tracking and analyzing the performance of language models."
    }
  ],
  "question": "What is Langfuse?"
}
```

In this example: 
- The chat prompt placeholder `message_history` maps to the JSON key `"message_history"`.
- The prompt variable `{{question}}` maps to the JSON key `"question"` in a variable not within a placeholder message.
- Both keys must exist in the dataset item's input JSON for the experiment to run successfully

</details>

### Create a usable dataset
Create a dataset with the inputs and expected outputs you want to use for your prompt experiments. [How to create a dataset?](/docs/evaluation/dataset-runs/datasets)

<Callout type="info">
**A dataset is usable when:** [1] the dataset items have JSON objects as input and [2] these objects have JSON keys that matches the prompt variables of the prompt(s) you will use. See the example below.
</Callout>

 <details>
<summary>Example: Prompt Variables & Dataset Item Keys Mapping</summary>
<div className="grid md:grid-cols-1 gap-4">
<div>

<br/>**Prompt:**
```bash
You are a Langfuse expert. Answer based on:
{{documentation}}

Question: {{question}}
```

</div>
<div>

<br/>**Dataset Item:**
```json
{
  "documentation": "Langfuse is an LLM Engineering Platform",
  "question": "What is Langfuse?"
}
```

</div>
</div>

In this example:
- The prompt variable `{{documentation}}` maps to the JSON key `"documentation"`
- The prompt variable `{{question}}` maps to the JSON key `"question"`
- Both keys must exist in the dataset item's input JSON for the experiment to run successfully

</details>

### Configure LLM connection

As your prompt will be executed for each dataset item, you need to configure an LLM connection in the project settings. [How to configure an LLM connection?](/faq/all/llm-connection)

### Optional: Set up LLM-as-a-judge

You can set up an LLM-as-a-judge evaluator to score the responses based on the expected outputs. Make sure to set the target of the LLM-as-a-Judge to "Experiment runs" and filter for the dataset you want to use. [How to set up LLM-as-a-judge?](/docs/evaluation/evaluation-methods/llm-as-a-judge)
</Steps>


## Trigger a dataset run via UI

<Steps>
### Navigate to the dataset
Dataset Runs are currently started from the detail page of a dataset. 

- **Navigate to** `Your Project` > `Datasets`
- **Click on** the dataset you want to start a Dataset Run for

<Frame border fullWidth>
![New Experiment Button](/images/docs/navigate-to-dataset.png)
</Frame>

### Open the setup page

**Click on** `Start Experiment` to open the setup page

<Frame border fullWidth>
![New Experiment Button](/images/docs/trigger-process.png)
</Frame>

**Click on** `Create` below `prompt Experiment`

<Frame border>
![New Experiment Button](/images/docs/trigger-process-2.png)
</Frame>

### Configure the Dataset Run

1. **Set** a Dataset Run name
2. **Select** the prompt you want to use
3. **Set up or select** the LLM connection you want to use
4. **Select** the dataset you want to use
5. **Optionally select** the evaluator you want to use
6. **Click on** `Create` to trigger the Dataset Run

<Frame border>
![New Experiment Button](/images/docs/configure_dataset_run.png)
</Frame>

This will trigger the Dataset Run and you will be redirected to the Dataset Runs page. The run might take a few seconds or minutes to complete depending on the prompt complexity and dataset size.

### Compare runs

After each experiment run, you can check the aggregated score in the dataset runs table and compare results side-by-side.

<CloudflareVideo
  videoId="f8f2cf7ff86f2b54d1b90c0921d2c7e9"
  aspectRatio={16 / 9}
  gifStyle
/>

</Steps>

## GitHub Discussions

import { GhDiscussionsPreview } from "@/components/gh-discussions/GhDiscussionsPreview";

<GhDiscussionsPreview labels={["feat-prompt-experiments"]} />

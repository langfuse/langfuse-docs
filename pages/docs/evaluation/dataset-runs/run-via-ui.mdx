---
title: Prompt Experiments
description: Run prompt experiments on datasets with LLM-as-a-Judge evaluations.
---

# Dataset Runs via the UI

You can execute Prompt Experiments in the Langfuse UI to test different prompt versions from [Prompt Management](/docs/prompt-management) or different models and compare the results side-by-side.

Optionally, you can use [LLM-as-a-Judge Evaluators](/docs/evaluation/evaluation-methods/llm-as-a-judge) to automatically evaluate the responses based on the expected outputs to further analyze the results on an aggregate level.

## Why use Prompt Experiments?

- **Feedback loop**: Quickly iterate on prompts by running experiments and directly comparing evaluation results side-by-side.
- **Regression prevention**: When making prompt changes, run an experiment to make sure that the new version does not cause bad outputs.

## Overview

<CloudflareVideo
  videoId="0e2811d0dbcd59000837773aef814963"
  aspectRatio={16 / 9}
  title="Introduction to Prompt Experiments"
/>

## Get started

<Steps>

### Create a prompt

<Frame border fullWidth>
![Create a prompt](/images/docs/prompt-experiments-create-prompt.png)
</Frame>

Create a prompt that you want to test on the dataset. As you are testing your prompt using all dataset items as input, your prompt needs to contain variables that match the keys in the dataset items.

import {FileJson} from "lucide-react";

<Cards num={1}>

  <Card
    icon={<FileJson size="24" />}
    title="How to create a prompt?"
    href="/docs/prompt-management/get-started"
    arrow
  />
</Cards>

<details>
<summary>Example: Prompt & Dataset Item Mapping</summary>

For Prompt Experiments, you need to map the variables in your prompt to the keys in your dataset items.

The following example demonstrates how prompt variables are mapped to dataset item inputs:

<div className="grid md:grid-cols-2 gap-4">
<div>

<br/>**Prompt:**
```bash
You are a Langfuse expert. Answer based on:
{{documentation}}

Question: {{question}}
```

</div>
<div>

<br/>**Dataset Item:**
```json
{
  "documentation": "Langfuse is an LLM Engineering Platform",
  "question": "What is Langfuse?"
}
```

</div>
</div>

In this example:
- The prompt variable `{{documentation}}` maps to the JSON key `"documentation"`
- The prompt variable `{{question}}` maps to the JSON key `"question"`
- Both keys must exist in the dataset item's input JSON for the experiment to run successfully

</details>

<details>
<summary>Example: Chat Message Placeholder Mapping</summary>

In addition to variables, you can also map placeholders in chat message prompts to dataset item keys.
This is useful when the dataset item also contains for example a chat message history to use.
Your chat prompt needs to contain a placeholder with a name. Variables within placeholders are not resolved.

**Chat Prompt:**
Placeholder named: `message_history`

**Dataset Item:**
```json
{
  "message_history": [
    {
      "role": "user",
      "content": "What is Langfuse?"
    },
    {
      "role": "assistant",
      "content": "Langfuse is a tool for tracking and analyzing the performance of language models."
    }
  ],
  "question": "What is Langfuse?"
}
```

In this example: 
- The chat prompt placeholder `message_history` maps to the JSON key `"message_history"`.
- The prompt variable `{{question}}` maps to the JSON key `"question"` in a variable not within a placeholder message.
- Both keys must exist in the dataset item's input JSON for the experiment to run successfully

</details>

### Create a dataset

<Frame border fullWidth>
![Create a dataset](/images/docs/prompt-experiments-create-dataset-item.png)
</Frame>

Create a dataset with the inputs and expected outputs that you want to test your prompt on. Make sure that the dataset items include the input variables that should be inserted into the prompt.

import {Database} from "lucide-react";

<Cards num={1}>

  <Card
    icon={<Database size="24" />}
    title="How to create a dataset?"
    href="/docs/evaluation/dataset-runs/datasets"
    arrow
  />
</Cards>

<details>
<summary>Example Dataset Item with variables</summary>

```json filename="input"
{
  "question": "What is Langfuse?",
  "documentation": "Langfuse - the LLM Engineering Platform"
}
```

```text filename="expected_output"
Langfuse is the LLM Engineering Platform.
```

</details>



### Configure LLM connection

<Frame border fullWidth>
![New LLM connection](/images/docs/prompt-experiments-new-llm-connection.png)
</Frame>

Prompt Experiments runs LLM calls within Langfuse. Thus, you need to configure an LLM connection in the project settings.

import {KeyRound} from "lucide-react";

<Cards num={1}>

  <Card
    icon={<KeyRound size="24" />}
    title="How to configure an LLM connection?"
    href="/faq/all/llm-connection"
    arrow
  />
</Cards>


### Optional: Set up LLM-as-a-judge

You can set up an LLM-as-a-judge to score the responses based on the expected outputs. Make sure to set the target of the LLM-as-a-Judge to "Experiment runs" and optionally filter for the dataset you want to use.


import {WandSparkles} from "lucide-react";

<Cards num={1}>

  <Card
    icon={<WandSparkles size="24" />}
    title="How to set up LLM-as-a-judge?"
    href="/docs/evaluation/evaluation-methods/llm-as-a-judge"
    arrow
  />
</Cards>

## Run a prompt experiment

Now that we have set up a prompt version and a dataset, we can run a prompt experiment in Langfuse for each prompt version that we want to test.

When viewing the prompt details or a dataset, use the following button to run a prompt experiment:

<Frame border fullWidth>
![New Experiment Button](/images/docs/prompt-experiments-new-experiment.png)
</Frame>

Select the prompt version, dataset, and model configuration that you want to test. Before running the experiment, you will see whether the prompt variables match the dataset variables.

## Compare runs

After each experiment run, you can check the aggregated score in the dataset runs table and compare results side-by-side.

<CloudflareVideo
  videoId="f8f2cf7ff86f2b54d1b90c0921d2c7e9"
  aspectRatio={16 / 9}
  gifStyle
/>

</Steps>

## Troubleshooting

If you see a warning about mismatched variables, ensure that:
- Every `{{variable}}` in your prompt has a matching key in your dataset items' input JSON
- The names match exactly (including case sensitivity)
- Your dataset input is valid JSON format


## GitHub Discussions

import { GhDiscussionsPreview } from "@/components/gh-discussions/GhDiscussionsPreview";

<GhDiscussionsPreview labels={["feat-prompt-experiments"]} />

---
title: LLM-as-a-Judge Evaluation
sidebarTitle: LLM-as-a-Judge
description: Configure, run, and monitor LLM-powered evaluators on observations, traces, and experiments.
---

# LLM-as-a-Judge

Use an LLM to automatically score your application outputs. For this evaluation method, the LLM is presented with your application's input and output and asked to score and reason about the output. It will then produce a [`score`](/docs/evaluation/core-concepts#scores) including a comment with chain-of-thought reasoning.

<Video
  src="https://static.langfuse.com/docs-videos/2025-12-19-llm-as-a-judge-overview.mp4"
  aspectRatio={16 / 9}
  gifStyle
/>

## Why use LLM-as-a-Judge?

- **Scalable:** Judge thousands of outputs quickly versus human annotators.
- **Human‑like:** Captures nuance (e.g. helpfulness, toxicity, relevance) better than simple metrics, especially when rubric‑guided.
- **Repeatable:** With a fixed rubric, you can rerun the same prompts to get consistent scores.

## How to use LLM-as-a-Judge?

LLM-as-a-Judge evaluators can run on three types of data: **Observations** (individual operations), **Traces** (complete workflows), or **Experiments** (controlled test datasets). Your choice depends on whether you're testing in development or monitoring production, and what level of granularity you need.

### Decision Tree

import { Card, CardContent, CardDescription, CardHeader, CardTitle } from "@/components/ui/card";

<div className="flex flex-col items-center gap-6 py-8 my-6">

<Card className="border-2 border-primary">
  <CardContent className="p-4 text-center font-semibold">
    Which data needs to be evaluated?
  </CardContent>
</Card>

<div className="text-2xl text-primary">↓</div>

<div className="flex flex-col md:flex-row gap-12 w-full justify-center">

{/* Live Production Branch */}
<div className="flex flex-col items-center gap-4 flex-1 min-w-[280px]">
  <Card className="border-2 border-primary w-full">
    <CardHeader className="p-4">
      <CardTitle className="text-base text-center">Live Production Data</CardTitle>
      <CardDescription className="text-center text-xs">Monitor real-time traffic</CardDescription>
    </CardHeader>
  </Card>

  <div className="text-xl text-primary">↓</div>

  <div className="flex flex-col gap-3 w-full">
    <Card className="border-2 border-primary">
      <CardContent className="p-3">
        <div className="font-semibold text-sm">Observations <span className="text-xs font-normal opacity-70">(Beta)</span></div>
        <div className="text-xs text-muted-foreground mt-1">Individual operations: LLM calls, retrievals, tool calls</div>
      </CardContent>
    </Card>

    <Card className="border">
      <CardContent className="p-3">
        <div className="font-semibold text-sm">Traces</div>
        <div className="text-xs text-muted-foreground mt-1">Complete workflow executions</div>
      </CardContent>
    </Card>
  </div>
</div>

{/* Offline Experiments Branch */}
<div className="flex flex-col items-center gap-4 flex-1 min-w-[280px]">
  <Card className="border-2 border-primary w-full">
    <CardHeader className="p-4">
      <CardTitle className="text-base text-center">Offline Experiment Data</CardTitle>
      <CardDescription className="text-center text-xs">Test in controlled environment</CardDescription>
    </CardHeader>
  </Card>

  <div className="text-xl text-primary">↓</div>

  <Card className="border-2 border-primary w-full">
    <CardContent className="p-3">
      <div className="font-semibold text-sm">Experiments</div>
      <div className="text-xs text-muted-foreground mt-1">Controlled test cases with datasets</div>
    </CardContent>
  </Card>
</div>

</div>

</div>

<Callout type="info">
**Production Pattern**: Teams typically use **Experiments** during development to validate changes, then deploy **Observation-level** evaluators in production for scalable, precise monitoring.
</Callout>

### Understanding Each Evaluation Target

<Tabs items={["Live Production Data", "Offline Experiment Data"]} storageKey="eval-data-type">
<Tab>

Evaluate live production traffic to monitor your LLM application performance in real-time.

<Tabs items={["Observations (Beta)", "Traces"]} storageKey="eval-live-target">
<Tab>

Run evaluators on individual observations within your traces—such as LLM calls, retrieval operations, embedding generations, or tool calls.

**Why target Observations**

- **Dramatically faster execution**: Evaluations complete in seconds, not minutes. Eliminates evaluation delays and backlogs. Asynchronous architecture processes thousands of evaluations per minute.
- **Operation-level precision**: Filter by observation type to evaluate only final LLM responses or retrieval steps, not entire workflows. Reduces evaluation volume and cost by targeting specific operations.
- **Compositional evaluation**: Run different evaluators on different operations within one trace. Toxicity on LLM outputs, relevance on retrievals, accuracy on generations—simultaneously.
- **Combined filtering**: Stack observation filters (type, name, metadata) with trace filters (userId, sessionId, tags, version). Example: "all LLM generations in conversations tagged 'customer-support' for premium users".

**Data Flow**

At ingest time, each observation is evaluated against your filter criteria. Matching observations are added to an evaluation queue. Evaluation jobs are then processed asynchronously. Scores are attached to the specific observation, resulting in one score per observation per evaluator. Depending on your filter criteria, multiple observations may match the criteria and result in multiple scores per trace.

**Example Use Cases**
- Evaluate helpfulness of only the final chatbot response to users
- Monitor toxicity scores on all customer-facing LLM generations
- Track retrieval relevance for RAG systems by targeting document retrieval observations

</Tab>

<Tab>

Run evaluators on complete traces, evaluating entire workflow executions from start to finish.

<Callout type="info">
**Consider targeting Observations instead**: Observation-level evaluators complete in seconds (vs minutes for trace-level), eliminating evaluation delays. They also offer better precision and cost efficiency for production monitoring. See [upgrade guide](/faq/all/llm-as-a-judge-migration).
</Callout>

**Why target Traces**

- You need to evaluate aggregate data across a full workflow
- Your evaluation requires full context spanning multiple operations
- You're on legacy SDK versions (Python v2 or JS/TS v3) and cannot upgrade

**Data Flow**

At ingest time, each trace is evaluated against your filter criteria. Matching traces are added to an evaluation queue. Evaluation jobs are then processed asynchronously. Scores are attached to the trace itself, resulting in one score per trace per evaluator.

**Example Use Cases**
- Score the accuracy of a multi-step agent workflow (e.g., retrieval → reranking → generation → citation)
- Track the effectiveness of a full agent execution (planning → tool use → synthesis)

</Tab>
</Tabs>

</Tab>

<Tab>

Run evaluators on controlled test datasets to compare model versions, prompt variations, or system configurations in a reproducible environment.

**Why target Experiments**

- You need reproducible benchmarks for decision-making
- Comparing multiple prompt versions or model configurations
- You have datasets with expected outputs (ground truth)

**Data Flow**

Each experiment run generates traces that are automatically scored by your selected evaluators. Think of each experiment item as a test case: input → execution → output → evaluation.

1. Create a dataset with test inputs and (optionally) expected outputs. You may also define your test data locally.
2. Run experiment via UI or SDK—this executes your application code for each dataset item. See [Experiments via UI](/docs/evaluation/experiments/experiments-via-ui) or [Experiments via SDK](/docs/evaluation/experiments/experiments-via-sdk) for more information.
3. Selected evaluators to automatically score the generated outputs
4. Compare results across experiment runs to make data-driven decisions

**Example Use Case**

- Compare GPT-4 vs Claude Opus on 50 customer support questions, evaluate both for accuracy and helpfulness, then deploy the better-performing model

</Tab>
</Tabs>



## Set up step-by-step

<Steps>

### Create a new LLM-as-a-Judge evaluator

Navigate to the Evaluators page and click on the `+ Set up Evaluator` button.

<Frame fullWidth>![Evaluator create](/images/docs/evaluator-create.png)</Frame>

### Set the default model

Next, define the default model used for the evaluations. This step requires an LLM Connection to be set up. Please see [LLM Connections](/docs/administration/llm-connection) for more information.

<Callout type="info">
  It's crucial that the chosen default model supports structured output. This is
  essential for our system to correctly interpret the evaluation results from
  the LLM judge.
</Callout>

### Pick an Evaluator

<Frame fullWidth>![Evaluator select](/images/docs/evaluator-select.png)</Frame>

Next, select an evaluator. There are two main ways:

<Tabs items={["Managed Evaluator", "Custom Evaluator"]}>
<Tab>

Langfuse ships a growing catalog of evaluators built and maintained by us and partners like **Ragas**. Each evaluator captures best-practice evaluation prompts for a specific quality dimension—e.g. _Hallucination_, _Context-Relevance_, _Toxicity_, _Helpfulness_.

- **Ready to use**: no prompt writing required.
- **Continuously expanded**: by adding OSS partner-maintained evaluators and more evaluator types in the future (e.g. regex-based).

</Tab>
<Tab>

When the library doesn't fit your specific needs, add your own:

1. Draft an evaluation prompt with `{{variables}}` placeholders (`input`, `output`, `ground_truth` …).
2. Optional: Customize the **score** (0-1) and **reasoning** prompts to guide the LLM in scoring.
3. Optional: Pin a custom dedicated model for this evaluator. If no custom model is specified, it will use the default evaluation model (see Section 2).
4. Save → the evaluator can now be reused across your project.

</Tab>
</Tabs>

### Choose which Data to Evaluate

With your evaluator and model selected, configure which data to run the evaluations on. See the [How it works](#how-it-works) section above to understand which option fits your use case.

<Tabs items={["Live Production Data", "Offline Experiment Data"]} storageKey="eval-data-type">
<Tab>

<Tabs items={["Observations (Beta)", "Traces"]} storageKey="eval-live-target">
<Tab>

**Configuration Steps**

1. Select "Live Observations" as your evaluation target
2. Filter to specific observations using observation type, trace name, trace tags, userId, sessionId, metadata, and other attributes
3. Configure sampling percentage (e.g., 5%) to manage evaluation costs and throughput

**Requirements**

- **SDK version**: Python v3+ (OTel-based) or JS/TS v4+ (OTel-based)
  - [Python v2 → v3 migration guide](/docs/observability/sdk/upgrade-path#python-sdk-v2--v3)
  - [JS/TS v3 → v4 migration guide](/docs/observability/sdk/upgrade-path#jsts-sdk-v3--v4)
- **When filtering by trace attributes**: To filter observations by trace-level attributes (`userId`, `sessionId`, `version`, `tags`, `metadata`, `traceName`), use [`propagate_attributes()`](/docs/observability/sdk/instrumentation#add-attributes-to-observations) in your instrumentation code. Without this, trace attributes will not be available on observations.

</Tab>

<Tab>

<Callout type="info">
**Performance consideration**: We recommend using Observation-level evaluators for production monitoring. They complete in seconds (vs minutes for trace-level), eliminating evaluation delays and backlogs. They also offer better precision and cost efficiency. See [upgrade guide](/faq/all/llm-as-a-judge-migration).
</Callout>

**Configuration Steps**

1. Select "Live Traces" as your evaluation target
2. Filter traces by name, tags, userId, and other trace-level attributes
3. Choose whether to run on new traces only or include existing traces (backfilling)
4. Configure sampling percentage (e.g., 5%) to manage evaluation costs and throughput
5. Preview matched traces from the last 24 hours to validate your filter configuration

<Frame fullWidth>
  ![Production tracing data](/images/docs/evaluator-trace-filter.png)
</Frame>

**Requirements**

- **OTel-based SDKs**: If you're using Python v3+ or JS/TS v4+, you must call `update_trace()` to populate trace input/output, or use observation-level evaluators instead.


</Tab>
</Tabs>

</Tab>
<Tab>

**Configuration Steps**

- **[Experiments via UI](/docs/evaluation/experiments/experiments-via-ui)**: When running experiments through the UI, select which evaluators to run. These evaluators will automatically execute on the data generated by your next run.

- **[Experiments via SDK](/docs/evaluation/experiments/experiments-via-sdk)**: Configure evaluators directly in code using the experiment runner SDK.

**Requirements (for Experiments via SDK)**

- **Recommended**: Python >= 3.9.0 or JS/TS >= 4.4.0 with experiment runner functions ([`run_experiment()`](/docs/evaluation/experiments/experiments-via-sdk) / [`experiment.run()`](/docs/evaluation/experiments/experiments-via-sdk)). More performant architecture with built-in evaluator orchestration.
- **Legacy support**: Older SDK versions supported. Upgrade recommended for better performance.



</Tab>
</Tabs>

### Map Variables & preview Evaluation Prompt

You now need to teach Langfuse _which properties_ of your observation, trace, or experiment item represent the actual data to populate these variables for a sensible evaluation. For instance, you might map your system's logged observation input to the prompt's `{{input}}` variable, and the LLM response (observation output) to the prompt's `{{output}}` variable. This mapping is crucial for ensuring the evaluation is sensible and relevant.

<Tabs items={["Live Production Data", "Offline Experiment Data"]} storageKey="eval-data-type">
<Tab>

- **Prompt Preview**: As you configure the mapping, Langfuse shows a **live preview of the evaluation prompt populated with actual data**. This preview uses historical data from the last 24 hours that matched your filters. You can navigate through several examples to see how their respective data fills the prompt, helping you build confidence that the mapping is correct.
- **JSONPath**: If the data is nested (e.g., within a JSON object), you can use a JSONPath expression (like `$.choices[0].message.content`) to precisely locate it.

<Frame fullWidth>![Filter preview](/images/docs/evaluator-mapping.png)</Frame>

</Tab>
<Tab>

- **Suggested mappings**: The system will often be able to autocomplete common mappings based on typical field names in experiments. For example, if you're evaluating for correctness, and your prompt includes `{{input}}`, `{{output}}`, and `{{ground_truth}}` variables, we would likely suggest mapping these to the experiment item's input, output, and expected_output respectively.
- **Edit mappings**: You can easily edit these suggestions if your experiment schema differs. You can map any properties of your experiment item (e.g., `input`, `expected_output`). Further, as experiments create traces under the hood, using the trace input/output as the evaluation input/output is a common pattern. Think of the trace output as your experiment run's output.

</Tab>
</Tabs>

### Trigger the evaluation

To see your evaluator in action, you need to either [send traces](/docs/observability/get-started) (fastest) or trigger an experiment run (takes longer to setup) via the [UI](/docs/evaluation/experiments/experiments-via-ui) or [SDK](/docs/evaluation/experiments/experiments-via-sdk). Make sure to set the correct target data in the evaluator settings according to how you want to trigger the evaluation.

</Steps>

✨ Done! You have successfully set up an evaluator which will run on your data.

<Callout type="info">
  Need custom logic? Use the SDK instead—see [Custom
  Scores](/docs/evaluation/evaluation-methods/custom-scores) or an [external
  pipeline example](/docs/scores/external-evaluation-pipelines).
</Callout>

## Debug LLM-as-a-Judge Executions

Every LLM-as-a-Judge evaluator execution creates a full trace, giving you complete visibility into the evaluation process. This allows you to debug prompt issues, inspect model responses, monitor token usage, and trace evaluation history.

You can show the LLM-as-a-Judge execution traces by filtering for the environment `langfuse-llm-as-a-judge` in the tracing table:

<Frame fullWidth>
  ![Tracing table filtered to langfuse-llm-as-a-judge
  environment](/images/docs/evaluation/llm-as-a-judge-debug-traces.png)
</Frame>

<details>
<summary>LLM-as-a-Judge Execution Status</summary>

- **Completed**: Evaluation finished successfully.
- **Error**: Evaluation failed (click execution trace ID for details).
- **Delayed**: Evaluation hit rate limits by the LLM provider and is being retried with exponential backoff.
- **Pending**: Evaluation is queued and waiting to run.

</details>

## Advanced Topics

### Migrating from Trace-Level to Observation-Level Evaluators

If you have existing evaluators running on traces and want to upgrade to running on observations for better performance and reliability, check out our comprehensive [Evaluator Migration Guide](/faq/all/llm-as-a-judge-migration).

## GitHub Discussions

import { GhDiscussionsPreview } from "@/components/gh-discussions/GhDiscussionsPreview";

<GhDiscussionsPreview labels={["feat-evals"]} />

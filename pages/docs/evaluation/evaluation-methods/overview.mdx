---
title: Evaluation Methods Overview
description: Overview of the different evaluation methods available in Langfuse.
---

# Evaluation Methods

Evaluation methods let you assign evaluation `scores` to `traces`, `observations`, `sessions`, or `dataset runs`. 

You can use the following evaluation methods to add `scores`:

- [LLM-as-a-Judge](/docs/evaluation/evaluation-methods/llm-as-a-judge)
- [Scores via UI](/docs/evaluation/evaluation-methods/scores-via-ui)
- [Annotation Queues](/docs/evaluation/evaluation-methods/annotation-queues)
- [Scores via API/SDK](/docs/evaluation/evaluation-methods/scores-via-sdk)

## Score Analytics

[Score Analytics](/docs/evaluation/evaluation-methods/score-analytics) provides an easy way to analyze your evaluation data out of the box. Whether you're validating that different LLM judges produce consistent results, checking if human scores align with automated evaluations, or exploring score distributions and trends, Score Analytics helps you build confidence in your evaluation process.


---
title: Add scores to traces via the UI
description: Annotate traces and observations with scores in the Langfuse UI to record human-in-the-loop evaluations.
sidebarTitle: Scores via UI
---

# Manual Scores via UI

Adding scores via the UI is a manual [evaluation method](/docs/evaluation/evaluation-methods/overview). It is used to collaboratively annotate traces, sessions and observations with evaluation scores.

<Video
  src="https://static.langfuse.com/docs-videos/2025-12-19-manual-scoring.mp4"
  aspectRatio={16 / 9}
  gifStyle
/>

<Callout type="info">
You can also use [Annotation Queues](docs/evaluation/evaluation-methods/annotation-queues) to streamline working through reviewing larger batches of of traces, sessions and observations.
</Callout>

## Why manually adding scores via UI?

- Allow multiple team members to manually review data and improve accuracy through diverse expertise.
- Standardized score configurations and criteria ensure consistent data labeling across different workflows and scoring types.
- Human baselines provide a reference point for benchmarking other scores and curating high-quality datasets from production logs.

## Set up step-by-step

<Steps>
### Create a Score Config

To add scores in the UI, you need to have at least one Score Config set up. See [how to create and manage Score Configs](/faq/all/manage-score-configs) for details.

### Add Scores

On a Trace, Session or Observation detail view click on `Annotate` to open the annotation form.

<Frame fullWidth>![Annotate](/images/docs/trigger_annotation.png)</Frame>

### Select Score Configs to use

<Frame fullWidth>![Annotate](/images/docs/select_score_configs.png)</Frame>

### Set Score values

<Frame fullWidth>![Annotate](/images/docs/set_score_values.png)</Frame>

### See the Scores
To see your newly added scores on traces or observations, **click on** the `Scores` tab on the trace or observation detail view.

<Frame fullWidth>
  ![Detail scores table](/images/docs/see_created_scores.png)
</Frame>

</Steps>

## Add scores to experiments

When running [experiments via UI](/docs/evaluation/experiments/experiments-via-ui) or via [SDK](/docs/evaluation/experiments/experiments-via-sdk), you can annotate results directly from the experiment compare view. 


<Callout type="info">
**Prerequisites:**
- Set up [score configurations](/faq/all/manage-score-configs) for the dimensions you want to evaluate
- Execute an [experiment via UI](/docs/evaluation/experiments/experiments-via-ui) or [SDK](/docs/evaluation/experiments/experiments-via-sdk) to generate results to review

</Callout>

<Frame fullWidth>
  ![Annotate from compare view](/images/changelog/2025-10-23-annotate-compare-view-overview.png)
</Frame>

The compare view maintains full experiment context: Inputs, outputs, and automated scores, while you review each item. Summary metrics update as you add annotation scores, allowing you to track progress across the experiment.

## GitHub Discussions

import { GhDiscussionsPreview } from "@/components/gh-discussions/GhDiscussionsPreview";

<GhDiscussionsPreview labels={["feat-annotation"]} />

---
title: Evaluation of LLM Applications
description: With Langfuse you can capture all your LLM evaluations in one place. You can combine a variety of different evaluation metrics like model-based evaluations (LLM-as-a-Judge), manual annotations or fully custom evaluation workflows via API/SDKs. This allows you to measure quality, tonality, factual accuracy, completeness, and other dimensions of your LLM application.
---

# Evaluation Overview

Evaluation is a critical aspect of developing and deploying LLM applications. Usually, teams use a multitude of different evaluation methods to score the performance of their AI application depending on the use case and the stage of the development process.

## Intro

### Why are LLM Evals important?

LLM evaluation is crucial for improving the accuracy and robustness of language models, ultimately enhancing the user experience and trust in your AI application. It helps detect hallucinations and measure performance across diverse tasks. A structured evaluation in production is vital for continuously improving your application.

### Core Concepts

| Concept | Description |
|---------|-------------|
| **Scores** | Scores are a flexible data object that can be used to store any evaluation metric and link it to other objects in Langfuse. |
| **Evaluation Methods** | Evaluation methods are functions or tools to assign scores to other objects. |
| **Datasets** | Datasets are a collection of inputs and, optionally, expected outputs that can be during Dataset runs. |
| **Dataset Runs** | Dataset runs are used to run a dataset through your LLM application and optionally apply evaluation methods to the results. |

### Online vs. Offline Evaluation
TBD

## Evaluation Methods

Langfuse uses the `score` object to store evaluation metrics, it is meant to be flexible to represent any evaluation metric. 

Learn more about the [data model](/docs/scores/data-model). This data model is utilized across all evaluation methods, both UI and API. It can also be accessed programmatically through the SDKs or API for custom workflows.

import { Lightbulb, SquarePercent, ClipboardPen} from "lucide-react";

<Cards num={2}>
  <Card
    icon={<Lightbulb size="24" />}
    title="LLM-as-a-Judge"
    href="/docs/evaluation/dataset-runs/datasets"
    arrow
  />
  <Card
    icon={<SquarePercent size="24" />}
    title="Custom Scores"
    href="/docs/evaluation/dataset-runs/data-model"
    arrow
  />
  <Card
    icon={<ClipboardPen size="24" />}
    title="Human Annotations"
    href="/docs/evaluation/dataset-runs/run-via-ui"
    arrow
  />
</Cards>

## Dataset runs

Dataset Runs in Langfuse allow you to test your application on a dataset of inputs and expected outputs. You can then compare the results of different prompts, models, or other parameters side-by-side.

<Frame border fullWidth>
![Dataset Runs](/images/docs/dataset-runs-comparison.png)
</Frame>


### Why Dataset Runs?

- **Continuous improvement:** Create datasets from production edge cases to improve your application
- **Pre-deployment testing:** Benchmark new releases before deploying to production
- **Structured testing:** Run experiments on collections of inputs and expected outputs
- **Flexible evaluation:** Add custom evaluation metrics or use llm-as-a-judge
- **Integrates well:** Works with popular frameworks like LangChain and LlamaIndex

### Where to start

Follow the quickstart to add tracing to your LLM app.

import { Table, WandSparkles, CodeXml, Database} from "lucide-react";

<Cards num={2}>
  <Card
    icon={<Table size="24" />}
    title="Create a dataset"
    href="/docs/evaluation/dataset-runs/datasets"
    arrow
  />
  <Card
    icon={<Database size="24" />}
    title="Datamodel"
    href="/docs/evaluation/dataset-runs/data-model"
    arrow
  />
  <Card
    icon={<WandSparkles size="24" />}
    title="Dataset runs via the UI"
    href="/docs/evaluation/dataset-runs/run-via-ui"
    arrow
  />
  <Card
    icon={<CodeXml size="24" />}
    title="Dataset runs via the SDK"
    href="/docs/evaluation/dataset-runs/run-via-sdk"
    arrow
  />
</Cards>


## GitHub Discussions

import { GhDiscussionsPreview } from "@/components/gh-discussions/GhDiscussionsPreview";

<GhDiscussionsPreview labels={["feat-scores"]} />

---
title: Evaluation of LLM Applications
description: With Langfuse you can capture all your LLM evaluations in one place. You can combine a variety of different evaluation metrics like model-based evaluations (LLM-as-a-Judge), human annotations or fully custom evaluation workflows via API/SDKs. This allows you to measure quality, tonality, factual accuracy, completeness, and other dimensions of your LLM application.
---

# Evaluation Overview

Evaluation is a critical aspect of developing and deploying LLM applications. Usually, teams use a multitude of different evaluation methods to score the performance of their AI application depending on the use case and the stage of the development process.

## Why use LLM Evaluation?

LLM evaluation is crucial for improving the accuracy and robustness of language models, ultimately enhancing the user experience and trust in your AI application. Here are the key benefits:

- **Quality Assurance**: Detect hallucinations, factual inaccuracies, and inconsistent outputs to ensure your AI app delivers reliable results
- **Performance Monitoring**: Measure response quality, relevance, and user satisfaction across different scenarios and edge cases
- **Continuous Improvement**: Identify areas for enhancement and track improvements over time through structured evaluation metrics
- **User Trust**: Build confidence in your AI application by demonstrating consistent, high-quality outputs through systematic evaluation
- **Risk Mitigation**: Catch potential issues before they reach production users, reducing the likelihood of poor user experiences or reputational damage

## Online & Offline Evaluation

**Offline Evaluation involves** 
- Evaluating the application in a controlled setting
- Typically using curated test Datasets instead of live user queries 
- Heavily used during development (can be part of CI/CD pipelines) to measure improvements / regressions
- Repeatable and you can get clear accuracy metrics since you have ground truth.

**Online Evaluation involves**
- Evaluating the application in a live, real-world environment, i.e. during actual usage in production. 
- Use Evaluation Methods that track success rates, user satisfaction scores, or other metrics on live traffic
- Advantage of online evaluation is that it captures things you might not anticipate in a lab setting 
- Can include collecting implicit and explicit user feedback, and possibly running shadow tests or A/B tests 

**In practice, successful evaluation blends online and offline evaluations.** Many teams adopt a loop-like approach. This way, evaluation is continuous and ever-improving.

<Frame border fullWidth>
![Continuous evaluation loop](/images/docs/evaluation/online-offline-loop.png)
</Frame>

_Adapted from: “How to continuously improve LLM products?”, Evidently_

## Core Concepts

| Concept | Description |
|---------|-------------|
| **Scores** | Scores are a flexible data object that can be used to store any evaluation metric and link it to other objects in Langfuse. |
| **Evaluation Methods** | Evaluation methods are functions or tools to assign scores to other objects. |
| **Datasets** | Datasets are a collection of inputs and, optionally, expected outputs that can be during Dataset runs. |
| **Dataset Runs** | Dataset runs are used to run a dataset through your LLM application and optionally apply evaluation methods to the results. |

## Evaluation Methods [#evaluation-methods]
Evaluation methods are functions or tools to assign evaluation `Score`s to other objects. Langfuse uses the Scores to store evaluation metrics, it is meant to be flexible to represent any evaluation metric. 

Langfuse currently supports: automatic scoring through **LLM-as-a-Judge**, manual **Human Annotations** or fully **Custom Scoring via API/SDKs**. We keep adding more evaluation methods fast, so stay tuned!

import { Lightbulb, SquarePercent, ClipboardPen} from "lucide-react";

<Cards num={1}>
  <Card
    icon={<Lightbulb size="24" />}
    title="LLM-as-a-Judge"
    href="/docs/evaluation/dataset-runs/datasets"
    arrow
  />
  <Card
    icon={<ClipboardPen size="24" />}
    title="Human Annotations"
    href="/docs/evaluation/dataset-runs/native-run"
    arrow
  />
    <Card
    icon={<SquarePercent size="24" />}
    title="Custom Scores"
    href="/docs/evaluation/dataset-runs/data-model"
    arrow
  />
</Cards>

Learn more about the [Scores Data Model](/docs/evaluation/evaluation-methods/data-model). 

## Dataset Runs [#dataset-runs]
**Dataset Runs** are used to loop your LLM application through **Datasets** and **optionally apply Evaluation Methods** to the results. This lets you strategically evaluate your application and compare the performance of different inputs, prompts, models, or other parameters side-by-side against controlled conditions.

In Langfuse we differentiate between **Native** vs. **Remote** Dataset Runs. Native Dataset Runs rely on Dataset, Prompts and optionally LLM-as-a-Judge Evaluators all being on the Langfuse platform. Remote Dataset Runs rely only on Datasets being on the Langfuse platform, prompts and evaluation methods can managed off platform – they are run via code.

All require managing the [Datasets](/docs/evaluation/dataset-runs/datasets) on the Langfuse platform.

import { Table, WandSparkles, CodeXml, Database, MousePointerClick} from "lucide-react";

<Cards num={1}>
  <Card
    icon={<Table size="24" />}
    title="Create a Dataset"
    href="/docs/evaluation/dataset-runs/datasets"
    arrow
  />
    <Card
    icon={<CodeXml size="24" />}
    title="Remote Dataset Runs"
    href="/docs/evaluation/dataset-runs/remote-run"
    arrow
  />
  <Card
    icon={<WandSparkles size="24" />}
    title="Native Dataset Runs"
    href="/docs/evaluation/dataset-runs/native-run"
    arrow
  />
</Cards>

Learn more about the [Dataset Runs Data Model](/docs/evaluation/dataset-runs/data-model).


## GitHub Discussions

import { GhDiscussionsPreview } from "@/components/gh-discussions/GhDiscussionsPreview";

<GhDiscussionsPreview labels={["feat-scores"]} />

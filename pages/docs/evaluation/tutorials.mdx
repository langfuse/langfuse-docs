---
description: Comprehensive guides and tutorials for evaluating LLM applications with Langfuse
---

# Evaluation Tutorials

Start with the fundamentals and progress through advanced techniques as your application complexity grows. Read our [blog post](/blog/2025-11-09-evals) for an overview.

import { TutorialCard, TutorialCards } from "@/components/TutorialCard";
import { Database, Bug, TestTube, Sparkles, TrendingUp, MessagesSquare, Play, Bot } from "lucide-react";

<TutorialCards>
  <TutorialCard
    title="RAG Observability & Evals"
    description="Specialized metrics for retrieval-augmented generation pipelines including retrieval relevance, answer faithfulness, and context completeness."
    href="/blog/2025-10-28-rag-observability-and-evals"
    icon={<Database />}
  />
  <TutorialCard
    title="Error Analysis"
    description="Review traces to classify issues like hallucinations, irrelevance, and formatting errors. Turn raw logs into actionable insights."
    href="/blog/2025-08-29-error-analysis-to-evaluate-llm-applications"
    icon={<Bug />}
  />
  <TutorialCard
    title="Testing LLM Applications"
    description="Build a testing foundation with deterministic checks and probabilistic ones using LLM judges to prevent regressions."
    href="/blog/2025-10-21-testing-llm-applications"
    icon={<TestTube />}
  />
  <TutorialCard
    title="Synthetic Dataset Generation"
    description="Scale your test coverage with LLM-generated synthetic data. Create diverse inputs and adversarial cases without waiting for users."
    href="/guides/cookbook/example_synthetic_datasets"
    icon={<Sparkles />}
  />
  <TutorialCard
    title="Experiment Interpretation"
    description="Run experiments to compare prompts, models, or pipelines. Measure metrics like accuracy and speed to quantify progress."
    href="/blog/2025-11-06-experiment-interpretation"
    icon={<TrendingUp />}
  />
</TutorialCards>

## Advanced Topics

Extend your evaluation approach for complex, multi-step applications.

<TutorialCards>
  <TutorialCard
    title="Evaluating Multi-Turn Conversations"
    description="Evaluate coherence, memory, and resolution in conversational apps that maintain context across multiple turns."
    href="/guides/cookbook/example_evaluating_multi_turn_conversations"
    icon={<MessagesSquare />}
  />
  <TutorialCard
    title="Simulated Multi-Turn Conversations"
    description="Generate and test user-AI exchanges safely before production. Build realistic dialogue scenarios for robust evaluation."
    href="/guides/cookbook/example_simulated_multi_turn_conversations"
    icon={<Play />}
  />
  <TutorialCard
    title="Agent Evaluation"
    description="Assess end-to-end trajectories for agents with tool use and planning. Evaluate action selection and task completion."
    href="/guides/cookbook/example_pydantic_ai_mcp_agent_evaluation"
    icon={<Bot />}
  />
</TutorialCards>

---
description: Follow the Quickstart to get started with Langfuse in minutes before diving into the details.
---

# Quickstart

This quickstart helps you to integrate your LLM application with Langfuse. It will log a single LLM call to get started.

## Create new project in Langfuse

1.  [Create Langfuse account](https://cloud.langfuse.com/auth/sign-up)
2.  Create a new project
3.  Create new API credentials in the project settings

## Log your first LLM call to Langfuse

<Tabs items={["Python","JS/TS","OpenAI SDK (Python)","Langchain","Langchain (JS)","API"]}>

<Tab>
{/* Python */}

```bash
pip install langfuse
```

```bash filename=".env"
LANGFUSE_SECRET_KEY="sk-lf-...";
LANGFUSE_PUBLIC_KEY="pk-lf-...";
LANGFUSE_HOST="https://cloud.langfuse.com"; # ðŸ‡ªðŸ‡º EU region
# LANGFUSE_HOST="https://us.cloud.langfuse.com"; # ðŸ‡ºðŸ‡¸ US region
```

Example usage, most of the parameters are optional and depend on the use case. For more information, see the [python docs](/docs/sdk/python).

```python filename="server.py"
from langfuse import Langfuse

# Create Langfuse client
langfuse = Langfuse()

# Create generation in Langfuse
generation = langfuse.generation(
    name="summary-generation",
    model="gpt-3.5-turbo",
    model_parameters={"maxTokens": "1000", "temperature": "0.9"},
    input=[{"role": "system", "content": "You are a helpful assistant."}, {"role": "user", "content": "Please generate a summary of the following documents \nThe engineering department defined the following OKR goals...\nThe marketing department defined the following OKR goals..."}],
    metadata={"interface": "whatsapp"}
)

# Execute model, mocked here
# chat_completion = openai.ChatCompletion.create(model="gpt-3.5-turbo", messages=[{"role": "user", "content": "Hello world"}])
chat_completion = "completion":"The Q3 OKRs contain goals for multiple teams..."

# Update span and sets end_time
generation.end(output=chat_completion)

# The SDK executes network requests in the background.
# To ensure that all requests are sent before the process exits, call flush()
# Not necessary in long-running production code
langfuse.flush()
```

</Tab>
<Tab>
{/* JS/TS */}

```sh
npm i langfuse
# or
yarn add langfuse

# Node.js < 18
npm i langfuse-node

# Deno
import { Langfuse } from "https://esm.sh/langfuse"
```

```bash filename=".env"
LANGFUSE_SECRET_KEY="sk-lf-...";
LANGFUSE_PUBLIC_KEY="pk-lf-...";
LANGFUSE_BASEURL="https://cloud.langfuse.com"; # ðŸ‡ªðŸ‡º EU region
# LANGFUSE_BASEURL="https://us.cloud.langfuse.com"; # ðŸ‡ºðŸ‡¸ US region
```

Example usage, most of the parameters are optional and depend on the use case. For more information, see the [typescript SDK docs](/docs/sdk/typescript).

```typescript filename="server.ts"
import { Langfuse } from "langfuse";

const langfuse = new Langfuse();

// Example generation creation
const generation = trace.generation({
  name: "chat-completion",
  model: "gpt-3.5-turbo",
  modelParameters: {
    temperature: 0.9,
    maxTokens: 2000,
  },
  input: messages,
});

// Application code
const chatCompletion = await llm.respond(prompt);

// End generation - sets endTime
generation.end({
  output: chatCompletion,
});
```

</Tab>
<Tab>
{/* OpenAI SDK (Python) */}
The integration is a drop-in replacement for the OpenAI Python SDK. By changing the import, Langfuse will capture all LLM calls and send them to Langfuse asynchronously.

```bash
pip install langfuse
```

```bash filename=".env"
LANGFUSE_SECRET_KEY="sk-lf-...";
LANGFUSE_PUBLIC_KEY="pk-lf-...";
LANGFUSE_HOST="https://cloud.langfuse.com"; # ðŸ‡ªðŸ‡º EU region
# LANGFUSE_HOST="https://us.cloud.langfuse.com"; # ðŸ‡ºðŸ‡¸ US region
```

```diff filename="server.py"
- import openai
+ from langfuse.openai import openai
```

Use the OpenAI SDK as you would normally. Example:

```python
completion = openai.chat.completions.create(
  model="gpt-3.5-turbo",
  messages=[
      {"role": "system", "content": "You are a very accurate calculator."},
      {"role": "user", "content": "1 + 1 = "}],
)
```

</Tab>
<Tab>
{/* Langchain (Python) */}

The integration uses the Langchain callback system to automatically capture detailed traces of your Langchain executions.

```bash
pip install langfuse
```

```bash filename=".env"
LANGFUSE_SECRET_KEY="sk-lf-...";
LANGFUSE_PUBLIC_KEY="pk-lf-...";
LANGFUSE_HOST="https://cloud.langfuse.com"; # ðŸ‡ªðŸ‡º EU region
# LANGFUSE_HOST="https://us.cloud.langfuse.com"; # ðŸ‡ºðŸ‡¸ US region
```

Add Langfuse callback handler to your Langchain agent/chain/... to automatically capture traces.

```python /callbacks=[handler]/
from langfuse.callback import CallbackHandler
handler = CallbackHandler(LANGFUSE_PUBLIC_KEY, LANGFUSE_SECRET_KEY)

# Langchain implementation

# Add handler as callback when running the Langchain agent
agent.run("<user_input>", callbacks=[handler])
```

For more examples, see the [Langchain integration docs](/docs/integrations/langchain/python).

</Tab>
<Tab>
{/* Langchain (JS) */}

The integration uses the Langchain callback system to automatically capture detailed traces of your Langchain executions.

```sh
npm i langfuse-langchain
# or
yarn add langfuse-langchain

# Deno
import CallbackHandler from 'https://esm.sh/langfuse-langchain'
```

```bash filename=".env"
LANGFUSE_SECRET_KEY="sk-lf-...";
LANGFUSE_PUBLIC_KEY="pk-lf-...";
LANGFUSE_BASEURL="https://cloud.langfuse.com"; # ðŸ‡ªðŸ‡º EU region
# LANGFUSE_BASEURL="https://us.cloud.langfuse.com"; # ðŸ‡ºðŸ‡¸ US region
```

Add Langfuse callback handler to your Langchain agent/chain/... to automatically capture traces.

```typescript /callbacks: [handler]/
import { CallbackHandler } from "langfuse-langchain";

// Initialize Langfuse callback handler
const handler = new CallbackHandler();

// Your Langchain implementation
const chain = new LLMChain(...);

// Add handler as callback when running the Langchain agent
await chain.call({ input: "<user_input>" }, { callbacks: [handler] });

```

</Tab>
<Tab>
{/* API */}

[API reference](https://api.reference.langfuse.com) for custom integrations. OpenAPI spec available.

</Tab>

</Tabs>

### Explore

Visit Langfuse interface to explore your data.

## More information

This is a brief summary of getting started with Langfuse. Have a look at the detailed SDK documentation for features such as:

- Nesting of observations to capture complex chains and agents
- Use of your own existing ids to group traces, observations and scores
- Optimizing asynchronous behavior of the SDK

import { SiTypescript, SiPython } from "react-icons/si";
import { FiCode } from "react-icons/fi";
import { SiOpenai } from "react-icons/si";

<Cards num={2}>
  <Card
    icon={<SiTypescript size="24" />}
    title="JS/TS SDK (Edge, Node, Web)"
    href="/docs/sdk/typescript"
    arrow
  />
  <Card
    icon={<SiPython size="24" />}
    title="Python SDK"
    href="/docs/sdk/python"
    arrow
  />
  <Card
    icon={<span>ðŸ¦œðŸ”—</span>}
    title="Langchain integration"
    href="/docs/langchain"
    arrow
  />
  <Card
    icon={<SiOpenai size="24" />}
    title="OpenAI SDK (Python)"
    href="/docs/integrations/openai"
    arrow
  />
  <Card
    icon={<FiCode size="24" />}
    title="API reference"
    href="https://api.reference.langfuse.com/"
    arrow
  />
  <Card
    icon={<FiCode size="24" />}
    title="Flowise, Langflow, Litellm, ..."
    href="/docs"
    arrow
  />
</Cards>

---
description: Follow the Quickstart to get started with Langfuse in minutes before diving into the details.
---

import { Steps, Tab, Tabs } from "nextra-theme-docs";

# Quickstart

This quickstart helps you to integrate your LLM application with Langfuse.

<Steps>

### Run Langfuse

The fastest way to get started is to use the Langfuse Cloud. Alternatively you can run Langfuse locally to test it out or self-host it.

<Tabs items={["Langfuse cloud", "local", "self-hosted"]}>
<Tab>

1. Create account and project on
   [cloud.langfuse.com](https://cloud.langfuse.com/auth/sign-up)
2. Copy API keys for your
   project

   ```bash filename=".env"
   LANGFUSE_SECRET_KEY="sk-lf-...";
   LANGFUSE_PUBLIC_KEY="pk-lf-...";
   ```

</Tab>
<Tab>

1.  Run Langfuse
    ```bash copy
    git clone https://github.com/langfuse/langfuse.git
    cd langfuse
    docker compose up
    ```
2.  Open http://localhost:3030, create a project
3.  Copy API keys for your project
    ```bash filename=".env"
    LANGFUSE_SECRET_KEY="sk-lf-...";
    LANGFUSE_PUBLIC_KEY="pk-lf-...";
    ```

Problems? Read more about running Langfuse [locally with
docker](local.md)

</Tab>
<Tab>

1. Run postgres database
2. Run `langfuse` docker container
3. Go to Langfuse UI on your host and create a project
4. Copy API keys for your project
   ```bash filename=".env"
   LANGFUSE_SECRET_KEY="sk-lf-...";
   LANGFUSE_PUBLIC_KEY="pk-lf-...";
   ```

To run Langfuse in production, see [self-host instructions](self-host.mdx)

</Tab>
</Tabs>

### Add tracing to your backend

<Tabs items={["Typescript", "Python", "Langchain", "API"]}>
<Tab>

```sh npm2yarn
npm i langfuse
```

For Deno (ESM CDN): `https://esm.sh/langfuse`

Example usage, most of the parameters are optional and depend on the use case.

For more information, see the [typescript docs](/docs/integrations/sdk/typescript).

```typescript filename="server.ts"
import { Langfuse } from "langfuse";

const langfuse = new Langfuse({
  secretKey: process.env.LANGFUSE_SECRET_KEY, // sk-lf-...
  publicKey: process.env.LANGFUSE_PUBLIC_KEY, // pk-lf-...
  // baseUrl: defaults to "https://cloud.langfuse.com"
});

const generationStartTime = new Date();

// const chatCompletion = await llm.respond(prompt);

langfuse.generation({
  startTime: generationStartTime,
  endTime: new Date(),
  name: "chat-completion",
  model: "gpt-3.5-turbo",
  modelParameters: {
    temperature: 0.9,
    maxTokens: 2000,
    topP: undefined,
  },
  prompt: messagesToSend,
  completion: chatCompletion.data.choices[0].message?.content,
  usage: {
    promptTokens: chatCompletion.data.usage?.prompt_tokens,
    completionTokens: chatCompletion.data.usage?.completion_tokens,
  },
  metadata: {
    userId: "user__935d7d1d-8625-4ef4-8651-544613e7bd22",
  },
});
```

</Tab>
<Tab>

```bash
pip install langfuse
```

Example usage, most of the parameters are optional and depend on the use case. For more information, see the [python docs](/docs/integrations/sdk/python).

```python filename="server.py"
from langfuse import Langfuse
from langfuse.api.model import Generation

langfuse = Langfuse(
  private_key="pk-lf-...",
  secret_key="sk-sk-...",
  host="https://cloud.langfuse.com"
)

generationStartTime = datetime.datetime.now()

# chatCompletion = callLLM(prompt)

generation = langfuse.generation(Generation(
    name="chat-completion",
    startTime=generationStartTime,
    endTime=datetime.datetime.now(),
    model="gpt-3.5-turbo",
    modelParameters= {
      "temperature":0.9,
      "maxTokens":1000,
      "topP": None,
    },
    prompt=[{"role": "user", "content": message_to_send}],
    completion=chat_response.json()["choices"][0]["message"],
    usage=LlmUsage(
      prompt_tokens=512,
      completion_tokens=49
    ),
    metadata= {
     "userid":'user__935d7d1d-8625-4ef4-8651-544613e7bd22',
    }
))
```

</Tab>
<Tab>

Install the Langfuse Python SDK

```bash
pip install langfuse
```

Add Langfuse callback handler to your Langchain agent/chain/... to automatically capture traces.

```python /callbacks=[handler]/
from langfuse.callback import CallbackHandler
handler = CallbackHandler(LANGFUSE_PUBLIC_KEY, LANGFUSE_SECRET_KEY)

# Langchain implementation

# Add handler as callback when running the Langchain agent
agent.run("<user_input>", callbacks=[handler])
```

For more examples, see the [Langchain integration docs](/docs/integrations/langchain).

</Tab>
<Tab>

[API reference](/docs/integrations/api) for custom integrations

</Tab>
</Tabs>

### Capture scoring (optional)

<Tabs items={["Typescript", "Python", "API"]}>
<Tab>

For typescript we offer a dedicated frontend package to directly report scores via the `publicKey`.

Simple feedback component in React:

```typescript filename="UserFeedbackComponent.tsx"
import { LangfuseWeb } from "langfuse";

export default function UserFeedbackComponent(props: { traceId: string }) {
  const langfuseWeb = new LangfuseWeb({
    publicKey: process.env.NEXT_PUBLIC_LANGFUSE_PUBLIC_KEY, // pk-lf-...
    // baseUrl: defaults to "https://cloud.langfuse.com"
  });

  const handleFeedback = async (value: number) => {
    await langfuseWeb.score({
      traceId: props.traceId,
      name: "user_feedback", // arbitrary name to identify the type of score
      value, // float, optional: scale it to e.g. 0..1
    });
  };

  return (
    <>
      <button onClick={() => handleFeedback(1)}>üëç</button>
      <button onClick={() => handleFeedback(0)}>üëé</button>
    </>
  );
}
```

</Tab>
<Tab>

You can use the Python SDK to report a score via your Python server. Alternatively, you can use the Typescript SDK or REST API to directly report scores from the frontend.

```python filename="server.py"
from langfuse.api.model import CreateScore

score = generation.score(CreateScore(
    name='user-explicit-feedback',
    value=1,
    comment="I like how personalized the response is"
))
```

</Tab>
<Tab>

```bash
curl --location 'http://cloud.langfuse.com/api/public/metrics' \
--header 'Content-Type: application/json' \
--header 'Accept: application/json' \
--header "Authorization: Bearer {langfuse_public_key}"
--data '{
  "traceId": "<string>",
  "name": "<string>",
  "value": "<integer>",
  "observationId": "<optional_string>"
}'
```

</Tab>
</Tabs>

### Explore

Visit Langfuse interface to explore your data.

</Steps>

## More information

This is a very short summary of how to get started with Langfuse, have a look at the detailed SDK documentation for features such as:

- Nesting of observations to capture complex chains and agents
- Use of existing `externalIds` to group traces, observations and scores
- Optimizing asynchronous behavior of the SDK

import { Card, Cards } from "nextra-theme-docs";
import { SiTypescript, SiPython } from "react-icons/si";

<Cards>
  <Card
    icon={<SiTypescript size="24" />}
    title="Typescript"
    href="/docs/sdk/typescript"
  />
  <Card icon={<SiPython size="24" />} title="Python" href="/docs/sdk/python" />
</Cards>

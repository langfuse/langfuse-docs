---
description: Get started with LLM observability with Langfuse in minutes before diving into all platform features.
---

# Get Started with Langfuse Tracing

This quickstart helps you to integrate your LLM application with [Langfuse Tracing](/docs/tracing). It will log a single LLM call to get you started.

If you are looking for other features, see the [overview](/docs).

## Create new project in Langfuse

1.  [Create Langfuse account](https://cloud.langfuse.com/auth/sign-up) or [self-host](/self-hosting)
2.  Create a new project
3.  Create new API credentials in the project settings

## Log your first LLM call to Langfuse

import GetStartedLangchainPythonArgs from "@/components-mdx/get-started-langchain-python-constructor-args.mdx";
import GetStartedLangchainJsArgs from "@/components-mdx/get-started-langchain-js-constructor-args.mdx";
import GetStartedJsSdk from "@/components-mdx/get-started-js-sdk.mdx";
import GetStartedLlamaindexPythonArgs from "@/components-mdx/get-started-llamaindex-python-constructor-args.mdx";
import EnvPython from "@/components-mdx/env-python.mdx";
import EnvJs from "@/components-mdx/env-js.mdx";
import GetStartedDecoratorOpenai from "@/components-mdx/get-started-python-decorator-openai.mdx";
import GetStartedDecoratorAnyLlm from "@/components-mdx/get-started-python-decorator-any-llm.mdx";

<Tabs items={["Python SDK","Python SDK + any LLM","JS/TS","OpenAI SDK (Python)","OpenAI SDK (JS)", "Langchain","Langchain (JS)","LlamaIndex","API", "Cursor AI Agent"]}>

<Tab>
{/* Python SDK + Decorator */}
The [`@observe()` decorator](/docs/sdk/python/sdk-v3#observe-decorator) is the recommended way to get started with Langfuse. It automatically captures traces with minimal code changes and works with any LLM provider.

```bash
pip install langfuse
```

<EnvPython />

```python /@observe()/ /from langfuse.openai import openai/ filename="main.py"
from langfuse import observe, get_client
from langfuse.openai import openai # OpenAI integration

@observe()
def story():
    return openai.chat.completions.create(
        model="gpt-4o",
        messages=[
          {"role": "system", "content": "You are a great storyteller."},
          {"role": "user", "content": "Once upon a time in a galaxy far, far away..."}
        ],
    ).choices[0].message.content

@observe()
def main():
    return story()

main()
```

For more advanced use cases with manual control, see the [Python SDK v3 documentation](/docs/sdk/python/sdk-v3).

</Tab>
<Tab>
{/* Python SDK + any LLM */}

**Recommended approach**: Use third-party, OTEL-based instrumentation packages for automatic tracing with popular LLM providers and frameworks. The Langfuse SDK will pick up emitted spans and process them accordingly.

<EnvPython />

In this example, we are using the [`opentelemetry-instrumentation-anthropic` library](https://pypi.org/project/opentelemetry-instrumentation-anthropic/).

```python
from langfuse import get_client

from anthropic import Anthropic
from opentelemetry.instrumentation.anthropic import AnthropicInstrumentor


# This will automatically emit OTEL-spans for all Anthropic API calls
AnthropicInstrumentor().instrument()

# Initialize Langfuse
langfuse = get_client()

# This will be traced as a Langfuse generation nested under the current span
message = Anthropic().messages.create(
    model="claude-3-7-sonnet-20250219",
    max_tokens=1024,
    messages=[{"role": "user", "content": "Hello, Claude"}],
)

print(message.content)
```

**Alternative**: Manual tracing with `@observe(as_type="generation")` for custom implementations. See [decorator documentation](/docs/sdk/python/decorators) for details.

</Tab>
<Tab>
{/* JS/TS */}

```sh
npm i langfuse
# or
yarn add langfuse

# Node.js < 18
npm i langfuse-node

# Deno
import { Langfuse } from "https://esm.sh/langfuse"
```

<EnvJs />

Example usage, most of the parameters are optional and depend on the use case. For more information, see the [JS/TS SDK docs](/docs/sdk/typescript/guide) or [end-to-end example notebook](/docs/sdk/typescript/example-notebook).

<GetStartedJsSdk />

</Tab>
<Tab>
{/* OpenAI SDK (Python) */}
The [integration](/docs/integrations/openai) is a drop-in replacement for the OpenAI Python SDK. By changing the import, Langfuse will capture all LLM calls and send them to Langfuse asynchronously (for Azure OpenAI, use `from langfuse.openai import AzureOpenAI`).

```bash
pip install langfuse
```

<EnvPython />

```diff filename=".py"
- import openai
+ from langfuse.openai import openai

Alternative imports:
+ from langfuse.openai import OpenAI, AsyncOpenAI, AzureOpenAI, AsyncAzureOpenAI
```

Use the OpenAI SDK as you would normally.

```python
from langfuse.openai import openai

completion = openai.chat.completions.create(
    model="gpt-4o",
    messages=[
        {"role": "system", "content": "You are a very accurate calculator."},
        {"role": "user", "content": "1 + 1 = "}
    ],
)
```

</Tab>
<Tab>
{/* OpenAI (JS/TS) */}

<EnvJs />

With your environment configured, call OpenAI SDK methods as usual from the wrapped client.

```ts
import OpenAI from "openai";
import { observeOpenAI } from "langfuse";

const openai = observeOpenAI(new OpenAI());

const res = await openai.chat.completions.create({
  messages: [{ role: "system", content: "Tell me a story about a dog." }],
  model: "gpt-4o",
  max_tokens: 300,
});
```

</Tab>
<Tab>
{/* LangChain */}

The [integration](/docs/integrations/langchain) uses the Langchain callback system. **Note**: Trace attributes must be set on an enclosing span.

```python
from langfuse.langchain import CallbackHandler

from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate

langfuse_handler = CallbackHandler()

llm = ChatOpenAI(model_name="gpt-4o")
prompt = ChatPromptTemplate.from_template("Tell me a joke about {topic}")
chain = prompt | llm

response = chain.invoke({"topic": "cats"}, config={"callbacks": [langfuse_handler]})
```

For more details, see the [Langchain integration docs](/docs/integrations/langchain/tracing).

</Tab>
<Tab>
{/* Langchain (JS) */}

The [integration](/docs/integrations/langchain) uses the Langchain callback system to automatically capture detailed traces of your Langchain executions.

<GetStartedLangchainJsArgs />

For more details, see the [Langchain integration docs](/docs/integrations/langchain/tracing).

</Tab>
<Tab>
{/* LlamaIndex */}

Use third-party OTEL-based instrumentation for LlamaIndex. In this example, we use the [`openinference-instrumentation-llama-index` library](https://pypi.org/project/openinference-instrumentation-llama-index/).

```bash
pip install langfuse llama-index openinference-instrumentation-llama-index
```

<EnvPython />

```python
from langfuse import get_client

from llama_index.core.llms.openai import OpenAI
from openinference.instrumentation.llama_index import LlamaIndexInstrumentor

# Enable OTEL-based instrumentation
LlamaIndexInstrumentor().instrument()

# Initialize Langfuse
langfuse = get_client()

response = OpenAI(model="gpt-4o").complete("Hello, world!")
```

</Tab>
<Tab>
{/* API */}

All features are available via the public API. See documentation below:

<Cards num={3}>
  <Card title="API Reference" href="https://api.reference.langfuse.com" arrow />
  <Card
    title="OpenAPI Specification (download)"
    href="https://cloud.langfuse.com/generated/api/openapi.yml"
    arrow
  />
  <Card
    title="Postman Collection (download)"
    href="https://cloud.langfuse.com/generated/postman/collection.json"
    arrow
  />
</Cards>

</Tab>
<Tab>
You can use the Cursor AI Agent to automatically integrate Langfuse into your codebase.

**1. Install the Langfuse Docs MCP Server**

Click the button below to add the Langfuse Docs MCP server to Cursor.

<a
  href="https://cursor.com/install-mcp?name=langfuse-docs&config=eyJ1cmwiOiJodHRwczovL2xhbmdmdXNlLmNvbS9hcGkvZG9jcy1tY3AifQ%3D%3D"
  className="block my-4"
>
  <img
    src="https://cursor.com/deeplink/mcp-install-light.svg"
    alt="Install MCP Server"
  />
</a>

Alternatively, you can add it manually to your `~/.cursor/mcp.json` file:

```json
{
  "mcpServers": {
    "langfuse-docs": {
      "url": "https://docs.langfuse.com/api/mcp/docs-mcp"
    }
  }
}
```

**2. Use the following prompt in Cursor**

Once the MCP server is installed, use the following prompt in Cursor agent mode:

```markdown
Before you begin, you must understand these three fundamental rules:

1.  Do Not Change Business Logic: You are strictly forbidden from changing, refactoring, or altering any of my existing code's logic. Your only task is to add the necessary code for Langfuse integration, such as decorators, imports, handlers, and environment variable initializations.

2.  Adhere to the Workflow: You must follow the step-by-step workflow outlined below in the exact sequence.

3.  Use the langfuse-docs mcp to fetch docs.

Integration Workflow

Step 1: Language and Compatibility Check

First, analyze the codebase to identify the primary programming language.

- If the language is Python or JavaScript/TypeScript, proceed to Step 2.
- If the language is not Python or JavaScript/TypeScript, you must stop immediately. Inform me that the codebase is currently unsupported for this AI-based setup, and do not proceed further.

Step 2: Codebase Discovery & Entrypoint Confirmation

Once you have confirmed the language is compatible, explore the entire codebase to understand its purpose.

- Identify all files and functions that contain LLM calls or are likely candidates for tracing.
- Present this list of files and function names to me.
- If you are unclear about the main entry point of the application (e.g., the primary API route or the main script to execute), you must ask me for confirmation on which parts are most critical to trace before proceeding to the next step.

Step 3: Discover Available Integrations

After I confirm the files and entry points, crawl the Langfuse integrations by triggering the `get-integration-docs-toc` tool of the langfuse-docs MCP server. Your goal is to get a high-level understanding of all the official SDKs and framework integrations Langfuse offers.

Step 4: Analyze Confirmed Files for Technologies

Based on the files we confirmed in Step 2, perform a deeper analysis to identify the specific LLM frameworks or SDKs being used (e.g., OpenAI SDK, LangChain, LlamaIndex, Anthropic SDK, etc.).

Step 5: Retrieve Specific Documentation

Based on your findings in Step 4, navigate to the exact integration documentation page on the Langfuse docs. You must find and crawl the guide that matches my specific setup to learn the precise steps, package names, and code modifications required.

Step 6: Propose a Development Plan

Before you write or modify a single line of code, you must present me with a clear, step-by-step development plan. This plan must include:

- The Langfuse package(s) you will install.
- The files you intend to modify.
- The specific code changes you will make, showing the exact additions.
- Instructions on where I will need to add my Langfuse API keys after your work is done.

I will review this plan and give you my approval before you proceed.

Step 7: Implement the Integration

Once I approve your plan, execute it. First, you must use your terminal access to run the necessary package installation command (e.g., pip install langfuse, npm install langfuse) yourself. After the installation is successful, modify the code exactly as described in the plan.

Step 8: Request User Review and Wait

After you have made all the changes, notify me that your work is complete. Explicitly ask me to review the changes and test my application to confirm that everything is still working correctly. You must state that you will wait for my feedback before doing anything else.

Step 9: Debug and Fix if Necessary

If I report that something is not working correctly, analyze my feedback. Use the knowledge you have to debug the issue. If required, re-crawl the relevant Langfuse documentation to find a solution, propose a fix to me, and then implement it.
```

</Tab>
</Tabs>

<Callout type="info" emoji="âœ…">
  Done, now visit the Langfuse interface to look at the trace you just created.
</Callout>

## All Langfuse platform features

This was a very brief introduction to get started with Langfuse. Explore all Langfuse platform features in detail.

**Develop**

<Cards num={3}>
  <Card title="Tracing" href="/docs/tracing" arrow />
  <Card title="Prompt Management" href="/docs/prompts/get-started" arrow />
  <Card
    title="Export & Fine-tuning"
    href="/docs/export-and-fine-tuning"
    arrow
  />
</Cards>

**Monitor**

<Cards num={3}>
  <Card title="Analytics" href="/docs/analytics" arrow />
  <Card title="Model Usage & Cost" href="/docs/model-usage-and-cost" arrow />
  <Card title="Evaluation" href="/docs/scores/overview" arrow />
</Cards>

**Test**

<Cards num={3}>
  <Card title="Datasets" href="/docs/datasets/overview" arrow />
</Cards>

## References

import { Code } from "lucide-react";
import IconPython from "@/components/icons/python";
import IconTypescript from "@/components/icons/typescript";
import IconOpenai from "@/components/icons/openai";

<Cards num={2}>
  <Card
    icon={<IconPython />}
    title="Python Decorator"
    href="/docs/sdk/python/decorators"
    arrow
  />
  <Card
    icon={<IconPython />}
    title="Python SDK (v3)"
    href="/docs/sdk/python/sdk-v3"
    arrow
  />
  <Card
    icon={<IconTypescript />}
    title="JS/TS SDK"
    href="/docs/sdk/typescript/guide"
    arrow
  />
  <Card
    icon={<IconOpenai />}
    title="OpenAI SDK"
    href="/docs/integrations/openai"
    arrow
  />
  <Card
    icon={<span>ðŸ¦œðŸ”—</span>}
    title="Langchain"
    href="/docs/integrations/langchain/tracing"
    arrow
  />
  <Card
    icon={<span>ðŸ¦™</span>}
    title="LlamaIndex"
    href="/docs/integrations/llama-index/get-started"
    arrow
  />
  <Card
    icon={<Code size="24" />}
    title="API reference"
    href="https://api.reference.langfuse.com/"
    arrow
  />
  <Card
    icon={<Code size="24" />}
    title="Flowise"
    href="/docs/integrations/flowise"
    arrow
  />
  <Card
    icon={<Code size="24" />}
    title="Langflow"
    href="/docs/integrations/langflow"
    arrow
  />
  <Card
    icon={<Code size="24" />}
    title="Litellm"
    href="/docs/integrations/litellm/tracing"
    arrow
  />
</Cards>

## FAQ

import { FaqPreview } from "@/components/faq/FaqPreview";

<FaqPreview tags={["setup"]} />

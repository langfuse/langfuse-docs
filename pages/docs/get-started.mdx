---
description: Get started with LLM observability with Langfuse in minutes before diving into all platform features.
---

# Get Started with Langfuse Tracing

This quickstart helps you to integrate your LLM application with [Langfuse Tracing](/docs/tracing). It will log a single LLM call to get you started.

If you are looking for other features, see the [overview](/docs).

## Create new project in Langfuse

1.  [Create Langfuse account](https://cloud.langfuse.com/auth/sign-up) or [self-host](/self-hosting)
2.  Create a new project
3.  Create new API credentials in the project settings

## Log your first LLM call to Langfuse

import GetStartedLangchainPythonArgs from "@/components-mdx/get-started-langchain-python-constructor-args.mdx";
import GetStartedLangchainJsArgs from "@/components-mdx/get-started-langchain-js-constructor-args.mdx";
import GetStartedJsSdk from "@/components-mdx/get-started-js-sdk.mdx";
import GetStartedLlamaindexPythonArgs from "@/components-mdx/get-started-llamaindex-python-constructor-args.mdx";
import EnvPython from "@/components-mdx/env-python.mdx";
import EnvJs from "@/components-mdx/env-js.mdx";
import GetStartedDecoratorOpenai from "@/components-mdx/get-started-python-decorator-openai.mdx";
import GetStartedDecoratorAnyLlm from "@/components-mdx/get-started-python-decorator-any-llm.mdx";

<Tabs items={["Python SDK (v3)","Python SDK (v2)","Python Decorator + OpenAI","Python Decorator + any LLM","JS/TS","OpenAI SDK (Python)","OpenAI SDK (JS)", "Langchain","Langchain (JS)","LlamaIndex","API"]}>

<Tab>
{/* Python SDK (v3) */}
The [OpenTelemetry-based SDK (v3)](/docs/sdk/python/sdk-v3) is the latest generation of the Python SDK with improved developer experience and enhanced ease of use.

```bash
pip install langfuse
```

<EnvPython />

Example usage of the v3 SDK using context managers:

```python filename="server.py"
from langfuse import get_client

# Create Langfuse client
langfuse = get_client()

# Create a trace with a root span
with langfuse.start_as_current_span(
    name="user-request-pipeline",
    input={"user_query": "Summarize the Q3 OKR documents"}
) as root_span:
    # Add trace attributes
    root_span.update_trace(
        user_id="user_123",
        session_id="session_abc",
        tags=["summary", "okr"]
    )

    # Create a nested generation for the LLM call
    with langfuse.start_as_current_generation(
        name="summary-generation",
        model="gpt-4o",
        model_parameters={"temperature": 0.7, "max_tokens": 1000},
        input=[
            {"role": "system", "content": "You are a helpful assistant."},
            {"role": "user", "content": "Please generate a summary of the following documents \\nThe engineering department defined the following OKR goals...\\nThe marketing department defined the following OKR goals..."}
        ]
    ) as generation:
        # Simulate an LLM call
        llm_response = "The Q3 OKRs contain goals for multiple teams..."
        token_usage = {"input_tokens": 50, "output_tokens": 120}

        # Update the generation with results
        generation.update(
            output=llm_response,
            usage_details=token_usage
        )

    # Update the root span with final output
    root_span.update(output={"summary": llm_response})

# The SDK executes network requests in the background.
# To ensure that all requests are sent before the process exits, call flush()
langfuse.flush()
```

For more information, see the [Python SDK v3 documentation](/docs/sdk/python/sdk-v3).

</Tab>
<Tab>
{/* Python SDK (v2) */}

The [low-level SDK (v2)](/docs/sdk/python/low-level-sdk) gives you full control over the traces logged to Langfuse. For a less verbose integration, consider using the `@observe()` decorator or the v3 SDK.

```bash
pip install langfuse
```

<EnvPython />

Example usage, most of the parameters are optional and depend on the use case. For more information, see the [python docs](/docs/sdk/python/low-level-sdk).

```python filename="server.py"
from langfuse import Langfuse

# Create Langfuse client
langfuse = Langfuse()

# Create generation in Langfuse
generation = langfuse.generation(
    name="summary-generation",
    model="gpt-4o",
    model_parameters={"maxTokens": "1000", "temperature": "0.9"},
    input=[{"role": "system", "content": "You are a helpful assistant."}, {"role": "user", "content": "Please generate a summary of the following documents \\nThe engineering department defined the following OKR goals...\\nThe marketing department defined the following OKR goals..."}],
    metadata={"interface": "whatsapp"}
)

# Execute model, mocked here
# chat_completion = openai.ChatCompletion.create(model="gpt-4o", messages=[{"role": "user", "content": "Hello world"}])
chat_completion = "completion":"The Q3 OKRs contain goals for multiple teams..."

# Update span and sets end_time
generation.end(output=chat_completion)

# The SDK executes network requests in the background.
# To ensure that all requests are sent before the process exits, call flush()
# Not necessary in long-running production code
langfuse.flush()
```

</Tab>
<Tab>
{/* Decorator + OpenAI */}

The [`@observe()` decorator](/docs/sdk/python/decorators) works with both SDK versions. This example uses the Langfuse [OpenAI integration](/docs/integrations/openai) to automatically capture all model parameters.

<Tabs items={["Python SDK v3", "Python SDK v2"]}>
<Tab>

```bash
pip install langfuse openai
```

<EnvPython />

```python /@observe()/ /from langfuse.openai import openai/ filename="main.py"
from langfuse import observe
from langfuse.openai import openai # OpenAI integration

@observe()
def story():
    return openai.chat.completions.create(
        model="gpt-4o",
        messages=[
          {"role": "system", "content": "You are a great storyteller."},
          {"role": "user", "content": "Once upon a time in a galaxy far, far away..."}
        ],
    ).choices[0].message.content

@observe()
def main():
    return story()

main()
```

</Tab>
<Tab>

```bash
pip install langfuse openai
```

<EnvPython />

```python /@observe()/ /from langfuse.openai import openai/ filename="main.py"
from langfuse.decorators import observe
from langfuse.openai import openai # OpenAI integration

@observe()
def story():
    return openai.chat.completions.create(
        model="gpt-4o",
        messages=[
          {"role": "system", "content": "You are a great storyteller."},
          {"role": "user", "content": "Once upon a time in a galaxy far, far away..."}
        ],
    ).choices[0].message.content

@observe()
def main():
    return story()

main()
```

</Tab>
</Tabs>

</Tab>
<Tab>
{/* Decorator + Any LLM */}

The [`@observe()` decorator](/docs/sdk/python/decorators) makes it easy to trace any Python LLM application. If you do not use any of our native integrations, you can trace any LLM call via `@observe(as_type="generation")`.

<Tabs items={["Python SDK v3", "Python SDK v2"]}>
<Tab>

```bash
pip install langfuse anthropic
```

<EnvPython />

```python /@observe(as_type="generation")/ /@observe()/ filename="main.py"
from langfuse import observe, get_client
import anthropic

langfuse = get_client()
anthopic_client = anthropic.Anthropic()

# Wrap LLM function with decorator
@observe(as_type="generation")
def anthropic_completion(**kwargs):
  # optional, extract some fields from kwargs
  kwargs_clone = kwargs.copy()
  input = kwargs_clone.pop('messages', None)
  model = kwargs_clone.pop('model', None)
  langfuse.update_current_generation(
      input=input,
      model=model,
      metadata=kwargs_clone
  )

  response = anthopic_client.messages.create(**kwargs)

  # See docs for more details on token counts and usd cost in Langfuse
  # https://langfuse.com/docs/model-usage-and-cost
  langfuse.update_current_generation(
      usage_details={
          "input": response.usage.input_tokens,
          "output": response.usage.output_tokens
      }
  )

  # return result
  return response.content[0].text

@observe()
def main():
  return anthropic_completion(
      model="claude-3-opus-20240229",
      max_tokens=1024,
      messages=[
          {"role": "user", "content": "Hello, Claude"}
      ]
  )

main()
```

You may alternatively use a third-party OTEL-based instrumentation library for your LLM. See our [docs](/docs/sdk/python/sdk-v3#third-party-integrations) for more details.

</Tab>
<Tab>

```bash
pip install langfuse anthropic
```

<EnvPython />

<GetStartedDecoratorAnyLlm />

</Tab>
</Tabs>

</Tab>
<Tab>
{/* JS/TS */}

```sh
npm i langfuse
# or
yarn add langfuse

# Node.js < 18
npm i langfuse-node

# Deno
import { Langfuse } from "https://esm.sh/langfuse"
```

<EnvJs />

Example usage, most of the parameters are optional and depend on the use case. For more information, see the [JS/TS SDK docs](/docs/sdk/typescript/guide) or [end-to-end example notebook](/docs/sdk/typescript/example-notebook).

<GetStartedJsSdk />

</Tab>
<Tab>
{/* OpenAI SDK (Python) */}
The [integration](/docs/integrations/openai) is a drop-in replacement for the OpenAI Python SDK. By changing the import, Langfuse will capture all LLM calls and send them to Langfuse asynchronously (for Azure OpenAI, use `from langfuse.openai import AzureOpenAI`).

<Tabs items={["Python SDK v3", "Python SDK v2"]}>
<Tab>
```bash
pip install langfuse
```

<EnvPython />

```diff filename=".py"
- import openai
+ from langfuse.openai import openai

Alternative imports:
+ from langfuse.openai import OpenAI, AsyncOpenAI, AzureOpenAI, AsyncAzureOpenAI
```

Use the OpenAI SDK as you would normally. **Note**: Trace attributes (user_id, session_id, tags) must be set on an enclosing span:

```python
from langfuse import get_client
from langfuse.openai import openai

langfuse = get_client()

# Trace attributes must be set on enclosing span
with langfuse.start_as_current_span(name="openai-call") as span:
    span.update_trace(
        user_id="user_123",
        session_id="session_abc", 
        tags=["calculator"]
    )
    
    completion = openai.chat.completions.create(
        model="gpt-4o",
        messages=[
            {"role": "system", "content": "You are a very accurate calculator."},
            {"role": "user", "content": "1 + 1 = "}
        ],
    )
```

</Tab>
<Tab>
```bash
pip install langfuse
```

<EnvPython />

```diff filename=".py"
- import openai
+ from langfuse.openai import openai

Alternative imports:
+ from langfuse.openai import OpenAI, AsyncOpenAI, AzureOpenAI, AsyncAzureOpenAI
```

Use the OpenAI SDK as you would normally. You can pass trace attributes directly:

```python
completion = openai.chat.completions.create(
    model="gpt-4o",
    messages=[
        {"role": "system", "content": "You are a very accurate calculator."},
        {"role": "user", "content": "1 + 1 = "}
    ],
    # Trace attributes can be passed directly in v2
    user_id="user_123",
    session_id="session_abc",
    tags=["calculator"]
)
```

</Tab>
</Tabs>

</Tab>
<Tab>
{/* OpenAI (JS/TS) */}

<EnvJs />

With your environment configured, call OpenAI SDK methods as usual from the wrapped client.

```ts
import OpenAI from "openai";
import { observeOpenAI } from "langfuse";

const openai = observeOpenAI(new OpenAI());

const res = await openai.chat.completions.create({
  messages: [{ role: "system", content: "Tell me a story about a dog." }],
  model: "gpt-4o",
  max_tokens: 300,
});
```

</Tab>
<Tab>
{/* LangChain */}

<Tabs items={["Python SDK v3", "Python SDK v2"]}>
<Tab>

The [integration](/docs/integrations/langchain) uses the Langchain callback system. **Note**: Trace attributes must be set on an enclosing span in v3.

```python
from langfuse import get_client
from langfuse.langchain import CallbackHandler
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate

langfuse = get_client()
langfuse_handler = CallbackHandler()

llm = ChatOpenAI(model_name="gpt-4o")
prompt = ChatPromptTemplate.from_template("Tell me a joke about {topic}")
chain = prompt | llm

# Trace attributes must be set on enclosing span
with langfuse.start_as_current_span(name="joke-chain") as span:
    span.update_trace(
        user_id="user_123",
        session_id="session_abc",
        tags=["joke-chain"]
    )

    response = chain.invoke({"topic": "cats"}, config={"callbacks": [langfuse_handler]})

# The SDK executes network requests in the background.
# To ensure that all requests are sent before the process exits, call flush()
langfuse.flush()
```

</Tab>
<Tab>

The [integration](/docs/integrations/langchain) uses the Langchain callback system to automatically capture detailed traces of your Langchain executions.

<GetStartedLangchainPythonArgs />

</Tab>
</Tabs>

For more details, see the [Langchain integration docs](/docs/integrations/langchain/tracing).

</Tab>
<Tab>
{/* Langchain (JS) */}

The [integration](/docs/integrations/langchain) uses the Langchain callback system to automatically capture detailed traces of your Langchain executions.

<GetStartedLangchainJsArgs />

For more details, see the [Langchain integration docs](/docs/integrations/langchain/tracing).

</Tab>
<Tab>
{/* LlamaIndex */}

<Tabs items={["Python SDK v3", "Python SDK v2"]}>
<Tab>

For Python SDK v3, use third-party OTEL-based instrumentation for LlamaIndex. In this example, we use the [`openinference-instrumentation-llama-index` library](https://pypi.org/project/openinference-instrumentation-llama-index/).

```bash
pip install langfuse llama-index openinference-instrumentation-llama-index
```

<EnvPython />

```python
from llama_index.core.llms.openai import OpenAI
from openinference.instrumentation.llama_index import LlamaIndexInstrumentor
from langfuse import get_client

# Enable OTEL-based instrumentation
LlamaIndexInstrumentor().instrument()

langfuse = get_client()
llm = OpenAI(model="gpt-4o")

with langfuse.start_as_current_span(name="llamaindex-query"):
    response = llm.complete("Hello, world!")
    print(response)

langfuse.flush()
```

</Tab>
<Tab>

The [integration](/docs/integrations/llama-index) uses the LlamaIndex callback system to automatically capture detailed traces of your LlamaIndex executions.

```bash
pip install llama-index langfuse
```

<GetStartedLlamaindexPythonArgs />

Traces and metrics from your LlamaIndex application are now automatically tracked in Langfuse.

</Tab>
</Tabs>

</Tab>
<Tab>
{/* API */}

All features are available via the public API. See documentation below:

<Cards num={3}>
  <Card title="API Reference" href="https://api.reference.langfuse.com" arrow />
  <Card
    title="OpenAPI Specification (download)"
    href="https://cloud.langfuse.com/generated/api/openapi.yml"
    arrow
  />
  <Card
    title="Postman Collection (download)"
    href="https://cloud.langfuse.com/generated/postman/collection.json"
    arrow
  />
</Cards>

</Tab>
</Tabs>

<Callout type="info" emoji="âœ…">
  Done, now visit the Langfuse interface to look at the trace you just created.
</Callout>

## All Langfuse platform features

This was a very brief introduction to get started with Langfuse. Explore all Langfuse platform features in detail.

**Develop**

<Cards num={3}>
  <Card title="Tracing" href="/docs/tracing" arrow />
  <Card title="Prompt Management" href="/docs/prompts/get-started" arrow />
  <Card
    title="Export & Fine-tuning"
    href="/docs/export-and-fine-tuning"
    arrow
  />
</Cards>

**Monitor**

<Cards num={3}>
  <Card title="Analytics" href="/docs/analytics" arrow />
  <Card title="Model Usage & Cost" href="/docs/model-usage-and-cost" arrow />
  <Card title="Evaluation" href="/docs/scores/overview" arrow />
</Cards>

**Test**

<Cards num={3}>
  <Card title="Datasets" href="/docs/datasets/overview" arrow />
</Cards>

## References

import { Code } from "lucide-react";
import IconPython from "@/components/icons/python";
import IconTypescript from "@/components/icons/typescript";
import IconOpenai from "@/components/icons/openai";

<Cards num={2}>
  <Card
    icon={<IconPython />}
    title="Python Decorator"
    href="/docs/sdk/python/decorators"
    arrow
  />
  <Card
    icon={<IconPython />}
    title="Python SDK (v3)"
    href="/docs/sdk/python/sdk-v3"
    arrow
  />
  <Card
    icon={<IconPython />}
    title="Python SDK (v2)"
    href="/docs/sdk/python/low-level-sdk"
    arrow
  />
  <Card
    icon={<IconTypescript />}
    title="JS/TS SDK"
    href="/docs/sdk/typescript/guide"
    arrow
  />
  <Card
    icon={<IconOpenai />}
    title="OpenAI SDK"
    href="/docs/integrations/openai"
    arrow
  />
  <Card
    icon={<span>ðŸ¦œðŸ”—</span>}
    title="Langchain"
    href="/docs/integrations/langchain/tracing"
    arrow
  />
  <Card
    icon={<span>ðŸ¦™</span>}
    title="LlamaIndex"
    href="/docs/integrations/llama-index/get-started"
    arrow
  />
  <Card
    icon={<Code size="24" />}
    title="API reference"
    href="https://api.reference.langfuse.com/"
    arrow
  />
  <Card
    icon={<Code size="24" />}
    title="Flowise"
    href="/docs/integrations/flowise"
    arrow
  />
  <Card
    icon={<Code size="24" />}
    title="Langflow"
    href="/docs/integrations/langflow"
    arrow
  />
  <Card
    icon={<Code size="24" />}
    title="Litellm"
    href="/docs/integrations/litellm/tracing"
    arrow
  />
</Cards>

## FAQ

import { FaqPreview } from "@/components/faq/FaqPreview";

<FaqPreview tags={["setup"]} />

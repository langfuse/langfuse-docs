---
description: Follow the Quickstart to get started with Langfuse in minutes before diving into the details.
---

# Quickstart

This quickstart helps you to integrate your LLM application with Langfuse.

<Steps>

### Run Langfuse

The fastest way to get started is to use the Langfuse Cloud. Alternatively you can run Langfuse locally to test it out or self-host it.

<Tabs items={["Langfuse cloud", "Docker"]}>
<Tab>

1. Create account and project on
   [cloud.langfuse.com](https://cloud.langfuse.com/auth/sign-up)
2. Copy API keys for your
   project

   ```bash filename=".env"
   LANGFUSE_SECRET_KEY="sk-lf-...";
   LANGFUSE_PUBLIC_KEY="pk-lf-...";
   ```

</Tab>
<Tab>

1.  Run Langfuse locally with docker compose

    ```bash
    # Clone repository
    git clone https://github.com/langfuse/langfuse.git
    cd langfuse

    # Run server and db
    docker compose up -d

    # Apply db migrations
    DATABASE_URL=postgresql://postgres:postgres@localhost:5432/postgres DIRECT_URL=postgresql://postgres:postgres@localhost:5432/postgres npx prisma migrate deploy
    ```

2.  Open http://localhost:3000, create a project
3.  Copy API keys for your project
    ```bash filename=".env"
    LANGFUSE_SECRET_KEY="sk-lf-...";
    LANGFUSE_PUBLIC_KEY="pk-lf-...";
    ```

Problems? Read more about running Langfuse [locally with
docker](/docs/local)

This setup is **not recommended for production use** as the core database is part of the docker compose. For details on how to setup Langfuse with a dedicated database for production use, see [self-hosting instructions](/docs/self-host).

</Tab>
<Tab>

1. Run postgres database, e.g. on AWS RDS or GCP Cloud SQL
2. Run docker container on VM or on your preferred cloud service, make sure to replace the database url with your own and add a random secret

```bash
docker run --name langfuse \
-e DATABASE_URL=postgresql://postgres:postgres@host.docker.internal:5432/postgres \
-e NEXTAUTH_SECRET=mysecret \
-e NEXTAUTH_URL=http://localhost:3000 \
-p 3000:3000 \
-a STDOUT \
ghcr.io/langfuse/langfuse:latest
```

3. Go to Langfuse UI on your host and create a project
4. Copy API keys for your project
   ```bash filename=".env"
   LANGFUSE_SECRET_KEY="sk-lf-...";
   LANGFUSE_PUBLIC_KEY="pk-lf-...";
   ```

To run Langfuse in production, see [self-host instructions](self-host.mdx)

</Tab>
</Tabs>

### Add tracing to your backend

<Tabs items={["JS/TS", "Python", "Langchain", "API"]}>
<Tab>

```sh
npm i langfuse
# or
yarn add langfuse

# Node.js < 18
npm i langfuse-node

# Deno
import { Langfuse } from "https://esm.sh/langfuse"
```

Example usage, most of the parameters are optional and depend on the use case.

For more information, see the [typescript SDK docs](/docs/integrations/sdk/typescript).

```typescript filename="server.ts"
import { Langfuse } from "langfuse";

const langfuse = new Langfuse({
  secretKey: process.env.LANGFUSE_SECRET_KEY, // sk-lf-...
  publicKey: process.env.LANGFUSE_PUBLIC_KEY, // pk-lf-...
  // baseUrl: defaults to "https://cloud.langfuse.com"
});

// Example generation creation
const generation = trace.generation({
  name: "chat-completion",
  model: "gpt-3.5-turbo",
  modelParameters: {
    temperature: 0.9,
    maxTokens: 2000,
  },
  prompt: messages,
});

// Application code
const chatCompletion = await llm.respond(prompt);

// End generation - sets endTime
generation.end({
  completion: chatCompletion,
});
```

</Tab>
<Tab>

```bash
pip install langfuse
```

Example usage, most of the parameters are optional and depend on the use case. For more information, see the [python docs](/docs/integrations/sdk/python).

```python filename="server.py"
from datetime import datetime
from langfuse import Langfuse
from langfuse.model import InitialGeneration, Usage

langfuse = Langfuse(
  private_key="pk-lf-...",
  secret_key="sk-sk-...",
  host="https://cloud.langfuse.com"
)

generationStartTime = datetime.now()

# chatCompletion = callLLM(prompt)

langfuse.generation(InitialGeneration(
    name="chat-completion",
    startTime=generationStartTime,
    endTime=datetime.now(),
    model="gpt-3.5-turbo",
    modelParameters= {
      "temperature":0.9,
      "maxTokens":1000,
      "topP": None,
    },
    prompt=[{"role": "user", "content": message_to_send}],
    completion=chat_response.json()["choices"][0]["message"],
    usage=Usage(
      prompt_tokens=512,
      completion_tokens=49
    ),
    version: '1.3.9',
    metadata= {
     "interface": 'whatsapp',
    }
))

# The SDK executes network requests in the background.
# To ensure that all requests are sent before the process exits, call flush()
langfuse.flush()
```

</Tab>
<Tab>

Install the Langfuse Python SDK

```bash
pip install langfuse
```

Add Langfuse callback handler to your Langchain agent/chain/... to automatically capture traces.

```python /callbacks=[handler]/
from langfuse.callback import CallbackHandler
handler = CallbackHandler(LANGFUSE_PUBLIC_KEY, LANGFUSE_SECRET_KEY)

# Langchain implementation

# Add handler as callback when running the Langchain agent
agent.run("<user_input>", callbacks=[handler])
```

For more examples, see the [Langchain integration docs](/docs/integrations/langchain).

</Tab>
<Tab>

[API reference](/docs/integrations/api) for custom integrations

</Tab>
</Tabs>

### Capture scores (optional)

<Tabs items={["Browser", "JS/TS", "Python", "API"]}>
<Tab>

Use LangfuseWeb to directly capture **user feedback** as scores in the browser.

```sh npm2yarn
npm i langfuse
```

Simple feedback component in React:

```typescript
import { LangfuseWeb } from "langfuse";

export default function UserFeedbackComponent(props: { traceId: string }) {
  const langfuseWeb = new LangfuseWeb({
    publicKey: process.env.NEXT_PUBLIC_LANGFUSE_PUBLIC_KEY, // pk-lf-...
    // baseUrl: defaults to "https://cloud.langfuse.com"
  });

  const handleFeedback = async (value: number) => {
    await langfuseWeb.score({
      traceId: props.traceId,
      name: "user_feedback", // arbitrary name to identify the type of score
      value, // float, optional: scale it to e.g. 0..1
    });
  };

  return (
    <>
      <button onClick={() => handleFeedback(1)}>üëç</button>
      <button onClick={() => handleFeedback(0)}>üëé</button>
    </>
  );
}
```

</Tab>
<Tab>

You can use the JS/TS SDK to report a score.

```typescript
trace.score({
  name: "user-explicit-feedback",
  value: 1,
  comment: "I like how personalized the response is",
});

// alternatively
span.score({});
generation.score({});
event.score({});
```

</Tab>
<Tab>

You can use the Python SDK to report a score via your Python server.

```python
from langfuse.model import CreateScore

score = generation.score(CreateScore(
    name='user-explicit-feedback',
    value=1,
    comment="I like how personalized the response is"
))
```

</Tab>
<Tab>

```bash
curl --location 'http://cloud.langfuse.com/api/public/metrics' \
--header 'Content-Type: application/json' \
--header 'Accept: application/json' \
--header "Authorization: Bearer {langfuse_public_key}"
--data '{
  "traceId": "<string>",
  "name": "<string>",
  "value": "<integer>",
  "observationId": "<optional_string>"
}'
```

</Tab>
</Tabs>

### Explore

Visit Langfuse interface to explore your data.

</Steps>

## More information

This is a very short summary of how to get started with Langfuse, have a look at the detailed SDK documentation for features such as:

- Nesting of observations to capture complex chains and agents
- Use of your own existing ids to group traces, observations and scores
- Optimizing asynchronous behavior of the SDK

import { SiTypescript, SiPython } from "react-icons/si";
import { FiCode } from "react-icons/fi";

<Cards num={2}>
  <Card
    icon={<SiTypescript size="24" />}
    title="JS/TS SDK (Edge, Node, Web)"
    href="/docs/integrations/sdk/typescript"
    arrow
  />
  <Card
    icon={<SiPython size="24" />}
    title="Python SDK"
    href="/docs/integrations/sdk/python"
    arrow
  />
  <Card
    icon={<span>ü¶úüîó</span>}
    title="Langchain integration"
    href="/docs/integrations/langchain"
    arrow
  />
  <Card
    icon={<FiCode size="24" />}
    title="API reference"
    href="/docs/integrations/api"
    arrow
  />
</Cards>

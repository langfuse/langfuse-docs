---
description: Langfuse integration to easily observe and monitor pipelines built by Haystack, an open-source Python framework developed by deepset.
---

# ðŸŒ¾ Haystack <> Langfuse Integration

We're excited to highlight a Langfuse integration with Haystack! This integration allows you to easily trace your Haystack pipelines in the Langfuse UI.

Thanks to the team at deepset for developing the integration. These docs are adapted from their write up, which you can read [here](https://haystack-home-git-tt-2024-may-langfuse-deepset-overnice.vercel.app/blog/langfuse-integration).

## What is Haystack?

[Haystack](https://haystack.deepset.ai/) is the open-source Python framework developed by deepset. Its modular design allows users to implement custom pipelines to build production-ready LLM applications, like retrieval-augmented generative pipelines and state-of-the-art search systems. It integrates with Hugging Face Transformers, Elasticsearch, OpenSearch, OpenAI, Cohere, Anthropic and others, making it an extremely popular framework for teams of all sizes.

## How Can Langfuse Help?

The `langfuse-haystack` package integrates tracing capabilities into Haystack (2.x) pipelines using Langfuse.

[Langfuse](https://langfuse.com) is helpful for Haystack pipelines in the following ways:

- Capture comprehensive details of the execution trace in a beautiful UI dashboard
  - Latency
  - Token usage
  - Cost
  - Scores
- Capture the full context of the execution
- Building fine-tuning and testing datasets

Langfuse integration with a tool like Haystack can help monitor model performance, can assist with pinpointing areas for improvement, or create datasets from your pipeline executions for fine-tuning and testing.

## Installation and Setup

Install the necessary packages:

```bash
pip install langfuse-haystack langfuse
```

And then some dependencies to parse data for a RAG pipeline. Read more about how Haystack approaches parsing and storing data for retrieval [here](https://docs.haystack.deepset.ai/docs/components_overview).

```bash
pip install sentence-transformers datasets mwparserfromhell
```

Then set the environment variables. You can find your Langfuse public and private API keys in the dashboard. Make sure to set `HAYSTACK_CONTENT_TRACING_ENABLED` to `true`.

```python
import os

# Get keys for your project from the project settings page
# https://cloud.langfuse.com
os.environ["LANGFUSE_PUBLIC_KEY"] = "pk-lf-..."
os.environ["LANGFUSE_SECRET_KEY"] = "sk-lf-..."
os.environ["LANGFUSE_HOST"] = "https://cloud.langfuse.com" # ðŸ‡ªðŸ‡º EU region
# os.environ["LANGFUSE_HOST"] = "https://us.cloud.langfuse.com" # ðŸ‡ºðŸ‡¸ US region
os.environ["HAYSTACK_CONTENT_TRACING_ENABLED"] = "True"

# Your openai key
os.environ["OPENAI_API_KEY"] = "sk-proj-..."
```

## Langfuse Integration with Basic RAG Pipeline

You can build a basic RAG pipeline very (based off of the Haystack [documentation](https://docs.haystack.deepset.ai/docs/get_started)) walks through building a basic retrieval-augmented generative (RAG) pipeline. First you'll load your data to the Document Store, then connect components together into a RAG pipeline, and finally ask a question.

First we build the pipeline. Add `LangfuseConnector` to the pipeline as a tracer. There's no need to connect it to any other component. The `LangfuseConnector` will automatically trace the operations and data flow within the pipeline. Then add the other components, like the text embedder, retriever, prompt builder and the model, and connect them together in the order they will be used in the pipeline.

<Steps>

### Import packages

```python
from datasets import load_dataset
from haystack import Document, Pipeline
from haystack.components.builders import PromptBuilder
from haystack.components.embedders import SentenceTransformersDocumentEmbedder, SentenceTransformersTextEmbedder
from haystack.components.generators import OpenAIGenerator
from haystack.components.retrievers import InMemoryEmbeddingRetriever
from haystack.document_stores.in_memory import InMemoryDocumentStore
from haystack_integrations.components.connectors.langfuse import LangfuseConnector
```

### Create the pipeline with necessary components

Add `LangfuseConnector` to the pipeline as a tracer. There's no need to connect it to any other component. Add the other components, like the text embedder, retriever, prompt builder and the model, and connect them together in the order they will be used in the pipeline.

```python
def get_pipeline(document_store: InMemoryDocumentStore):
    retriever = InMemoryEmbeddingRetriever(document_store=document_store, top_k=2)

    # A prompt corresponds to an NLP task and contains instructions for the model. Here, the pipeline will go through each Document to figure out the answer.
    template = """
    Given the following information, answer the question.
    Context:
    {% for document in documents %}
        {{ document.content }}
    {% endfor %}
    Question: {{question}}
    Answer:
    """

    prompt_builder = PromptBuilder(template=template)

    basic_rag_pipeline = Pipeline()
    # Add components to your pipeline
    basic_rag_pipeline.add_component("tracer", LangfuseConnector("Basic RAG Pipeline"))
    basic_rag_pipeline.add_component(
        "text_embedder", SentenceTransformersTextEmbedder(model="sentence-transformers/all-MiniLM-L6-v2")
    )
    basic_rag_pipeline.add_component("retriever", retriever)
    basic_rag_pipeline.add_component("prompt_builder", prompt_builder)
    basic_rag_pipeline.add_component("llm", OpenAIGenerator(model="gpt-3.5-turbo", generation_kwargs={"n": 2}))

    # Now, connect the components to each other
    # NOTE: the tracer component doesn't need to be connected to anything in order to work
    basic_rag_pipeline.connect("text_embedder.embedding", "retriever.query_embedding")
    basic_rag_pipeline.connect("retriever", "prompt_builder.documents")
    basic_rag_pipeline.connect("prompt_builder", "llm")

    return basic_rag_pipeline
```

### Load data

Then we load data into DocumentStore. In this example, we use the `trivia_qa_tiny` [dataset](https://huggingface.co/datasets/SpeedOfMagic/trivia_qa_tiny?row=26).

```python
document_store = InMemoryDocumentStore()
dataset = load_dataset("SpeedOfMagic/trivia_qa_tiny", split="train")
embedder = SentenceTransformersDocumentEmbedder("sentence-transformers/all-MiniLM-L6-v2")
embedder.warm_up()

docs_with_embeddings = []
for entry in dataset:
    # Create a Document object for each entry, handling the question (str) and answer (str) data correctly
    content = f"Question: {entry['question']} Answer: {entry['answer']}"
    doc = Document(content=content)

    # Embed the document using the embedder
    # Only takes in list of Documents
    embedder.run([doc])

    # Collect the embedded documents
    docs_with_embeddings.append(doc)

# Write the embedded documents to the document store
document_store.write_documents(docs_with_embeddings)
```

### Run the pipeline

Now you can ask a question and you will get a response informed by the data we loaded into the pipeline.

```python
pipeline = get_pipeline(document_store)
question = "What can you tell me about Truman Capote?"
response = pipeline.run({"text_embedder": {"text": question}, "prompt_builder": {"question": question}})
print(response["llm"]["replies"][0])
print(response["tracer"]["trace_url"])
print(response)
```

The output should look like this.

```text
{'tracer': {'name': 'Basic RAG Pipeline', 'trace_url': 'https://cloud.langfuse.com/trace/cb283594-b2fa-487b-ac6f-d7dc1dbdc7b0'}, 'llm': {'replies': ['Truman Capote was an American author known for his works such as "In Cold Blood" and "Breakfast at Tiffany\'s." He was born Truman Streckfus Persons, but later adopted his stepfather\'s surname, Capote. He was known for his unique writing style and for popularizing the genre of true crime writing. Capote was also a prominent figure in the social scene of New York City during the 1950s and 1960s.', 'Truman Capote was an American writer known for his literary works such as "Breakfast at Tiffany\'s" and "In Cold Blood." He was born Truman Streckfus Persons before being adopted by his stepfather and taking on the last name Capote. Capote was known for his unique writing style and his ability to blur the lines between fiction and non-fiction. He also had a close relationship with fellow writer Harper Lee, author of "To Kill a Mockingbird."'], 'meta': [{'model': 'gpt-3.5-turbo-0125', 'index': 0, 'finish_reason': 'stop', 'usage': {'completion_tokens': 192, 'prompt_tokens': 87, 'total_tokens': 279}}, {'model': 'gpt-3.5-turbo-0125', 'index': 1, 'finish_reason': 'stop', 'usage': {'completion_tokens': 192, 'prompt_tokens': 87, 'total_tokens': 279}}]}}
```

You will notice that the response contains trivia information Truman Capote, such as his birth name, that would not have been included in a standard model call without RAG.

Head over to the Langfuse dashboard to see the trace at this URL: https://cloud.langfuse.com/project/clvh71wlt0000wpuqhferayy5/traces/cb283594-b2fa-487b-ac6f-d7dc1dbdc7b0

![Trace of Haystack pipeline](/public/images/docs/haystack-langfuse-1.png)

</Steps>

For each trace, you can see:

- Latency for each component of the pipeline
- Input and output for each step
- For generations, token usage and costs are automatically calculated.

Here's where you can read more about [traces](https://langfuse.com/docs/tracing) in Langfuse.

## Adding Scores via Langfuse to Traces

Scoring is a key part of the monitoring process. You can sort through traces by their scores to identify low-quality output or to monitor the quality of responses.

You can score traces using a number of methods:

- Through user feedback
- Model-based evaluation
- Through SDK/API
- Manually, in the Langfuse UI

Read more about how to score [here](https://langfuse.com/docs/scores/overview).

The example below walks through a simple way to score the chat generator's response via the Python SDK. It adds a score of 1 to the trace above with the comment "Cordial and relevant" because the model's response was very polite and factually correct.

```python
from langfuse import Langfuse

langfuse = Langfuse()

url = "https://cloud.langfuse.com/project/clvh71wlt0000wpuqhferayy5/traces/345f3776-76e0-4053-95c9-d4e865302ff1"

# Split the string at each forward slash (/) and get the last element
id = url.split('/')[-1]

langfuse.score(
    trace_id=id,
    name="quality",
    value=1,
    comment="Cordial and relevant", # optional
)
```

See the trace with the score added: https://cloud.langfuse.com/project/clvh71wlt0000wpuqhferayy5/traces/345f3776-76e0-4053-95c9-d4e865302ff1

## Acknowledgement

We're thrilled to collaborate with deepset and great products like Haystack to give the best possible experience to devs. Thanks to the deepset team for developing this integration and read their [write up](https://haystack-home-git-tt-2024-may-langfuse-deepset-overnice.vercel.app/blog/langfuse-integration)!

We're committed to being open source and shipping as fast as possible. If you're interested in collaborating, raise an issue on [Github](https://github.com/orgs/langfuse/discussions), make sure to join [Discord](https://discord.com/invite/7NXusRtqYU), and follow us on [Twitter](https://twitter.com/langfuse) to stay up-to-date on all new feature launches!

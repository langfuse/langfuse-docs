---
title: Open Source Observability & Tracing for LangChain
description: Open source tracing and monitoring for your LangChain application. Python and JS/TS. Automatically capture rich traces and metrics and evaluate outputs.
ogImage: /images/docs/langchain_og.png
---

import GetStartedLangchainPythonArgs from "@/components-mdx/get-started-langchain-python-constructor-args.mdx";
import GetStartedLangchainPythonEnv from "@/components-mdx/get-started-langchain-python-env.mdx";
import GetStartedLangchainJsArgs from "@/components-mdx/get-started-langchain-js-constructor-args.mdx";
import GetStartedLangchainJsEnv from "@/components-mdx/get-started-langchain-js-env.mdx";

# Observability & Tracing for Langchain (Python & JS/TS)

[Langfuse Tracing](/docs/tracing) integrates with Langchain using Langchain Callbacks ([Python](https://python.langchain.com/docs/how_to/#callbacks), [JS](https://js.langchain.com/docs/how_to/#callbacks)). Thereby, the Langfuse SDK automatically creates a nested trace for every run of your Langchain applications. This allows you to log, analyze and debug your LangChain application.

<CloudflareVideo
  videoId="dbe2ea8e8d64f27126a3734fa51cd0e7"
  gifStyle
  className="max-w-2xl"
/>

## Add Langfuse to your Langchain Application

You can configure the integration via (1) constructor arguments or (2) environment variables. Get your Langfuse credentials from the Langfuse dashboard.

<Tabs
  items={[
    "Python SDK v3 (env)",
    "Python SDK v3 (constructor args)",
    "Python SDK v2 (constructor args)",
    "Python SDK v2 (env)",
    "JS (constructor args)",
    "JS (env)",
  ]}
>
  <Tab>
    Set environment variables:

    ```bash
    export LANGFUSE_PUBLIC_KEY="your-public-key"
    export LANGFUSE_SECRET_KEY="your-secret-key"
    export LANGFUSE_HOST="https://cloud.langfuse.com"  # Optional: defaults to https://cloud.langfuse.com
    ```

    ```bash
    pip install langfuse
    ```

    ```python
    from langfuse import get_client
    from langfuse.langchain import CallbackHandler
    from langchain_openai import ChatOpenAI  # Example LLM
    from langchain_core.prompts import ChatPromptTemplate

    # Initialize Langfuse client (uses environment variables)
    langfuse = get_client()

    # Initialize the Langfuse handler
    langfuse_handler = CallbackHandler()

    # Example: Using it with an LLM call
    llm = ChatOpenAI(model_name="gpt-4o")
    prompt = ChatPromptTemplate.from_template("Tell me a joke about {topic}")
    chain = prompt | llm

    # Wrap your chain execution with a Langfuse span to set trace attributes
    with langfuse.start_as_current_span(name="joke-chain") as span:
        # Set trace attributes for the entire chain execution
        span.update_trace(
            user_id="user_123",
            session_id="session_456",
            tags=["joke-chain"],
            input={"topic": "cats"}
        )

        response = chain.invoke({"topic": "cats"}, config={"callbacks": [langfuse_handler]})
        
        # Set the trace output
        span.update_trace(output={"response": response.content})
        
        print(response.content)

    # Flush events to Langfuse in short-lived applications
    langfuse.flush()
    ```
  </Tab>
  <Tab>
    ```bash
    pip install langfuse
    ```

    ```python
    from langfuse import Langfuse
    from langfuse.langchain import CallbackHandler
    from langchain_openai import ChatOpenAI  # Example LLM
    from langchain_core.prompts import ChatPromptTemplate

    # Initialize Langfuse client with constructor arguments
    langfuse = Langfuse(
        public_key="your-public-key",
        secret_key="your-secret-key",
        host="https://cloud.langfuse.com"  # Optional: defaults to https://cloud.langfuse.com
    )

    # Initialize the Langfuse handler
    langfuse_handler = CallbackHandler()

    # Example: Using it with an LLM call
    llm = ChatOpenAI(model_name="gpt-4o")
    prompt = ChatPromptTemplate.from_template("Tell me a joke about {topic}")
    chain = prompt | llm

    # Wrap your chain execution with a Langfuse span to set trace attributes
    with langfuse.start_as_current_span(name="joke-chain") as span:
        # Set trace attributes for the entire chain execution
        span.update_trace(
            user_id="user_123",
            session_id="session_456",
            tags=["joke-chain"],
            input={"topic": "cats"}
        )

        response = chain.invoke({"topic": "cats"}, config={"callbacks": [langfuse_handler]})
        
        # Set the trace output
        span.update_trace(output={"response": response.content})
        
        print(response.content)

    # Flush events to Langfuse in short-lived applications
    langfuse.flush()
    ```
  </Tab>
  <Tab>
    <GetStartedLangchainPythonArgs />
  </Tab>
  <Tab>
    <GetStartedLangchainPythonEnv />
  </Tab>
  <Tab>
    <GetStartedLangchainJsArgs />
  </Tab>
  <Tab>
    <GetStartedLangchainJsEnv />
  </Tab>
</Tabs>

<Callout type="info" emoji="‚ú®">
  Done. Now you can explore detailed traces and metrics in the Langfuse
  dashboard.
</Callout>

Prefer end-to-end examples?

import { FileCode, GitPullRequestArrow } from "lucide-react";

<Cards num={3}>
  <Card
    title="Example (Python)"
    href="/docs/integrations/langchain/example-python"
    icon={<FileCode />}
  />
  <Card
    title="Example (JS/TS)"
    href="/docs/integrations/langchain/example-javascript"
    icon={<FileCode />}
  />
  <Card
    title="Example LangGraph (Python)"
    href="/docs/integrations/langchain/example-python-langgraph"
    icon={<FileCode />}
  />
  <Card
    title="Example LangServe (Python)"
    href="/docs/integrations/langchain/example-python-langserve"
    icon={<FileCode />}
  />
</Cards>

## Supported LangChain interfaces

| Feature/interface | Python | JS/TS |
| ----------------- | ------ | ----- |
| LCEL              | ‚úÖ     | ‚úÖ    |
| `invoke()`        | ‚úÖ     | ‚úÖ    |
| `run()`           | ‚úÖ     | ‚úÖ    |
| `call()`          | ‚úÖ     | ‚úÖ    |
| `predict()`       | ‚úÖ     | ‚úÖ    |
| async             | ‚úÖ     | ‚úÖ    |
| `batch()`         | ‚úÖ     | (‚úÖ)  |
| streaming         | ‚úÖ     | ‚úÖ    |

We are interested in your feedback! Raise an issue on [GitHub](/ideas) to request support for additional interfaces.

## Supported LangChain features

- **üï∏Ô∏è LangGraph**: Works with Langfuse Integration. Requires Python 3.11+ ([GH issue](https://github.com/langfuse/langfuse/issues/1926)). See notebook for [example integration](/docs/integrations/langchain/example-python-langgraph).
- **üèì LangServe**: See notebook for [example integration](/docs/integrations/langchain/example-python-langserve).

## Additional Configuration

### Optional constructor arguments

When initializing the Langfuse handler, you can pass the following **optional** arguments to use more advanced features.

| Python        | JS/TS       | Type    | Description                                                                            |
| ------------- | ----------- | ------- | -------------------------------------------------------------------------------------- |
| `user_id`     | `userId`    | string  | The current [user](/docs/tracing-features/users).                                      |
| `session_id`  | `sessionId` | string  | The current [session](/docs/tracing-features/sessions).                                |
| `release`     | `release`   | string  | The [release](/docs/tracing-features/releases-and-versioning) tag of your application. |
| `version`     | `version`   | string  | The [version](/docs/tracing-features/releases-and-versioning) of your application.     |
| `trace_name`  |             | string  | Customize the name of the created traces. Defaults to name of chain.                   |
| `enabled`     | `enabled`   | boolean | Enable or disable the Langfuse integration. Defaults to `true`.                        |
| `sample_rate` | `-`         | float   | [Sample rate](/docs/tracing-features/sampling) for tracing.                            |

<Callout type="warning">

**Python SDK v3 Note**: The constructor arguments above are only available in **Python SDK v2**. 

For **Python SDK v3**:
- **Global settings** (like `sample_rate`, `enabled`) must be set when initializing the Langfuse client globally via `Langfuse()` constructor or environment variables
- **Trace attributes** (like `user_id`, `session_id`, `release`, `version`) must be set via an enclosing span using `span.update_trace()` as shown in the examples above

</Callout>

### Dynamic Trace Attributes in Chain Invocation

You can also set the `trace_name`, [`user_id`](/docs/tracing-features/users), [`session_id`](/docs/tracing-features/sessions), and [`tags`](/docs/tracing-features/tags) for a trace that corresponds to a LangChain execution through the runnable configuration of the chain without instantiating a new `CallbackHandler` each time. This allows you to dynamically set these attributes for each specific execution. Here's an example:

<Tabs items={["Langchain (Python)", "Langchain (JS/TS)"]}>
<Tab>

```python
from langfuse.callback import CallbackHandler

langfuse_handler = CallbackHandler()

trace_name = "langchain_trace_name"
session_id = "random-session"
user_id = "random-user"
tags = ["random-tag-1", "random-tag-2"]

# Your existing Langchain code to create the chain

# Pass config to the chain invocation to be parsed as Langfuse trace attributes

chain.invoke(
    {"animal": "dog"},
    config={
        "callbacks": [langfuse_handler],
        "run_name": trace_name,
        "tags": tags,
        "metadata": {
            "langfuse_session_id": session_id,
            "langfuse_user_id": user_id,
        },
    },
)
```

Setting trace attributes dynamically requires the execution of a Langchain chain. Otherwise the dynamically provided chain configuration is not passed to the Langfuse `CallbackHandler` instance. Thus when invoking Langchain LLM class instances with dynamic trace metadata, an additional wrapping is required to force the execution as a chain:

```python
from langchain_core.runnables import RunnablePassthrough
from langchain_openai import ChatOpenAI

from langfuse.callback import CallbackHandler

handler = CallbackHandler()


def main():
    llm = ChatOpenAI()

    runnable = RunnablePassthrough() | llm # force execution as chain
    result = runnable.invoke(
        "Whassup?",
        {
            "callbacks": [handler],
            "metadata": {
                "langfuse_session_id": "my-session",
                "langfuse_user_id": "my-user",
            },
        },
    )

    handler.flush()
```
</Tab>

<Tab>

```ts
import { CallbackHandler } from "langfuse-langchain";

const langfuseHandler = new CallbackHandler();

const traceName = "langchain_trace_name";
const sessionId = "random-session";
const userId = "random-user";
const tags = ["random-tag-1", "random-tag-2"];

await chain.invoke(
  { animal: "dog" },
  {
    callbacks: [langfuseHandler],
    runName: traceName,
    tags,
    metadata: { langfuseUserId: userId, langfuseSessionId: sessionId },
  }
);
```

</Tab>

</Tabs>

### Predefined Trace ID + Add Evaluation or User Feedback Score

**Predefined Trace ID**

To query traces or add evaluation and feedback scores, you need to know the ID of a trace. The LangChain integration automatically assigns the `trace_id` to the `run_id` of the LangChain run. You can set a predefined `run_id` through the run configuration.

<Tabs items={["Python", "JS/TS"]}>
<Tab>

```python
import uuid

predefined_run_id = str(uuid.uuid4())

langfuse_handler = CallbackHandler()

# Your existing Langchain code to create the chain

# Pass run_id to the chain invocation
chain.invoke({"person": "Ada Lovelace"}, config={
    "run_id": predefined_run_id,
    "callbacks": [langfuse_handler],
  }
)
```

</Tab>
<Tab>

```typescript
import { CallbackHandler } from "langfuse-langchain";
import { v4 as uuidv4 } from "uuid";

const langfuseHandler = new CallbackHandler();

const predefinedRunId = uuid4();

await chain.invoke(
  { animal: "dog" },
  {
    callbacks: [langfuseHandler],
    runId: predefinedRunId,
  }
);
```

</Tab>
</Tabs>

**Add Score to Trace**

Evaluation results and user feedback are recorded as [scores](https://langfuse.com/docs/scores) in Langfuse. You can add them to a trace via the `trace_id`. Scores do not need to be numeric, see scores documentation for more details.

<Tabs items={["Python", "JS/TS"]}>
<Tab>

```python
from langfuse import Langfuse

langfuse = Langfuse()

langfuse.score(
    trace_id=predefined_run_id,
    name="user-feedback",
    value=1,
    comment="This was correct, thank you"
)
```

</Tab>

<Tab>

```typescript
import { Langfuse } from "langfuse";

const langfuse = new Langfuse();

await langfuse.score({
  traceId: predefinedRunId,
  name: "user-feedback",
  value: 1,
  comment: "This was correct, thank you",
});
```

</Tab>
</Tabs>

### Interoperability with Langfuse SDKs [#interoperability]

Use the Langchain integration in combination with the regular Langfuse SDKs if you want to:

- Add non-Langchain related observations to the trace.
- Group multiple Langchain runs into a single trace.

Learn more about the structure of a trace [here](/docs/tracing).

<Tabs items={["Python SDK v3 with @observe", "Python SDK v3 Manual", "Python SDK v2 Decorator", "JS", "Python SDK v2 low-level"]}>
<Tab>
{/* Python SDK v3 decorator */}

```python
from langfuse import observe, get_client
from langfuse.langchain import CallbackHandler

@observe() # automatically log function as a trace to Langfuse
def main():
    langfuse = get_client()
    
    # Update trace attributes (e.g, name, session_id, user_id)
    langfuse.update_current_trace(
        name="custom-trace",
        session_id="user-1234",
        user_id="session-1234",
    )
    
    # Initialize the Langfuse handler for the current trace context
    langfuse_handler = CallbackHandler()

    # Your Langchain code
    # Add Langfuse handler as callback (classic and LCEL)
    chain.invoke({"input": "<user_input>"}, config={"callbacks": [langfuse_handler]})

main()
```

</Tab>
<Tab>
{/* Python SDK v3 manual */}

```python
from langfuse import get_client
from langfuse.langchain import CallbackHandler

langfuse = get_client()

# Create a trace via Langfuse spans and use Langchain within it
with langfuse.start_as_current_span(name="custom-trace") as span:
    # Update trace attributes
    span.update_trace(
        session_id="user-1234",
        user_id="session-1234",
        input={"user_input": "<user_input>"}
    )
    
    # Initialize the Langfuse handler
    langfuse_handler = CallbackHandler()

    # Your Langchain code
    # Add Langfuse handler as callback (classic and LCEL)
    result = chain.invoke({"input": "<user_input>"}, config={"callbacks": [langfuse_handler]})
    
    # Update trace output
    span.update_trace(output={"result": result})
```

</Tab>
<Tab>
{/* Python SDK v2 decorator */}

```python /@observe()/ /from langfuse.decorators import langfuse_context, observe/ /langfuse_context.get_current_langchain_handler()/
from langfuse.decorators import langfuse_context, observe

# Create a trace via Langfuse decorators and get a Langchain Callback handler for it
@observe() # automtically log function as a trace to Langfuse
def main():
    # update trace attributes (e.g, name, session_id, user_id)
    langfuse_context.update_current_trace(
        name="custom-trace",
        session_id="user-1234",
        user_id="session-1234",
    )
    # get the langchain handler for the current trace
    langfuse_handler = langfuse_context.get_current_langchain_handler()

    # Your Langchain code

    # Add Langfuse handler as callback (classic and LCEL)
    chain.invoke({"input": "<user_input>"}, config={"callbacks": [langfuse_handler]})

main()
```

Limitation (decorator + langchain): The input/output of the Langchain code will not be added to the trace or span created by the decorator but to a child. Adding them would cause unwanted side-effects if they are set manually or if you add multiple Langchain runs.

</Tab>
<Tab>
{/* JS */}

```ts
import { CallbackHandler, Langfuse } from "langfuse-langchain";
const langfuse = new Langfuse();

// Get Langchain handler for a trace
const trace = langfuse.trace();
const langfuseHandlerTrace = new CallbackHandler({ root: trace });

// Add Langfuse handler as callback (classic and LCEL)
await chain.invoke(
  { input: "<user_input>" },
  { callbacks: [langfuseHandlerTrace] }
);

// Get Langchain handler for a span
const span = trace.span();
const langfuseHandlerSpan = new CallbackHandler({ root: span });

// Add Langfuse handler as callback (classic and LCEL)
await chain.invoke(
  { input: "<user_input>" },
  { callbacks: [langfuseHandlerSpan] }
);
```

If you want to add the input/output of the Langchain run to the trace or span itself, use the `updateRoot` flag in the `CallbackHandler` constructor.

```ts
const langfuseHandlerTrace = new CallbackHandler({
  root: trace,
  updateRoot: true,
});
```

</Tab>
<Tab>
{/* Python low-level */}

```python
# Initialize Langfuse Client
from langfuse import Langfuse
langfuse = Langfuse()

# Get Langchain handler for a trace
trace = langfuse.trace()
langfuse_handler_trace = trace.get_langchain_handler()

# Add Langfuse handler as callback (classic and LCEL)
chain.invoke({"input": "<user_input>"}, config={"callbacks": [langfuse_handler_trace]})

# Get Langchain handler for a span
span = trace.span()
langfuse_handler_span = span.get_langchain_handler()

# Add Langfuse handler as callback (classic and LCEL)
chain.invoke({"input": "<user_input>"}, config={"callbacks": [langfuse_handler_span]})
```

If you want to add the input/output of the Langchain run to the trace or span itself, use the `update_parent` flag.

```python
langfuse_handler_trace = trace.get_langchain_handler(update_parent=True)
```

</Tab>
</Tabs>

If you pass these callback handlers to your Langchain code, the events will be nested under the respective trace or span in the Langfuse.

See the [Langchain observability cookbook](/docs/integrations/langchain/example-python) for an example of this in action (Python).

### Queuing and flushing

The Langfuse SDKs queue and batch events in the background to reduce the number of network requests and improve overall performance. In a long-running application, this works without any additional configuration.

If you are running a short-lived application, you need to shutdown Langfuse to ensure that all events are flushed before the application exits.

<Tabs items={["Python SDK v3", "Python SDK v2", "JS"]}>
<Tab>
{/* Python SDK v3 */}

```python
# For the main Langfuse client
langfuse.shutdown()

# For the callback handler
langfuse_handler.shutdown()
```

</Tab>
<Tab>
{/* Python SDK v2 */}

```python
langfuse_handler.shutdown()
```

</Tab>
<Tab>

{/* JS */}

```ts
await langfuseHandler.shutdownAsync();
```

</Tab>
</Tabs>

If you want to flush events synchronously at a certain point, you can use the `flush` method. This will wait for all events that are still in the background queue to be sent to the Langfuse API. This is usually discouraged in production environments.

<Tabs items={["Python SDK v3", "Python SDK v2", "JS"]}>
<Tab>
{/* Python SDK v3 */}

```python
from langfuse import get_client

get_client().flush()
```

</Tab>
<Tab>
{/* Python SDK v2 */}

```python
langfuse_handler.flush()
```

</Tab>
<Tab>

{/* JS */}

```ts
await langfuseHandler.flushAsync();
```

</Tab>
</Tabs>

### Serverless environments (JS/TS)

Since Langchain version > 0.3.0, the callbacks on which Langfuse relies have been backgrounded. This means that execution will not wait for the callback to either return before continuing. Prior to 0.3.0, this behavior was the opposite. If you are running code in serverless environments such as Google Cloud Functions, AWS Lambda or Cloudflare Workers you should set your callbacks to be blocking to allow them time to finish or timeout. This can be done either by

- setting the `LANGCHAIN_CALLBACKS_BACKGROUND` environment variable to "false"
- importing the global `awaitAllCallbacks` method to ensure all callbacks finish if necessary

Read more about awaiting callbacks here in the [Langchain docs](https://js.langchain.com/docs/how_to/callbacks_serverless).

### Azure OpenAI model names

Please add the `model` keyword argument to the `AzureOpenAI` or `AzureChatOpenAI` class to have the model name parsed correctly in Langfuse.

<Tabs items={["Python", "JS/TS"]}> 
<Tab>
```python
from langchain_openai import AzureChatOpenAI

llm = AzureChatOpenAI(
azure_deployment="my-gpt-4o-deployment",
model="gpt-4o",
)

````
</Tab>
<Tab>
```typescript
import { AzureChatOpenAI } from "@langchain/openai";

const llm = new AzureChatOpenAI({
  azureOpenAIApiDeploymentName: "my-gpt-4o-deployment",
  model: "gpt-4o",
});
````

</Tab>
</Tabs>

## FAQ

import { FaqPreview } from "@/components/faq/FaqPreview";

<FaqPreview tags={["integration-langchain"]} />

## GitHub Discussions

import { GhDiscussionsPreview } from "@/components/gh-discussions/GhDiscussionsPreview";

<GhDiscussionsPreview labels={["integration-langchain"]} />

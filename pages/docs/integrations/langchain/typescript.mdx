---
description: Open source observability for your Langchain (JS) application. Automatically captures rich traces and metrics.
---

import { Callout } from "nextra/components";

# JS/TS Langchain SDK

<div className="flex flex-row flex-wrap gap-2 mt-2">
  <a href="https://github.com/langfuse/langfuse-langchain">
    <img
      alt="Github repository langfuse/langfuse-js"
      src="https://img.shields.io/badge/repo-langfuse--js-blue?style=flat-square&logo=Github"
    />
  </a>
  <a href="https://github.com/langfuse/langfuse-js/actions/workflows/ci.yml?query=branch%3Amain">
    <img
      src="https://img.shields.io/github/actions/workflow/status/langfuse/langfuse-js/ci.yml?style=flat-square&logo=Github&label=tests"
      alt="CI test status"
    />
  </a>
  <a href="https://www.npmjs.com/package/langfuse-langchain">
    <img
      src="https://img.shields.io/npm/v/langfuse-langchain?style=flat-square&label=npm+langfuse-langchain"
      alt="npm langfuse-langchain"
    />
  </a>
</div>

If you are working with Node.js, Deno, or Edge functions, the `langfuse` library is the simplest way to integrate Langfuse into your Langchain application. The library queues calls to make them non-blocking.

**Supported runtimes**

- [x] Node.js
- [x] Edge: Vercel, Cloudflare, ...
- [x] Deno

Want to work without Langchain? Use [Langfuse](/docs/sdk/typescript)
for tracing and [LangfuseWeb](/docs/sdk/typescript-web) to capture feedback from the browser.

## Installation

```sh
# npm
npm i langfuse-langchain

# or yarn
yarn add langfuse-langchain

# or deno
import CallbackHandler from 'https://esm.sh/langfuse-langchain'
```

In your application, set your **API keys** from the project settings in the Langfuse UI to create a client.

```typescript
import { CallbackHandler } from "langfuse-langchain";

const langfuseHandler = new CallbackHandler({
  secretKey: "sk-lf-...",
  publicKey: "pk-lf-...",
  // options
});
```

### Options

| Variable  | Description                                                                                    | Default value                                                                                                                                           |
| --------- | ---------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------- |
| baseUrl   | BaseUrl of the Langfuse API                                                                    | `"https://cloud.langfuse.com"`                                                                                                                          |
| release   | The release number/hash of the application to provide analytics grouped by release.            | `process.env.LANGFUSE_RELEASE` or [common system environment names](https://github.com/langfuse/langfuse-js/blob/main/langfuse-core/src/release-env.ts) |
| version   | The version of the application, see [experimentation docs](/docs/experimentation) for details. | `undefined`                                                                                                                                             |
| userId    | For user-level analytics ([docs](/docs/tracing/users))                                         | `undefined`                                                                                                                                             |
| sessionId | For session-level tracing ([docs](/docs/tracing/sessions))                                     | `undefined`                                                                                                                                             |

<Callout type="info" emoji="ℹ️">
  In short-lived environments (e.g. serverless functions), make sure to always
  call `langfuseHandler.shutdownAsync()` at the end to flush the queue and await
  all pending requests. ([Learn more](/docs/sdk/typescript#lambda))
</Callout>

#### Create a simple LLM call using Langchain

```typescript /callbacks: [langfuseHandler]/
import { PromptTemplate } from "@langchain/core/prompts";
import { OpenAI } from "@langchain/openai";

import { CallbackHandler } from "langfuse-langchain";

// Create a callback handler
const langfuseHandler = new CallbackHandler({
  publicKey: LANGFUSE_PUBLIC_KEY,
  secretKey: LANGFUSE_SECRET_KEY,
});

// Choose a model
const model = new OpenAI({
  temperature: 0,
  openAIApiKey: "YOUR-API-KEY",
  callbacks: [langfuseHandler] // Register your Langfuse callback in the constructor
});

// Create a prompt
const prompt = PromptTemplate.fromTemplate(
  "What is a good name for a company that makes {product}?"
);

// Create a chain and add the Langfuse callback handler
const chain = prompt.pipe(model).withConfig({ callbacks: [langfuseHandler] }); // Register your Langfuse callback on chain creation

// Invoke the chain
const result = await chain.invoke(
  { product: "colorful hockey sticks" },
  { callbacks: [langfuseHandler] } // Register your Langfuse callback as run config
);

...

await langfuseHandler.flushAsync(); // Flush queued events to Langfuse
```

There are two ways to integrate callbacks into Langchain:

- **Constructor Callbacks**: Set when initializing an object, like `new LLMChain({ ..., callbacks: [langfuseHandler] })`. This approach will use the callback for every call made on that specific object. However, it won't apply to its child objects, making it limited in scope.
- **Request Callbacks**: Defined when issuing a request, like `chain.invoke(..., { callbacks: [langfuseHandler] })`. This not only uses the callback for that specific request but also for any subsequent sub-requests it triggers.

For comprehensive data capture especially for complex chains or agents, it's advised to use the both approaches, as demonstrated above. [Langchain docs.](https://js.langchain.com/docs/modules/callbacks/#when-do-you-want-to-use-each-of-these).

#### Stateful Langchain callbacks

The Langchain client can also be constructed with `Traces` or `Spans`. This allows to nest Langchain executions anywhere in the `Trace` an hence to add any metadata and ids that are important.

```typescript /callbacks: [langfuseHandler]/ /callbacks: [langfuseSpanHandler]/
import { OpenAI } from "@langchain/openai";
import { CallbackHandler, Langfuse } from "langfuse-langchain";

// Instantiate the standard Langfuse SDK
const langfuse = new Langfuse({
  publicKey: LANGFUSE_PUBLIC_KEY,
  secretKey: LANGFUSE_SECRET_KEY,
  baseUrl: LANGFUSE_BASEURL,
});

// Create a trace and a handler nested into the trace.
const parentTrace = langfuse.trace({ name: "parent-trace" });
const langfuseHandler = new CallbackHandler({ root: parentTrace });

// Call the LLM with the handler
const llm = new OpenAI({ callbacks: [langfuseHandler] });
await llm.call("Tell me a joke", { callbacks: [langfuseHandler] });

// Create a span within the trace
const childSpan = parentTrace.span({ name: "child-span" });
const langfuseSpanHandler = new CallbackHandler({ root: span });

// Invoke the nested call to the LLm with the corresponding handler
const llmSpan = new OpenAI({ callbacks: [langfuseSpanHandler] });
await llmSpan.call("Tell me a better joke", {
  callbacks: [langfuseSpanHandler],
});

await langfuse.flushAsync();
```

## Shutdown [#shutdown]

The Langfuse SDKs buffer events and flush them asynchronously to the Langfuse server. You should call shutdown to exit cleanly before your application exits.

```typescript
await langfuseHandler.shutdownAsync();
```

## Execution identifier

The SDK provides a function to return the `traceId` of the trace which is currently used. Similarly, there is another function to expose the langchain `runId`. This `runId` is the latest top-level id of a Langchain run which is also used to create `Spans` or `Generations` in Langfuse.

Both of these ids can be used to create scores for the correct `Span` in Langfuse.

```typescript /handler.getTraceId();/ /handler.getLangchainRunId();/ /callbacks: [langfuseHandler]/
import { OpenAI } from "@langchain/openai";

import { CallbackHandler, Langfuse } from "langfuse-langchain";

// Create a Langfuse JS client
const langfuse = new Langfuse();

// Create a trace and a handler nested into the trace.
const trace = langfuse.trace({ id: "special-id" });
const langfuseHandler = new CallbackHandler({ root: trace });

langfuseHandler.getTraceId(); // returns "special-id"

// Call the LLM with the handler
const llm = new OpenAI({ callbacks: [langfuseHandler] });
await llm.call("Tell me a joke", { callbacks: [langfuseHandler] });

langfuseHandler.getLangchainRunId(); // returns the latest run id

await llm.call("Tell me a better joke", { callbacks: [langfuseHandler] });

langfuseHandler.getLangchainRunId(); // returns the latest run id, different from the first one
```

## Upgrading from v2.x.x to v3.x.x [#upgrade2to3]

Requires [`langchain ^0.1.10`](https://github.com/langchain-ai/langchainjs/releases/tag/0.1.10). Langchain released a new stable version of the Callback Handler interface and this version of the Langfuse SDK implements it. Older versions are no longer supported.

## Upgrading from v1.x.x to v2.x.x [#upgrade1to2]

The `CallbackHandler` can be used in multiple invocations of a Langchain chain as shown below.

```typescript
import { CallbackHandler } from "langfuse-langchain";

// create a handler
const langfuseHandler = new CallbackHandler({
  publicKey: LANGFUSE_PUBLIC_KEY,
  secretKey: LANGFUSE_SECRET_KEY,
});

import { LLMChain } from "langchain/chains";

// create a chain
const chain = new LLMChain({
  llm: model,
  prompt,
  callbacks: [langfuseHandler],
});

// execute the chain
await chain.call(
  { product: "<user_input_one>" },
  { callbacks: [langfuseHandler] }
);
await chain.call(
  { product: "<user_input_two>" },
  { callbacks: [langfuseHandler] }
);
```

So far, invoking the chain multiple times would group the observations in one trace.

```bash
TRACE
|
|-- SPAN: Retrieval
|   |
|   |-- SPAN: LLM Chain
|   |   |
|   |   |-- GENERATION: ChatOpenAi
|-- SPAN: Retrieval
|   |
|   |-- SPAN: LLM Chain
|   |   |
|   |   |-- GENERATION: ChatOpenAi
```

We changed this, so that each invocation will end up on its own trace. This is a more sensible default setting for most users.

```bash
TRACE_1
|
|-- SPAN: Retrieval
|   |
|   |-- SPAN: LLM Chain
|   |   |
|   |   |-- GENERATION: ChatOpenAi

TRACE_2
|
|-- SPAN: Retrieval
|   |
|   |-- SPAN: LLM Chain
|   |   |
|   |   |-- GENERATION: ChatOpenAi
```

If you still want to group multiple invocations on one trace, you can scope the CallbackHandler to a single trace using the following approach. See [docs above](#execution-identifier) for more information.

```typescript
const trace = langfuse.trace({ id: "special-id" });
const langfuseHandler = new CallbackHandler({ root: trace });
```

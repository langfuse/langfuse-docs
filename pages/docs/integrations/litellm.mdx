---
description: Open source observability for LiteLLM via the native integration. Automatically capture detailed traces and metrics for every request.
---

# ðŸš… LiteLLM Integration

**LiteLLM** ([GitHub](https://github.com/BerriAI/litellm)): Use any LLM as a drop in replacement for GPT. Use Azure, OpenAI, Cohere, Anthropic, Ollama, VLLM, Sagemaker, HuggingFace, Replicate (100+ LLMs).

You can find more in-depth documentation in the [LiteLLM docs](https://litellm.vercel.app/docs/observability/langfuse_integration).

## Integration

<Tabs items={["LiteLLM Python SDK", "LiteLLM (Proxy) with OpenAI SDK Wrapper", "LiteLLM (Proxy) with UI-enabled Integration"]} >
<Tab>
By adding the callback you can instantly log your responses across all providers with Langfuse.

## Setup

```sh
pip install langfuse>=2.0.0 litellm
```

```python filename="main.py"
from litellm import completion

## set env variables
os.environ["LANGFUSE_PUBLIC_KEY"] = "your key"
os.environ["LANGFUSE_SECRET_KEY"] = "your key"

# Langfuse host
os.environ["LANGFUSE_HOST"]="https://cloud.langfuse.com" # ðŸ‡ªðŸ‡º EU region
# os.environ["LANGFUSE_HOST"]="https://us.cloud.langfuse.com" # ðŸ‡ºðŸ‡¸ US region

os.environ["OPENAI_API_KEY"] = ""
os.environ["COHERE_API_KEY"] = ""

# set callbacks
litellm.success_callback = ["langfuse"]
litellm.failure_callback = ["langfuse"] 

```

## Quick Example

```python filename="main.py"
import litellm

# openai call
openai_response = litellm.completion(
  model="gpt-3.5-turbo",
  messages=[
    {"role": "user", "content": "Hi ðŸ‘‹ - i'm openai"}
  ]
)

print(openai_response)

# cohere call
cohere_response = litellm.completion(
  model="command-nightly",
  messages=[
    {"role": "user", "content": "Hi ðŸ‘‹ - i'm cohere"}
  ]
)
print(cohere_response)

```

## Set Custom Trace ID, Trace User ID and Tags

```python filename="main.py"
from litellm import completion

# set custom langfuse trace params and generation params
response = completion(
  model="gpt-3.5-turbo",
  messages=[
    {"role": "user", "content": "Hi ðŸ‘‹ - i'm openai"}
  ],
  metadata={
      "generation_name": "test-generation",   # set langfuse Generation Name
      "generation_id": "gen-id",              # set langfuse Generation ID 
      "trace_id": "trace-id",                 # set langfuse Trace ID
      "trace_user_id": "user-id",             # set langfuse Trace User ID
      "session_id": "session-id",             # set langfuse Session ID
      "tags": ["tag1", "tag2"]                # set langfuse Tags
  },
)

print(response)

```

## Use LangChain ChatLiteLLM + Langfuse

```sh
pip install langchain
```

```python filename="main.py"
from langchain.chat_models import ChatLiteLLM
from langchain.schema import HumanMessage
import litellm


chat = ChatLiteLLM(
  model="gpt-3.5-turbo"
  model_kwargs={
      "metadata": {
        "trace_user_id": "user-id", # set Langfuse Trace User ID
        "session_id": "session-id", # set Langfuse Session ID
        "tags": ["tag1", "tag2"] # set Langfuse Tags
      }
    }
  )
messages = [
    HumanMessage(
        content="what model are you"
    )
]
chat(messages)

```

</Tab>
<Tab>
LiteLLM (Proxy) with OpenAI SDK Wrapper

By adding the callback you can instantly log your responses across all providers with Langfuse.

</Tab>
<Tab>
LiteLLM (Proxy) with UI-enabled Integration
</Tab>
</Tabs>
---
description: Fully async and typed SDK, runs in Node.js, edge, Deno
---

# LiteLLM Tutorial

LiteLLM is an open-source library to simplify LLM completion + embedding calls.

<img src="https://docs.litellm.ai/assets/ideal-img/langfuse.f4da332.1920.png"/>

## Use Langfuse to log requests across all LLM Providers (OpenAI, Azure, Anthropic, Cohere, Replicate, PaLM)

liteLLM provides callbacks, making it easy for you to log data depending on the status of your responses.

### Using Callbacks

Get your Langfuse API Keys from https://cloud.langfuse.com/

Use just 2 lines of code, to instantly log your responses across all providers with langfuse:

```sh
litellm.success_callback = ["langfuse"]

```

Complete code

```sh
from litellm import completion

## set env variables
os.environ["LANGFUSE_PUBLIC_KEY"] = "your key"
os.environ["LANGFUSE_SECRET_KEY"] = "your key"

os.environ["OPENAI_API_KEY"], os.environ["COHERE_API_KEY"] = "", ""

# set callbacks
litellm.success_callback = ["langfuse"]

#openai call
response = completion(model="gpt-3.5-turbo", messages=[{"role": "user", "content": "Hi ðŸ‘‹ - i'm openai"}])

#cohere call
response = completion(model="command-nightly", messages=[{"role": "user", "content": "Hi ðŸ‘‹ - i'm cohere"}])
```


---
description: Open source observability for LiteLLM via the native integration. Automatically capture detailed traces and metrics for every request.
---

# ðŸš… LiteLLM Integration

**LiteLLM** ([GitHub](https://github.com/BerriAI/litellm)): Use any LLM as a drop in replacement for GPT. Use Azure, OpenAI, Cohere, Anthropic, Ollama, VLLM, Sagemaker, HuggingFace, Replicate (100+ LLMs).

You can find more in-depth documentation in the [LiteLLM docs](https://litellm.vercel.app/docs/observability/langfuse_integration).

There are three ways to integrate LiteLLM with Langfuse:

1. LiteLLM Proxy with OpenAI SDK Wrapper, the proxy standardizes 100+ models on the OpenAI API schema and the Langfuse OpenAI SDK wrapper instruments the LLM calls.
2. LiteLLM Python SDK which can send logs to Langfuse if the environment variables are set.
3. LiteLLM Proxy which can send logs to Langfuse if enabled in the UI.

## Integration

<Tabs items={["LiteLLM Proxy + Langfuse OpenAI SDK Wrapper", "LiteLLM Python SDK", "LiteLLM Proxy (UI)"]} >
<Tab>
import { FileCode } from "lucide-react";

The LiteLLM proxy is a simple way to integrate LiteLLM with Langfuse. You can set the success and failure callbacks to Langfuse to log all responses.

<br />
To see a full end-to-end example, check out the LiteLLM cookbook.

<Cards num={2}>
<Card
    title="Example LiteLLM (Proxy)"
    href="/guides/cookbook/integration_litellm_proxy"
    icon={<FileCode />}
  />
</Cards>
</Tab>
<Tab>
The LiteLLM Python SDK is a simple way to integrate LiteLLM with Langfuse. You can set the success and failure callbacks to Langfuse to log all responses.

## Setup

```
pip install langfuse>=2.0.0 litellm
```

```python filename="main.py"
from litellm import completion

## set env variables
os.environ["LANGFUSE_PUBLIC_KEY"] = ""
os.environ["LANGFUSE_SECRET_KEY"] = ""

# Langfuse host
os.environ["LANGFUSE_HOST"]="https://cloud.langfuse.com" # ðŸ‡ªðŸ‡º EU region
# os.environ["LANGFUSE_HOST"]="https://us.cloud.langfuse.com" # ðŸ‡ºðŸ‡¸ US region

os.environ["OPENAI_API_KEY"] = ""
os.environ["COHERE_API_KEY"] = ""

# set callbacks
litellm.success_callback = ["langfuse"]
litellm.failure_callback = ["langfuse"]

```

## Quick Example

```python filename="main.py"
import litellm

# openai call
openai_response = litellm.completion(
  model="gpt-3.5-turbo",
  messages=[
    {"role": "user", "content": "Hi ðŸ‘‹ - i'm openai"}
  ]
)

print(openai_response)

# cohere call
cohere_response = litellm.completion(
  model="command-nightly",
  messages=[
    {"role": "user", "content": "Hi ðŸ‘‹ - i'm cohere"}
  ]
)
print(cohere_response)

```

## Set Custom Trace ID, Trace User ID and Tags

```python filename="main.py"
from litellm import completion

# set custom langfuse trace params and generation params
response = completion(
  model="gpt-3.5-turbo",
  messages=[
    {"role": "user", "content": "Hi ðŸ‘‹ - i'm openai"}
  ],
  metadata={
      "generation_name": "test-generation",   # set langfuse Generation Name
      "generation_id": "gen-id",              # set langfuse Generation ID
      "trace_id": "trace-id",                 # set langfuse Trace ID
      "trace_user_id": "user-id",             # set langfuse Trace User ID
      "session_id": "session-id",             # set langfuse Session ID
      "tags": ["tag1", "tag2"]                # set langfuse Tags
  },
)

print(response)

```

## Use LangChain ChatLiteLLM + Langfuse

```sh
pip install langchain
```

```python filename="main.py"
from langchain.chat_models import ChatLiteLLM
from langchain.schema import HumanMessage
import litellm


chat = ChatLiteLLM(
  model="gpt-3.5-turbo"
  model_kwargs={
      "metadata": {
        "trace_user_id": "user-id", # set Langfuse Trace User ID
        "session_id": "session-id", # set Langfuse Session ID
        "tags": ["tag1", "tag2"] # set Langfuse Tags
      }
    }
  )
messages = [
    HumanMessage(
        content="what model are you"
    )
]
chat(messages)

```

</Tab>
<Tab>
By setting the callback to Langfuse in the LiteLLM UI you can instantly log your responses across all providers. <br />
For more information on how to setup the Proxy UI, see the [LiteLLM docs](https://docs.litellm.ai/docs/proxy/ui).

![Set Langfuse as callback in Proxy UI](https://langfuse.com/images/docs/litellm-ui.png)

</Tab>
</Tabs>

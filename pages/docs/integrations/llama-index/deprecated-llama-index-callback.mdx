---
title: (Deprecated) Callback-based LlamaIndex Integration
description: Open source observability for LlamaIndex. Automatically capture detailed traces and metrics for every request of your RAG application.
---

# (Deprecated) Callback-based LlamaIndex Integration

<Callout type="warning">
  This integration is deprecated. We recommend using the new
  instrumentation-based integration with Langfuse as described
  [here](/docs/integrations/llama-index/get-started).
</Callout>

## Add Langfuse to your LlamaIndex application

Make sure you have both `llama-index` and `langfuse` installed.

```bash
pip install llama-index langfuse
```

At the root of your LlamaIndex application, register Langfuse's `LlamaIndexCallbackHandler` in the LlamaIndex `Settings.callback_manager`. When instantiating `LlamaIndexCallbackHandler`, make sure to configure it correctly with your Langfuse API keys and the Host URL.

import GetStartedLlamaindexPythonArgs from "@/components-mdx/get-started-llamaindex-python-constructor-args.mdx";
import GetStartedLlamaindexPythonEnv from "@/components-mdx/get-started-llamaindex-python-env.mdx";

<Tabs items={["Environment variables", "Constructor arguments"]}>
  <Tab>
    <GetStartedLlamaindexPythonEnv />
  </Tab>
  <Tab>
    <GetStartedLlamaindexPythonArgs />
  </Tab>
</Tabs>

<Callout type="info" emoji="âœ¨">
  Done! Traces and metrics from your LlamaIndex application are now
  automatically tracked in Langfuse. If you construct a new index or query an
  LLM with your documents in context, your traces and metrics are immediately
  visible in the Langfuse UI.
</Callout>

Check out the notebook for end-to-end examples of the integration:

import { FileCode } from "lucide-react";

<Cards>
  <Card
    title="Example Notebook"
    href="/docs/integrations/llama-index/example-python"
    icon={<FileCode />}
  />
</Cards>

## Additional configuration

### Queuing and flushing

The Langfuse SDKs queue and batches events in the background to reduce the number of network requests and improve overall performance. In a long-running application, this works without any additional configuration.

If you are running a short-lived application, you need to flush Langfuse to ensure that all events are flushed before the application exits.

```python
langfuse_handler.flush()
```

Learn more about queuing and batching of events [here](/docs/tracing).

### Custom trace parameters [#set-trace-params]

You can update trace parameters at any time to add additional context to a trace, such as a user ID, session ID, or tags. See the [Python SDK Trace documentation](/docs/sdk/python#traces) for more information. All _subsequent_ traces will include these set parameters.

| Property      | Description                                                                            |
| ------------- | -------------------------------------------------------------------------------------- |
| `name`        | Identify a specific type of trace, e.g. a use case or functionality.                   |
| `metadata`    | Additional information that you want to see in Langfuse. Can be any JSON.              |
| `session_id`  | The current [session](/docs/tracing-features/sessions).                                |
| `user_id`     | The current [user_id](/docs/tracing-features/users).                                   |
| `tags`        | [Tags](/docs/tracing-features/tags) to categorize and filter traces.                   |
| `release`     | The [release](/docs/tracing-features/releases-and-versioning) tag of your application. |
| `version`     | The [version](/docs/tracing-features/releases-and-versioning) of your application.     |
| `sample_rate` | [Sample rate](/docs/tracing-features/sampling) for tracing.                            |

```python {11-15}
from llama_index.core import Settings
from llama_index.core.callbacks import CallbackManager
from langfuse import langfuse

# Instantiate a new LlamaIndexCallbackHandler and register it in the LlamaIndex Settings
langfuse_handler = LlamaIndexCallbackHandler()
Settings.callback_manager = CallbackManager([langfuse_handler])

def my_func():
  # Set trace parameters before executing your LlamaIndex code
  langfuse_callback_handler.set_trace_params(
    user_id="user-123",
    session_id="session-abc",
    tags=["production"]
  )

  # Your LlamaIndex code, trace will include the set parameters
```

> Notes
>
> - The params will be applied to all traces and spans created after the `set_trace_params` call. You can unset them by calling e.g. `set_trace_params(user_id=None)`.
> - If you run this in a Jupyter Notebook, you need to run `set_trace_params` in the same cell as your LlamaIndex code.
> - When setting a root trace or span, this setting will have no effect as the root trace or span will be used. See next section for more information.

### Interoperability with Langfuse SDK

The Langfuse Python SDK is fully interoperable with the LlamaIndex integration.

This is useful when your **LlamaIndex executions are part of a larger application** and you want to link all traces and spans together. This can also be useful when you'd like to **group multiple LlamaIndex executions** to be part of the same trace or span.

<Tabs items={["Python Decorator", "Low-level SDK"]}>
<Tab>

When using the [Langfuse `@observe()` decorator](/docs/sdk/python/decorators), `langfuse_context.get_current_llama_index_handler()` exposes a callback handler scoped to the current trace context, in this case `llama_index_fn()`. Pass it to the LlamaIndex `Settings.callback_manager` to trace subsequent LlamaIndex executions.

```python showLineNumbers {1,6,9-10}
from langfuse.decorators import langfuse_context, observe
from llama_index.core import Document, VectorStoreIndex
from llama_index.core import Settings
from llama_index.core.callbacks import CallbackManager

@observe()
def llama_index_fn(question: str):
    # Set callback manager for LlamaIndex, will apply to all LlamaIndex executions in this function
    langfuse_handler = langfuse_context.get_current_llama_index_handler()
    Settings.callback_manager = CallbackManager([langfuse_handler])

    # Run application
    index = VectorStoreIndex.from_documents([doc1,doc2])
    response = index.as_query_engine().query(question)
    return response
```

> Notes
>
> - The Llamaindex intergation will not make any changes to your provided root trace or span. If you want to add additional context or input/output to your root trace or span, you can do so via the Python SDK.
> - This uses context vars and will work reliably when run in the same cell in Jupyter.

</Tab>
<Tab>

If you have an existing Langfuse trace or span object, you can set it as the root observation for your `LlamaIndexCallbackHandler`. All _subsequent_ traces and spans will be nested under this root.

```python {14,17,22}
from llama_index.core import Settings
from llama_index.core.callbacks import CallbackManager
from langfuse import Langfuse

# Instantiate a Langfuse Client
langfuse = Langfuse()

# Instantiate a new LlamaIndexCallbackHandler and register it in the LlamaIndex Settings
langfuse_callback_handler = LlamaIndexCallbackHandler()
Settings.callback_manager = CallbackManager([langfuse_callback_handler])

def my_func():
  # Create a new trace on your main execution path
  root_trace = langfuse.trace(name="trace-name")

  # Set the root trace, subsequent LlamaIndex observations will be nested under root
  langfuse_callback_handler.set_root(root_trace)

  # Your LlamaIndex code here

  # Reset root, subsequent LlamaIndex observations will now use the default grouping and trace creation
  langfuse_callback_handler.set_root(None)
```

> Notes
>
> - The Llamaindex intergation will not make any changes to your provided root trace or span. If you want to add additional context or input/output to your root trace or span, you can do so via the Python SDK.
> - If you run this in a Jupyter Notebook, you need to run `set_root` in the same cell as your LlamaIndex code.

</Tab>
</Tabs>

---
description: Open source observability for LlamaIndex. Automatically capture detailed traces and metrics for every request of your RAG application.
---

import { Callout } from "nextra-theme-docs";

# ðŸ¦™ LlamaIndex Integration

**LlamaIndex** ([GitHub](https://github.com/run-llama/llama_index)) is an advanced "data framework" tailored for augmenting Large Language Models (LLMs) with private data.

> It streamlines the integration of diverse data sources and formats (APIs, PDFs, docs, SQL, etc.) through versatile data connectors and structures data into indices and graphs for LLM compatibility. The platform offers a sophisticated retrieval/query interface for enriching LLM inputs with context-specific outputs. Designed for both beginners and experts, LlamaIndex provides a user-friendly high-level API for easy data ingestion and querying, alongside customizable lower-level APIs for detailed module adaptation.

_See our announcement [blog post](/blog/llama-index-integration)_

Langfuse offers a simple integration for automatic capture of [traces](/docs/tracing/overview) and metrics generated in LlamaIndex applications. **Any feedback?** Let us know on Discord or GitHub. This is a new integration, and we'd love to hear your thoughts.

<Callout type="info">
  Currently only Python is supported by this integration. If you are interested
  in an integration with **LlamaIndex.TS**, add your upvote/comments to [this
  issue](https://github.com/orgs/langfuse/discussions/1291).
</Callout>

import { CloudflareVideo } from "@/components/Video";

<CloudflareVideo
  videoId="590b7c143b13a0f4e40bd181175f1a00"
  aspectRatio={16 / 9.86}
  gifStyle
/>
<span>
  _Example LlamaIndex trace in Langfuse. See a full video demo
  [here](/guides/videos/llama-index)._
</span>

## Add Langfuse to your LlamaIndex application

Make sure you have both `llama-index` and `langfuse` installed.

```bash
pip install llama-index langfuse
```

At the root of your LlamaIndex application, register Langfuse's `LlamaIndexCallbackHandler` in the LlamaIndex `Settings.callback_manager`. When instantiating `LlamaIndexCallbackHandler`, make sure to configure it correctly with your Langfuse API keys and the Host URL.

import GetStartedLlamaindexPythonArgs from "@/components-mdx/get-started-llamaindex-python-constructor-args.mdx";
import GetStartedLlamaindexPythonEnv from "@/components-mdx/get-started-llamaindex-python-env.mdx";

<Tabs items={["Environment variables", "Constructor arguments"]}>
  <Tab>
    <GetStartedLlamaindexPythonEnv />
  </Tab>
  <Tab>
    <GetStartedLlamaindexPythonArgs />
  </Tab>
</Tabs>

<Callout type="info" emoji="âœ¨">
  Done! Traces and metrics from your LlamaIndex application are now
  automatically tracked in Langfuse. If you construct a new index or query an
  LLM with your documents in context, your traces and metrics are immediately
  visible in the Langfuse UI.
</Callout>

Check out the notebook for end-to-end examples of the integration:

import { FileCode } from "lucide-react";

<Cards>
  <Card
    title="Example Notebook"
    href="/docs/integrations/llama-index/example-python"
    icon={<FileCode />}
  />
</Cards>

## Additional configuration

### Queuing and flushing

The Langfuse SDKs queue and batches events in the background to reduce the number of network requests and improve overall performance. In a long-running application, this works without any additional configuration.

If you are running a short-lived application, you need to flush Langfuse to ensure that all events are flushed before the application exits.

```python
langfuse_handler.flush()
```

Learn more about queuing and batching of events [here](/docs/tracing/overview).

### Custom trace parameters [#set-trace-params]

You can update trace parameters at any time to add additional context to a trace, such as a user ID, session ID, or tags. See the [Python SDK Trace documentation](/docs/sdk/python#traces) for more information. All _subsequent_ traces will include these set parameters.

| Property     | Description                                                               |
| ------------ | ------------------------------------------------------------------------- |
| `name`       | Identify a specific type of trace, e.g. a use case or functionality.      |
| `metadata`   | Additional information that you want to see in Langfuse. Can be any JSON. |
| `session_id` | The current [session](/docs/tracing/sessions).                            |
| `user_id`    | The current [user_id](/docs/tracing/users).                               |
| `tags`       | [Tags](/docs/tracing/tags) to categorize and filter traces.               |
| `version`    | The specified version to trace [experiments](/docs/experimentation).      |
| `release`    | The specified release to trace [experiments](/docs/experimentation).      |

```python {11-15}
from llama_index.core import Settings
from llama_index.core.callbacks import CallbackManager
from langfuse import langfuse

# Instantiate a new LlamaIndexCallbackHandler and register it in the LlamaIndex Settings
langfuse_handler = LlamaIndexCallbackHandler()
Settings.callback_manager = CallbackManager([langfuse_handler])

def my_func():
  # Set trace parameters before executing your LlamaIndex code
  langfuse_callback_handler.set_trace_params(
    user_id="user-123",
    session_id="session-abc",
    tags=["production"]
  )

  # Your LlamaIndex code, trace will include the set parameters
```

> Notes
>
> - The params will be applied to all traces and spans created after the `set_trace_params` call. You can unset them by calling e.g. `set_trace_params(user_id=None)`.
> - If you run this in a Jupyter Notebook, you need to run `set_trace_params` in the same cell as your LlamaIndex code.
> - When setting a root trace or span, this setting will have no effect as the root trace or span will be used. See next section for more information.

### Interoperability with Langfuse SDKs

If you have an existing Langfuse trace or span object, you can set it as the root observation for your `LlamaIndexCallbackHandler`. All _subsequent_ traces and spans will be nested under this root.

This is useful when your **LlamaIndex executions are part of a larger application** and you want to link all traces and spans together. This can also be useful when you'd like to **group multiple LlamaIndex executions** to be part of the same trace or span.

```python {14,17,22}
from llama_index.core import Settings
from llama_index.core.callbacks import CallbackManager
from langfuse import Langfuse

# Instantiate a Langfuse Client
langfuse = Langfuse()

# Instantiate a new LlamaIndexCallbackHandler and register it in the LlamaIndex Settings
langfuse_callback_handler = LlamaIndexCallbackHandler()
Settings.callback_manager = CallbackManager([langfuse_callback_handler])

def my_func():
  # Create a new trace on your main execution path
  root_trace = langfuse.trace(name="trace-name")

  # Set the root trace, subsequent LlamaIndex observations will be nested under root
  langfuse_callback_handler.set_root(root_trace)

  # Your LlamaIndex code here

  # Reset root, subsequent LlamaIndex observations will now use the default grouping and trace creation
  langfuse_callback_handler.set_root(None)
```

> Notes
>
> - The Llamaindex intergation will not make any changes to your provided root trace or span. If you want to add additional context or input/output to your root trace or span, you can do so via the Python SDK.
> - If you run this in a Jupyter Notebook, you need to run `set_root` in the same cell as your LlamaIndex code.

---
description: Drop-in replacement of OpenAI SDK to get full observability in Langfuse by changing only the import
---

# OpenAI Integration (Python)

If you use the OpenAI Python SDK, you can use the Langfuse **drop-in replacement** to get full logging by changing only the import.

```diff
- import openai
+ from langfuse.openai import openai
```

Langfuse automatically tracks:

- All prompts/completions with support for streaming, async and functions
- Latencies
- Errors ([example](/docs/integrations/openai/track-errors))
- Model usage (tokens) and cost (USD) ([learn more](/docs/model-usage-and-cost))

## Get started

### 1. Setup

The integration is compatible with OpenAI SDK versions `>=0.27.8`. It supports async functions and streaming for OpenAI SDK versions `>=1.0.0`.

```python
%pip install langfuse openai --upgrade
```

```python
import os

# get keys for your project from https://cloud.langfuse.com
os.environ["LANGFUSE_PUBLIC_KEY"] = ""
os.environ["LANGFUSE_SECRET_KEY"] = ""

# your openai key
os.environ["OPENAI_API_KEY"] = ""

# Your host, defaults to https://cloud.langfuse.com
# For US data region, set to "https://us.cloud.langfuse.com"
# os.environ["LANGFUSE_HOST"] = "http://localhost:3000"
```

### 2. Replace import

```diff
- import openai
+ from langfuse.openai import openai
```

```python
# Optional, checks the SDK connection with the server. Not recommended for production usage.
from langfuse.openai import auth_check

auth_check()
```

#### Optional attributes

Instead of setting the environment variables before importing the SDK, you can also use the following attributes after the import. This works for the async OpenAI client as well:

| Attribute                    | Description                     | Default value                                                                                                                                  |
| ---------------------------- | ------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------- |
| `openai.langfuse_host`       | Host of the Langfuse API        | `LANGFUSE_HOST` environment variable, defaults to `"https://cloud.langfuse.com"`. Set to `"https://us.cloud.langfuse.com"` for US data region. |
| `openai.langfuse_public_key` | Public key of the Langfuse API  | `LANGFUSE_PUBLIC_KEY` environment variable                                                                                                     |
| `openai.langfuse_secret_key` | Private key of the Langfuse API | `LANGFUSE_SECRET_KEY` environment variable                                                                                                     |
| `openai.langfuse_debug`      | Debug mode of Langfuse SDK      | `False`                                                                                                                                        |

### 3. Use SDK as usual

_No changes required._

For end-to-end examples, check out the [example notebook](/docs/integrations/openai/examples).

import { FileCode } from "lucide-react";

<Cards>
  <Card
    title="Examples"
    href="/docs/integrations/openai/examples"
    icon={<FileCode />}
  />
</Cards>

## Advanced usage

### Custom trace properties

You can add the following properties to the openai method, e.g. `openai.chat.completions.create()`, to use additional Langfuse features:

| Property                | Description                                                                  |
| ----------------------- | ---------------------------------------------------------------------------- |
| `name`                  | Set `name` to identify a specific type of generation.                        |
| `metadata`              | Set `metadata` with additional information that you want to see in Langfuse. |
| `session_id`            | TODO                                                                         |
| `user_id`               | TODO                                                                         |
| `tags`                  | TODO                                                                         |
| `trace_id`              | See "Interoperability with Langfuse Python SDK" (below) for more details.    |
| `parent_observation_id` | See "Interoperability with Langfuse Python SDK" (below) for more details.    |

Example:

```python
openai.chat.completions.create(
    name="test-chat",
    model="gpt-3.5-turbo",
    messages=[
      {"role": "system", "content": "You are a very accurate calculator. You output only the result of the calculation."},
      {"role": "user", "content": "1 + 1 = "}],
    temperature=0,
    metadata={"someMetadataKey": "someValue"},
)
```

### Interoperability with Langfuse Python SDK

Use the OpenAI integration in combination with the regular Langfuse SDKs if you want to:

- Add non-OpenAI related observations to the trace.
- Group multiple OpenAI calls into a single trace.
- Have more control over the trace structure.

Learn more about the structure of a trace [here](/docs/tracing/overview).

**TODO TODO TODO TODO TODO**

## Troubleshooting

### Shutdown behavior

The Langfuse SDK executes network requests in the background on a separate thread for better performance of your application. This can lead to lost events in short lived environments like AWS Lambda functions when the Python process is terminated before the SDK sent all events to the Langfuse backend.

To avoid this, ensure that the `openai.flush_langfuse()` function is called before termination. This method is blocking as it awaits all requests to be completed.

```python
openai.flush_langfuse()
```

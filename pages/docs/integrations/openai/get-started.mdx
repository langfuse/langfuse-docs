---
description: Drop-in replacement of OpenAI SDK to get full observability in Langfuse by changing only the import.
---

# OpenAI Integration (Python)

If you use the OpenAI Python SDK, you can use the Langfuse **drop-in replacement** to get full logging by changing only the import.

```diff
- import openai
+ from langfuse.openai import openai
```

Langfuse automatically tracks:

- All prompts/completions with support for streaming, async and functions
- Latencies
- API Errors ([example](/docs/integrations/openai/track-errors))
- Model usage (tokens) and cost (USD) ([learn more](/docs/model-usage-and-cost))

## Get started

### 1. Setup

The integration is compatible with OpenAI SDK versions `>=0.27.8`. It supports async functions and streaming for OpenAI SDK versions `>=1.0.0`.

```python
%pip install langfuse openai --upgrade
```

```python
import os

# get keys for your project from https://cloud.langfuse.com
os.environ["LANGFUSE_PUBLIC_KEY"] = ""
os.environ["LANGFUSE_SECRET_KEY"] = ""

# your openai key
os.environ["OPENAI_API_KEY"] = ""

# Your host, defaults to https://cloud.langfuse.com
# For US data region, set to "https://us.cloud.langfuse.com"
# os.environ["LANGFUSE_HOST"] = "http://localhost:3000"
```

### 2. Replace import

```diff
- import openai
+ from langfuse.openai import openai
```

```python
# Optional, checks the SDK connection with the server. Not recommended for production usage.
from langfuse.openai import auth_check

auth_check()
```

#### Optional attributes

Instead of setting the environment variables before importing the SDK, you can also use the following attributes after the import. This works for the async OpenAI client as well:

| Attribute                    | Description                     | Default value                                                                                                                                  |
| ---------------------------- | ------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------- |
| `openai.langfuse_host`       | Host of the Langfuse API        | `LANGFUSE_HOST` environment variable, defaults to `"https://cloud.langfuse.com"`. Set to `"https://us.cloud.langfuse.com"` for US data region. |
| `openai.langfuse_public_key` | Public key of the Langfuse API  | `LANGFUSE_PUBLIC_KEY` environment variable                                                                                                     |
| `openai.langfuse_secret_key` | Private key of the Langfuse API | `LANGFUSE_SECRET_KEY` environment variable                                                                                                     |
| `openai.langfuse_debug`      | Debug mode of Langfuse SDK      | `False`                                                                                                                                        |

### 3. Use SDK as usual

_No changes required._

For end-to-end examples, check out the [example notebook](/docs/integrations/openai/examples).

import { FileCode } from "lucide-react";

<Cards>
  <Card
    title="Examples"
    href="/docs/integrations/openai/examples"
    icon={<FileCode />}
  />
</Cards>

## Troubleshooting

### Queuing and batching of events

The Langfuse SDK executes network requests in the background on a separate thread for better performance of your application. This can lead to lost events in short lived environments like AWS Lambda functions when the Python process is terminated before the SDK sent all events to the Langfuse backend.

To avoid this, ensure that the `openai.flush_langfuse()` function is called before the process is terminated. This method is blocking as it awaits all requests to be completed.

```python
openai.flush_langfuse()
```

Learn more about queuing and batching of events [here](/docs/tracing/overview).

## Advanced usage

### Custom trace properties

You can add the following properties to the openai method, e.g. `openai.chat.completions.create()`, to use additional Langfuse features:

| Property                | Description                                                                  |
| ----------------------- | ---------------------------------------------------------------------------- |
| `name`                  | Set `name` to identify a specific type of generation.                        |
| `metadata`              | Set `metadata` with additional information that you want to see in Langfuse. |
| `session_id`            | The current [session](/docs/tracing/sessions).                               |
| `user_id`               | The current [user_id](/docs/tracing/users).                                  |
| `tags`                  | Set [tags](/docs/tracing/tags) to categorize and filter traces.              |
| `trace_id`              | See "Interoperability with Langfuse Python SDK" (below) for more details.    |
| `parent_observation_id` | See "Interoperability with Langfuse Python SDK" (below) for more details.    |

Example:

```python {6-7}
openai.chat.completions.create(
    model="gpt-3.5-turbo",
    messages=[
      {"role": "system", "content": "You are a very accurate calculator. You output only the result of the calculation."},
      {"role": "user", "content": "1 + 1 = "}],
    name="test-chat",
    metadata={"someMetadataKey": "someValue"},
)
```

### Group multiple LLM calls into a single trace

Usually, this integration creates a single trace in Langfuse for each openai call. If you want to group multiple openai calls into a single trace, you can use the `trace_id` property. This can be either a random id or an existing id from your application that will be unique to this execution.

```python {3,14,23}
from langfuse.openai import openai
from uuid import uuid4
trace_id = str(uuid4())

country = "Bulgaria"

capital = openai.chat.completions.create(
  model="gpt-3.5-turbo",
  messages=[
      {"role": "system", "content": "What is the capital of the country?"},
      {"role": "user", "content": country}],
  temperature=0,
  name="get-capital",
  trace_id=trace_id
).choices[0].message.content

poem = openai.chat.completions.create(
  model="gpt-3.5-turbo",
  messages=[
      {"role": "system", "content": "You are a poet. Create a poem about this city."},
      {"role": "user", "content": capital}],
  name="generate-poem",
  trace_id=trace_id
).choices[0].message.content
```

Now both generations are grouped in the same trace with the `trace_id` assigned by you.

```
TRACE (id: trace_id)
|
|-- GENERATION: get-capital
|
|-- GENERATION: generate-poem
```

If you want to add further properties or observations to the trace, read the next section.

### Interoperability with Langfuse Python SDK

Use the OpenAI integration in combination with the regular Langfuse SDKs ([full reference](/docs/sdk/python)) if you want to:

- Add non-OpenAI related observations to the trace.
- Group multiple OpenAI calls into a single trace while customizing the trace.
- Have more control over the trace structure.

import { Callout } from "nextra/components";

<Callout type="info" emoji="ℹ️">
  New to Langfuse Tracing? Checkout this [introduction](/docs/tracing/overview)
  to the basic concepts.
</Callout>

<Tabs items={["Trace", "Trace and nested spans"]}>
<Tab>

Desired trace structure:

```
TRACE: capital-poem-generator
|
|-- GENERATION: get-capital
|
|-- GENERATION: generate-poem
```

Implementation:

```python {5,8,20,29}
from langfuse import Langfuse
from langfuse.openai import openai

# initialize SDK
langfuse = Langfuse()

# create trace and add params
trace = langfuse.trace(name="capital-poem-generator")

# create multiple completions, pass trace_id to each

country = "Bulgaria"

capital = openai.chat.completions.create(
  model="gpt-3.5-turbo",
  messages=[
      {"role": "system", "content": "What is the capital of the country?"},
      {"role": "user", "content": country}],
  name="get-capital",
  trace_id=trace_id
).choices[0].message.content

poem = openai.chat.completions.create(
  model="gpt-3.5-turbo",
  messages=[
      {"role": "system", "content": "You are a poet. Create a poem about this city."},
      {"role": "user", "content": capital}],
  name="generate-poem",
  trace_id=trace_id
).choices[0].message.content
```

</Tab>
<Tab>
Desired trace structure:

```
TRACE: capital-poem-generator
|
|-- SPAN: Bulgaria
|   |
|   |-- GENERATION: get-capital
|   |
|   |-- GENERATION: generate-poem
|
|-- SPAN: France
|   |
|   |-- GENERATION: get-capital
|   |
|   |-- GENERATION: generate-poem
```

Implementation

```python showLineNumbers {5,8,12,20-21,30-31,35}
from langfuse import Langfuse
from langfuse.openai import openai

# initialize SDK
langfuse = Langfuse()

# create trace and add params
trace = langfuse.trace(name="capital-poem-generator")

for country in ["Bulgaria", "France"]:
  # create span
  span = trace.span(name=country)

  capital = openai.chat.completions.create(
    model="gpt-3.5-turbo",
    messages=[
        {"role": "system", "content": "What is the capital of the country?"},
        {"role": "user", "content": country}],
    name="get-capital",
    trace_id=trace.id,
    parent_observation_id=span.id,
  ).choices[0].message.content

  poem = openai.chat.completions.create(
    model="gpt-3.5-turbo",
    messages=[
        {"role": "system", "content": "You are a poet. Create a poem about this city."},
        {"role": "user", "content": capital}],
    name="generate-poem",
    trace_id=trace.id,
    parent_observation_id=span.id,
  ).choices[0].message.content

  # End span to get span-level latencies
  span.end()
```

_Note that you need to provide `trace_id` and `parent_observation_id` to each generation._

</Tab>
</Tabs>

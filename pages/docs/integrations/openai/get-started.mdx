---
description: Drop-in replacement of OpenAI SDK to get full observability in Langfuse by changing only the import
---

# OpenAI Integration (Python)

If you use the OpenAI Python SDK, you can use the Langfuse **drop-in replacement** to get full logging by changing only the import.

```diff
- import openai
+ from langfuse.openai import openai
```

Langfuse automatically tracks:

- All prompts/completions with support for streaming, async and functions
- Latencies
- Errors ([example](/docs/integrations/openai/track-errors))
- Model usage (tokens) and cost (USD) ([learn more](/docs/model-usage-and-cost))

## Get started

### 1. Setup

The integration is compatible with OpenAI SDK versions `>=0.27.8`. It supports async functions and streaming for OpenAI SDK versions `>=1.0.0`.

```python
%pip install langfuse openai --upgrade
```

```python
import os

# get keys for your project from https://cloud.langfuse.com
os.environ["LANGFUSE_PUBLIC_KEY"] = ""
os.environ["LANGFUSE_SECRET_KEY"] = ""

# your openai key
os.environ["OPENAI_API_KEY"] = ""

# Your host, defaults to https://cloud.langfuse.com
# For US data region, set to "https://us.cloud.langfuse.com"
# os.environ["LANGFUSE_HOST"] = "http://localhost:3000"
```

### 2. Replace import

```diff
- import openai
+ from langfuse.openai import openai
```

```python
# Optional, checks the SDK connection with the server. Not recommended for production usage.
from langfuse.openai import auth_check

auth_check()
```

#### Optional attributes

Instead of setting the environment variables before importing the SDK, you can also use the following attributes after the import. This works for the async OpenAI client as well:

| Attribute                    | Description                     | Default value                                                                                                                                  |
| ---------------------------- | ------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------- |
| `openai.langfuse_host`       | Host of the Langfuse API        | `LANGFUSE_HOST` environment variable, defaults to `"https://cloud.langfuse.com"`. Set to `"https://us.cloud.langfuse.com"` for US data region. |
| `openai.langfuse_public_key` | Public key of the Langfuse API  | `LANGFUSE_PUBLIC_KEY` environment variable                                                                                                     |
| `openai.langfuse_secret_key` | Private key of the Langfuse API | `LANGFUSE_SECRET_KEY` environment variable                                                                                                     |
| `openai.langfuse_debug`      | Debug mode of Langfuse SDK      | `False`                                                                                                                                        |

### 3. Use SDK as usual

_No changes required._

For end-to-end examples, check out the [example notebook](/docs/integrations/openai/examples).

import { FileCode } from "lucide-react";

<Cards>
  <Card
    title="Examples"
    href="/docs/integrations/openai/examples"
    icon={<FileCode />}
  />
</Cards>

## Advanced usage

### Custom trace properties

You can add the following properties to the openai method, e.g. `openai.chat.completions.create()`, to use additional Langfuse features:

| Property                | Description                                                                  |
| ----------------------- | ---------------------------------------------------------------------------- |
| `name`                  | Set `name` to identify a specific type of generation.                        |
| `metadata`              | Set `metadata` with additional information that you want to see in Langfuse. |
| `session_id`            | The current [session](/docs/tracing/sessions).                                                         |
| `user_id`               | The current [user_id](/docs/tracing/users).                                                                         |
| `tags`                  | Set [tags](/docs/tracing/tags) to categorize and filter traces.                                                                         |
| `trace_id`              | See "Interoperability with Langfuse Python SDK" (below) for more details.    |
| `parent_observation_id` | See "Interoperability with Langfuse Python SDK" (below) for more details.    |

Example:

```python
openai.chat.completions.create(
    name="test-chat",
    model="gpt-3.5-turbo",
    messages=[
      {"role": "system", "content": "You are a very accurate calculator. You output only the result of the calculation."},
      {"role": "user", "content": "1 + 1 = "}],
    temperature=0,
    metadata={"someMetadataKey": "someValue"},
)
```

### Group multiple LLM calls into a single trace

To get started, you can just add an identifier from your own application (e.g., conversation-id) to the openai calls â€“ or create a random id.

```python
# create random trace_id
# could also use existing id from your application, e.g. conversation id
from uuid import uuid4
trace_id = str(uuid4())

# create multiple completions, pass trace_id to each

country = "Bulgaria"

capital = openai.chat.completions.create(
  name="geography-teacher",
  model="gpt-3.5-turbo",
  messages=[
      {"role": "system", "content": "You are a Geography teacher helping students learn the capitals of countries. Output only the capital when being asked."},
      {"role": "user", "content": country}],
  temperature=0,
  trace_id=trace_id
).choices[0].message.content

poem = openai.chat.completions.create(
  name="poet",
  model="gpt-3.5-turbo",
  messages=[
      {"role": "system", "content": "You are a poet. Create a poem about a city."},
      {"role": "user", "content": capital}],
  temperature=1,
  max_tokens=200,
  trace_id=trace_id
).choices[0].message.content
```

Now both generations are regrouped in a trace without any name and a random `trace_id`.

```
TRACE: (no name)
|
|-- GENERATION: geography-teacher
|
|-- GENERATION: poet
```

### Interoperability with Langfuse Python SDK

Use the OpenAI integration in combination with the regular Langfuse SDKs if you want to:

- Add non-OpenAI related observations to the trace.
- Group multiple OpenAI calls into a single trace.
- Have more control over the trace structure.

Learn more about the structure of a trace [here](/docs/tracing/overview).

<Tabs items={["Trace", "Trace and nested spans"]}>
<Tab>
We can easily create a trace with custom parameters in Langfuse and use this trace's `trace_id` to group OpenAI generations.

```python
from langfuse import Langfuse

# initialize SDK
langfuse = Langfuse()

# create trace and add params
trace = langfuse.trace(
    # optional, if you want to use your own id
    # id = "my-trace-id"
    name = "capital-poem-generator"
)

# get traceid to pass to openai calls
trace_id = trace.id

# create multiple completions, pass trace_id to each

country = "Bulgaria"

capital = openai.chat.completions.create(
  name="geography-teacher",
  model="gpt-3.5-turbo",
  messages=[
      {"role": "system", "content": "You are a Geography teacher helping students learn the capitals of countries. Output only the capital when being asked."},
      {"role": "user", "content": country}],
  temperature=0,
  trace_id=trace_id
).choices[0].message.content

poem = openai.chat.completions.create(
  name="poet",
  model="gpt-3.5-turbo",
  messages=[
      {"role": "system", "content": "You are a poet. Create a poem about a city."},
      {"role": "user", "content": capital}],
  temperature=1,
  max_tokens=200,
  trace_id=trace_id
).choices[0].message.content
```

Now both generations are regrouped in the trace "capital-poem-generator" with the `trace_id` assigned by Langfuse.

```
TRACE: capital-poem-generator
|
|-- GENERATION: geography-teacher
|
|-- GENERATION: poet
```

</Tab>
<Tab>
We can also use the nesting capabilities of Langfuse Tracing by providing a `parent_observation_id`. For example, this can be the id of a span that you created via the Langfuse Python SDK.

```python
from langfuse import Langfuse

# initialize SDK
langfuse = Langfuse()

# create trace and add params
trace = langfuse.trace(
    # optional, if you want to use your own id
    # id = "my-trace-id"
    name = "capital-poem-generator"
)

for country in ["Bulgaria", "France"]:
  # create span
  span = trace.span(name=country)

  # get span_id to provide for OpenAI calls
  span_id = span.id

  capital = openai.chat.completions.create(
    name="geography-teacher",
    model="gpt-3.5-turbo",
    messages=[
        {"role": "system", "content": "You are a Geography teacher helping students learn the capitals of countries. Output only the capital when being asked."},
        {"role": "user", "content": country}],
    temperature=0,
    parent_observation_id=span_id
  ).choices[0].message.content

  poem = openai.chat.completions.create(
    name="poet",
    model="gpt-3.5-turbo",
    messages=[
        {"role": "system", "content": "You are a poet. Create a poem about a city."},
        {"role": "user", "content": capital}],
    temperature=1,
    max_tokens=200,
    parent_observation_id=span_id
  ).choices[0].message.content
  
  # End span to get span-level latencies
  span.end()
```

Now we have two sets of two nested generations which regrouped under a span each using their respective `span_id` as the generation's `parent_observation_id`.
  
```
TRACE: capital-poem-generator
|
|-- SPAN: Bulgaria
|   |
|   |-- GENERATION: geography-teacher
|   |
|   |-- GENERATION: poet
|
|-- SPAN: France
|   |
|   |-- GENERATION: geography-teacher
|   |
|   |-- GENERATION: poet

```
  
</Tab>
</Tabs>

## Troubleshooting

### Shutdown behavior

The Langfuse SDK executes network requests in the background on a separate thread for better performance of your application. This can lead to lost events in short lived environments like AWS Lambda functions when the Python process is terminated before the SDK sent all events to the Langfuse backend.

To avoid this, ensure that the `openai.flush_langfuse()` function is called before termination. This method is blocking as it awaits all requests to be completed.

```python
openai.flush_langfuse()
```

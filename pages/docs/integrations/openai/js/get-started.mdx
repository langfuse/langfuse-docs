---
description: Simple wrapper around OpenAI SDK to get full observability in Langfuse
---

# Observability for OpenAI SDK (JS/TS)

The Langfuse JS/TS SDK provides a wrapper function around the OpenAI SDK that lets you easily add observability to your OpenAI calls, including latencies, time-to-first-token on stream responses, errors, and model usage.

_Looking for the Python version? [Check it out here](/docs/integrations/openai/python/get-started)._

## Quickstart

```typescript {2, 4}
import OpenAI from "openai";
import { observeOpenAI } from "langfuse";

const openai = observeOpenAI(new OpenAI());

openai.chat.completions
  .create({
    messages: [{ role: "system", content: "Tell me a story about a dog." }],
  })
  .then((res) => console.log(res));

```

Langfuse automatically tracks:

- All prompts/completions with support for streaming and function calling
- Total latencies and time-to-first-token
- OpenAI API Errors
- Model usage (tokens) and cost (USD) ([learn more](/docs/model-usage-and-cost))

_In the Langfuse Console_

import { CloudflareVideo } from "@/components/Video";

<CloudflareVideo
  videoId="53740b20a378b166552ff1941ed38e4c"
  aspectRatio={16 / 9.8}
  gifStyle
/>

## How it works

<Steps>

### Install Langfuse SDK

The integration is compatible with OpenAI SDK versions `>=4.32.0`. // TODO: check backward compat

```sh
npm install langfuse openai
```

### Wrap the OpenAI SDK

```typescript
import OpenAI from 'openai';
import { observeOpenAI } from "langfuse"

const openai = observeOpenAI(new OpenAI());
```

### Call OpenAI methods with the wrapped client

Langfuse wraps the OpenAI SDK and provides the functionality and method signatures. You can call them as usual.

The `observeOpenAI` function automatically instantiates a Langfuse client in the background. You can either configure the client with environment variables or pass the configuration directly to the `observeOpenAI` function.

<Tabs items={["Environment variables", "Directly passed variables"]}>
  <Tab>
    Add your Langfuse credentials to your environment variables. You can find your credentials in your project settings in the Langfuse UI. Make sure that you have a `.env` file in your project root and a package like `dotenv` to load the variables.

import EnvJS from "@/components-mdx/env-js.mdx";

    <EnvJS />

    With your environment configured, call OpenAI SDK methods as usual from the wrapped client.

    ```typescript
    const res = await openai.chat.completions.create({ 
      messages: [{ role: "system", content: "Tell me a story about a dog." }],
      model: "gpt-3.5-turbo",
      max_tokens: 300,
    });
    ```

  </Tab>
  <Tab>
    Pass your Langfuse credentials directly as `clientInitParams` to the `observeOpenAI` function. You can find your credentials in your project settings in the Langfuse UI.

    ```typescript
    const res = await observeOpenAI(new OpenAI(), {
      clientInitParams: {
        publicKey: "pk-lf-...",
        secretKey: "sk-lf-...",
        baseUrl: "https://cloud.langfuse.com",
      },
    }).chat.completions.create({ 
      messages: [{ role: "system", content: "Tell me a story about a dog." }],
      model: "gpt-3.5-turbo",
      max_tokens: 300,
    });
    ```

import { Callout } from "nextra/components";
  
  
  <Callout type="info">
    The Langfuse client is a singleton. Initialization params are only used for the first call. Subsequent calls will use the same client.
  </Callout>
  </Tab>
</Tabs>

Done!âœ¨ You now have full observability of your OpenAI calls in Langfuse.

</Steps>

## Troubleshooting

### Queuing and batching of events

The Langfuse SDKs queue and batches events in the background to reduce the number of network requests and improve overall performance. In a long-running application, this works without any additional configuration.

If you are running a short-lived application, you need to flush Langfuse to ensure that all events are flushed before the application exits.

```typescript
await openai.flushAsync() // method added by Langfuse wrapper

// If you have previously passed a parent span or trace for nesting, use that client for the flush call
await langfuse.flushAsync()
```

Learn more about queuing and batching of events [here](/docs/tracing).

## Advanced usage

### Custom trace properties

You can add the following properties to the `langfuseConfig` of the `observeOpenAI` function to use additional Langfuse features:

| Property                | Description                                                                  |
| ----------------------- | ---------------------------------------------------------------------------- |
| `generationName`        | Set `generationName` to identify a specific type of generation.              |
| `metadata`              | Set `metadata` with additional information that you want to see in Langfuse. |
| `sessionId`             | The current [session](/docs/tracing-features/sessions).                      |
| `userId`                | The current [user_id](/docs/tracing-features/users).                         |
| `version`               | Track different versions in Langfuse analytics                               |
| `release`               | Track different releases in Langfuse analytics                               |
| `tags`                  | Set [tags](/docs/tracing-features/tags) to categorize and filter traces.     |

Example:

```typescript
const res = await observeOpenAI(openai, {
  generationName: "Traced generation",
  metadata: { someMetadataKey: "someValue" },
  sessionId: "session-id",
  userId: "user-id",
  tags: ["tag1", "tag2"],
  version: "0.0.1",
  release: "beta",
}).chat.completions.create({ 
  messages: [{ role: "system", content: "Tell me a story about a dog." }],
  model: "gpt-3.5-turbo",
  max_tokens: 300,
});
```
<Callout type="info">
  Adding custom properties requires you to wrap the OpenAI SDK with the `observeOpenAI` function and pass the properties as the second `langfuseConfig` argument. Since the Langfuse client here is a singleton and the same client is used for all calls, you do not need to worry about mistakingly having multiple clients running.
</Callout>

### Nested traces

[Langfuse Tracing](/docs/tracing) groups multiple observations (can be any LLM or non-LLM calls) into a single trace. This integration by default creates a single trace for each OpenAI call. 

By passing an existing trace or span to the `observeOpenAI` function as the `parent`, you can:

- add non-OpenAI related observations to the trace.
- group multiple OpenAI calls into a single trace while customizing the trace.
- exert more control over the trace structure.
- leverage all Langfuse Tracing features.

<Callout type="info">
  New to Langfuse Tracing? Checkout this [introduction](/docs/tracing) to the
  basic concepts.
</Callout>

Use the [Langfuse JS/TS SDK](/docs/sdk/typescript/guide) to create traces or spans manually and add OpenAI calls to it.

#### Example

**Desired trace structure**

```
TRACE: capital-poem-generator
|
|-- SPAN: France
|   |
|   |-- GENERATION: get-capital
|   |
|   |-- GENERATION: generate-poem
```

**Implementation**

```typescript {12, 17, 30}
import langfuse, { observeOpenAI } from "langfuse";

// Initialize SDKs
const langfuse = new Langfuse();
const openai = new OpenAI();

// Create trace and add params
const trace = langfuse.trace({ name: "capital-poem-generator" });

// Create span
const country = "France";
const span = trace.span({ name: country });

// Call OpenAI
const capital = (
  await observeOpenAI(openai, {
    parent: span,
    generationName: "get-capital",
  }).chat.completions.create({
    model: "gpt-3.5-turbo",
    messages: [
      { role: "system", content: "What is the capital of the country?" },
      { role: "user", content: country },
    ],
  })
).choices[0].message.content;

const poem = (
  await observeOpenAI(openai, {
    parent: span,
    generationName: "generate-poem",
  }).chat.completions.create({
    model: "gpt-3.5-turbo",
    messages: [
      {
        role: "system",
        content: "You are a poet. Create a poem about this city.",
      },
      { role: "user", content: capital },
    ],
  })
).choices[0].message.content;

// End span to get span-level latencies
span.end();

// Flush the Langfuse client belonging to the parent span
await langfuse.flushAsync()
```
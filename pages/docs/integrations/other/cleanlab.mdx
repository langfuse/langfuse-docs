---
title: "Langfuse Integration with Cleanlab"
description: "Automatically evaluate LLMs in real time with Cleanlab's trustworthy Languae Model (TLM)"
---

# Automatically Find Inaccurate LLM Responses with Cleanlab

Cleanlab’s [Trustworthy Language Model](https://cleanlab.ai/tlm/) (TLM) enables Langfuse users to quickly identify low quality and hallucinated responses from any LLM trace.

## What is TLM?

TLM is an automated evaluation tool that add reliability and explainability to every LLM output. TLM automatically finds the poor quality and incorrect LLM responses lurking within your production logs and traces. This helps you perform better Evals, with significantly less manual review and annotation work to find these bad responses yourself. TLM also enables smart-routing for LLM-automated responses and decision-making using trustworthiness scores for every LLM output.

### The Problem with LLMs
Generative AI and Large Language Models (LLMs) are revolutionizing automation and data-driven decision-making. But there‘s a catch: LLMs often produce "hallucinations", generating incorrect or nonsensical answers that can undermine your business.

### The Solution: Add Trust to Every Response
Cleanlab's Trustworthy Language Model (TLM) scores the trustworthiness of every LLM response, letting you know which responses are reliable and which ones need extra scrutiny. TLM automatically detects incorrect LLM outputs in real-time—perfect for enterprise applications where unchecked hallucinations are unacceptable.

**TLM provides users with:**

- Trustworthiness scores and explanation for every LLM response
- Higher accuracy: rigorous [benchmarks](https://cleanlab.ai/blog/trustworthy-language-model/) show TLM consistently produces more accurate results than other LLMs like GPT 4/4o and Claude.
- Scalable API: designed to handle large datasets, TLM is suitable for most enterprise applications, including data extraction, tagging/labeling, Q&A (RAG), and more.

## Getting Started

Check out the notebook for an end-to-end example of LLM evaluations with Cleanlab and Langfuse:

import { FileCode } from "lucide-react";

<Cards>
  <Card
    title="Evaluation of LLMs with Cleanlab"
    href="/guides/cookbook/evaluation_of_llms_with_cleanlab"
    icon={<FileCode />}
  />
</Cards>
---
source: ⚠️ Jupyter Notebook
title: Integrate Portkey LLM Gateway with Langfuse
description: Guide on using Portkey's AI gateway to access 250+ LLM models with Langfuse via the OpenAI SDK.
category: Integrations
---

# Observability for Portkey LLM Gateway with Langfuse

This guide shows you how to integrate Portkey's AI gateway with Langfuse. Portkey's API endpoints are fully [compatible](https://portkey.ai/docs/api-reference/inference-api/introduction) with the OpenAI SDK, allowing you to trace and monitor your AI applications seamlessly.

> **What is Portkey?** [Portkey](https://portkey.ai/) is an AI gateway that provides a unified interface to interact with 250+ AI models, offering advanced tools for control, visibility, and security in your Generative AI apps.

> **What is Langfuse?** [Langfuse](https://langfuse.com) is an open source LLM engineering platform that helps teams trace API calls, monitor performance, and debug issues in their AI applications.

<Steps>
## Step 1: Install Dependencies


```python
%pip install openai langfuse
```

## Step 2: Set Up Environment Variables


```python
import os

# Get keys for your project from the project settings page
# https://cloud.langfuse.com

os.environ["LANGFUSE_PUBLIC_KEY"] = "pk-lf-..." 
os.environ["LANGFUSE_SECRET_KEY"] = "sk-lf-..."
os.environ["LANGFUSE_HOST"] = "https://cloud.langfuse.com" # 🇪🇺 EU region
# os.environ["LANGFUSE_HOST"] = "https://us.cloud.langfuse.com" # 🇺🇸 US region

# Set your Portkey API details
os.environ["PORTKEY_API_BASE"] = "https://api.portkey.ai/v1"
os.environ["PORTKEY_API_KEY"] = "portkey-..."  # Your Portkey API key
os.environ["PORTKEY_PROVIDER"] = "openai"  # Or any other provider supported by Portkey
os.environ["PROVIDER_API_KEY"] = "sk-..."  # Your provider's API key (e.g., OpenAI API key)
```

## Step 3: Use Langfuse OpenAI Drop-in Replacement


```python
from langfuse.openai import openai

client = openai.OpenAI(
  api_key=os.environ.get("PROVIDER_API_KEY"),
  base_url=os.environ.get("PORTKEY_API_BASE"),
  default_headers={
    "x-portkey-api-key": os.environ.get("PORTKEY_API_KEY"),
    "x-portkey-provider": os.environ.get("PORTKEY_PROVIDER")
  }
)
```

## Step 4: Run an Example


```python
response = client.chat.completions.create(
  model="gpt-3.5-turbo",  # Or any model supported by your chosen provider
  messages=[
    {"role": "system", "content": "You are a helpful assistant."},
    {"role": "user", "content": "What are the benefits of using an AI gateway?"},
  ],
  name = "Portkey-Gateway-Trace" # name of the trace
)
print(response.choices[0].message.content)
```

## Step 5: See Traces in Langfuse

After running the example, log in to Langfuse to view the detailed traces, including:

- Request parameters
- Response content
- Token usage and latency metrics
- Provider information through Portkey gateway

![Langfuse Trace Example](https://langfuse.com/images/cookbook/integration-portkey/portkey-example-trace.png)

_[Public example trace link in Langfuse](https://cloud.langfuse.com/project/cloramnkj0002jz088vzn1ja4/traces/2c11b0e4-eb40-49de-aee9-2ed11bed2839?timestamp=2025-03-05T13%3A31%3A34.781Z&observation=e9668bb4-29d7-4239-87be-e3019480f71f)_
</Steps>

import LearnMore from "@/components-mdx/integration-learn-more.mdx";

<LearnMore />
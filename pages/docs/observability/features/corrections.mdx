---
title: Corrected Outputs
description: Capture improved versions of LLM outputs directly in traces and observations to build better datasets and drive continuous improvement.
sidebarTitle: Corrections
---

# Corrected Outputs

Corrections allow you to capture improved versions of LLM outputs directly in trace and observation views. Domain experts can document what the model should have generated, creating a foundation for fine-tuning datasets and continuous improvement.

<Frame>
  ![Corrected output with diff view](/images/docs/corrections-diff-view.png)
</Frame>

## Why Use Corrections?

- **Domain expert feedback**: Subject matter experts provide what the model should have output based on their expertise
- **Fine-tuning datasets**: Export corrected outputs alongside original inputs to create high-quality training data from production traces
- **Quality benchmarking**: Compare actual vs expected outputs across your production traces to identify systematic issues
- **Human-in-the-loop workflows**: Capture corrections during review processes, especially useful in [annotation queues](/docs/evaluation/evaluation-methods/annotation-queues)

## How It Works

Add corrected outputs to any trace or observation through the UI or API. Corrections appear alongside the original output with a diff view showing what changed. Each trace or observation can have one corrected output.

## Adding Corrections

<LangTabs items={["Langfuse UI", "API/SDK"]}>

<Tab>

### Via the UI

Navigate to any trace or observation detail page:

1. Find the **"Corrected Output"** field below the original output
2. Click to add or edit the correction
3. Enter the improved version of the output
4. Toggle between **JSON validation mode** and **plain text mode** to match your data format
5. View the **diff** to compare original vs corrected output

<Frame fullWidth>
  ![Adding a correction in the UI](/images/docs/corrections-add-ui.png)
</Frame>

The editor auto-saves as you type and provides real-time validation feedback in JSON mode.

</Tab>

<Tab>

### Via API/SDK

Corrections are created as scores with `dataType: "CORRECTION"` and `name: "output"`.

<Tabs items={["Python", "TypeScript", "HTTP"]}>
<Tab>

```python
from langfuse import Langfuse

langfuse = Langfuse()

# Add correction to a trace
langfuse.create_score(
    trace_id="trace-123",
    name="output",
    value="The corrected output text here",
    data_type="CORRECTION"
)

# Add correction to an observation
langfuse.create_score(
    trace_id="trace-123",
    observation_id="obs-456",
    name="output",
    value="The corrected output text here",
    data_type="CORRECTION"
)
```

</Tab>
<Tab>

```typescript
import { Langfuse } from "langfuse";

const langfuse = new Langfuse();

// Add correction to a trace
langfuse.score.create({
  traceId: "trace-123",
  name: "output",
  value: "The corrected output text here",
  dataType: "CORRECTION"
});

// Add correction to an observation
langfuse.score.create({
  traceId: "trace-123",
  observationId: "obs-456",
  name: "output",
  value: "The corrected output text here",
  dataType: "CORRECTION"
});
```

</Tab>
<Tab>

```bash
curl -X POST https://cloud.langfuse.com/api/public/scores \
  -H "Content-Type: application/json" \
  -H "Authorization: Basic <base64_encoded_credentials>" \
  -d '{
    "traceId": "trace-123",
    "observationId": "obs-456",
    "name": "output",
    "value": "The corrected output text here",
    "dataType": "CORRECTION"
  }'
```

</Tab>
</Tabs>

</Tab>

</LangTabs>

## Fetching Corrections

Corrections are stored as scores and can be fetched programmatically to build datasets or analyze model performance.

<Tabs items={["Python", "TypeScript", "HTTP"]}>
<Tab>

Coming soon: Fetch corrections via the SDK.

</Tab>
<Tab>

Coming soon: Fetch corrections via the SDK.

</Tab>
<Tab>

```bash
curl -X GET "https://cloud.langfuse.com/api/public/scores?dataType=CORRECTION" \
  -H "Authorization: Basic <base64_encoded_credentials>"
```

</Tab>
</Tabs>
---
title: Corrected Outputs
description: Capture improved versions of LLM outputs directly in traces and observations to build better datasets and drive continuous improvement.
sidebarTitle: Corrections
---

# Corrected Outputs

Corrections allow you to capture improved versions of LLM outputs directly in trace and observation views. Domain experts can document what the model should have generated, creating a foundation for fine-tuning datasets and continuous improvement.

<Frame>
  ![Corrected output with diff view](/images/docs/corrections-diff-view.png)
</Frame>

## Why Use Corrections?

- **Domain expert feedback**: Subject matter experts provide what the model should have output based on their expertise
- **Fine-tuning datasets**: Export corrected outputs alongside original inputs to create high-quality training data from production traces
- **Quality benchmarking**: Compare actual vs expected outputs across your production traces to identify systematic issues
- **Human-in-the-loop workflows**: Capture corrections during review processes, especially useful in [annotation queues](/docs/evaluation/evaluation-methods/annotation-queues)

## How It Works

Add corrected outputs to any trace or observation through the UI or API. Corrections appear alongside the original output with a diff view showing what changed. Each trace or observation can have one corrected output.

## Adding Corrections

<LangTabs items={["Langfuse UI", "API/SDK"]}>

<Tab>

### Via the UI

Navigate to any trace or observation detail page:

1. Find the **"Corrected Output"** field below the original output
2. Click to add or edit the correction
3. Enter the improved version of the output
4. Toggle between **JSON validation mode** and **plain text mode** to match your data format
5. View the **diff** to compare original vs corrected output

<Frame fullWidth>
  ![Adding a correction in the UI](/images/docs/corrections-add-ui.png)
</Frame>

The editor auto-saves as you type and provides real-time validation feedback in JSON mode.

</Tab>

<Tab>

### Via API/SDK

Corrections are created as scores with `dataType: "CORRECTION"` and `name: "output"`.

<Tabs items={["Python", "TypeScript", "HTTP"]}>
<Tab>

```python
from langfuse import Langfuse

langfuse = Langfuse()

# Add correction to a trace
langfuse.score(
    trace_id="trace-123",
    name="output",
    value="The corrected output text here",
    data_type="CORRECTION"
)

# Add correction to an observation
langfuse.score(
    trace_id="trace-123",
    observation_id="obs-456",
    name="output",
    value="The corrected output text here",
    data_type="CORRECTION"
)
```

</Tab>
<Tab>

```typescript
import { Langfuse } from "langfuse";

const langfuse = new Langfuse();

// Add correction to a trace
langfuse.score({
  traceId: "trace-123",
  name: "output",
  value: "The corrected output text here",
  dataType: "CORRECTION"
});

// Add correction to an observation
langfuse.score({
  traceId: "trace-123",
  observationId: "obs-456",
  name: "output",
  value: "The corrected output text here",
  dataType: "CORRECTION"
});
```

</Tab>
<Tab>

```bash
curl -X POST https://cloud.langfuse.com/api/public/scores \
  -H "Content-Type: application/json" \
  -H "Authorization: Basic <base64_encoded_credentials>" \
  -d '{
    "traceId": "trace-123",
    "observationId": "obs-456",
    "name": "output",
    "value": "The corrected output text here",
    "dataType": "CORRECTION"
  }'
```

</Tab>
</Tabs>

</Tab>

</LangTabs>

## Fetching Corrections

Corrections are stored as scores and can be fetched programmatically to build datasets or analyze model performance.

<Tabs items={["Python", "TypeScript", "HTTP"]}>
<Tab>

```python
from langfuse import Langfuse

langfuse = Langfuse()

# Fetch all corrections for a project
corrections = langfuse.fetch_scores(
    data_type="CORRECTION"
)

for correction in corrections.data:
    print(f"Trace: {correction.trace_id}")
    print(f"Original: {correction.trace.output}")
    print(f"Corrected: {correction.string_value}")
```

</Tab>
<Tab>

```typescript
import { Langfuse } from "langfuse";

const langfuse = new Langfuse();

// Fetch all corrections for a project
const corrections = await langfuse.fetchScores({
  dataType: "CORRECTION"
});

corrections.data.forEach(correction => {
  console.log(`Trace: ${correction.traceId}`);
  console.log(`Corrected: ${correction.stringValue}`);
});
```

</Tab>
<Tab>

```bash
curl -X GET "https://cloud.langfuse.com/api/public/scores?dataType=CORRECTION" \
  -H "Authorization: Basic <base64_encoded_credentials>"
```

</Tab>
</Tabs>

## Use Cases

### Customer Support

Capture expert agent responses when reviewing support tickets. Export corrections to train models on your company's support standards.

### Content Generation

Document preferred outputs for style, tone, and formatting. Build a dataset of high-quality examples that reflect your brand voice.

### Code Generation

Record working code when the model output needed fixes. Create a fine-tuning dataset from real production scenarios.

### Structured Extraction

Provide correctly formatted outputs when the model returns malformed JSON or misses fields. Improve extraction accuracy over time.

## Workflow Integration

Corrections work seamlessly with other Langfuse features:

- **[Annotation Queues](/docs/evaluation/evaluation-methods/annotation-queues)**: Review traces systematically and add corrections as part of your annotation workflow
- **[Comments](/docs/observability/features/comments)**: Use comments to explain why a correction was needed or discuss edge cases with your team
- **[User Feedback](/docs/observability/features/user-feedback)**: Combine user satisfaction scores with corrections to prioritize improvement areas

## Best Practices

- **Be specific**: Provide complete, production-ready outputs rather than partial fixes
- **Add context**: Use the comment field on corrections to explain what was wrong and why the correction is better
- **Review systematically**: Use annotation queues to ensure consistent correction quality across multiple reviewers
- **Export regularly**: Build fine-tuning datasets incrementally as you accumulate high-quality corrections

## API Reference

For complete API details, see the [Scores API Reference](https://api.reference.langfuse.com/#tag/scores).

---
description: Add any customer metadata to your traces to better understand your users, application and experiments.
sidebarTitle: Metadata
---

# Metadata

Traces and observations (see [Langfuse Data Model](/docs/tracing-data-model)) can be enriched with metadata to better understand your users, application, and experiments.
Metadata can be added to traces in the form of arbitrary JSON.

Metadata updates are merged based on the top-level key.
We strongly discourage writing the same key multiple times within your instrumentation.

<LangTabs items={["Python SDK", "JS/TS SDK", "OpenAI (Python)", "OpenAI (JS/TS)", "Langchain (Python)", "Langchain (JS/TS)", "Flowise"]}>
<Tab>

### Trace-Level Metadata (Propagated to All Spans)

Use `propagate_attributes()` to set metadata that applies to the current span AND all child spans:

```python
from langfuse import observe, propagate_attributes

@observe()
def process_user_request(request):
    with propagate_attributes(metadata={
        "customer_tier": "premium",
        "ab_test_variant": "variant_a"
    }):
        return process_pipeline(request)
```

See [Attribute Propagation](/docs/observability/features/attribute-propagation) for more details.

### Observation-Level Metadata (Single Span Only)

Use `.update(metadata={...})` to set metadata on a single span:

```python
from langfuse import get_client

langfuse = get_client()

with langfuse.start_as_current_span(name="operation") as span:
    # Observation-level: only on THIS span
    span.update(metadata={
        "request_id": "req-abc-123",
        "retry_count": "2"
    })
```

<Callout type="info">
**When to use each:**

- **Trace-level metadata** (via `propagate_attributes()`): For dimensions that apply to the entire workflow - feature flags, experiment variants, user properties, deployment context. See [Attribute Propagation](/docs/observability/features/attribute-propagation#trace-level-vs-observation-level) for details.
- **Observation-level metadata** (via `span.update()`): For operation-specific details - retry counts, step information, operation-specific parameters
</Callout>

</Tab>
<Tab title="JS/TS SDK">

### Trace-Level Metadata (Propagated to All Spans)

Use `propagateAttributes()` to set metadata that applies to the current span AND all child spans:

```ts
import { observe, propagateAttributes } from "@langfuse/tracing";

const processUserRequest = observe(async (request: Request) => {
  return await propagateAttributes({
    metadata: {
      customer_tier: "premium",
      ab_test_variant: "variant_a"
    }
  }, async () => {
    return await processPipeline(request);
  });
}, { name: 'process-user-request' });
```

See [Attribute Propagation](/docs/observability/features/attribute-propagation) for more details.

### Observation-Level Metadata (Single Span Only)

Use `.update({ metadata: {...} })` to set metadata on a single span:

```ts
import { startActiveObservation } from "@langfuse/tracing";

await startActiveObservation("operation", async (span) => {
  // Observation-level: only on THIS span
  span.update({
    metadata: {
      request_id: "req-abc-123",
      retry_count: "2"
    }
  });
});
```

<Callout type="info">
**When to use each:**

- **Trace-level metadata** (via `propagateAttributes()`): For dimensions that apply to the entire workflow - feature flags, experiment variants, user properties, deployment context. See [Attribute Propagation](/docs/observability/features/attribute-propagation#trace-level-vs-observation-level) for details.
- **Observation-level metadata** (via `span.update({ metadata: {...} })`): For operation-specific details - retry counts, step information, operation-specific parameters

**Important:** All metadata values must be strings (US-ASCII, â‰¤200 characters). Convert numbers, booleans, etc. to strings.
</Callout>

</Tab>
<Tab title="OpenAI (Python v2)">

When using the [OpenAI SDK Integration](/integrations/model-providers/openai-py), pass `metadata` as an additional argument:

```python
from langfuse.openai import openai

completion = openai.chat.completions.create(
  name="test-chat",
  model="gpt-3.5-turbo",
  messages=[
    {"role": "system", "content": "You are a calculator."},
    {"role": "user", "content": "1 + 1 = "}],
  temperature=0,

  # add metadata as additional argument
  metadata={"key":"value"}
)
```

</Tab>
<Tab title="OpenAI (JS/TS)">

When using the [OpenAI SDK Integration (JS)](/integrations/model-providers/openai-js), pass `metadata` as an additional argument:

```ts
import OpenAI from "openai";
import { observeOpenAI } from "@langfuse/openai";

const res = await observeOpenAI(new OpenAI(), {
  metadata: { someMetadataKey: "someValue" },
}).chat.completions.create({
  messages: [{ role: "system", content: "Tell me a story about a dog." }],
  model: "gpt-3.5-turbo",
  max_tokens: 300,
});
```

</Tab>
<Tab>

Option 1: Via metadata fields in chain invocation (simplest approach):

```python
from langfuse.langchain import CallbackHandler
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate

langfuse_handler = CallbackHandler()

llm = ChatOpenAI(model_name="gpt-4o")
prompt = ChatPromptTemplate.from_template("Tell me a joke about {topic}")
chain = prompt | llm

# Set trace attributes dynamically via metadata
response = chain.invoke(
    {"topic": "cats"},
    config={
        "callbacks": [langfuse_handler],
        "metadata": {
            "foo": "bar",
            "baz": "qux"
        }
    }
)
```

Option 2: Via enclosing span (for more control):

```python
from langfuse import get_client
from langfuse.langchain import CallbackHandler
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate

langfuse = get_client()
langfuse_handler = CallbackHandler()

llm = ChatOpenAI(model_name="gpt-4o")
prompt = ChatPromptTemplate.from_template("Tell me a joke about {topic}")
chain = prompt | llm

# Set trace attributes dynamically via enclosing span
with langfuse.start_as_current_span(name="dynamic-langchain-trace") as span:
    span.update_trace(
        metadata={"foo": "bar", "baz": "qux"}
    )

    response = chain.invoke({"topic": "cats"}, config={"callbacks": [langfuse_handler]})

    span.update_trace(output={"response": response.content})
```

</Tab>
<Tab title="Langchain (JS/TS)">

When using the [CallbackHandler](/integrations/frameworks/langchain), you can pass `metadata` to the constructor:

```ts
const handler = new CallbackHandler({
  metadata: { key: "value" },
});
```

When using the integration with the JS SDK (see [interop docs](/integrations/frameworks/langchain#interoperability)), set `metadata` via `langfuse.trace()`:

```ts
import { CallbackHandler, Langfuse } from "langfuse-langchain";
const langfuse = new Langfuse();

const trace = langfuse.trace({
  metadata: { key: "value" },
});
const langfuseHandler = new CallbackHandler({ root: trace });

// Add Langfuse handler as callback to your langchain chain/agent
await chain.invoke({ input: "<user_input>" }, { callbacks: [langfuseHandler] });
```

</Tab>

<Tab title="Flowise">
You can set the `metadata` via the override configs, see the [Flowise Integration docs](/docs/flowise) for more details.

</Tab>

</LangTabs>

## GitHub Discussions

import { GhDiscussionsPreview } from "@/components/gh-discussions/GhDiscussionsPreview";

<GhDiscussionsPreview labels={["feat-metadata"]} />

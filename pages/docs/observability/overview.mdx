---
title: LLM Observability & Application Tracing (open source)
description: Open Source LLM Observability and Tracing with Langfuse. Integrates with OpenAI, LlamaIndex, LangChain, Python Decorators and more.
---

# Observability & Tracing

Because AI is inherently non-deterministic, debugging your application without any observability tool is more like guesswork.
Well implemented observability gives you the tools to understand what's happening inside your application and why.

The core of this is tracing. It gives you structured logs of every request: the exact prompt sent, the model's response, token usage, latency, and any tools or retrieval steps in between.

Langfuse captures all of this for you as you build. Here's an example of a trace in the Langfuse UI:

<Frame fullWidth>
  <img src="/images/docs/tracing-overview.png" alt="Example of a trace showing nested observations: an initial model call, multiple tool executions, and a final summarization step. Each observation includes timing, inputs, outputs, and cost information." />
</Frame>

<Callout type="info" emoji="ðŸŽ¥">

[**Watch this walkthrough**](/watch-demo?tab=observability) of Langfuse Observability and how to integrate it with your application.

</Callout>


## Getting started

Start by [setting up your first trace](/docs/observability/get-started).

Take a moment to understand the core concepts of tracing in Langfuse: [traces, sessions, and observations](/docs/observability/data-model).

Once you're up and running, you can start adding on more functionality to your traces. We recommend starting with the following:
- [Group traces into sessions for multi-turn applications](/docs/observability/features/sessions)
- [Split traces into environments for different stages of your application](/docs/observability/features/environments)
- [Add attributes to your traces so you can filter them in the future](/docs/observability/features/tags)

Already know what you want? Take a look under _Features_ for guides on specific topics. 


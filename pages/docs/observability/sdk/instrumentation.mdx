---
title: Instrument your application with the Langfuse SDKs
description: Use native integrations or custom instrumentation patterns in Python and JavaScript/TypeScript to capture rich traces.
category: SDKs
---

import GetStartedJsLangchain from "@/components-mdx/get-started/js-langchain.mdx";
import { PropagationRestrictionsCallout } from "@/components/PropagationRestrictionsCallout";

# Instrumentation

Langfuse SDKs lets you manually create observations and traces. You can also use the manual creation patters together with one of the [native integrations](/integrations).


## Native integrations [#native-integrations]

Langfuse supports [native integrations](/integrations) for popular LLM and agent libraries such as OpenAI, LangChain or the Vercel AI SDK. They automatically create observations and traces and capture prompts, responses, usage, and errors. 

## Custom observations [#custom]

For some use cases you might want to have more control over the observations and traces. For this, you can create custom observations using the Langfuse SDK. The SDKs provide 3 ways to create custom observations:

- **[Context manager](#context-manager)**
- **[Observe wrapper](#observe-wrapper)** 
- **[Manual observations](#manual-observations)**

All custom patterns are interoperable. You can nest a decorator-created observation inside a context manager or mix manual spans with native integrations.

### Context manager [#context-manager]

The context manager allows you to create a new span and set it as the currently active observation in the OTel context for its duration. Any new observations created within this block will be its children. 

<Tabs items={["Python", "TypeScript"]}>
<Tab title="Python">
`langfuse.start_as_current_observation()` is the primary way to create observations while ensuring the active OpenTelemetry context is updated. Any child observations created inside the `with` block inherit the parent automatically.

```python
from langfuse import get_client, propagate_attributes

langfuse = get_client()

with langfuse.start_as_current_observation(
    as_type="span",
    name="user-request-pipeline",
    input={"user_query": "Tell me a joke"},
) as root_span:
    with propagate_attributes(user_id="user_123", session_id="session_abc"):
        with langfuse.start_as_current_observation(
            as_type="generation",
            name="joke-generation",
            model="gpt-4o",
        ) as generation:
            generation.update(output="Why did the span cross the road?")

    root_span.update(output={"final_joke": "..."})
```
</Tab>
<Tab title="TypeScript">
`startActiveObservation` accepts a callback, makes the new span active for the callback scope, and ends it automatically, even across async boundaries.

```ts /startActiveObservation/
import { startActiveObservation, startObservation } from "@langfuse/tracing";

await startActiveObservation("user-request", async (span) => {
  span.update({ input: { query: "Capital of France?" } });

  const generation = startObservation(
    "llm-call",
    { model: "gpt-4", input: [{ role: "user", content: "Capital of France?" }] },
    { asType: "generation" }
  );
  generation.update({ output: { content: "Paris." } }).end();

  span.update({ output: "Answered." });
});
```
</Tab>
</Tabs>

### Observe wrapper [#observe-wrapper]

Use the `observe` wrapper to automatically capture inputs, outputs, timings, and errors of a wrapped function without modifying the function's internal logic.

<Tabs items={["Python", "TypeScript"]}>
<Tab title="Python">

```python /@observe/
from langfuse import observe

@observe()
def my_data_processing_function(data, parameter):
    return {"processed_data": data, "status": "ok"}

@observe(name="llm-call", as_type="generation")
async def my_async_llm_call(prompt_text):
    return "LLM response"
```

**Parameters**: `name`, `as_type`, `capture_input`, `capture_output`, `transform_to_string`. Special kwargs such as `langfuse_trace_id` or `langfuse_parent_observation_id` let you stitch into existing traces.

The decorator automatically propagates the OTEL trace context. Pass `langfuse_trace_id` when you need to force a specific trace ID (e.g., to align with an external system) and `langfuse_parent_observation_id` to attach to an existing parent span.

<Callout type="info">
Capturing large inputs/outputs may add overhead. Disable IO capture per decorator (`capture_input=False`, `capture_output=False`) or via the `LANGFUSE_OBSERVE_DECORATOR_IO_CAPTURE_ENABLED` env var.
</Callout>
</Tab>
<Tab title="TypeScript">

```ts /observe/ /updateActiveObservation/
import { observe, updateActiveObservation } from "@langfuse/tracing";

async function fetchData(source: string) {
  updateActiveObservation({ metadata: { source: "API" } });
  return { data: `some data from ${source}` };
}

const tracedFetchData = observe(fetchData, {});

const result = await tracedFetchData("API");
```

| Option | Description | Default |
| ------ | ----------- | ------- |
| `name` | Observation name | Function name |
| `asType` | Observation type (`span`, `generation`, etc.) | `"span"` |
| `captureInput` | Capture arguments as `input` | `true` |
| `captureOutput` | Capture return values/errors as `output` | `true` |
</Tab>
</Tabs>

### Manual observations [#manual-observations]

Manual APIs are useful when you need to create spans without altering the currently active context (e.g., background work or parallel tasks).

<Tabs items={["Python", "TypeScript"]}>
<Tab title="Python">
Use `start_span()` / `start_generation()` when you need manual control without changing the active context.

```python
from langfuse import get_client

langfuse = get_client()

span = langfuse.start_span(name="manual-span")
span.update(input="Data for side task")
child = span.start_span(name="child-span")
child.end()
span.end()
```

<Callout type="warning" title="Manual ending required">
Spans created via `start_span()` / `start_generation()` must be ended explicitly via `.end()`.
</Callout>
</Tab>
<Tab title="TypeScript">
`startObservation` gives you full control over lifecycle and nesting.

```ts /startObservation/
import { startObservation } from "@langfuse/tracing";

const span = startObservation("user-request", {
  input: { query: "Capital of France?" },
});

const toolCall = span.startObservation("fetch-weather", { input: { city: "Paris" } });
await new Promise((resolve) => setTimeout(resolve, 100));
toolCall.update({ output: { temperature: "15°C" } }).end();

const generation = span.startObservation(
  "llm-call",
  { model: "gpt-4", input: [{ role: "user", content: "Capital of France?" }] },
  { asType: "generation" }
);
generation.update({ output: { content: "Paris." } }).end();

span.update({ output: "Done." }).end();
```
</Tab>
</Tabs>

### Nesting observations

<Tabs items={["Python", "TypeScript"]}>
<Tab title="Python">
The function call hierarchy is automatically captured by the `@observe` decorator and reflected in the trace.

```python
from langfuse import observe

@observe
def my_data_processing_function(data, parameter):
    return {"processed_data": data, "status": "ok"}


@observe
def main_function(data, parameter):
    return my_data_processing_function(data, parameter)
```
</Tab>
<Tab title="TypeScript">
Nesting happens automatically via OpenTelemetry context propagation. When you create a new observation with `startActiveObservation`, it becomes a child of whatever was active at the time.

```ts
import { startActiveObservation } from "@langfuse/tracing";

await startActiveObservation("outer-process", async () => {
  await startActiveObservation("llm-step-1", async (span) => {
    span.update({ output: "LLM 1 output" });
  });

  await startActiveObservation("intermediate-step", async (span) => {
    await startActiveObservation("llm-step-2", async (child) => {
      child.update({ output: "LLM 2 output" });
    });

    span.update({ output: "Intermediate processing done" });
  });
});
```
</Tab>
</Tabs>

### Update observations

<Tabs items={["Python", "TypeScript"]}>
<Tab title="Python">
Update observation objects directly with `.update()` or use context-aware helpers with `.update_current_span()`.

```python /.update/ /.update_current_span/
from langfuse import get_client

langfuse = get_client()

with langfuse.start_as_current_observation(as_type="generation", name="llm-call") as gen:
    gen.update(
        input={"prompt": "Why is the sky blue?"},
        output="Rayleigh scattering",
        usage_details={"input_tokens": 5, "output_tokens": 50},
    )

with langfuse.start_as_current_observation(as_type="span", name="data-processing"):
    langfuse.update_current_span(metadata={"step1_complete": True})
```
</Tab>
<Tab title="TypeScript">
Update the active observation or trace without holding a reference.

```ts /.update/ /updateActiveTrace/
import { startActiveObservation, updateActiveTrace } from "@langfuse/tracing";

await startActiveObservation("user-request", async (span) => {
  span.update({ input: { path: "/api/process" } });

  await new Promise((resolve) => setTimeout(resolve, 50));
  const user = { id: "user-5678", name: "Jane Doe" };
  updateActiveTrace({
    userId: user.id,
    metadata: { userName: user.name },
  });

  span.update({ output: { status: "success" } }).end();
});
```
</Tab>
</Tabs>

### Add attributes to observations
Propagate attributes such as `userId`, `sessionId`, `metadata`, `version`, and `tags` to keep downstream analytics consistent. These helpers mirror the Python `propagate_attributes` context manager and the TypeScript `propagateAttributes` callback wrapper from the standalone SDK docs. Use propagation for attributes that should appear on **every** observation and `updateTrace()`/`update_current_trace()` for single-trace fields like `name`, `input`, `output`, or `public`.

**Propagatable attributes**

- `userId` / `user_id`
- `sessionId` / `session_id`
- `metadata`
- `version`
- `tags`

**Trace-only attributes** (use `updateTrace` / `update_current_trace`)

- `name`
- `input`
- `output`
- `public`

<Tabs items={["Python", "TypeScript"]}>
<Tab title="Python">
```python /propagate_attributes/
from langfuse import get_client, propagate_attributes

langfuse = get_client()

with langfuse.start_as_current_observation(as_type="span", name="user-workflow"):
    with propagate_attributes(
        user_id="user_123",
        session_id="session_abc",
        metadata={"experiment": "variant_a"},
        version="1.0",
    ):
        with langfuse.start_as_current_observation(as_type="generation", name="llm-call"):
            pass
```
</Tab>
<Tab title="TypeScript">
```ts /propagateAttributes/
import { startActiveObservation, propagateAttributes, startObservation } from "@langfuse/tracing";

await startActiveObservation("user-workflow", async () => {
  await propagateAttributes(
    {
      userId: "user_123",
      sessionId: "session_abc",
      metadata: { experiment: "variant_a", env: "prod" },
      version: "1.0",
    },
    async () => {
      const generation = startObservation("llm-call", { model: "gpt-4" }, { asType: "generation" });
      generation.end();
    }
  );
});
```
</Tab>
</Tabs>

<PropagationRestrictionsCallout attributes={[]} />

### Cross-service propagation

Use baggage propagation only when you need to forward attributes across HTTP boundaries. It pushes the values into every outbound request header, so prefer non-sensitive identifiers (session IDs, experiment versions, etc.).



<Tabs items={["Python", "TypeScript"]}>
<Tab title="Python">
```python /as_baggage=True/
from langfuse import get_client, propagate_attributes
import requests

langfuse = get_client()

with langfuse.start_as_current_observation(as_type="span", name="api-request"):
    with propagate_attributes(
        user_id="user_123",
        session_id="session_abc",
        as_baggage=True,
    ):
        requests.get("https://service-b.example.com/api")
```
</Tab>
<Tab title="TypeScript">
```ts /asBaggage/
import { propagateAttributes, startActiveObservation } from "@langfuse/tracing";

await startActiveObservation("api-request", async () => {
  await propagateAttributes(
    {
      userId: "user_123",
      sessionId: "session_abc",
      asBaggage: true,
    },
    async () => {
      await fetch("https://service-b.example.com/api");
    }
  );
});
```
</Tab>
</Tabs>

<Callout type="warning">
When baggage propagation is enabled, attributes are added to **all** outbound HTTP headers. Only use it for non-sensitive values needed for distributed tracing.
</Callout>

### Trace-level metadata & inputs/outputs [#trace-inputoutput-behavior]

By default, trace input/output mirror whatever you set on the **root observation**. Override them explicitly whenever evaluations, AB-tests, or judge models need a different payload than the root span captured.

The snippets below illustrate both the default behavior and how to call `update_current_trace` / `updateActiveTrace()` to set trace-level payloads later in the workflow.

<Callout type="info">
LLM-as-a-judge and evaluation workflows typically rely on trace-level inputs/outputs. Make sure to set them deliberately rather than relying on the root span if your evaluation payload differs.
</Callout>

<Tabs items={["Python", "TypeScript"]}>
<Tab title="Python">
Trace input/output default to the root observation. Override them explicitly when needed (e.g., for evaluations).

```python
from langfuse import get_client

langfuse = get_client()

with langfuse.start_as_current_observation(as_type="span", name="complex-pipeline") as root_span:
    root_span.update(input="Step 1 data", output="Step 1 result")
    root_span.update_trace(
        input={"original_query": "User question"},
        output={"final_answer": "Complete response", "confidence": 0.95},
    )
```

```python /update_current_trace/
from langfuse import observe, get_client

langfuse = get_client()

@observe()
def process_user_query(user_question: str):
    answer = call_llm(user_question)
    langfuse.update_current_trace(
        input={"question": user_question},
        output={"answer": answer},
    )
    return answer
```
</Tab>
<Tab title="TypeScript">
Update trace-level fields via the observation object itself.

```ts /updateTrace/
import { startObservation } from "@langfuse/tracing";

const rootSpan = startObservation("data-processing");

// ... some initial steps ...

const userId = "user-123";
const sessionId = "session-abc";
rootSpan.updateTrace({
  userId: userId,
  sessionId: sessionId,
  tags: ["authenticated-user"],
  metadata: { plan: "premium" },
});

const generation = rootSpan.startObservation(
  "llm-call",
  {},
  { asType: "generation" }
);

generation.end();

rootSpan.end();
```
</Tab>
</Tabs>

### Trace and observation IDs [#managing-trace-and-observation-ids]

Langfuse follows the W3C Trace Context standard: trace IDs are 32-character lowercase hex strings (16 bytes) and observation IDs are 16-character lowercase hex strings (8 bytes). You cannot set arbitrary observation IDs, but you can generate deterministic trace IDs to correlate with external systems.

<Tabs items={["Python", "TypeScript"]}>
<Tab title="Python">
Langfuse uses W3C Trace Context IDs. Access current IDs or create deterministic ones.

```python
from langfuse import get_client, Langfuse

langfuse = get_client()

with langfuse.start_as_current_observation(as_type="span", name="my-op") as current_op:
    trace_id = langfuse.get_current_trace_id()
    observation_id = langfuse.get_current_observation_id()
    print(trace_id, observation_id)

external_request_id = "req_12345"
deterministic_trace_id = Langfuse.create_trace_id(seed=external_request_id)
```
</Tab>
<Tab title="TypeScript">
Generate deterministic IDs or read the active trace ID.

```ts
import { createTraceId, startObservation } from "@langfuse/tracing";

const externalId = "support-ticket-54321";

const langfuseTraceId = await createTraceId(externalId);

const rootSpan = startObservation(
  "process-ticket",
  {},
  {
    parentSpanContext: {
      traceId: langfuseTraceId,
      spanId: "0123456789abcdef",
      traceFlags: 1,
    },
  }
);
```

```ts
import { startObservation, getActiveTraceId } from "@langfuse/tracing";

await startObservation("run", async (span) => {
  const traceId = getActiveTraceId();
  console.log(`Current trace ID: ${traceId}`);
});
```
</Tab>
</Tabs>

### Link to existing traces

When integrating with upstream services that already have trace IDs, supply the W3C trace context so Langfuse spans join the existing tree rather than creating a new one.

<Tabs items={["Python", "TypeScript"]}>
<Tab title="Python">
```python
from langfuse import get_client

langfuse = get_client()

existing_trace_id = "abcdef1234567890abcdef1234567890"
existing_parent_span_id = "fedcba0987654321"

with langfuse.start_as_current_observation(
    as_type="span",
    name="process-downstream-task",
    trace_context={
        "trace_id": existing_trace_id,
        "parent_span_id": existing_parent_span_id,
    },
):
    pass
```
</Tab>
<Tab title="TypeScript">
```ts
import { startObservation } from "@langfuse/tracing";

const span = startObservation(
  "downstream-task",
  {},
  {
    parentSpanContext: {
      traceId: "abcdef1234567890abcdef1234567890",
      spanId: "fedcba0987654321",
      traceFlags: 1,
    },
  }
);

span.end();
```
</Tab>
</Tabs>

### Client lifecycle & flushing

Both SDKs buffer spans in the background. Always flush or shut down the exporter in short-lived processes (scripts, serverless functions, workers) to avoid losing data.

<Tabs items={["Python", "TypeScript"]}>
<Tab title="Python">
Flush or shut down the client to ensure all buffered data is delivered—especially in short-lived jobs.

```python
from langfuse import get_client

langfuse = get_client()
# ... create traces ...
langfuse.flush()
langfuse.shutdown()
```
</Tab>
<Tab title="TypeScript">
Export the span processor from your OTEL setup and flush before the process exits (or use `after` in Vercel Functions).

```ts filename="instrumentation.ts" /langfuseSpanProcessor/
import { NodeSDK } from "@opentelemetry/sdk-node";
import { LangfuseSpanProcessor } from "@langfuse/otel";

export const langfuseSpanProcessor = new LangfuseSpanProcessor({
  exportMode: "immediate",
});

const sdk = new NodeSDK({
  spanProcessors: [langfuseSpanProcessor],
});

sdk.start();
```

```ts filename="handler.ts" /forceFlush/
import { langfuseSpanProcessor } from "./instrumentation";

export async function handler(event, context) {
  // ... logic ...
  await langfuseSpanProcessor.forceFlush();
}
```
</Tab>
</Tabs>

---
title: Instrument your application with the Langfuse SDKs
description: Use native integrations or custom instrumentation patterns in Python and JavaScript/TypeScript to capture rich traces.
category: SDKs
---

import GetStartedJsLangchain from "@/components-mdx/get-started/js-langchain.mdx";
import { PropagationRestrictionsCallout } from "@/components/PropagationRestrictionsCallout";

# Instrumentation

Langfuse SDKs build on OpenTelemetry so you can mix native integrations, wrappers, and fully custom spans. Use the tabs below to see the equivalent Python and JS/TS patterns side-by-side.

## Native integrations [#native-integrations]

Langfuse supports native integrations for popular LLM and agent libraries. They capture prompts, responses, usage, and errors automatically while preserving full OpenTelemetry context. Explore the full ecosystem in the [Integrations gallery](/integrations).

### OpenAI

<Tabs items={["Python", "TypeScript"]}>
<Tab title="Python">
Langfuse provides a drop-in replacement for the OpenAI Python SDK to automatically trace all API calls.

```diff
- import openai
+ from langfuse.openai import openai

# Your existing OpenAI code continues to work as is
# For example:
# client = openai.OpenAI()
# completion = client.chat.completions.create(...)
```

**What's captured automatically:** prompts (incl. streaming), responses, timings, errors, token usage, estimated costs, and multimedia payloads. If the OpenAI call runs inside an active Langfuse span it is nested correctly.

You can also pass Langfuse-specific metadata to enrich the trace:

```python /metadata={"langfuse_session_id":/
from langfuse.openai import openai

client = openai.OpenAI()

response = client.chat.completions.create(
    model="gpt-4o",
    messages=[{"role": "user", "content": "What is OpenTelemetry?"}],
    metadata={
        "langfuse_session_id": "session_123",
        "langfuse_user_id": "user_456",
        "langfuse_tags": ["production", "chat-bot"],
        "custom_field": "additional metadata",
    },
)
```

Integrations are interoperable with manual tracing. Wrap calls with Langfuse spans to propagate tags or session data:

```python
from langfuse import get_client, propagate_attributes
from langfuse.openai import openai

langfuse = get_client()
client = openai.OpenAI()

with langfuse.start_as_current_observation(as_type="span", name="qna-bot-openai"):
    with propagate_attributes(tags=["qna-bot-openai"]):
        response = client.chat.completions.create(
            name="qna-bot-openai",
            model="gpt-4o",
            messages=[{"role": "user", "content": "What is OpenTelemetry?"}],
        )
```
</Tab>
<Tab title="TypeScript">
Wrap any OpenAI client with `observeOpenAI`. All subsequent calls emit generations that inherit the current OpenTelemetry context.

```ts /observeOpenAI/
import { OpenAI } from "openai";
import { observeOpenAI } from "@langfuse/openai";

const openai = new OpenAI();

const tracedOpenAI = observeOpenAI(openai, {
  traceName: "my-openai-trace",
  sessionId: "user-session-123",
  userId: "user-abc",
  tags: ["openai-integration"],
});

const completion = await tracedOpenAI.chat.completions.create({
  model: "gpt-4",
  messages: [{ role: "user", content: "What is OpenTelemetry?" }],
});
```

Mix native integrations with custom spans (e.g., `startActiveObservation`) whenever you need additional metadata or nesting control.
</Tab>
</Tabs>

### LangChain

<Tabs items={["Python", "TypeScript"]}>
<Tab title="Python">
Use the Langfuse LangChain callback handler to automatically trace chains, tools, retrievers, and LLM calls.

```python
from langfuse import get_client, propagate_attributes
from langfuse.langchain import CallbackHandler
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate

langfuse = get_client()
langfuse_handler = CallbackHandler()
llm = ChatOpenAI(model_name="gpt-4o")
prompt = ChatPromptTemplate.from_template("Tell me a joke about {topic}")
chain = prompt | llm

with langfuse.start_as_current_observation(as_type="span", name="joke-chain"):
    with propagate_attributes(tags=["joke-chain"]):
        response = chain.invoke({"topic": "cats"}, config={"callbacks": [langfuse_handler]})
```

Trace attributes (`session_id`, `user_id`, `tags`) can be set via `metadata` on the LangChain call so every observation inherits them.
</Tab>
<Tab title="TypeScript">
<GetStartedJsLangchain />
</Tab>
</Tabs>

### Framework & third-party telemetry

<Tabs items={["Python", "TypeScript"]}>
<Tab title="Python">
Any OTEL-instrumented library can forward spans to Langfuse. Example: tracing Anthropic via the community instrumentation package.

```python
from anthropic import Anthropic
from opentelemetry.instrumentation.anthropic import AnthropicInstrumentor

from langfuse import get_client

AnthropicInstrumentor().instrument()

langfuse = get_client()
client = Anthropic()

with langfuse.start_as_current_observation(as_type="span", name="myspan"):
    message = client.messages.create(
        model="claude-3-7-sonnet-20250219",
        max_tokens=1024,
        messages=[{"role": "user", "content": "Hello, Claude"}],
    )

langfuse.flush()
```

See the [Langfuse + LlamaIndex integration](/integrations/frameworks/llamaindex) for another OTEL-based example.
</Tab>
<Tab title="TypeScript">
The JS SDK integrates with frameworks like the Vercel AI SDK. Register the `LangfuseSpanProcessor` in your OTEL setup and flush spans after streaming responses.

```ts filename="instrumentation.ts"
import { LangfuseSpanProcessor, ShouldExportSpan } from "@langfuse/otel";
import { NodeTracerProvider } from "@opentelemetry/sdk-trace-node";

const shouldExportSpan: ShouldExportSpan = (span) =>
  span.otelSpan.instrumentationScope.name !== "next.js";

export const langfuseSpanProcessor = new LangfuseSpanProcessor({ shouldExportSpan });

const tracerProvider = new NodeTracerProvider({
  spanProcessors: [langfuseSpanProcessor],
});

tracerProvider.register();
```

```ts filename="route.ts" /observe/ /forceFlush/
import { streamText } from "ai";
import { after } from "next/server";

import { openai } from "@ai-sdk/openai";
import { observe, updateActiveObservation, updateActiveTrace } from "@langfuse/tracing";
import { trace } from "@opentelemetry/api";

import { langfuseSpanProcessor } from "@/src/instrumentation";

const handler = async (req: Request) => {
  const { messages, chatId, userId } = await req.json();

  updateActiveObservation({ input: messages[messages.length - 1] });
  updateActiveTrace({ name: "my-ai-sdk-trace", sessionId: chatId, userId });

  const result = streamText({
    experimental_telemetry: { isEnabled: true },
    onFinish: async (result) => {
      updateActiveObservation({ output: result.content });
      updateActiveTrace({ output: result.content });
      trace.getActiveSpan()?.end();
    },
  });

  after(async () => await langfuseSpanProcessor.forceFlush());

  return result.toUIMessageStreamResponse();
};

export const POST = observe(handler, {
  name: "handle-chat-message",
  endOnExit: false,
});
```

Any OTEL-instrumented library (databases, HTTP clients, agents) automatically exports spans to Langfuse once the `LangfuseSpanProcessor` is registered.
</Tab>
</Tabs>

## Custom instrumentation patterns [#custom-instrumentation]

You can also create custom instrumentation patterns using the Langfuse SDK. 

All custom patterns are interoperable—you can nest a decorator-created observation inside a context manager or mix manual spans with native integrations.

### Observe decorator [#observe-wrapper]

<Tabs items={["Python", "TypeScript"]}>
<Tab title="Python">
Use the `@observe()` decorator to automatically capture inputs, outputs, timings, and errors of a wrapped function.

```python
from langfuse import observe

@observe()
def my_data_processing_function(data, parameter):
    return {"processed_data": data, "status": "ok"}

@observe(name="llm-call", as_type="generation")
async def my_async_llm_call(prompt_text):
    return "LLM response"
```

**Parameters**: `name`, `as_type`, `capture_input`, `capture_output`, `transform_to_string`. Special kwargs such as `langfuse_trace_id` or `langfuse_parent_observation_id` let you stitch into existing traces.

The decorator automatically propagates the OTEL trace context. Pass `langfuse_trace_id` when you need to force a specific trace ID (e.g., to align with an external system) and `langfuse_parent_observation_id` to attach to an existing parent span.

<Callout type="info">
Capturing large inputs/outputs may add overhead. Disable IO capture per decorator (`capture_input=False`, `capture_output=False`) or via the `LANGFUSE_OBSERVE_DECORATOR_IO_CAPTURE_ENABLED` env var.
</Callout>
</Tab>
<Tab title="TypeScript">
Wrap existing functions with `observe` to trace calls without modifying their internals.

```ts /observe/ /updateActiveObservation/
import { observe, updateActiveObservation } from "@langfuse/tracing";

async function fetchData(source: string) {
  updateActiveObservation({ metadata: { source: "API" } });
  return { data: `some data from ${source}` };
}

const tracedFetchData = observe(fetchData, {});

const result = await tracedFetchData("API");
```

| Option | Description | Default |
| ------ | ----------- | ------- |
| `name` | Observation name | Function name |
| `asType` | Observation type (`span`, `generation`, etc.) | `"span"` |
| `captureInput` | Capture arguments as `input` | `true` |
| `captureOutput` | Capture return values/errors as `output` | `true` |
</Tab>
</Tabs>

### Context managers & callbacks [#context-management-with-callbacks]

Context helpers keep spans correctly parented and automatically close them for you.

<Tabs items={["Python", "TypeScript"]}>
<Tab title="Python">
`langfuse.start_as_current_observation()` is the primary way to create spans or generations while ensuring the active OpenTelemetry context is updated. Any child spans created inside the `with` block inherit the parent automatically.

```python
from langfuse import get_client, propagate_attributes

langfuse = get_client()

with langfuse.start_as_current_observation(
    as_type="span",
    name="user-request-pipeline",
    input={"user_query": "Tell me a joke"},
) as root_span:
    with propagate_attributes(user_id="user_123", session_id="session_abc"):
        with langfuse.start_as_current_observation(
            as_type="generation",
            name="joke-generation",
            model="gpt-4o",
        ) as generation:
            generation.update(output="Why did the span cross the road?")

    root_span.update(output={"final_joke": "..."})
```
</Tab>
<Tab title="TypeScript">
`startActiveObservation` accepts a callback, makes the new span active for the callback scope, and ends it automatically—even across async boundaries.

```ts /startActiveObservation/
import { startActiveObservation, startObservation } from "@langfuse/tracing";

await startActiveObservation("user-request", async (span) => {
  span.update({ input: { query: "Capital of France?" } });

  const generation = startObservation(
    "llm-call",
    { model: "gpt-4", input: [{ role: "user", content: "Capital of France?" }] },
    { asType: "generation" }
  );
  generation.update({ output: { content: "Paris." } }).end();

  span.update({ output: "Answered." });
});
```
</Tab>
</Tabs>

### Manual observations [#manual-observations]

Manual APIs are useful when you need to create spans without altering the currently active context (e.g., background work or parallel tasks).

<Tabs items={["Python", "TypeScript"]}>
<Tab title="Python">
Use `start_span()` / `start_generation()` when you need manual control without changing the active context.

```python
from langfuse import get_client

langfuse = get_client()

span = langfuse.start_span(name="manual-span")
span.update(input="Data for side task")
child = span.start_span(name="child-span")
child.end()
span.end()
```

<Callout type="warning" title="Manual ending required">
Spans created via `start_span()` / `start_generation()` must be ended explicitly via `.end()`.
</Callout>
</Tab>
<Tab title="TypeScript">
`startObservation` gives you full control over lifecycle and nesting.

```ts /startObservation/
import { startObservation } from "@langfuse/tracing";

const span = startObservation("user-request", {
  input: { query: "Capital of France?" },
});

const toolCall = span.startObservation("fetch-weather", { input: { city: "Paris" } });
await new Promise((resolve) => setTimeout(resolve, 100));
toolCall.update({ output: { temperature: "15°C" } }).end();

const generation = span.startObservation(
  "llm-call",
  { model: "gpt-4", input: [{ role: "user", content: "Capital of France?" }] },
  { asType: "generation" }
);
generation.update({ output: { content: "Paris." } }).end();

span.update({ output: "Done." }).end();
```
</Tab>
</Tabs>

### Nesting observations

<Tabs items={["Python", "TypeScript"]}>
<Tab title="Python">
The function call hierarchy is automatically captured by the `@observe` decorator and reflected in the trace.

```python
from langfuse import observe

@observe
def my_data_processing_function(data, parameter):
    return {"processed_data": data, "status": "ok"}


@observe
def main_function(data, parameter):
    return my_data_processing_function(data, parameter)
```
</Tab>
<Tab title="TypeScript">
Nesting happens automatically via OpenTelemetry context propagation. When you create a new observation with `startActiveObservation`, it becomes a child of whatever was active at the time.

```ts
import { startActiveObservation } from "@langfuse/tracing";

await startActiveObservation("outer-process", async () => {
  await startActiveObservation("llm-step-1", async (span) => {
    span.update({ output: "LLM 1 output" });
  });

  await startActiveObservation("intermediate-step", async (span) => {
    await startActiveObservation("llm-step-2", async (child) => {
      child.update({ output: "LLM 2 output" });
    });

    span.update({ output: "Intermediate processing done" });
  });
});
```
</Tab>
</Tabs>

### Update observations

<Tabs items={["Python", "TypeScript"]}>
<Tab title="Python">
Update observation objects directly or use context-aware helpers.

```python
from langfuse import get_client

langfuse = get_client()

with langfuse.start_as_current_observation(as_type="generation", name="llm-call") as gen:
    gen.update(
        input={"prompt": "Why is the sky blue?"},
        output="Rayleigh scattering",
        usage_details={"input_tokens": 5, "output_tokens": 50},
    )

with langfuse.start_as_current_observation(as_type="span", name="data-processing"):
    langfuse.update_current_span(metadata={"step1_complete": True})
```
</Tab>
<Tab title="TypeScript">
Update the active observation or trace without holding a reference.

```ts /updateActiveTrace/
import { startActiveObservation, updateActiveTrace } from "@langfuse/tracing";

await startActiveObservation("user-request", async (span) => {
  span.update({ input: { path: "/api/process" } });

  await new Promise((resolve) => setTimeout(resolve, 50));
  const user = { id: "user-5678", name: "Jane Doe" };
  updateActiveTrace({
    userId: user.id,
    metadata: { userName: user.name },
  });

  span.update({ output: { status: "success" } }).end();
});
```
</Tab>
</Tabs>

### Add attributes to observations
Propagate attributes such as `userId`, `sessionId`, `metadata`, `version`, and `tags` to keep downstream analytics consistent. These helpers mirror the Python `propagate_attributes` context manager and the TypeScript `propagateAttributes` callback wrapper from the standalone SDK docs. Use propagation for attributes that should appear on **every** observation and `updateTrace()`/`update_current_trace()` for single-trace fields like `name`, `input`, `output`, or `public`.

**Propagatable attributes**

- `userId` / `user_id`
- `sessionId` / `session_id`
- `metadata`
- `version`
- `tags`

**Trace-only attributes** (use `updateTrace` / `update_current_trace`)

- `name`
- `input`
- `output`
- `public`

<Tabs items={["Python", "TypeScript"]}>
<Tab title="Python">
```python /propagate_attributes/
from langfuse import get_client, propagate_attributes

langfuse = get_client()

with langfuse.start_as_current_observation(as_type="span", name="user-workflow"):
    with propagate_attributes(
        user_id="user_123",
        session_id="session_abc",
        metadata={"experiment": "variant_a"},
        version="1.0",
    ):
        with langfuse.start_as_current_observation(as_type="generation", name="llm-call"):
            pass
```
</Tab>
<Tab title="TypeScript">
```ts /propagateAttributes/
import { startActiveObservation, propagateAttributes, startObservation } from "@langfuse/tracing";

await startActiveObservation("user-workflow", async () => {
  await propagateAttributes(
    {
      userId: "user_123",
      sessionId: "session_abc",
      metadata: { experiment: "variant_a", env: "prod" },
      version: "1.0",
    },
    async () => {
      const generation = startObservation("llm-call", { model: "gpt-4" }, { asType: "generation" });
      generation.end();
    }
  );
});
```
</Tab>
</Tabs>

<PropagationRestrictionsCallout attributes={[]} />

### Cross-service propagation

Use baggage propagation only when you need to forward attributes across HTTP boundaries. It pushes the values into every outbound request header, so prefer non-sensitive identifiers (session IDs, experiment versions, etc.).



<Tabs items={["Python", "TypeScript"]}>
<Tab title="Python">
```python /as_baggage=True/
from langfuse import get_client, propagate_attributes
import requests

langfuse = get_client()

with langfuse.start_as_current_observation(as_type="span", name="api-request"):
    with propagate_attributes(
        user_id="user_123",
        session_id="session_abc",
        as_baggage=True,
    ):
        requests.get("https://service-b.example.com/api")
```
</Tab>
<Tab title="TypeScript">
```ts /asBaggage/
import { propagateAttributes, startActiveObservation } from "@langfuse/tracing";

await startActiveObservation("api-request", async () => {
  await propagateAttributes(
    {
      userId: "user_123",
      sessionId: "session_abc",
      asBaggage: true,
    },
    async () => {
      await fetch("https://service-b.example.com/api");
    }
  );
});
```
</Tab>
</Tabs>

<Callout type="warning">
When baggage propagation is enabled, attributes are added to **all** outbound HTTP headers. Only use it for non-sensitive values needed for distributed tracing.
</Callout>

### Trace-level metadata & inputs/outputs [#trace-inputoutput-behavior]

By default, trace input/output mirror whatever you set on the **root observation**. Override them explicitly whenever evaluations, AB-tests, or judge models need a different payload than the root span captured.

The snippets below illustrate both the default behavior and how to call `update_current_trace` / `updateActiveTrace()` to set trace-level payloads later in the workflow.

<Callout type="info">
LLM-as-a-judge and evaluation workflows typically rely on trace-level inputs/outputs. Make sure to set them deliberately rather than relying on the root span if your evaluation payload differs.
</Callout>

<Tabs items={["Python", "TypeScript"]}>
<Tab title="Python">
Trace input/output default to the root observation. Override them explicitly when needed (e.g., for evaluations).

```python
from langfuse import get_client

langfuse = get_client()

with langfuse.start_as_current_observation(as_type="span", name="complex-pipeline") as root_span:
    root_span.update(input="Step 1 data", output="Step 1 result")
    root_span.update_trace(
        input={"original_query": "User question"},
        output={"final_answer": "Complete response", "confidence": 0.95},
    )
```

```python /update_current_trace/
from langfuse import observe, get_client

langfuse = get_client()

@observe()
def process_user_query(user_question: str):
    answer = call_llm(user_question)
    langfuse.update_current_trace(
        input={"question": user_question},
        output={"answer": answer},
    )
    return answer
```
</Tab>
<Tab title="TypeScript">
Update trace-level fields via the observation object itself.

```ts /updateTrace/
import { startObservation } from "@langfuse/tracing";

const rootSpan = startObservation("data-processing");

// ... some initial steps ...

const userId = "user-123";
const sessionId = "session-abc";
rootSpan.updateTrace({
  userId: userId,
  sessionId: sessionId,
  tags: ["authenticated-user"],
  metadata: { plan: "premium" },
});

const generation = rootSpan.startObservation(
  "llm-call",
  {},
  { asType: "generation" }
);

generation.end();

rootSpan.end();
```
</Tab>
</Tabs>

### Trace and observation IDs [#managing-trace-and-observation-ids]

Langfuse follows the W3C Trace Context standard: trace IDs are 32-character lowercase hex strings (16 bytes) and observation IDs are 16-character lowercase hex strings (8 bytes). You cannot set arbitrary observation IDs, but you can generate deterministic trace IDs to correlate with external systems.

<Tabs items={["Python", "TypeScript"]}>
<Tab title="Python">
Langfuse uses W3C Trace Context IDs. Access current IDs or create deterministic ones.

```python
from langfuse import get_client, Langfuse

langfuse = get_client()

with langfuse.start_as_current_observation(as_type="span", name="my-op") as current_op:
    trace_id = langfuse.get_current_trace_id()
    observation_id = langfuse.get_current_observation_id()
    print(trace_id, observation_id)

external_request_id = "req_12345"
deterministic_trace_id = Langfuse.create_trace_id(seed=external_request_id)
```
</Tab>
<Tab title="TypeScript">
Generate deterministic IDs or read the active trace ID.

```ts
import { createTraceId, startObservation } from "@langfuse/tracing";

const externalId = "support-ticket-54321";

const langfuseTraceId = await createTraceId(externalId);

const rootSpan = startObservation(
  "process-ticket",
  {},
  {
    parentSpanContext: {
      traceId: langfuseTraceId,
      spanId: "0123456789abcdef",
      traceFlags: 1,
    },
  }
);
```

```ts
import { startObservation, getActiveTraceId } from "@langfuse/tracing";

await startObservation("run", async (span) => {
  const traceId = getActiveTraceId();
  console.log(`Current trace ID: ${traceId}`);
});
```
</Tab>
</Tabs>

### Link to existing traces

When integrating with upstream services that already have trace IDs, supply the W3C trace context so Langfuse spans join the existing tree rather than creating a new one.

<Tabs items={["Python", "TypeScript"]}>
<Tab title="Python">
```python
from langfuse import get_client

langfuse = get_client()

existing_trace_id = "abcdef1234567890abcdef1234567890"
existing_parent_span_id = "fedcba0987654321"

with langfuse.start_as_current_observation(
    as_type="span",
    name="process-downstream-task",
    trace_context={
        "trace_id": existing_trace_id,
        "parent_span_id": existing_parent_span_id,
    },
):
    pass
```
</Tab>
<Tab title="TypeScript">
```ts
import { startObservation } from "@langfuse/tracing";

const span = startObservation(
  "downstream-task",
  {},
  {
    parentSpanContext: {
      traceId: "abcdef1234567890abcdef1234567890",
      spanId: "fedcba0987654321",
      traceFlags: 1,
    },
  }
);

span.end();
```
</Tab>
</Tabs>

### Client lifecycle & flushing

Both SDKs buffer spans in the background. Always flush or shut down the exporter in short-lived processes (scripts, serverless functions, workers) to avoid losing data.

<Tabs items={["Python", "TypeScript"]}>

<Tabs items={["Python", "TypeScript"]}>
<Tab title="Python">
Flush or shut down the client to ensure all buffered data is delivered—especially in short-lived jobs.

```python
from langfuse import get_client

langfuse = get_client()
# ... create traces ...
langfuse.flush()
langfuse.shutdown()
```
</Tab>
<Tab title="TypeScript">
Export the span processor from your OTEL setup and flush before the process exits (or use `after` in Vercel Functions).

```ts filename="instrumentation.ts" /langfuseSpanProcessor/
import { NodeSDK } from "@opentelemetry/sdk-node";
import { LangfuseSpanProcessor } from "@langfuse/otel";

export const langfuseSpanProcessor = new LangfuseSpanProcessor({
  exportMode: "immediate",
});

const sdk = new NodeSDK({
  spanProcessors: [langfuseSpanProcessor],
});

sdk.start();
```

```ts filename="handler.ts" /forceFlush/
import { langfuseSpanProcessor } from "./instrumentation";

export async function handler(event, context) {
  // ... logic ...
  await langfuseSpanProcessor.forceFlush();
}
```
</Tab>
</Tabs>

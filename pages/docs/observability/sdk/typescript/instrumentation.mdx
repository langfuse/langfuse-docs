---
title: TypeScript SDK - Instrumentation
description: Instrumentation methods for the TypeScript SDK.
label: "Version: JS SDK v4"
---

import GetStartedJsLangchain from "@/components-mdx/get-started/js-langchain.mdx";
import { BookOpen, Code } from "lucide-react";

# TypeScript SDK - Instrumentation

To instrument your application to send traces to Langfuse, you can use

1. [**Native instrumentation**](#native-instrumentation) of llm/agent libraries for out-of-the-box tracing
2. [**Custom instrumentation**](#custom-instrumentation) methods for fine-grained control
   - Context manager: `startActiveObservation`
   - Wrapper: `observe`
   - Manual: `startObservation`

These components are interoperable. Please refer to this [API route handler](https://github.com/langfuse/langfuse-docs/blob/main/components/qaChatbot/apiHandler.ts), which powers [langfuse.com/demo](/demo), as an example of how to combine the auto-instrumentation of the AI SDK V5 with custom instrumentation. This approach captures more details and groups multiple LLM calls into a single trace.

## Native instrumentation [#native-instrumentation]

Langfuse integrates with many llm/agent libraries to automatically trace your application. For a full list, see the [Langfuse Integrations](/integrations) page.

These are the most popular ones:

<LangTabs items={["OpenAI SDK", "Vercel AI SDK", "LangChainJS", "Third-party OTel Instrumentations"]}>

<Tab>

The `@langfuse/openai` package provides a wrapper to automatically trace calls to the OpenAI SDK.

For an end-to-end example, see the [Langfuse + OpenAI JS/TS Cookbook](https://langfuse.com/guides/cookbook/js_integration_openai).

**Installation:**

```bash
npm install @langfuse/openai
```

**Usage:**

The `observeOpenAI` function wraps your OpenAI client instance. All subsequent API calls made with the wrapped client will be traced as generations and nested automatically in the current trace tree. If there's no active trace in context, a new one will be created automatically.

```typescript /observeOpenAI/
import { OpenAI } from "openai";
import { observeOpenAI } from "@langfuse/openai";

// Instantiate the OpenAI client as usual
const openai = new OpenAI();

// Wrap it with Langfuse
const tracedOpenAI = observeOpenAI(openai, {
  // Pass trace-level attributes that will be applied to all calls
  traceName: "my-openai-trace",
  sessionId: "user-session-123",
  userId: "user-abc",
  tags: ["openai-integration"],
});

// Use the wrapped client just like the original
const completion = await tracedOpenAI.chat.completions.create({
  model: "gpt-4",
  messages: [{ role: "user", content: "What is OpenTelemetry?" }],
});
```

</Tab>

<Tab>

The Vercel AI SDK offers native instrumentation with OpenTelemetry. You can enable the Vercel AI SDK telemetry by passing `{ experimental_telemetry: { isEnabled: true }}` to your AI SDK function calls.

The LangfuseSpanProcessor automatically detects and handles multimodal data in your traces.

Here is a full example on how to set up tracing with the

- AI SDK v5
- Next JS
- deployed on Vercel

```ts filename="instrumentation.ts"
import { LangfuseSpanProcessor, ShouldExportSpan } from "@langfuse/otel";
import { NodeTracerProvider } from "@opentelemetry/sdk-trace-node";

// Optional: filter our NextJS infra spans
const shouldExportSpan: ShouldExportSpan = (span) => {
  return span.otelSpan.instrumentationScope.name !== "next.js";
};

const langfuseSpanProcessor = new LangfuseSpanProcessor({
  shouldExportSpan,
});

const tracerProvider = new NodeTracerProvider({
  spanProcessors: [langfuseSpanProcessor],
});

tracerProvider.register();
```

<Callout>
  If you are using Next.js, please use a manual OpenTelemetry setup via the
  `NodeTracerProvider` rather than via `registerOTel` from `@vercel/otel`. This
  is because [the `@vercel/otel` package does not yet support the OpenTelemetry
  JS SDK v2](https://github.com/vercel/otel/issues/154) on which the
  `@langfuse/tracing` and `@langfuse/otel` packages are based.
</Callout>

```ts filename="route.ts"
import { streamText } from "ai";
import { after } from "next/server";

import { openai } from "@ai-sdk/openai";
import {
  observe,
  updateActiveObservation,
  updateActiveTrace,
} from "@langfuse/tracing";
import { trace } from "@opentelemetry/api";

import { langfuseSpanProcessor } from "@/src/instrumentation";

const handler = async (req: Request) => {
  const {
    messages,
    chatId,
    userId,
  }: { messages: UIMessage[]; chatId: string; userId: string } =
    await req.json();

  // Set session id and user id on active trace
  const inputText = messages[messages.length - 1].parts.find(
    (part) => part.type === "text"
  )?.text;

  updateActiveObservation({
    input: inputText,
  });

  updateActiveTrace({
    name: "my-ai-sdk-trace",
    sessionId: chatId,
    userId,
    input: inputText,
  });

  const result = streamText({
    // ... other streamText options ...
    experimental_telemetry: {
      isEnabled: true,
    },
    onFinish: async (result) => {
      updateActiveObservation({
        output: result.content,
      });
      updateActiveTrace({
        output: result.content,
      });

      // End span manually after stream has finished
      trace.getActiveSpan().end();
    },
  });

  // Important in serverless environments: schedule flush after request is finished
  after(async () => await langfuseSpanProcessor.forceFlush());

  return result.toUIMessageStreamResponse();
};

export const POST = observe(handler, {
  name: "handle-chat-message",
  endOnExit: false, // end observation _after_ stream has finished
});
```

Learn more about the AI SDK Telemetry in the [Vercel AI SDK documentation on Telemetry](https://ai-sdk.dev/docs/ai-sdk-core/telemetry).

</Tab>

<Tab>

The `@langfuse/langchain` package offers a `CallbackHandler` to integrate Langfuse tracing into your LangChain applications.

<GetStartedJsLangchain />

<Cards num={2}>
  <Card
    icon={<BookOpen />}
    title="Documentation"
    href="/integrations/frameworks/langchain"
    arrow
  />
  <Card
    icon={<Code />}
    title="Notebook"
    href="/guides/cookbook/js_integration_langchain"
    arrow
  />
</Cards>


</Tab>

<Tab>

Many LLM and data libraries are built with OpenTelemetry support. If a library you use supports OTEL, you just need to ensure the `LangfuseSpanProcessor` is registered in your OTEL setup. All traces generated by that library will automatically be sent to Langfuse.

</Tab>

</LangTabs>

## Custom instrumentation [#custom-instrumentation]

You can add custom instrumentations to your application via

- the `observe` wrapper
- `startActiveObservation` context managers
- manually managing the observation lifecycle and its nesting with the `startObservation` function

<Callout>

For an end-to-end example, see the [JS Instrumentation Cookbook](/guides/cookbook/js_langfuse_sdk).

</Callout>

### Context management with callbacks

To simplify nesting and context management, you can use `startActiveObservation`. These functions take a callback and automatically manage the observation's lifecycle and the OpenTelemetry context. Any observation created inside the callback will automatically be nested under the active observation, and the observation will be ended when the callback finishes.

This is the recommended approach for most use cases as it prevents context leakage and ensures observations are properly ended.

```typescript /startActiveObservation/ /span.update/
import { startActiveObservation, startObservation } from "@langfuse/tracing";

await startActiveObservation(
  // name
  "user-request",
  // callback
  async (span) => {
    span.update({
      input: { query: "What is the capital of France?" },
    });

    // Example child, could also use startActiveObservation
    // This manually created generation (see docs below) will automatically be a child of "user-request"
    const generation = startObservation(
      "llm-call",
      {
        model: "gpt-4",
        input: [{ role: "user", content: "What is the capital of France?" }],
      },
      { asType: "generation" }
    );
    generation.update({
      usageDetails: { input: 10, output: 5 },
      output: { content: "The capital of France is Paris." },
    });
    generation.end();

    span.update({ output: "Successfully answered." });
  }
);
```

### `observe` wrapper

The `observe` wrapper is a powerful tool for tracing existing functions without modifying their internal logic. It acts as a decorator that automatically creates a **span** or **generation** around the function call. You can use the `updateActiveObservation` function to add attributes to the observation from within the wrapped function.

```typescript /observe/ /updateActiveObservation/
import { observe, updateActiveObservation } from "@langfuse/tracing";

// An existing function
async function fetchData(source: string) {
  updateActiveObservation({ metadata: { source: "API" } });
  // ... logic to fetch data
  return { data: `some data from ${source}` };
}

// Wrap the function to trace it
const tracedFetchData = observe(
  // method
  fetchData,
  // options, optional, see below
  {}
);

// Now, every time you call tracedFetchData, a span is created.
// Its input and output are automatically populated with the
// function's arguments and return value.
const result = await tracedFetchData("API");
```

You can configure the `observe` wrapper by passing an options object as the second argument:

| Option          | Description                                                                                                      | Default                       |
| --------------- | ---------------------------------------------------------------------------------------------------------------- | ----------------------------- |
| `name`          | The name of the observation.                                                                                     | The original function's name. |
| `asType`        | The [type of observation](/docs/observability/features/observation-types) to create (e.g. `span`, `generation`). | `"span"`                      |
| `captureInput`  | Whether to capture the function's arguments as the `input` of the observation.                                   | `true`                        |
| `captureOutput` | Whether to capture the function's return value or thrown error as the `output` of the observation.               | `true`                        |

### Manual observations

The core tracing function (`startObservation`) gives you full control over creating observations. You can pass the `asType` option to specify the [type of observation](/docs/observability/features/observation-types) to create.

When you call one of these functions, the new observation is automatically linked as a child of the currently active operation in the OpenTelemetry context. However, it does **not** make this new observation the active one. This means any further operations you trace will still be linked to the _original_ parent, not the one you just created.

To create nested observations manually, use the methods on the returned object (e.g., `parentSpan.startObservation(...)`).

```typescript /startObservation/ /end/ /asType/
import { startObservation } from "@langfuse/tracing";

// Start a root span for a user request
const span = startObservation(
  // name
  "user-request",
  // params
  {
    input: { query: "What is the capital of France?" },
  }
);

// Create a nested span for, e.g., a tool call
const toolCall = span.startObservation(
  // name
  "fetch-weather",
  // params
  {
    input: { city: "Paris" },
  },
  // Specify observation type in asType
  // This will type the attributes argument accordingly
  // Default is 'span'
  { asType: "tool" }
);

// Simulate work and end the tool call span
await new Promise((resolve) => setTimeout(resolve, 100));
toolCall.update({ output: { temperature: "15°C" } }).end();

// Create a nested generation for the LLM call
const generation = span.startObservation(
  "llm-call",
  {
    model: "gpt-4",
    input: [{ role: "user", content: "What is the capital of France?" }],
  },
  { asType: "generation" }
);

generation.update({
  usageDetails: { input: 10, output: 5 },
  output: { content: "The capital of France is Paris." },
});

generation.end();

// End the root span
span.update({ output: "Successfully answered user request." }).end();
```

<Callout type="warning" title="Manual Ending Required">
  If you use `startObservation()`, you are responsible for calling `.end()` on
  the returned observation object. Failure to do so will result in incomplete or
  missing observations in Langfuse.
</Callout>

### Updating Traces

Often, you might not have all the information about a trace (like a `userId` or `sessionId`) when you start it. The SDK lets you add or update trace-level attributes at any point during its execution.

#### `.updateTrace()` on an observation

When you create an observation manually with `startObservation`, the returned object has an `.updateTrace()` method. You can call this at any time before the root span ends to apply attributes to the entire trace.

```typescript /updateTrace/
import { startObservation } from "@langfuse/tracing";

// Start a trace without knowing the user yet
const rootSpan = startObservation("data-processing");

// ... some initial steps ...

// Later, once the user is authenticated, update the trace
const userId = "user-123";
const sessionId = "session-abc";
rootSpan.updateTrace({
  userId: userId,
  sessionId: sessionId,
  tags: ["authenticated-user"],
  metadata: { plan: "premium" },
});

// ... continue with the rest of the trace ...
const generation = rootSpan.startObservation(
  "llm-call",
  {},
  { asType: "generation" }
);

generation.end();

rootSpan.end();
```

#### `updateActiveTrace()`

<Callout type="info">
  **Note:** For `userId`, `sessionId`, `metadata`, and `version`, use
  `propagateAttributes()` (see below) to ensure these attributes are applied to
  **all observations**, not just the trace object.

  In the near-term future filtering and aggregating observations by these attributes requires them to be present on all observations, and `propagateAttributes` is the future-proof solution.
</Callout>

When you're inside a callback from `startActiveObservation`, or a function wrapped with `observe`, you might not have a direct reference to an observation object. In these cases, use the `updateActiveTrace()` function. It automatically finds the currently active trace in the context and applies the new attributes.

```typescript /updateActiveTrace/
import { startActiveObservation, updateActiveTrace } from "@langfuse/tracing";

await startActiveObservation("user-request", async (span) => {
  // Initial part of the request
  span.update({ input: { path: "/api/process" } });

  // Simulate fetching user data
  await new Promise((resolve) => setTimeout(resolve, 50));
  const user = { id: "user-5678", name: "Jane Doe" };

  // Update the active trace with the user's information
  updateActiveTrace({
    userId: user.id,
    metadata: { userName: user.name },
  });

  // ... continue logic ...
  span.update({ output: { status: "success" } }).end();
});
```


### Propagating Attributes

Certain attributes (`userId`, `sessionId`, `metadata`, `version`, `tags`) should be applied to **all spans** created within some execution scope. This is important because Langfuse aggregation queries (e.g., filtering by userId, calculating costs by sessionId) will soon operate across individual observations rather than the trace level.

Use the `propagateAttributes()` function to automatically propagate these attributes to all child observations:

```typescript /propagateAttributes/
import { startActiveObservation, propagateAttributes } from "@langfuse/tracing";

await startActiveObservation("user-workflow", async (span) => {
  await propagateAttributes(
    {
      userId: "user_123",
      sessionId: "session_abc",
      metadata: { experiment: "variant_a", env: "prod" },
      version: "1.0",
    },
    async () => {
      // All spans created here inherit these attributes
      const generation = startObservation(
        "llm-call",
        { model: "gpt-4" },
        { asType: "generation" }
      );
      // This generation automatically has userId, sessionId, metadata, version
      generation.end();
    }
  );
});
```

<Callout type="warning">
  **Important Timing:** Call `propagateAttributes()` as early as possible in
  your trace. Only the currently active observation and observations created within the
  callback will have these attributes. Pre-existing observations will NOT be updated.
</Callout>

**Attribute Restrictions:**

- **Values**: Must be strings ≤200 characters
- **Metadata Keys**: Alphanumeric characters only (no whitespace or special characters)
- **Metadata Values**: Same as other values (strings, ≤200 chars)
- Invalid values are dropped with a warning logged

**Propagatable Attributes:**

- `userId` - User identifier
- `sessionId` - Session identifier
- `metadata` - Key-value metadata (both keys and values must be strings)
- `version` - Version identifier
- `tags` - Array of tag strings

**Non-Propagatable Attributes** (use `updateTrace()` instead):

- `name` - Trace name
- `input` / `output` - Trace input/output
- `public` - Public visibility flag

#### Advanced: Cross-Service Propagation

For distributed tracing across multiple services, use the `asBaggage` parameter (see [OpenTelemetry documentation](https://opentelemetry.io/docs/concepts/signals/baggage/) for more details) to propagate attributes via HTTP headers:

```typescript /propagateAttributes/
import { propagateAttributes, startActiveObservation } from "@langfuse/tracing";

await startActiveObservation("api-request", async () => {
  await propagateAttributes(
    {
      userId: "user_123",
      sessionId: "session_abc",
      asBaggage: true, // Propagate via HTTP headers
    },
    async () => {
      // HTTP request to Service B
      const response = await fetch("https://service-b.example.com/api");
      // userId and sessionId are now in HTTP headers
    }
  );
});
```

<Callout type="warning">
  **Security Warning:** When `asBaggage: true`, attribute values are added to
  HTTP headers on ALL outbound requests. Only enable for non-sensitive values
  and when you need cross-service tracing.
</Callout>

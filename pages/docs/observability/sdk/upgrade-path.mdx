---
title: Langfuse SDK upgrade paths
description: Migration guides for Python SDK v3 → v4, v2 → v3 and JS/TS SDK v4 → v5, v3 → v4.
category: SDKs
---

# Upgrade paths

This page shows the migration guides to the latest versions of the Langfuse SDKs. Pick your SDK to follow the relevant migration steps.

- [Python SDK v3 → v4](#python-v3--v4)
- [Python SDK v2 → v3](#python-v2--v3)
- [JS/TS SDK v4 → v5](#jsts-v4--v5)
- [JS/TS SDK v3 → v4](#jsts-v3--v4)

## Python SDK

### Python v3 → v4 [#python-v3--v4]

The Python SDK v4 introduces the **[observation-centric data model](/docs/observability/data-model)**. In this model, correlating attributes (`user_id`, `session_id`, `metadata`, `tags`) propagate to every observation rather than living only on the trace. This enables single-table queries without expensive joins, significantly improving query performance at scale.

This changes how you set trace attributes: instead of imperatively updating the trace object with `update_current_trace()`, you use `propagate_attributes()` — a context manager that automatically applies attributes to the current and all child observations created within its scope.

#### Breaking Changes

**1. `update_current_trace()` decomposed into 3 methods**

In the new model, correlating attributes (`user_id`, `session_id`, `metadata`, `tags`) must live on every observation, not just the trace. This is why they move to `propagate_attributes()` — a context manager that automatically applies these attributes to the current and all child observations created within its scope.

**v3:**

```python
langfuse.update_current_trace(
    name="trace-name",
    user_id="user-123",
    session_id="session-abc",
    version="1.0",
    input={"query": "hello"},
    output={"result": "world"},
    metadata={"key": "value"},
    tags=["tag1"],
    public=True,
)
```

**v4 (decomposed):**

```python
from langfuse import observe, propagate_attributes, get_client

langfuse = get_client()

@observe()
def my_function():
    # (a) Correlating attributes → propagate_attributes() context manager
    with propagate_attributes(
        trace_name="trace-name",  # note: 'name' is now 'trace_name'
        user_id="user-123",
        session_id="session-abc",
        version="1.0",
        metadata={"key": "value"},
        tags=["tag1"],
    ):
        result = call_llm("hello")

    # (b) Trace I/O (deprecated, only for legacy trace-level LLM-as-a-judge configurations)
    langfuse.set_current_trace_io(input={"query": "hello"}, output={"result": result})

    # (c) Public flag
    langfuse.set_current_trace_as_public()
```

**Key differences:**

| Attribute                                  | v3                                      | v4                                                   |
| ------------------------------------------ | --------------------------------------- | ---------------------------------------------------- |
| `name`                                     | `update_current_trace(name=...)`        | `propagate_attributes(trace_name=...)`               |
| `user_id`, `session_id`, `tags`, `version` | `update_current_trace(...)`             | `propagate_attributes(...)`                          |
| `metadata`                                 | `update_current_trace(metadata=any)`    | `propagate_attributes(metadata=dict[str,str])`       |
| `input`, `output`                          | `update_current_trace(...)`             | `set_current_trace_io(...)` (deprecated)             |
| `public`                                   | `update_current_trace(public=True)`     | `set_current_trace_as_public()`                      |
| `release`                                  | `update_current_trace(release=...)`     | Removed — use `LANGFUSE_RELEASE` env var             |
| `environment`                              | `update_current_trace(environment=...)` | Removed — use `LANGFUSE_TRACING_ENVIRONMENT` env var |

<Callout type="warning">
  `set_current_trace_io()` is deprecated and exists only for backward
  compatibility with trace-level
  [LLM-as-a-judge](/docs/evaluation/evaluation-methods/llm-as-a-judge)
  evaluators that rely on trace input/output. For new code, set input/output on
  the root observation directly.
</Callout>

**2. `span.update_trace()` decomposed into 3 methods**

The same decomposition applies to the observation-level `update_trace()` method.

**v3:**

```python
span.update_trace(name=..., user_id=..., input=..., output=..., public=True)
```

**v4:**

```python
from langfuse import get_client, propagate_attributes

langfuse = get_client()

with langfuse.start_as_current_observation(as_type="span", name="my-operation") as span:
    with propagate_attributes(trace_name="my-trace", user_id="user-123"):
        result = call_llm("Hello")

    span.set_trace_io(input={"query": "Hello"}, output={"result": result})  # deprecated
    span.set_trace_as_public()
```

<Callout type="info">
  For integrations ([LangChain](/integrations/frameworks/langchain),
  [OpenAI](/integrations/model-providers/openai-py)), passed-in trace attributes
  now propagate to children only — they do not bubble up to the trace.
</Callout>

**3. `start_span()` / `start_generation()` → `start_observation()`**

Observations are the primary concept in the new model. The unified `start_observation()` API with `as_type` parameter replaces the separate methods.

| v3                                                              | v4                                                                                     |
| --------------------------------------------------------------- | -------------------------------------------------------------------------------------- |
| `langfuse.start_span(name="x")`                                 | `langfuse.start_observation(name="x")`                                                 |
| `langfuse.start_as_current_span(name="x")`                      | `langfuse.start_as_current_observation(name="x")`                                      |
| `langfuse.start_generation(name="x", model="gpt-4")`            | `langfuse.start_observation(name="x", as_type="generation", model="gpt-4")`            |
| `langfuse.start_as_current_generation(name="x", model="gpt-4")` | `langfuse.start_as_current_observation(name="x", as_type="generation", model="gpt-4")` |
| `span.start_span(name="x")`                                     | `span.start_observation(name="x")`                                                     |
| `span.start_as_current_span(name="x")`                          | `span.start_as_current_observation(name="x")`                                          |
| `span.start_generation(name="x")`                               | `span.start_observation(name="x", as_type="generation")`                               |
| `span.start_as_current_generation(name="x")`                    | `span.start_as_current_observation(name="x", as_type="generation")`                    |

**4. `DatasetItemClient.run()` removed → use [Experiment SDK](/docs/evaluation/experiments/experiments-via-sdk)**

The [Experiment SDK](/docs/evaluation/experiments/experiments-via-sdk) (`dataset.run_experiment()`) handles propagation of experiment attributes (run metadata, dataset item linking) under the hood.

**v3:**

```python
for item in dataset.items:
    with item.run(run_name="my-run", run_metadata={...}) as span:
        result = my_llm(item.input)
        span.update(output=result)
```

**v4:**

```python
from langfuse import get_client

dataset = get_client().get_dataset("my-dataset")

def my_task(*, item, **kwargs):
    return my_llm(item.input)

dataset.run_experiment(name="my-run", task=my_task)
```

The `DatasetItem` objects still have the same data attributes (`id`, `input`, `expected_output`, `metadata`, etc.) but the `run()` method is removed.

**5. [LangChain](/integrations/frameworks/langchain) `CallbackHandler`: `update_trace` parameter removed**

The handler now uses `propagate_attributes()` internally. The `update_trace` parameter no longer exists — passing it raises a `TypeError`.

**v3:**

```python
from langfuse.langchain import CallbackHandler

handler = CallbackHandler(update_trace=True, trace_context={...})
```

**v4:**

```python
handler = CallbackHandler(trace_context={...})
```

<Callout type="info">
  You can still set trace attributes (`user_id`, `session_id`, `tags`, etc.) by
  wrapping your LangChain call in an enclosing span with
  `propagate_attributes()`. See the [LangChain integration
  example](/docs/observability/sdk/upgrade-path#langchain-integration) in the v2
  → v3 migration guide or the [custom trace
  properties](/docs/observability/sdk/instrumentation#add-attributes)
  documentation.
</Callout>

**6. Removed types**

The following types have been removed from `langfuse.types`:

| Removed Type                             | Description                                                                                        |
| ---------------------------------------- | -------------------------------------------------------------------------------------------------- |
| `TraceMetadata`                          | TypedDict with `name`, `user_id`, `session_id`, `version`, `release`, `metadata`, `tags`, `public` |
| `ObservationParams`                      | TypedDict extending `TraceMetadata` with observation fields                                        |
| `MapValue`, `ModelUsage`, `PromptClient` | No longer re-exported from `langfuse.types`, import from `langfuse.model` instead                  |

**7. Pydantic v1 support dropped**

The SDK now requires Pydantic v2. If your application still uses Pydantic v1, you must use the `pydantic.v1` [compatibility shim](https://docs.pydantic.dev/latest/migration/#continue-using-pydantic-v1-features).

**8. Validation changes**

- propagated `metadata`: now `dict[str, str]` with values limited to 200 characters (was `Any`). Non-string values are coerced to strings. Values exceeding the limit are dropped with a warning.
- `user_id`, `session_id`: validated as strings with a maximum length of 200 characters. Values exceeding the limit are dropped with a warning.

#### Migration Checklist

1. Search for `update_current_trace` → split into `propagate_attributes()` + `set_current_trace_io()` (only when relying on legacy trace-level LLM-as-a-judge configurations) + `set_current_trace_as_public()`
2. Search for `.update_trace(` → same split on observation objects
3. Search for `start_span` / `start_generation` → replace with `start_observation`
4. Search for `item.run(` → replace with `dataset.run_experiment()`
5. Search for `CallbackHandler(update_trace=` → remove parameter
6. Verify metadata values are `dict[str, str]` with values ≤200 chars
7. Upgrade Pydantic to v2 if still on v1

### Python v2 → v3 [#python-v2--v3]

The Python SDK v3 introduces significant improvements and changes compared to the legacy v2 SDK. It is **not fully backward compatible**. This comprehensive guide will help you migrate based on your current integration.

<Callout type="info">
  You can find a snapshot of the v2 SDK documentation
  [here](https://python-sdk-v2.docs-snapshot.langfuse.com/docs/observability/sdk/python/decorators).
</Callout>

**Core Changes to SDK v2:**

- **OpenTelemetry Foundation**: v3 is built on OpenTelemetry standards
- **Trace Input/Output**: Now derived from root observation by default
- **Trace Attributes** (`user_id`, `session_id`, etc.) Can be set via enclosing spans OR directly on integrations using metadata fields (OpenAI call, Langchain invocation)
- **Context Management**: Automatic OTEL [context propagation](https://opentelemetry.io/docs/concepts/context-propagation/)

#### Migration Path by Integration Type

**`@observe` Decorator Users**

**v2 Pattern:**

```python
from langfuse.decorators import langfuse_context, observe

@observe()
def my_function():
    # This was the trace
    langfuse_context.update_current_trace(user_id="user_123")
    return "result"
```

**v3 Migration:**

```python
from langfuse import observe, get_client # new import

@observe()
def my_function():
    # This is now the root span, not the trace
    langfuse = get_client()

    # Update trace explicitly
    langfuse.update_current_trace(user_id="user_123")
    return "result"
```

**[OpenAI Integration](/integrations/model-providers/openai-py)**

**v2 Pattern:**

```python
from langfuse.openai import openai

response = openai.chat.completions.create(
    model="gpt-4o",
    messages=[{"role": "user", "content": "Hello"}],
    # Trace attributes directly on the call
    user_id="user_123",
    session_id="session_456",
    tags=["chat"],
    metadata={"source": "app"}
)
```

**v3 Migration:**

If you do not set additional trace attributes, no changes are needed.

If you set additional trace attributes, you have two options:

**Option 1: Use metadata fields (simplest migration):**

```python
from langfuse.openai import openai

response = openai.chat.completions.create(
    model="gpt-4o",
    messages=[{"role": "user", "content": "Hello"}],
    metadata={
        "langfuse_user_id": "user_123",
        "langfuse_session_id": "session_456",
        "langfuse_tags": ["chat"],
        "source": "app"  # Regular metadata still works
    }
)
```

**Option 2: Use enclosing span (for more control):**

```python
from langfuse import get_client, propagate_attributes
from langfuse.openai import openai

langfuse = get_client()

with langfuse.start_as_current_observation(as_type="span", name="chat-request") as span:

    with propagate_attributes(
        user_id="user_123",
        session_id="session_456",
        tags=["chat"],
    ):

        response = openai.chat.completions.create(
            model="gpt-4o",
            messages=[{"role": "user", "content": "Hello"}],
            metadata={"source": "app"}
        )

        # Set trace input and output explicitly
        span.update_trace(
            output={"response": response.choices[0].message.content},
            input={"query": "Hello"},
            )
```

<Callout type="warning">
  `update_trace()` is deprecated in Python SDK v4. See the [v3 → v4 migration
  guide](#python-v3--v4).
</Callout>

**[LangChain Integration](/integrations/frameworks/langchain)**

**v2 Pattern:**

```python
from langfuse.callback import CallbackHandler

handler = CallbackHandler(
    user_id="user_123",
    session_id="session_456",
    tags=["langchain"]
)

response = chain.invoke({"input": "Hello"}, config={"callbacks": [handler]})
```

**v3 Migration:**

You have two options for setting trace attributes:

**Option 1: Use metadata fields in chain invocation (simplest migration):**

```python
from langfuse.langchain import CallbackHandler

handler = CallbackHandler()

response = chain.invoke(
    {"input": "Hello"},
    config={
        "callbacks": [handler],
        "metadata": {
            "langfuse_user_id": "user_123",
            "langfuse_session_id": "session_456",
            "langfuse_tags": ["langchain"]
        }
    }
)
```

**Option 2: Use enclosing span (for more control):**

```python
from langfuse import get_client, propagate_attributes
from langfuse.langchain import CallbackHandler

langfuse = get_client()

with langfuse.start_as_current_observation(as_type="span", name="langchain-request") as span:

    with propagate_attributes(
        user_id="user_123",
        session_id="session_456",
        tags=["langchain"],
    ):

        handler = CallbackHandler()
        response = chain.invoke({"input": "Hello"}, config={"callbacks": [handler]})

        # Set trace input and output explicitly
        span.update_trace(
            input={"query": "Hello"},
            output={"response": response}
            )
```

<Callout type="warning">
  `update_trace()` is deprecated in Python SDK v4. See the [v3 → v4 migration
  guide](#python-v3--v4).
</Callout>

**[LlamaIndex Integration](/integrations/frameworks/llamaindex) Users**

**v2 Pattern:**

```python
from langfuse.llama_index import LlamaIndexCallbackHandler

handler = LlamaIndexCallbackHandler()
Settings.callback_manager = CallbackManager([handler])

response = index.as_query_engine().query("Hello")
```

**v3 Migration:**

```python
from langfuse import get_client, propagate_attributes
from openinference.instrumentation.llama_index import LlamaIndexInstrumentor

# Use third-party OTEL instrumentation
LlamaIndexInstrumentor().instrument()

langfuse = get_client()

with langfuse.start_as_current_observation(as_type="span", name="llamaindex-query") as span:

    with propagate_attributes(
        user_id="user_123",
    ):
        response = index.as_query_engine().query("Hello")

    span.update_trace(
        input={"query": "Hello"},
        output={"response": str(response)}
        )
```

<Callout type="warning">
  `update_trace()` is deprecated in Python SDK v4. See the [v3 → v4 migration
  guide](#python-v3--v4).
</Callout>

**Low-Level SDK Users**

**v2 Pattern:**

```python
from langfuse import Langfuse

langfuse = Langfuse()

trace = langfuse.trace(
    name="my-trace",
    user_id="user_123",
    input={"query": "Hello"}
)

generation = trace.generation(
    name="llm-call",
    model="gpt-4o"
)
generation.end(output="Response")
```

**v3 Migration:**

<Callout type="info">
  In v3, all spans / generations must be ended by calling `.end()` on the
  returned object.
</Callout>

```python
from langfuse import get_client, propagate_attributes

langfuse = get_client()

# Use context managers instead of manual objects
with langfuse.start_as_current_observation(
    as_type="span",
    name="my-trace",
    input={"query": "Hello"}  # Becomes trace input automatically
) as root_span:

    # Propagate trace attributes to all child observations
    with propagate_attributes(
        user_id="user_123",
    ):

        with langfuse.start_as_current_observation(
            as_type="generation",
            name="llm-call",
            model="gpt-4o"
        ) as generation:
            generation.update(output="Response")

        # If needed, override trace output
        root_span.update_trace(
            input={"query": "Hello"},
            output={"response": "Response"}
            )
```

<Callout type="warning">
  `update_trace()` is deprecated in Python SDK v4. See the [v3 → v4 migration
  guide](#python-v3--v4).
</Callout>

#### Key Migration Checklist

1. **Update Imports**:
   - Use `from langfuse import get_client` to access global client instance configured via environment variables
   - Use `from langfuse import Langfuse` to create a new client instance configured via constructor parameters
   - Use `from langfuse import observe` to import the observe decorator
   - Update integration imports: `from langfuse.langchain import CallbackHandler`

2. **Trace Attributes Pattern**:
   - **Option 1**: Use metadata fields (`langfuse_user_id`, `langfuse_session_id`, `langfuse_tags`) directly in integration calls
   - **Option 2**: Move `user_id`, `session_id`, `tags` to `propagate_attributes()`

3. **Trace Input/Output**:
   - **Critical for [LLM-as-a-judge](/docs/evaluation/evaluation-methods/llm-as-a-judge)**: Explicitly set trace input/output
   - Don't rely on automatic derivation from root observation if you need specific values

4. **Context Managers**:
   - Replace manual `langfuse.trace()`, `trace.span()` with context managers if you want to use them
   - Use [`with langfuse.start_as_current_observation()`](https://python.reference.langfuse.com/langfuse#Langfuse.start_as_current_observation) instead

5. **LlamaIndex Migration**:
   - Replace Langfuse callback with third-party OTEL instrumentation
   - Install: `pip install openinference-instrumentation-llama-index`

6. **ID Management**:
   - **No Custom Observation IDs**: v3 uses W3C Trace Context standard - you cannot set custom observation IDs
   - **Trace ID Format**: Must be 32-character lowercase hexadecimal (16 bytes)
   - **External ID Correlation**: Use [`Langfuse.create_trace_id(seed=external_id)`](https://python.reference.langfuse.com/langfuse#Langfuse.create_trace_id) to generate deterministic trace IDs from external systems

   ```python
   from langfuse import Langfuse, observe

   # v3: Generate deterministic trace ID from external system
   external_request_id = "req_12345"
   trace_id = Langfuse.create_trace_id(seed=external_request_id)

   @observe(langfuse_trace_id=trace_id)
   def my_function():
       # This trace will have the deterministic ID
       pass
   ```

7. **Initialization**:
   - Replace constructor parameters:
     - `enabled` → `tracing_enabled`
     - `threads` → `media_upload_thread_count`

8. **Datasets**

The `link` method on the dataset item objects has been replaced by a context manager that can be accessed via the `run` method on the dataset items. This is a higher level abstraction that manages trace creation and linking of the dataset item with the resulting trace.

See the [datasets documentation](/docs/evaluation/dataset-runs/remote-run) for more details.

#### Detailed Change Summary

1.  **Core Change: OpenTelemetry Foundation**
    - Built on OpenTelemetry standards for better ecosystem compatibility

2.  **Trace Input/Output Behavior**
    - **v2**: Integrations could set trace input/output directly
    - **v3**: Trace input/output derived from root observation by default
    - **Migration**: Explicitly set via `span.update_trace(input=..., output=...)`

3.  **Trace Attributes Location**
    - **v2**: Could be set directly on integration calls
    - **v3**: Must be set on enclosing spans
    - **Migration**: Wrap integration calls with [`langfuse.start_as_current_observation()`](https://python.reference.langfuse.com/langfuse#Langfuse.start_as_current_observation)

4.  **Creating Observations**:
    - **v2**: `langfuse.trace()`, `langfuse.span()`, `langfuse.generation()`
    - **v3**: `langfuse.start_as_current_observation()`
    - **Migration**: Use context managers, ensure `.end()` is called or use `with` statements

5.  **IDs and Context**:

- **v3**: W3C Trace Context format, automatic [context propagation](https://opentelemetry.io/docs/concepts/context-propagation/)
- **Migration**: Use [`langfuse.get_current_trace_id()`](https://python.reference.langfuse.com/langfuse#Langfuse.get_current_trace_id) instead of `get_trace_id()`

6.  **Event Size Limitations**:
    - **v2**: Events were limited to 1MB in size
    - **v3**: No size limits enforced on the SDK-side for events

#### Future support for v2

We will continue to support the v2 SDK for the foreseeable future with critical bug fixes and security patches. We will not be adding any new features to the v2 SDK. You can find a snapshot of the v2 SDK documentation [here](https://python-sdk-v2.docs-snapshot.langfuse.com/docs/observability/sdk/python/decorators).

## JS/TS SDK

### JS/TS v4 → v5 [#jsts-v4--v5]

The JS/TS SDK v5 introduces the **[observation-centric data model](/docs/observability/data-model)**. In this model, correlating attributes (`userId`, `sessionId`, `metadata`, `tags`) propagate to every observation rather than living only on the trace. This enables single-table queries without expensive joins, significantly improving query performance at scale.

This changes how you set trace attributes: instead of imperatively updating the trace with `updateActiveTrace()`, you use `propagateAttributes()` — a function that wraps a callback, automatically applying attributes to all child observations created within its scope.

#### Breaking Changes

**1. `updateActiveTrace()` decomposed into 3 functions**

In the new model, correlating attributes (`userId`, `sessionId`, `metadata`, `tags`) must live on every observation, not just the trace. `propagateAttributes()` wraps a callback — the current and all child spans created inside the callback automatically inherit the attributes. Spans created _before_ the callback are **not** retroactively updated.

**v4:**

```typescript
import { updateActiveTrace, startActiveObservation } from "@langfuse/tracing";

await startActiveObservation("my-operation", async (span) => {
  updateActiveTrace({
    name: "user-workflow",
    userId: "user-123",
    sessionId: "session-456",
    tags: ["production"],
    public: true,
    metadata: { testRun: "server-export" },
    input: { query: "hello" },
    output: { response: "world" },
  });
});
```

**v5:**

```typescript
import {
  propagateAttributes,
  startActiveObservation,
  setActiveTraceIO,
  setActiveTraceAsPublic,
} from "@langfuse/tracing";

await propagateAttributes(
  {
    traceName: "user-workflow", // was "name"
    userId: "user-123",
    sessionId: "session-456",
    tags: ["production"],
    metadata: { testRun: "server-export" },
  },
  async () => {
    await startActiveObservation("my-operation", async (span) => {
      setActiveTraceIO({
        input: { query: "hello" },
        output: { response: "world" },
      });
      setActiveTraceAsPublic();
    });
  },
);
```

**Key differences:**

| Attribute                                | v4                                      | v5                                                           |
| ---------------------------------------- | --------------------------------------- | ------------------------------------------------------------ |
| `name`                                   | `updateActiveTrace({name: ...})`        | `propagateAttributes({traceName: ...}, cb)`                  |
| `userId`, `sessionId`, `tags`, `version` | `updateActiveTrace({...})`              | `propagateAttributes({...}, cb)`                             |
| `metadata`                               | `updateActiveTrace({metadata: any})`    | `propagateAttributes({metadata: Record<string,string>}, cb)` |
| `input`, `output`                        | `updateActiveTrace({...})`              | `setActiveTraceIO({...})` (deprecated)                       |
| `public`                                 | `updateActiveTrace({public: true})`     | `setActiveTraceAsPublic()`                                   |
| `release`                                | `updateActiveTrace({release: ...})`     | Removed — use `LANGFUSE_RELEASE` env var                     |
| `environment`                            | `updateActiveTrace({environment: ...})` | Removed — use `LANGFUSE_TRACING_ENVIRONMENT` env var         |

<Callout type="warning">
  `setActiveTraceIO()` is deprecated and exists only for backward compatibility
  with trace-level
  [LLM-as-a-judge](/docs/evaluation/evaluation-methods/llm-as-a-judge)
  evaluators that rely on trace input/output. For new code, set input/output on
  the root observation directly.
</Callout>

**2. `.updateTrace()` → `.setTraceIO()` + `.setTraceAsPublic()`**

The same decomposition applies on all observation wrapper classes (`LangfuseSpan`, `LangfuseGeneration`, etc.).

**v4:**

```typescript
import { startObservation } from "@langfuse/tracing";

const span = startObservation("my-op");
span.updateTrace({
  name: "my-trace",
  userId: "user-123",
  sessionId: "session-456",
  tags: ["prod"],
  public: true,
  input: { query: "hello" },
  output: { response: "world" },
});
```

**v5:**

```typescript
import { propagateAttributes, startObservation } from "@langfuse/tracing";

propagateAttributes(
  {
    traceName: "my-trace",
    userId: "user-123",
    sessionId: "session-456",
    tags: ["prod"],
  },
  () => {
    const span = startObservation("my-op");
    span.setTraceIO({
      input: { query: "hello" },
      output: { response: "world" },
    });
    span.setTraceAsPublic();
    span.end();
  },
);
```

<Callout type="warning">
  `.setTraceIO()` is deprecated and exists only for backward compatibility with
  trace-level
  [LLM-as-a-judge](/docs/evaluation/evaluation-methods/llm-as-a-judge)
  evaluators that rely on trace input/output.
</Callout>

**3. [`@langfuse/langchain`](/integrations/frameworks/langchain) internal changes**

The `CallbackHandler` now uses `propagateAttributes()` for trace-level attributes. This affects users who:

- Subclass `CallbackHandler`
- Depend on the internal span-creation behavior
- Rely on `traceMetadata` accepting non-string values — non-string values are now serialized via `JSON.stringify` before being passed to `propagateAttributes`, which requires `Record<string, string>`

**4. [`@langfuse/openai`](/integrations/model-providers/openai-py) internal changes**

The `traceMethod` wrapper now wraps the traced call in `propagateAttributes()` to set `userId`, `sessionId`, `tags`, and `traceName`, instead of calling `.updateTrace()` on the observation. (If you rely on attributes being set on parent observations as well, wrap the entire execution in with `propagateAttributes`).

**5. Removed attributes**

| Removed       | Replacement                                                    |
| ------------- | -------------------------------------------------------------- |
| `release`     | Set via `LANGFUSE_RELEASE` env var                             |
| `environment` | Set via `LANGFUSE_TRACING_ENVIRONMENT` env var                 |
| `public`      | Replaced by `setActiveTraceAsPublic()` / `.setTraceAsPublic()` |

#### Migration Checklist

1. Search for `updateActiveTrace` → split into `propagateAttributes()` + `setActiveTraceIO()` (when relying on legacy trace-level LLM-as-a-judge configurations) + `setActiveTraceAsPublic()`
2. Search for `.updateTrace(` → split into `propagateAttributes()` + `.setTraceIO()` + `.setTraceAsPublic()`
3. Verify propagated metadata values are `Record<string, string>` with values ≤200 characters
4. Replace `release`/`environment` attribute usage with env vars (`LANGFUSE_RELEASE`, `LANGFUSE_TRACING_ENVIRONMENT`)

### JS/TS v3 → v4 [#jsts-v3--v4]

Please follow each section below to upgrade your application from v3 to v4.

If you encounter any questions or issues while upgrading, please raise an [issue](/issues) on GitHub.

#### Initialization

The Langfuse base URL environment variable is now `LANGFUSE_BASE_URL` and no longer `LANGFUSE_BASEURL`. For backward compatibility however, the latter will still work in v4 but not in future versions.

#### Tracing

The v4 SDK tracing is a major rewrite based on OpenTelemetry and introduces several breaking changes.

1.  **OTEL-based Architecture**: The SDK is now built on top of OpenTelemetry. An OpenTelemetry Setup is required now and done by registering the [`LangfuseSpanProcessor`](https://langfuse-js-git-main-langfuse.vercel.app/classes/_langfuse_otel.LangfuseSpanProcessor.html) with an OpenTelemetry `NodeSDK`.
2.  **New Tracing Functions**: The `langfuse.trace()`, `langfuse.span()`, and `langfuse.generation()` methods have been replaced by [`startObservation`](https://langfuse-js-git-main-langfuse.vercel.app/functions/_langfuse_tracing.startObservation.html), [`startActiveObservation`](https://langfuse-js-git-main-langfuse.vercel.app/functions/_langfuse_tracing.startActiveObservation.html), etc., from the `@langfuse/tracing` package.
3.  **Separation of Concerns**:
    - The **`@langfuse/tracing`** and **`@langfuse/otel`** packages are for tracing.
    - The **`@langfuse/client`** package and the [`LangfuseClient`](https://langfuse-js-git-main-langfuse.vercel.app/classes/_langfuse_client.LangfuseClient.html) class are now only for non-tracing features like scoring, prompt management, and datasets.

See the [SDK v4 docs](/docs/observability/sdk/overview) for details on each.

#### [Prompt Management](/docs/prompt-management/overview)

- **Import**: The import of the Langfuse client is now:

  ```typescript
  import { LangfuseClient } from "@langfuse/client";
  ```

- **Usage**: The usage of the Langfuse client is now:

  ```typescript
  const langfuse = new LangfuseClient();

  const prompt = await langfuse.prompt.get("my-prompt");

  const compiledPrompt = prompt.compile({ topic: "developers" });

  const response = await openai.chat.completions.create({
    model: "gpt-4o",
    messages: [{ role: "user", content: compiledPrompt }],
  });
  ```

- `version` is now an optional property of the options object of `langfuse.prompt.get()` instead of a positional argument.

  ```typescript
  const prompt = await langfuse.prompt.get("my-prompt", { version: "1.0" });
  ```

#### OpenAI integration

- **Import**: The import of the OpenAI integration is now:

  ```typescript
  import { observeOpenAI } from "@langfuse/openai";
  ```

- You can set the `environment` and `release` now via the `LANGFUSE_TRACING_ENVIRONMENT` and `LANGFUSE_TRACING_RELEASE` environment variables.

#### Vercel AI SDK

Works very similarly to v3, but replaces `LangfuseExporter` from `langfuse-vercel` with the regular `LangfuseSpanProcessor` from `@langfuse/otel`.

Please see [full example on usage with the AI SDK](/docs/observability/sdk/instrumentation#framework-third-party-telemetry) for more details.

<Callout>

Please note that provided tool definitions to the LLM are now mapped to `metadata.tools` and no longer in `input.tools`. This is relevant in case you are running evaluations on your generations.

</Callout>

#### [Langchain integration](/integrations/frameworks/langchain)

- **Import**: The import of the Langchain integration is now:

  ```typescript
  import { CallbackHandler } from "@langfuse/langchain";
  ```

- You can set the `environment` and `release` now via the `LANGFUSE_TRACING_ENVIRONMENT` and `LANGFUSE_TRACING_RELEASE` environment variables.

#### `langfuseClient.getTraceUrl`

- method is now asynchronous and returns a promise

  ```typescript
  const traceUrl = await langfuseClient.getTraceUrl(traceId);
  ```

#### Scoring

- **Import**: The import of the Langfuse client is now:

  ```typescript
  import { LangfuseClient } from "@langfuse/client";
  ```

- **Usage**: The usage of the Langfuse client is now:

  ```typescript
  const langfuse = new LangfuseClient();

  await langfuse.score.create({
    traceId: "trace_id_here",
    name: "accuracy",
    value: 0.9,
  });
  ```

See [custom scores documentation](/docs/evaluation/evaluation-methods/custom-scores) for new scoring methods.

#### Datasets

See [datasets documentation](/docs/evaluation/dataset-runs/remote-run#setup--run-via-sdk) for new dataset methods.

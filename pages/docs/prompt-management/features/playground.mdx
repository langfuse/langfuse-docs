---
description: Test, iterate, and compare different prompts and models within the LLM Playground.
---

# LLM Playground

Test and iterate on your prompts directly in the Langfuse Prompt Playground. Tweak the prompt and model parameters to see how different models respond to these input changes. This allows you to quickly iterate on your prompts and optimize them for the best results in your LLM app without having to switch between tools or use any code.

<CloudflareVideo
  videoId="3cfab665df39518f15fc18813cf82e3f"
  aspectRatio={16 / 10.47}
  title="LLM Playground"
/>

## Core features

### Open your prompt in the playground

You can either start from scratch or open a prompt you created with [Langfuse Prompt Management](/docs/prompt-management/get-started) in the playground.

<CloudflareVideo
  videoId="e0b7f4d5aa2da38fc9c2c36d0f2b3883"
  gifStyle
  aspectRatio={16 / 9}
/>

### Open a generation in the playground

You can open a generation in the playground by clicking the `Open in Playground` button in the generation details page.

<CloudflareVideo
  videoId="707d08e15f28fcced0acbbadd5d123df"
  gifStyle
  aspectRatio={16 / 9}
/>

### Tool calling and structured outputs

The Langfuse Playground supports tool calling and structured output schemas, enabling you to define, test, and validate LLM executions that rely on tool calls and enforce specific response formats.

<CloudflareVideo
  videoId="8a19028067239a8e9ffe929d6dfd426e"
  gifStyle
/>

**Tool Calling**

- Define custom tools with JSON schema definitions
- Test prompts relying on tools in real-time by mocking tool responses
- Save tool definitions to your project

**Structured Output**

- Enforce response formats using JSON schemas
- Save schemas to your project
- Jump into the playground from your OpenAI generation using structured output

### Add prompt variables

You can add prompt variables in the playground to simulate different inputs to your prompt.

<Frame border fullWidth>
  ![Add prompt variables](/images/docs/playground-variables.png)
</Frame>

### Select a model

You can use your favorite model by adding the API key for the model you want to use in the Langfuse project settings. 

import {
  Accordion,
  AccordionItem,
  AccordionTrigger,
  AccordionContent,
} from "@/components/ui/accordion";

<Accordion type="single" collapsible>
<AccordionItem value="supported-models">
<AccordionTrigger>Supported models</AccordionTrigger>
<AccordionContent>

Currently the playground supports the following models by default. You may configure additional custom model names when adding your LLM API Key in the Langfuse project settings, e.g. when using a custom model or proxy.

export function ModelList() {
  const openAIModels = [
    "o3",
    "o3-2025-04-16",
    "o4-mini",
    "o4-mini-2025-04-16",
    "gpt-4.1",
    "gpt-4.1-2025-04-14",
    "gpt-4.1-mini-2025-04-14",
    "gpt-4.1-nano-2025-04-14",
    "gpt-4o",
    "gpt-4o-2024-08-06",
    "gpt-4o-2024-05-13",
    "gpt-4o-mini",
    "gpt-4o-mini-2024-07-18",
    "o3-mini",
    "o3-mini-2025-01-31",
    "o1-preview",
    "o1-preview-2024-09-12",
    "o1-mini",
    "o1-mini-2024-09-12",
    "gpt-4-turbo-preview",
    "gpt-4-1106-preview",
    "gpt-4-0613",
    "gpt-4-0125-preview",
    "gpt-4",
    "gpt-3.5-turbo-16k-0613",
    "gpt-3.5-turbo-16k",
    "gpt-3.5-turbo-1106",
    "gpt-3.5-turbo-0613",
    "gpt-3.5-turbo-0301",
    "gpt-3.5-turbo-0125",
    "gpt-3.5-turbo",
  ];
  const anthropicModels = [
    "claude-3-7-sonnet-20250219",
    "claude-3-5-sonnet-20241022",
    "claude-3-5-sonnet-20240620",
    "claude-3-opus-20240229",
    "claude-3-sonnet-20240229",
    "claude-3-5-haiku-20241022",
    "claude-3-haiku-20240307",
    "claude-2.1",
    "claude-2.0",
    "claude-instant-1.2",
  ];
  const vertexAIModels = [
    "gemini-2.5-pro-exp-03-25",
    "gemini-2.0-pro-exp-02-05",
    "gemini-2.0-flash-001",
    "gemini-2.0-flash-lite-preview-02-05",
    "gemini-2.0-flash-exp",
    "gemini-1.5-pro",
    "gemini-1.5-flash",
    "gemini-1.0-pro",
  ];
  const googleAIStudioModels = [
    "gemini-2.5-pro-exp-03-25",
    "gemini-2.0-flash",
    "gemini-2.0-flash-lite-preview-02-05",
    "gemini-2.0-flash-thinking-exp-01-21",
    "gemini-1.5-pro",
    "gemini-1.5-flash",
    "gemini-1.5-flash-8b",
  ];
  return (
    <div>
      <ul>
        <li>
          <strong>Any model that supports the OpenAI API schema:</strong> The
          Playground and LLM-as-a-Judge evaluations can be used by any framework
          that supports the OpenAI API schema such as Groq, OpenRouter, LiteLLM,
          Hugging Face, and more. Just replace the API Base URL with the
          appropriate endpoint for the model you want to use and add the
          providers API keys for authentication.
        </li>
        <li>
          <strong>OpenAI / Azure OpenAI:</strong> {openAIModels.join(", ")}
        </li>
        <li>
          <strong>Anthropic:</strong> {anthropicModels.join(", ")}
        </li>
        <li>
          <strong>Google Vertex AI:</strong> {vertexAIModels.join(", ")}. You
          may also add additional model names supported by Google Vertex AI
          platform and enabled in your GCP account through the `Custom model
          names` section in the LLM API Key creation form.
        </li>
        <li>
          <strong>Google AI Studio:</strong> {vertexAIModels.join(", ")}
        </li>
        <li>
          <strong>Amazon Bedrock:</strong> All Amazon Bedrock models are
          supported. The required permission on AWS is `bedrock:InvokeModel`.
        </li>
      </ul>
    </div>
  );
}

<ModelList />
</AccordionContent>
</AccordionItem>
</Accordion>

<Frame border fullWidth>
  ![Select the model you want to use](/images/docs/playground-model-selection.png)
</Frame>

## GitHub Discussions

import { GhDiscussionsPreview } from "@/components/gh-discussions/GhDiscussionsPreview";

<GhDiscussionsPreview labels={["feat-playground"]} />

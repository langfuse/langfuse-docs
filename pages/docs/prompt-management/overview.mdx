---
title: Open Source Prompt Management
description: Manage and version your prompts in Langfuse (open source). When retrieved, they are cached by the Langfuse SDKs for low latency.
---

# Prompt Management

Prompt management is a systematic approach to storing, versioning, and retrieving prompts for your LLM application. Instead of hardcoding prompts in your application code, you manage them centrally in Langfuse.


<Frame fullWidth>
  <img src="/images/docs/prompt-management.png" alt="Prompt Management in Langfuse showing version control, playground, and deployment labels" />
</Frame>

<Callout type="info" emoji="ðŸŽ¥">

[**Watch this walkthrough**](/watch-demo?tab=prompt) of Langfuse Prompt Management and how to integrate it with your application.

</Callout>

### Decouple Prompt Updates from Code Deployment

In most LLM applications, prompt iteration and code deployment are managed by different people. Product managers and domain experts iterate on prompts, while engineers manage deployments.

With your prompts in code, when a PM wants to improve a chatbot's greeting, they need to

_ask engineering â†’ wait for code review â†’ wait for deployment â†’ test in production_

**A simple text change takes hours or days, disrupting the workflow of different people.**

When your prompts are in Langfuse, the PM updates the prompt in Langfuse UI. Your application fetches the latest version automatically. 

This **separation of concerns** enables **deploying updates instantly, without involving engineering or going through a deployment cycle**.

### No latency, no availability risk

**Langfuse Prompt Management adds no latency to your application**. Prompts are cached client-side by the SDK, so retrieving them is as fast as reading from memory. See [the caching docs page](/docs/prompt-management/features/caching) for more details.



## Getting started

Start by [adding your first prompt](/docs/prompt-management/get-started) to Langfuse, and connecting it to your application. You can either create a prompt from scratch in the UI or import existing prompts from your application.

Take a moment to understand the core concepts: [prompt types, versioning, labels, and configuration](/docs/prompt-management/data-model).

Once you have prompts in Langfuse and are using them in your application, there are a few things you can do to get the most out of Langfuse Prompt Management:
- [Link prompts to traces](/docs/prompt-management/features/link-to-traces) to analyze performance by prompt version
- [Use version control and labels](/docs/prompt-management/features/prompt-version-control) to manage deployments across environments

Looking for something specific? Take a look under _Features_ for guides on specific topics.

---
title: Open Source Prompt Management
description: Manage and version your prompts in Langfuse (open source). When retrieved, they are cached by the Langfuse SDKs for low latency.
---

# Prompt Management

Prompt management is a systematic approach to storing, versioning and retrieving prompts in LLM applications.

<Frame border fullWidth>
  <img src="/images/docs/prompt-management.png" alt="Prompt Management" />
</Frame>

## Why Prompt Management?

- Involve non-technical users through UI
- Decoupling: edit prompts without redeploying
- Quickly rollback to a previous version of a prompt
- Compare different prompt versions side-by-side
- No latency impact after first use of a prompt due to [client-side caching](/docs/prompt-management/features/caching)

## Where to start

Follow the quickstart to create your first prompt and use it in your application.

import {
  Rocket,
  BookOpen,
  Joystick,
  Layers,
  MessageSquare,
  Tag,
  GitPullRequestArrow,
  Diff,
  PlaySquare,
  Link,
  Folder,
  Settings,
  TestTube,
  Zap,
  Gauge,
  BarChart3,
  Webhook,
  Server,
  Workflow,
} from "lucide-react";

<Cards num={1}>
  <Card
    icon={<Rocket size="24" />}
    title="Quickstart"
    href="/docs/prompt-management/get-started"
    arrow
  />
  <Card
    icon={<BookOpen size="24" />}
    title="Data Model"
    href="/docs/prompt-management/data-model"
    arrow
  />
</Cards>

## Core Features

Essential prompt management features that form the foundation of prompt operations in Langfuse:

<Cards num={2}>
   <Card
    title="Prompt Version Control"
    href="/docs/prompt-management/features/prompt-version-control"
    icon={<GitPullRequestArrow />}
    arrow
  />
  <Card
    title="Composability"
    href="/docs/prompt-management/features/composability"
    icon={<Layers />}
    arrow
  />
  <Card
    title="Message Placeholders"
    href="/docs/prompt-management/features/message-placeholders"
    icon={<MessageSquare />}
    arrow
  />
  <Card
    title="Changes"
    href="/docs/prompt-management/features/changes"
    icon={<Diff />}
    arrow
  />
  <Card
    title="Playground"
    href="/docs/prompt-management/features/playground"
    icon={<PlaySquare />}
    arrow
  />
</Cards>

## Advanced Features

Extended capabilities for sophisticated prompt management and analysis:

<Cards num={2}>
  <Card
    title="Link Tracing"
    href="/docs/prompt-management/features/link-tracing"
    icon={<Link />}
    arrow
  />
  <Card
    title="Folders"
    href="/docs/prompt-management/features/folders"
    icon={<Folder />}
    arrow
  />
  <Card
    title="Config"
    href="/docs/prompt-management/features/config"
    icon={<Settings />}
    arrow
  />
  <Card
    title="A/B Testing"
    href="/docs/prompt-management/features/a-b-testing"
    icon={<TestTube />}
    arrow
  />
  <Card
    title="Caching"
    href="/docs/prompt-management/features/caching"
    icon={<Zap />}
    arrow
  />
  <Card
    title="Availability"
    href="/docs/prompt-management/features/guaranteed-availability"
    icon={<Gauge />}
    arrow
  />
  <Card
    title="Metrics"
    href="/docs/prompt-management/features/metrics"
    icon={<BarChart3 />}
    arrow
  />
  <Card
    title="Webhooks"
    href="/docs/prompt-management/features/webhooks"
    icon={<Webhook />}
    arrow
  />
  <Card
    title="MCP Server"
    href="/docs/prompt-management/features/mcp-server"
    icon={<Server />}
    arrow
  />
  <Card
    title="n8n Node"
    href="/docs/prompt-management/features/n8n-node"
    icon={<Workflow />}
    arrow
  />
</Cards>

## Prompt Engineering FAQ

import {
  Accordion,
  AccordionItem,
  AccordionTrigger,
  AccordionContent,
} from "@/components/ui/accordion";

<Accordion type="single" collapsible>
<AccordionItem value="what-is-prompt-engineering">
<AccordionTrigger>What is prompt engineering?</AccordionTrigger>
<AccordionContent>

Prompt engineering is the practice of designing and refining prompts to effectively communicate with and guide AI models, particularly large language models (LLMs), to produce desired outputs.

Prompt engineering involves:

1. Crafting clear and specific instructions for AI models
2. Utilizing techniques like role assignment, few-shot prompting, and chain-of-thought reasoning
3. Optimizing prompts for different applications such as text generation, summarization, and problem-solving
4. Understanding the capabilities and limitations of AI models
5. Iteratively refining prompts to improve output quality and reliability
6. Applying various advanced techniques like self-consistency, generated knowledge, and least-to-most prompting
7. Considering ethical implications and potential biases in prompt design

By using Langfuse prompt management, you can version and manage your prompts collaboratively to execute on the steps above.

Recommended readings:

- [Learn Prompting Documentation](https://learnprompting.org/docs)
- [The Prompt Report](https://arxiv.org/abs/2406.06608)
- [Prompting Fundamentals and How to Apply them Effectively](https://eugeneyan.com/writing/prompting/)

</AccordionContent>
</AccordionItem>
<AccordionItem value="how-to-measure-prompt-performance">
<AccordionTrigger>How to measure prompt performance?</AccordionTrigger>
<AccordionContent>

Depending on the scale of your experiments, there are different approaches you can take:

1. [Playground](/docs/playground) for single prompt experiments in UI.
2. [Releases and Versioning](/docs/tracing-features/releases-and-versioning) for A/B testing and structured experiments in production.
3. [Datasets](/docs/datasets/overview) for offline/dev benchmarking prompts or applications on a set of reference inputs.

</AccordionContent>
</AccordionItem>
</Accordion>

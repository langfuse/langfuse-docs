---
title: Open Source Prompt Management
description: Manage and version your prompts in Langfuse (open source). When retrieved, they are cached by the Langfuse SDKs for low latency.
---

# What is prompt management?

Prompt management is a systematic approach to storing, versioning and retrieving prompts in LLM applications.

## Why prompt management?

- Involve non-technical users through UI
- Decoupling: edit prompts without redeploying
- Quickly rollback to a previous version of a prompt
- Compare different prompt versions side-by-side
- No latency impact after first use of a prompt due to [client-side caching](/docs/prompt-management/features/caching)

## How it looks

import PromptOverview from "@/components-mdx/prompt-overview-gifs.mdx";

<PromptOverview />

## Where to start

Follow the quickstart to create your first prompt and use it in your application.

import { Rocket, BookOpen, Joystick } from "lucide-react";

<Cards num={1}>
  <Card
    icon={<Rocket size="24" />}
    title="Quickstart"
    href="/docs/prompt-management/get-started"
    arrow
  />
  <Card
    icon={<BookOpen size="24" />}
    title="Data Model"
    href="/docs/prompt-management/data-model"
    arrow
  />
</Cards>

## Prompt Engineering FAQ

import {
  Accordion,
  AccordionItem,
  AccordionTrigger,
  AccordionContent,
} from "@/components/ui/accordion";

<Accordion>
<AccordionItem value="what-is-prompt-engineering">
<AccordionTrigger>What is prompt engineering?</AccordionTrigger>
<AccordionContent>

Prompt engineering is the practice of designing and refining prompts to effectively communicate with and guide AI models, particularly large language models (LLMs), to produce desired outputs.

Prompt engineering involves:

1. Crafting clear and specific instructions for AI models
2. Utilizing techniques like role assignment, few-shot prompting, and chain-of-thought reasoning
3. Optimizing prompts for different applications such as text generation, summarization, and problem-solving
4. Understanding the capabilities and limitations of AI models
5. Iteratively refining prompts to improve output quality and reliability
6. Applying various advanced techniques like self-consistency, generated knowledge, and least-to-most prompting
7. Considering ethical implications and potential biases in prompt design

By using Langfuse prompt management, you can version and manage your prompts collaboratively to execute on the steps above.

Recommended readings:

- [Learn Prompting Documentation](https://learnprompting.org/docs)
- [The Prompt Report](https://arxiv.org/abs/2406.06608)
- [Prompting Fundamentals and How to Apply them Effectively](https://eugeneyan.com/writing/prompting/)

</AccordionContent>
</AccordionItem>
<AccordionItem value="how-to-measure-prompt-performance">
<AccordionTrigger>How to measure prompt performance?</AccordionTrigger>
<AccordionContent>

Depending on the scale of your experiments, there are different approaches you can take:

1. [Playground](/docs/playground) for single prompt experiments in UI.
2. [Releases and Versioning](/docs/tracing-features/releases-and-versioning) for A/B testing and structured experiments in production.
3. [Datasets](/docs/datasets/overview) for offline/dev benchmarking prompts or applications on a set of reference inputs.

</AccordionContent>
</AccordionItem>
</Accordion>

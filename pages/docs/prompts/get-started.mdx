---
title: Open Source Prompt Management
description: Manage and version your prompts in Langfuse (open source). When retrieved, they are cached by the Langfuse SDKs for low latency.
---

# Prompt Management

Use Langfuse to effectively **manage** and **version** your prompts. Langfuse prompt management is a **Prompt CMS** (Content Management System).

import PromptOverview from "@/components-mdx/prompt-overview-gifs.mdx";

<PromptOverview />

## What is prompt management?

**Prompt management is a systematic approach to storing, versioning and retrieving prompts in LLM applications.** Key aspects of prompt management include version control, decoupling prompts from code, monitoring, logging and optimizing prompts as well as integrating prompts with the rest of your application and tool stack.

## Why use prompt management?

> Can't I just hardcode my prompts in my application and track them in Git? Yes, well... you can and all of us have done it.

Typical benefits of using a CMS apply here:

- Decoupling: deploy new prompts without redeploying your application.
- Non-technical users can create and update prompts via Langfuse Console.
- Quickly rollback to a previous version of a prompt.
- Compare different prompt versions side-by-side.

Platform benefits:

- Track performance of prompt versions in [Langfuse Tracing](/docs/tracing).

Performance benefits compared to other implementations:

- No latency impact after first use of a prompt due to client-side caching and asynchronous cache refreshing.
- Support for text and chat prompts.
- Edit/manage via UI, SDKs, or API.

## Prompt Engineering FAQ

import {
  Accordion,
  AccordionItem,
  AccordionTrigger,
  AccordionContent,
} from "@/components/ui/accordion";

<Accordion>
<AccordionItem value="what-is-prompt-engineering">
<AccordionTrigger>What is prompt engineering?</AccordionTrigger>
<AccordionContent>

Prompt engineering is the practice of designing and refining prompts to effectively communicate with and guide AI models, particularly large language models (LLMs), to produce desired outputs.

Prompt engineering involves:

1. Crafting clear and specific instructions for AI models
2. Utilizing techniques like role assignment, few-shot prompting, and chain-of-thought reasoning
3. Optimizing prompts for different applications such as text generation, summarization, and problem-solving
4. Understanding the capabilities and limitations of AI models
5. Iteratively refining prompts to improve output quality and reliability
6. Applying various advanced techniques like self-consistency, generated knowledge, and least-to-most prompting
7. Considering ethical implications and potential biases in prompt design

By using Langfuse prompt management, you can version and manage your prompts collaboratively to execute on the steps above.

Recommended readings:

- [Learn Prompting Documentation](https://learnprompting.org/docs)
- [The Prompt Report](https://arxiv.org/abs/2406.06608)
- [Prompting Fundamentals and How to Apply them Effectively](https://eugeneyan.com/writing/prompting/)

</AccordionContent>
</AccordionItem>
<AccordionItem value="how-to-measure-prompt-performance">
<AccordionTrigger>How to measure prompt performance?</AccordionTrigger>
<AccordionContent>

Depending on the scale of your experiments, there are different approaches you can take:

1. [Playground](/docs/playground) for single prompt experiments in UI.
2. [Releases and Versioning](/docs/tracing-features/releases-and-versioning) for A/B testing and structured experiments in production.
3. [Datasets](/docs/datasets/overview) for offline/dev benchmarking prompts or applications on a set of reference inputs.

</AccordionContent>
</AccordionItem>
</Accordion>

## Langfuse prompt object

```json filename="Example prompt in Langfuse with custom config"
{
  "name": "movie-critic",
  "type": "text",
  "prompt": "As a {{criticLevel}} movie critic, do you like {{movie}}?",
  "config": {
    "model": "gpt-3.5-turbo",
    "temperature": 0.5,
    "supported_languages": ["en", "fr"]
  },
  "version": 1,
  "labels": ["production", "staging", "latest"],
  "tags": ["movies"]
}
```

- `name`: Unique name of the prompt within a Langfuse project.
- `type`: The type of the prompt content (`text` or `chat`). Default is `text`.
- `prompt`: The text template with variables (e.g. `This is a prompt with a {{variable}}`). For chat prompts, this is a list of chat messages each with `role` and `content`.
- `config`: Optional JSON object to store any parameters (e.g. model parameters or model tools).
- `version`: Integer to indicate the version of the prompt. The version is automatically incremented when creating a new prompt version.
- `labels`: [Labels](#labels) that can be used to fetch specific prompt versions in the SDKs.
  - When using a prompt without specifying a label, Langfuse will serve the version with the `production` label.
  - `latest` points to the most recently created version.
  - You can create any additional labels, e.g. for different environments (`staging`, `production`) or tenants (`tenant-1`, `tenant-2`).

## How it works

<Steps>

### Create/update prompt [#create-update-prompt]

If you already have a prompt with the same `name`, the prompt will be added as a new version.

import PromptCreate from "@/components-mdx/prompt-create.mdx";

<PromptCreate />

### Use prompt [#use-prompt]

At runtime, you can fetch the latest production version from Langfuse.

<Tabs items={["Python", "JS/TS", "Langchain (Python)", "Langchain (JS)"]}>
<Tab>

```python
from langfuse import Langfuse

# Initialize Langfuse client
langfuse = Langfuse()

# Get current `production` version of a text prompt
prompt = langfuse.get_prompt("movie-critic")

# Insert variables into prompt template
compiled_prompt = prompt.compile(criticlevel="expert", movie="Dune 2")
# -> "As an expert movie critic, do you like Dune 2?"
```

Chat prompts

```python
# Get current `production` version of a chat prompt
chat_prompt = langfuse.get_prompt("movie-critic-chat", type="chat") # type arg infers the prompt type (default is 'text')

# Insert variables into chat prompt template
compiled_chat_prompt = chat_prompt.compile(criticlevel="expert", movie="Dune 2")
# -> [{"role": "system", "content": "You are an expert movie critic"}, {"role": "user", "content": "Do you like Dune 2?"}]
```

Optional parameters

```python
# Get specific version
prompt = langfuse.get_prompt("movie-critic", version=1)

# Get specific label
prompt = langfuse.get_prompt("movie-critic", label="staging")

# Get latest prompt version. The 'latest' label is automatically maintained by Langfuse.
prompt = langfuse.get_prompt("movie-critic", label="latest")

# Extend cache TTL from default 60 to 300 seconds
prompt = langfuse.get_prompt("movie-critic", cache_ttl_seconds=300)

# Number of retries on fetching prompts from the server. Default is 2.
prompt = langfuse.get_prompt("movie-critic", max_retries=3)

# Timeout per call to the Langfuse API in seconds. Default is 20.
prompt = langfuse.get_prompt("movie-critic", fetch_timeout_seconds=3)
```

Attributes

```python
# Raw prompt including {{variables}}. For chat prompts, this is a list of chat messages.
prompt.prompt

# Config object
prompt.config
```

</Tab>

<Tab>

```ts
import { Langfuse } from "langfuse";

// Iniitialize the Langfuse client
const langfuse = new Langfuse();

// Get current `production` version
const prompt = await langfuse.getPrompt("movie-critic");

// Insert variables into prompt template
const compiledPrompt = prompt.compile({
  criticlevel: "expert",
  movie: "Dune 2",
});
// -> "As an expert movie critic, do you like Dune 2?"
```

Chat prompts

```ts
// Get current `production` version of a chat prompt
const chatPrompt = await langfuse.getPrompt("movie-critic-chat", undefined, {
  type: "chat",
}); // type option infers the prompt type (default is 'text')

// Insert variables into chat prompt template
const compiledChatPrompt = chatPrompt.compile({
  criticlevel: "expert",
  movie: "Dune 2",
});
// -> [{"role": "system", "content": "You are an expert movie critic"}, {"role": "user", "content": "Do you like Dune 2?"}]
```

Optional parameters

```ts
// Get specific version of a prompt (here version 1)
const prompt = await langfuse.getPrompt("movie-critic", 1);

// Get specific label
const prompt = await langfuse.getPrompt("movie-critic", undefined, {
  label: "staging",
});

// Get latest prompt version. The 'latest' label is automatically maintained by Langfuse.
const prompt = await langfuse.getPrompt("movie-critic", undefined, {
  label: "latest",
});

// Extend cache TTL from default 1 to 5 minutes
const prompt = await langfuse.getPrompt("movie-critic", undefined, {
  cacheTtlSeconds: 300,
});

// Number of retries on fetching prompts from the server. Default is 2.
const promptWithMaxRetries = await langfuse.getPrompt(
  "movie-critic",
  undefined,
  {
    maxRetries: 5,
  }
);

// Timeout per call to the Langfuse API in milliseconds. Default is 10 seconds.
const promptWithFetchTimeout = await langfuse.getPrompt(
  "movie-critic",
  undefined,
  {
    fetchTimeoutMs: 5000,
  }
);
```

Attributes

```ts
// Raw prompt including {{variables}}. For chat prompts, this is a list of chat messages.
prompt.prompt;

// Config object
prompt.config;
```

</Tab>

<Tab>

As Langfuse and Langchain process input variables of prompt templates differently (`{}` instead of `{{}}`), we provide the `prompt.get_langchain_prompt()` method to transform the Langfuse prompt into a string that can be used with Langchain's PromptTemplate. You can pass optional keyword arguments to `prompt.get_langchain_prompt(**kwargs)` in order to precompile some variables and handle the others with Langchain's PromptTemplate.

```python
from langfuse import Langfuse
from langchain_core.prompts import ChatPromptTemplate

# Initialize Langfuse client
langfuse = Langfuse()

# Get current `production` version
langfuse_prompt = langfuse.get_prompt("movie-critic")

# Example using ChatPromptTemplate
langchain_prompt = ChatPromptTemplate.from_template(langfuse_prompt.get_langchain_prompt())

# Example using ChatPromptTemplate with pre-compiled variables.
langchain_prompt = ChatPromptTemplate.from_template(langfuse_prompt.get_langchain_prompt(strictness='tough'))
```

Chat prompts

```python
# Get current `production` version of a chat prompt
langfuse_prompt = langfuse.get_prompt("movie-critic-chat", type="chat")

# Create a Langchain ChatPromptTemplate from the Langfuse prompt chat messages
langchain_prompt = ChatPromptTemplate.from_messages(langfuse_prompt.get_langchain_prompt())
```

Optional parameters

```python
# Get specific version
prompt = langfuse.get_prompt("movie-critic", version=1)

# Get specific label
prompt = langfuse.get_prompt("movie-critic", label="staging")

# Get latest prompt version. The 'latest' label is automatically maintained by Langfuse.
prompt = langfuse.get_prompt("movie-critic", label="latest")

# Extend cache TTL from default 60 to 300 seconds
prompt = langfuse.get_prompt("movie-critic", cache_ttl_seconds=300)
```

Attributes

```python
# Raw prompt including {{variables}}. For chat prompts, this is a list of chat messages.
prompt.prompt

# Config object
prompt.config
```

</Tab>

<Tab>

As Langfuse and Langchain process input variables of prompt templates differently (`{}` instead of `{{}}`), we provide the `prompt.getLangchainPrompt()` method to transform the Langfuse prompt into a string that can be used with Langchain's PromptTemplate.

```ts
import { Langfuse } from "langfuse";
import { ChatPromptTemplate } from "@langchain/core/prompts";

const langfuse = new Langfuse();

// Get current `production` version
const langfusePrompt = await langfuse.getPrompt("movie-critic");

// Example using ChatPromptTemplate
const promptTemplate = PromptTemplate.fromTemplate(
  langfusePrompt.getLangchainPrompt()
);
```

Chat prompts

```ts
// Get current `production` version of a chat prompt
const langfusePrompt = await langfuse.getPrompt(
  "movie-critic-chat",
  undefined,
  { type: "chat" }
);

// Example using ChatPromptTemplate
const promptTemplate = ChatPromptTemplate.fromMessages(
  langfusePrompt.getLangchainPrompt().map((msg) => [msg.role, msg.content])
);
```

Optional parameters

```ts
// Get specific version of a prompt (here version 1)
const prompt = await langfuse.getPrompt("movie-critic", 1);

// Get specific label
const prompt = await langfuse.getPrompt("movie-critic", undefined, {
  label: "staging",
});

// Get latest prompt version. The 'latest' label is automatically maintained by Langfuse.
const prompt = await langfuse.getPrompt("movie-critic", undefined, {
  label: "latest",
});

// Extend cache TTL from default 1 to 5 minutes
const prompt = await langfuse.getPrompt("movie-critic", undefined, {
  cacheTtlSeconds: 300,
});
```

Attributes

```ts
// Raw prompt including {{variables}}. For chat prompts, this is a list of chat messages.
prompt.prompt;

// Config object
prompt.config;
```

</Tab>

</Tabs>

### Link with Langfuse Tracing (optional)

Add the prompt object to the `generation` call in the SDKs to link the generation in [Langfuse Tracing](/docs/tracing) to the prompt version. This linkage enables tracking of metrics by prompt version and name, such as "movie-critic", directly in the Langfuse UI. Metrics like scores per prompt version provide insights into how modifications to prompts impact the quality of the generations. If a [fallback prompt](#fallback) is used, no link will be created.

This is currently unavailable when using the LlamaIndex integration.

<Tabs items={["Python SDK", "JS/TS SDK", "OpenAI SDK (Python)", "OpenAI SDK (JS/TS)", "Langchain (Python)", "Langchain (JS/TS)", "Vercel AI SDK"]}>
<Tab>

Decorators

```python
from langfuse.decorators import langfuse_context, observe

@observe(as_type="generation")
def nested_generation():
    prompt = langfuse.get_prompt("movie-critic")

    langfuse_context.update_current_observation(
        prompt=prompt,
    )

@observe()
def main():
  nested_generation()

main()
```

Low-level SDK

```diff
langfuse.generation(
    ...
+   prompt=prompt
    ...
)
```

</Tab>

<Tab>

```diff
langfuse.generation({
    ...
+   prompt: prompt
    ...
})
```

</Tab>

<Tab>

```python /langfuse_prompt=prompt/
prompt = langfuse.get_prompt("calculator")

openai.chat.completions.create(
  model="gpt-3.5-turbo",
  messages=[
    {"role": "system", "content": prompt.compile(base=10)},
    {"role": "user", "content": "1 + 1 = "}],
  langfuse_prompt=prompt
)
```

</Tab>

<Tab>

```ts /langfusePrompt,/
import { observeOpenAI } from "langfuse";
import OpenAI from "openai";

const langfusePrompt = await langfuse.getPrompt("prompt-name"); // Fetch a previously created prompt

const res = await observeOpenAI(new OpenAI(), {
  langfusePrompt,
}).completions.create({
  prompt: langfusePrompt.prompt,
  model: "gpt-3.5-turbo-instruct",
  max_tokens: 300,
});
```

</Tab>

<Tab>

```python /"langfuse_prompt"/
from langfuse import Langfuse
from langchain_core.prompts import ChatPromptTemplate, PromptTemplate
from langchain_openai import ChatOpenAI, OpenAI

langfuse = Langfuse()

# Text prompts:
langfuse_text_prompt = langfuse.get_prompt("movie-critic")

## Pass the langfuse_text_prompt to the PromptTemplate as metadata to link it to generations that use it
langchain_text_prompt = PromptTemplate.from_template(
    langfuse_text_prompt.get_langchain_prompt(),
    metadata={"langfuse_prompt": langfuse_text_prompt},
)

## Use the text prompt in a Langchain chain
llm = OpenAI()
completion_chain = langchain_text_prompt | llm

completion_chain.invoke({"movie": "Dune 2", "criticlevel": "expert"})

# Chat prompts:
langfuse_chat_prompt = langfuse.get_prompt("movie-critic-chat", type="chat")

## Manually set the metadata on the langchain_chat_prompt to link it to generations that use it
langchain_chat_prompt = ChatPromptTemplate.from_messages(
    langfuse_chat_prompt.get_langchain_prompt()
)
langchain_chat_prompt.metadata = {"langfuse_prompt": langfuse_chat_prompt}

## or use the ChatPromptTemplate constructor directly.
## Note that using ChatPromptTemplate.from_template led to issues in the past
## See: https://github.com/langfuse/langfuse/issues/5374
langchain_chat_prompt = ChatPromptTemplate(
    langfuse_chat_prompt.get_langchain_prompt(),
    metadata={"langfuse_prompt": langfuse_prompt}
)

## Use the chat prompt in a Langchain chain
chat_llm = ChatOpenAI()
chat_chain = langchain_chat_prompt | chat_llm

chat_chain.invoke({"movie": "Dune 2", "criticlevel": "expert"})
```

<Callout type="info">
  If you use the `with_config` method on the PromptTemplate to create a new
  Langchain Runnable with updated config, please make sure to pass the
  `langfuse_prompt` in the `metadata` key as well.
</Callout>

<Callout type="info">
  Set the `langfuse_prompt` metadata key only on PromptTemplates and not
  additionally on the LLM calls or elsewhere in your chains.
</Callout>

</Tab>

<Tab>

```ts /metadata: { langfusePrompt:/
import { Langfuse } from "langfuse";
import { PromptTemplate } from "@langchain/core/prompts";
import { ChatOpenAI, OpenAI } from "@langchain/openai";

const langfuse = new Langfuse();

// Text prompts
const langfuseTextPrompt = await langfuse.getPrompt("movie-critic"); // Fetch a previously created text prompt

// Pass the langfuseTextPrompt to the PromptTemplate as metadata to link it to generations that use it
const langchainTextPrompt = PromptTemplate.fromTemplate(
  langfuseTextPrompt.getLangchainPrompt()
).withConfig({
  metadata: { langfusePrompt: langfuseTextPrompt },
});

const model = new OpenAI();
const chain = langchainTextPrompt.pipe(model);

await chain.invoke({ movie: "Dune 2", criticlevel: "expert" });

// Chat prompts
const langfuseChatPrompt = await langfuse.getPrompt(
  "movie-critic-chat",
  undefined,
  {
    type: "chat",
  }
); // type option infers the prompt type as chat (default is 'text')

const langchainChatPrompt = ChatPromptTemplate.fromMessages(
  langfuseChatPrompt.getLangchainPrompt().map((m) => [m.role, m.content])
).withConfig({
  metadata: { langfusePrompt: langfuseChatPrompt },
});

const chatModel = new ChatOpenAI();
const chatChain = langchainChatPrompt.pipe(chatModel);

await chatChain.invoke({ movie: "Dune 2", criticlevel: "expert" });
```

</Tab>

<Tab>

Link Langfuse prompts to Vercel AI SDK generations by setting the `langfusePrompt` property in the `metadata` field:

```typescript /langfusePrompt: fetchedPrompt.toJSON()/
import { generateText } from "ai";
import { Langfuse } from "langfuse";

const langfuse = new Langfuse();

const fetchedPrompt = await langfuse.getPrompt("my-prompt");

const result = await generateText({
  model: openai("gpt-4o"),
  prompt: fetchedPrompt.prompt,
  experimental_telemetry: {
    isEnabled: true,
    metadata: {
      langfusePrompt: fetchedPrompt.toJSON(),
    },
  },
});
```

</Tab>

</Tabs>

### Rollbacks (optional)

When a prompt has a `production` label, then that version will be served by default in the SDKs. You can quickly rollback to a previous version by setting the `production` label to that previous version in the Langfuse UI.

</Steps>

## End-to-end examples

The following example notebooks include end-to-end examples of prompt management:

import { Terminal, FileCode } from "lucide-react";

<Cards num={3}>
  <Card
    title="Example OpenAI Functions"
    href="/docs/prompts/example-openai-functions"
    icon={<FileCode />}
  />
  <Card
    title="Example Langchain (Python)"
    href="/docs/prompts/example-langchain"
    icon={<FileCode />}
  />
  <Card
    title="Example Langchain (JS/TS)"
    href="/docs/prompts/example-langchain-js"
    icon={<FileCode />}
  />
</Cards>

We also used Prompt Management for our Docs Q&A Chatbot and traced it with Langfuse. You can get view-only access to the project by signing up to the [public demo](/docs/demo).

## Prompt Deployment Labels [#labels]

Labels can be used to fetch specific prompt versions in the SDKs. When [using a prompt](#use-prompt) without specifying a label, Langfuse will serve the version with the `production` label. The `latest` label points to the most recently created version. You can create any additional labels, for example for different environments (`staging`, `production`), tenants (`tenant-1`, `tenant-2`), or experiments (`prod-a`, `prod-b`).

**How to assign a label to a prompt:**

<Tabs items={["UI","Python", "JS/TS"]}>
<Tab>

<CloudflareVideo
  videoId="2141209791bd771000103d9703e2557b"
  aspectRatio={1696 / 1080}
  gifStyle
/>

</Tab>
<Tab>

You can assign labels when [creating a new prompt (version)](#create-update-prompt).

Alternatively, you can also update the labels of an existing prompt version:

```python
langfuse = Langfuse()
langfuse.update_prompt(
    name="movie-critic",
    version=1,
    new_labels=["john", "doe"], # assign these labels to the prompt version
)
```

</Tab>
<Tab>

You can assign labels when [creating a new prompt (version)](#create-update-prompt).

Alternatively, you can also update the labels of an existing prompt version:

```ts
langfuse = Langfuse();
await langfuse.updatePrompt({
  name: "movie-critic",
  version: 1,
  newLabels: ["john", "doe"],
});
```

</Tab>
</Tabs>

### Protected prompt labels

<AvailabilityBanner
  availability={{
    hobby: "not-available",
    core: "not-available",
    pro: "team-add-on",
    enterprise: "full",
    selfHosted: "ee",
  }}
/>

Protected prompt labels give project admins and owners ([RBAC docs](/docs/rbac)) the ability to prevent labels from being modified or deleted, ensuring better control over prompt deployment.

Once a label such as `production` is marked as protected:

- `viewer` and `member` roles cannot modify or delete the label from prompts, preventing changes to the `production` prompt version.
- `admin` and `owner` roles can still modify or delete it, effectively changing the `production` prompt version

Admins and owners can update a label's protection status in the project settings.

<CloudflareVideo
  videoId="512685314ff082abb62233baba411395"
  aspectRatio={16 / 9}
  gifStyle
/>

## Prompt Composability [#composability]

<CloudflareVideo
  videoId="6e9c25ba3d4c72363680af5dfb6a9bd2"
  aspectRatio={16 / 9}
/>

You can reference other **text** prompts in your promptsusing a simple tag format:

```
@@@langfusePrompt:name=PromptName|version=1@@@
```

or using a label instead of a specific version for dynamic resolution:

```
@@@langfusePrompt:name=PromptName|label=production@@@
```

When creating the prompt via the Langfuse UI, you can use the `Add prompt reference` button to insert a prompt reference tag into your prompt.

When fetched via the SDK / API, these tags are automatically replaced with the referenced prompt content, enabling you to:

- Create modular prompt components that can be reused across multiple prompts
- Maintain common instructions, examples, or context in a single place
- Update dependent prompts automatically when base prompts change

## Caching in client SDKs

Langfuse prompts are served from a client-side cache in the SDKs. Therefore, **Langfuse Prompt Management does not add any latency to your application when a cached prompt is available from a previous use**. Optionally, you can pre-fetch prompts on application startup to ensure that the cache is populated (example below).

### Optional: Pre-fetch prompts on application start

**To ensure that your application never hits an empty cache at runtime** (and thus adding an initial delay of fetching the prompt), you can pre-fetch the prompts during the application startup. This pre-fetching will populate the cache and ensure that the prompts are readily available when needed.

_Example implementations:_

<Tabs items={["Python (Flask)", "JS/TS (Express)"]}>
<Tab>

```python
from flask import Flask, jsonify
from langfuse import Langfuse

# Initialize the Flask app and Langfuse client
app = Flask(__name__)
langfuse = Langfuse()

def fetch_prompts_on_startup():
    # Fetch and cache the production version of the prompt
    langfuse.get_prompt("movie-critic")

# Call the function during application startup
fetch_prompts_on_startup()

@app.route('/get-movie-prompt/<movie>', methods=['GET'])
def get_movie_prompt(movie):
    prompt = langfuse.get_prompt("movie-critic")
    compiled_prompt = prompt.compile(criticlevel="expert", movie=movie)
    return jsonify({"prompt": compiled_prompt})

if __name__ == '__main__':
    app.run(debug=True)
```

</Tab>
<Tab>

```ts
import express from "express";
import { Langfuse } from "langfuse";

// Initialize the Express app and Langfuse client
const app = express();
const langfuse = new Langfuse();

async function fetchPromptsOnStartup() {
  // Fetch and cache the production version of the prompt
  await langfuse.getPrompt("movie-critic");
}

// Call the function during application startup
fetchPromptsOnStartup();

app.get("/get-movie-prompt/:movie", async (req, res) => {
  const movie = req.params.movie;
  const prompt = await langfuse.getPrompt("movie-critic");
  const compiledPrompt = prompt.compile({ criticlevel: "expert", movie });
  res.json({ prompt: compiledPrompt });
});

app.listen(3000, () => {
  console.log("Server is running on port 3000");
});
```

</Tab>
</Tabs>

### Optional: Customize caching duration (TTL)

The caching duration is configurable if you wish to reduce network overhead of the Langfuse Client. The default cache TTL is 60 seconds. After the TTL expires, the SDKs will refetch the prompt in the background and update the cache. Refetching is done asynchronously and does not block the application.

<Tabs items={["Python", "JS/TS"]}>
<Tab>

```python
# Get current `production` prompt version and cache for 5 minutes
prompt = langfuse.get_prompt("movie-critic", cache_ttl_seconds=300)
```

</Tab>

<Tab>

```ts
// Get current `production` version and cache prompt for 5 minutes
const prompt = await langfuse.getPrompt("movie-critic", undefined, {
  cacheTtlSeconds: 300,
});
```

</Tab>

</Tabs>

### Optional: Disable caching [#disable-caching]

You can disable caching by setting the `cacheTtlSeconds` to `0`. This will ensure that the prompt is fetched from the Langfuse API on every call. This is recommended for non-production use cases where you want to ensure that the prompt is always up to date with the latest version in Langfuse.

<Tabs items={["Python", "JS/TS"]} >
<Tab>

```python
prompt = langfuse.get_prompt("movie-critic", cache_ttl_seconds=0)

# Common in non-production environments, no cache + latest version
prompt = langfuse.get_prompt("movie-critic", cache_ttl_seconds=0, label="latest")
```

</Tab>

<Tab>

```ts
const prompt = await langfuse.getPrompt("movie-critic", undefined, {
  cacheTtlSeconds: 0,
});

// Common in non-production environments, no cache + latest version
const prompt = await langfuse.getPrompt("movie-critic", undefined, {
  cacheTtlSeconds: 0,
  label: "latest",
});
```

</Tab>
</Tabs>

### Performance measurement of inital fetch (empty client-side cache)

We measured the execution time of the following snippet with fully disabled caching.

```python
prompt = langfuse.get_prompt("perf-test")
prompt.compile(input="test")
```

Results from 1000 sequential executions in a local jupyter notebook using Langfuse Cloud (includes network latency):

<div className="sm:grid sm:grid-cols-2 gap-4">

<Frame className="max-w-md">
  ![Performance Chart](/images/docs/prompt-performance-chart.png)
</Frame>

```
count  1000.000000
mean      0.178465 sec
std       0.058125 sec
min       0.137314 sec
25%       0.161333 sec
50%       0.165919 sec
75%       0.171736 sec
max       0.687994 sec
```

</div>

## Optional: Guaranteed availability

<Callout>

Implementing this is usually not necessary as it adds complexity to your application and the Langfuse API is highly available. However, if you require 100% availability, you can use the following options.

</Callout>

The Langfuse API has high uptime and prompts are cached locally in the SDKs to prevent network issues from affecting your application.

However, `get_prompt()`/`getPrompt()` will throw an exception if:

1. No local (fresh or stale) cached prompt is available -> new application instance fetching prompt for the first time
2. _and_ network request fails -> networking or Langfuse API issue (after retries)

To gurantee 100% availability, there are two options:

1. Pre-fetch prompts on application startup and exit the application if the prompt is not available.
2. Provide a `fallback` prompt that will be used in these cases.

### Option 1: Pre-fetch prompts on application startup and exit if not available

<Tabs items={["Python", "JS/TS"]}>
<Tab>

```python
from langfuse import Langfuse
import sys

# Initialize Langfuse client
langfuse = Langfuse()

def fetch_prompts_on_startup():
    try:
        # Fetch and cache the production version of the prompt
        langfuse.get_prompt("movie-critic")
    except Exception as e:
        print(f"Failed to fetch prompt on startup: {e}")
        sys.exit(1)  # Exit the application if the prompt is not available

# Call the function during application startup
fetch_prompts_on_startup()

# Your application code here
```

</Tab>
<Tab>

```ts
import { Langfuse } from "langfuse";

// Initialize Langfuse client
const langfuse = new Langfuse();

async function fetchPromptsOnStartup() {
  try {
    // Fetch and cache the production version of the prompt
    await langfuse.getPrompt("movie-critic");
  } catch (error) {
    console.error("Failed to fetch prompt on startup:", error);
    process.exit(1); // Exit the application if the prompt is not available
  }
}

// Call the function during application startup
fetchPromptsOnStartup();

// Your application code here
```

</Tab>
</Tabs>

### Option 2: Fallback [#fallback]

<Tabs items={["Python", "JS/TS"]}>
<Tab>

```python /fallback="Do you like {{movie}}?"/ /fallback=[{"role": "system", "content": "You are an expert on {{movie}}"}]/
from langfuse import Langfuse
langfuse = Langfuse()

# Get `text` prompt with fallback
prompt = langfuse.get_prompt(
  "movie-critic",
  fallback="Do you like {{movie}}?"
)

# Get `chat` prompt with fallback
chat_prompt = langfuse.get_prompt(
  "movie-critic-chat",
  type="chat",
  fallback=[{"role": "system", "content": "You are an expert on {{movie}}"}]
)

# True if the prompt is a fallback
prompt.is_fallback
```

</Tab>
<Tab>

```ts /fallback: "Do you like {{movie}}?"/ /fallback: [{ role: "system", content: "You are an expert on {{movie}}" }]/
import { Langfuse } from "langfuse";
const langfuse = new Langfuse();

// Get `text` prompt with fallback
const prompt = await langfuse.getPrompt("movie-critic", undefined, {
  fallback: "Do you like {{movie}}?",
});

// Get `chat` prompt with fallback
const chatPrompt = await langfuse.getPrompt("movie-critic-chat", undefined, {
  type: "chat",
  fallback: [{ role: "system", content: "You are an expert on {{movie}}" }],
});

// True if the prompt is a fallback
prompt.isFallback;
```

</Tab>
</Tabs>

## Additional features

- [Prompt Management MCP Server](/docs/prompts/mcp-server)
- [Prompt A/B Testing](/docs/prompts/a-b-testing)

## FAQ

import { FaqPreview } from "@/components/faq/FaqPreview";

<FaqPreview tags={["prompt-management"]} />

## GitHub Discussions

import { GhDiscussionsPreview } from "@/components/gh-discussions/GhDiscussionsPreview";

<GhDiscussionsPreview labels={["feat-prompt-management"]} />

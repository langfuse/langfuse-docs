---
description: Manage and version your prompts in Langfuse (open source). When retrieved, they are cached by the Langfuse SDKs for low latency.
---

# Prompt Management

Use Langfuse to effectively **manage** and **version** your prompts. Langfuse prompt management is basically a **Prompt CMS** (Content Management System).

## Why use prompt management?

> Can't I just hardcode my prompts in my application and track them in Git? Yes, well... you can and all of us have done it.

Typical benefits of using a CMS apply here:

- Decoupling: deploy new prompts without redeploying your application.
- Non-technical users can create and update prompts via Langfuse Console.
- Quickly rollback to a previous version of a prompt.

Platform benefits:

- Track performance of prompt versions in Langfuse Tracing.

## Langfuse prompt object

```json filename="Example prompt in Langfuse with custom config"
{
  "name": "movie-critic",
  "type": "text",
  "prompt": "Do you like {{movie}}?",
  "config": {
    "model": "gpt-3.5-turbo",
    "temperature": 0.5,
    "supported_languages": ["en", "fr"]
  },
  "version": 1,
  "is_active": true
}
```

- `name`: Unique name of the prompt within a Langfuse project.
- `type`: The type of the prompt content (`text` or `chat`). Default is `text`.
- `prompt`: The text template with variables (e.g. `This is a prompt with a {{variable}}`). For chat prompts, this is a list of chat messages each with `role` and `content`.
- `config`: Optional JSON object to store any parameters (e.g. model parameters or model tools).
- `version`: Integer to indicate the version of the prompt. The version is automatically incremented when creating a new prompt version.
- `is_active`: Boolean to indicate if the prompt version is in production. This is the default version that is returned when fetching the prompt by name.

## How it works

<Steps>

### Create/update prompt

If you already have a prompt with the same `name`, the prompt will be added as a new version.

<Tabs items={["Langfuse UI", "Python", "JS/TS"]}>
<Tab>

<CloudflareVideo
  videoId="3c9bf36417e79dd2d68c3bba2a8f0a98"
  aspectRatio={1.24}
  gifStyle
  className="max-w-xl"
/>

</Tab>
<Tab>

```python
# Create a text prompt
langfuse.create_prompt(
    name="movie-critic",
    type="text",
    prompt="Do you like {{movie}}?",
    is_active=True,  # directly promote to production?
    config={
        "model": "gpt-3.5-turbo",
        "temperature": 0.7,
        "supported_languages": ["en", "fr"],
    },  # optionally, add configs (e.g. model parameters or model tools)
)

# Create a chat prompt
langfuse.create_prompt(
    name="movie-critic-chat",
    type="chat",
    prompt=[{"role": "system", "content": "You are an expert on {{movie}}"}],
    is_active=True,  # directly promote to production?
    config={
        "model": "gpt-3.5-turbo",
        "temperature": 0.7,
        "supported_languages": ["en", "fr"],
    },  # optionally, add configs (e.g. model parameters or model tools)
)
```

If you already have a prompt with the same name, the prompt will be added as a new version.

</Tab>

<Tab>

```typescript
// Create a text prompt
await langfuse.createPrompt({
  name: "movie-critic",
  type: "text",
  prompt: "Do you like {{movie}}?",
  isActive: true, // directly promote to production?
  config: {
    model: "gpt-3.5-turbo",
    temperature: 0.7,
    supported_languages: ["en", "fr"],
  }, // optionally, add configs (e.g. model parameters or model tools)
});

// Create a chat prompt
await langfuse.createPrompt({
  name: "movie-critic-chat",
  type: "chat",
  prompt: [{ role: "system", content: "You are an expert on {{movie}}" }],
  isActive: true, // directly promote to production?
  config: {
    model: "gpt-3.5-turbo",
    temperature: 0.7,
    supported_languages: ["en", "fr"],
  }, // optionally, add configs (e.g. model parameters or model tools)
});
```

If you already have a prompt with the same name, the prompt will be added as a new version.

</Tab>

</Tabs>

### Use prompt

At runtime, you can fetch the latest production version from Langfuse.

<Tabs items={["Python", "JS/TS", "Langchain (Python)", "Langchain (JS)"]}>
<Tab>

```python
from langfuse import Langfuse

# Initialize Langfuse client
langfuse = Langfuse()

# Get current production version of a text prompt
prompt = langfuse.get_prompt("movie-critic")

# Insert variables into prompt template
compiled_prompt = prompt.compile(movie="Dune 2")
# -> "Do you like Dune 2?"
```

Chat prompts

```python
# Get current production version of a chat prompt
chat_prompt = langfuse.get_prompt("movie-critic-chat", type="chat") # type arg infers the prompt type (default is 'text')

# Insert variables into chat prompt template
compiled_chat_prompt = chat_prompt.compile(movie="Dune 2")
# -> [{"role": "system", "content": "You are an expert on Dune 2"}]
```

Optional parameters

```python
# Get specific version
prompt = langfuse.get_prompt("movie-critic", version=1)

# Extend cache TTL from default 60 to 300 seconds
prompt = langfuse.get_prompt("movie-critic", cache_ttl_seconds=300)
```

Attributes

```python
# Raw prompt including {{variables}}. For chat prompts, this is a list of chat messages.
prompt.prompt

# Config object
prompt.config
```

</Tab>

<Tab>

```typescript
import { Langfuse } from "langfuse";

// Iniitialize the Langfuse client
const langfuse = new Langfuse();

// Get current production version
const prompt = await langfuse.getPrompt("movie-critic");

// Insert variables into prompt template
const compiledPrompt = prompt.compile({ movie: "Dune 2" });
```

Chat prompts

```typescript
// Get current production version of a chat prompt
const chatPrompt = await langfuse.getPrompt("movie-critic-chat", undefined, {
  type: "chat",
}); // type option infers the prompt type (default is 'text')

// Insert variables into chat prompt template
const compiledChatPrompt = chatPrompt.compile({ movie: "Dune 2" });
```

Optional parameters

```typescript
// Get specific version of a prompt (here version 1). Prompt is returned even if not active.
const prompt = await langfuse.getPrompt("movie-critic", 1);

// Extend cache TTL from default 1 to 5 minutes
const prompt = await langfuse.getPrompt("movie-critic", undefined, {
  cacheTtlSeconds: 300,
});
```

Attributes

```typescript
// Raw prompt including {{variables}}. For chat prompts, this is a list of chat messages.
prompt.prompt;

// Config object
prompt.config;
```

</Tab>

<Tab>

As Langfuse and Langchain process input variables of prompt templates differently (`{}` instead of `{{}}`), we provide the `prompt.get_langchain_prompt()` method to transform the Langfuse prompt into a string that can be used with Langchain's PromptTemplate.

```python
from langfuse import Langfuse
from langchain_core.prompts import ChatPromptTemplate

# Initialize Langfuse client
langfuse = Langfuse()

# Get current production version
langfuse_prompt = langfuse.get_prompt("movie-critic")

# Example using ChatPromptTemplate
langchain_prompt = ChatPromptTemplate.from_template(langfuse_prompt.get_langchain_prompt())
```

Chat prompts

```python
# Get current production version of a chat prompt
langfuse_prompt = langfuse.get_prompt("movie-critic-chat", type="chat")

# Create a Langchain ChatPromptTemplate from the Langfuse prompt chat messages
langchain_prompt = ChatPromptTemplate.from_messages(langfuse_prompt.get_langchain_prompt())
```

Optional parameters

```python
# Get specific version
prompt = langfuse.get_prompt("movie-critic", version=1)

# Extend cache TTL from default 60 to 300 seconds
prompt = langfuse.get_prompt("movie-critic", cache_ttl_seconds=300)
```

Attributes

```python
# Raw prompt including {{variables}}. For chat prompts, this is a list of chat messages.
prompt.prompt

# Config object
prompt.config
```

</Tab>

<Tab>

As Langfuse and Langchain process input variables of prompt templates differently (`{}` instead of `{{}}`), we provide the `prompt.getLangchainPrompt()` method to transform the Langfuse prompt into a string that can be used with Langchain's PromptTemplate.

```typescript
import { Langfuse } from "langfuse";
import { ChatPromptTemplate } from "@langchain/core/prompts";

const langfuse = new Langfuse();

// Get current production version
const langfusePrompt = await langfuse.getPrompt("movie-critic");

// Example using ChatPromptTemplate
const promptTemplate = PromptTemplate.fromTemplate(
  langfusePrompt.getLangchainPrompt()
);
```

Chat prompts

```typescript
// Get current production version of a chat prompt
const langfusePrompt = await langfuse.getPrompt(
  "movie-critic-chat",
  undefined,
  { type: "chat" }
);

// Example using ChatPromptTemplate
const promptTemplate = ChatPromptTemplate.fromMessages(
  langfusePrompt.getLangchainPrompt().map((msg) => [msg.role, msg.content])
);
```

Optional parameters

```typescript
// Get specific version of a prompt (here version 1). Prompt is returned even if not active.
const prompt = await langfuse.getPrompt("movie-critic", 1);

// Extend cache TTL from default 1 to 5 minutes
const prompt = await langfuse.getPrompt("movie-critic", undefined, {
  cacheTtlSeconds: 300,
});
```

Attributes

```typescript
// Raw prompt including {{variables}}. For chat prompts, this is a list of chat messages.
prompt.prompt;

// Config object
prompt.config;
```

</Tab>

</Tabs>

### Link with Langfuse Tracing (optional)

Add the prompt object to the `generation` call in the SDKs to link the generation in [Langfuse Tracing](/docs/tracing) to the prompt version. This allows you to track metrics by movie-critic and version in the Langfuse UI.

This is currently unavailable when using the LangChain or LlamaIndex integration.

<Tabs items={["Python SDK", "JS/TS SDK", "OpenAI SDK (Python)"]}>
<Tab>

Decorators

```python
from langfuse.decorators import langfuse_context, observe

@observe(as_type="generation")
def nested_generation():
    prompt = langfuse.get_prompt("movie-critic")

    langfuse_context.update_current_observation(
        prompt=prompt,
    )

@observe()
def main():
  nested_generation()

main()
```

Low-level SDK

```diff
langfuse.generation(
    ...
+   prompt=prompt
    ...
)
```

</Tab>

<Tab>

```diff
langfuse.generation({
    ...
+   prompt: prompt
    ...
})
```

</Tab>

<Tab>

```python /langfuse_prompt=prompt/
prompt = langfuse.get_prompt("calculator")

openai.chat.completions.create(
  model="gpt-3.5-turbo",
  messages=[
    {"role": "system", "content": prompt.compile(base=10)},
    {"role": "user", "content": "1 + 1 = "}],
  langfuse_prompt=prompt
)
```

</Tab>

</Tabs>

### Rollbacks (optional)

Set a prompt version to `active` when creating it via the SDKs. In the Langfuse UI, you can promote a specific prompt version to quickly rollback:

<CloudflareVideo
  videoId="0a1cc222b766594947efd00af62e7a67"
  aspectRatio={16.2 / 9}
  gifStyle
  className="max-w-xl"
/>

</Steps>

## End-to-end examples

The following example notebooks include end-to-end examples of prompt management:

import { Terminal, FileCode } from "lucide-react";

<Cards num={3}>
  <Card
    title="Example OpenAI Functions"
    href="/docs/prompts/example-openai-functions"
    icon={<FileCode />}
  />
  <Card
    title="Example Langchain (Python)"
    href="/docs/prompts/example-langchain"
    icon={<FileCode />}
  />
  <Card
    title="Example Langchain (JS/TS)"
    href="/docs/prompts/example-langchain-js"
    icon={<FileCode />}
  />
</Cards>

We also used Prompt Management for our Docs Q&A Chatbot and traced it with Langfuse. You can get view-only access to the project by signing up to the [public demo](/docs/demo).

## Performance

### Caching in client SDKs

While [Langfuse Tracing](/docs/tracing) is fully asynchronous and non-blocking, managing prompts in Langfuse adds latency to your application when retrieving the prompt. To minimize the impact on your application, prompts are cached in the client SDKs. The default cache TTL is 60 seconds and is configurable.

<Callout type="info">

When refetching a prompt fails but an expired version is in the cache, the SDKs will return the expired version, preventing application blockage due to network issues.

</Callout>

<Tabs items={["Python", "JS/TS"]}>
<Tab>

```python
# Get current production prompt version and cache for 5 minutes
prompt = langfuse.get_prompt("movie-critic", cache_ttl_seconds=300)

# Disable caching for a prompt
prompt = langfuse.get_prompt("movie-critic", cache_ttl_seconds=0)
```

</Tab>

<Tab>

```typescript
// Get current production version and cache prompt for 5 minutes
const prompt = await langfuse.getPrompt("movie-critic", undefined, {
  cacheTtlSeconds: 300,
});

// Disable caching for a prompt
const prompt = await langfuse.getPrompt("movie-critic", undefined, {
  cacheTtlSeconds: 0,
});
```

</Tab>

</Tabs>

### Performance measurement (excluding cache hits)

We measured the execution time of the following snippet. We disabled the cache to measure the performance of the prompt retrieval and compilation.

```python
prompt = langfuse.get_prompt("perf-test", cache_ttl_seconds=0) # disable cache
prompt.compile(input="test")
```

Results from 1000 sequential executions in a local jupyter notebook using Langfuse Cloud (includes network latency):

<div className="sm:grid sm:grid-cols-2 gap-4">

<Frame className="max-w-md">
  ![Performance Chart](/images/docs/prompt-performance-chart.png)
</Frame>

```
count  1000.000000
mean      0.178465 sec
std       0.058125 sec
min       0.137314 sec
25%       0.161333 sec
50%       0.165919 sec
75%       0.171736 sec
max       0.687994 sec
```

</div>

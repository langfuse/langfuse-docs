---
description: Manage and version your prompts in Langfuse (open source). When retrieved, they are cached by the Langfuse SDKs for low latency.
---

import { Callout } from "nextra-theme-docs";
import { CloudflareVideo } from "@/components/Video";

# Prompt Management

Use Langfuse to effectively manage and version your prompts. This allows you to iterate quickly, publish new prompt versions without redeploying your app, and track metrics by prompt version.

<Tabs items={["Python", "JS/TS", "Langchain (Python)"]}>
<Tab>

```python
from langfuse import Langfuse

langfuse = Langfuse()

# Get current production prompt in application
prompt = langfuse.get_prompt("prompt name")
```

</Tab>
<Tab>

```typescript
import { Langfuse } from "langfuse";

const langfuse = new Langfuse();

// Get current production prompt in application
const prompt = await langfuse.getPrompt("prompt name");
```

</Tab>
<Tab>

```python
from langfuse import Langfuse
from langchain_core.prompts import ChatPromptTemplate

langfuse = Langfuse()

# Get current production prompt in application
prompt = langfuse.get_prompt("prompt name")

# Example using ChatPromptTemplate
langchain_prompt = ChatPromptTemplate.from_template(prompt.get_langchain_prompt())
```

</Tab>
</Tabs>

The workflow for managing prompts in Langfuse includes the following steps:

```mermaid
graph LR
    A["1. Create new prompt version (SDK or UI)"] --> PV
    subgraph S1["Prompt (identified by name)"]
        PV["Prompt Version"] -->|"2. Promote"| PP["Production Prompt"]
    end
    PP --> D["3. Retrieve prompt using SDK"]
    PP --> E["4. View metrics in Langfuse for prompt version"]
```

## Prompt object

Attributes of a prompt in Langfuse:

- `prompt`: The text template with variables (e.g. `This is a prompt with a {{variable}}`)
- `config`: Optional JSON object to store any parameters (e.g. model parameters or model tools).
- `version`: Integer to indicate the version of the prompt. The version is automatically incremented when creating a new prompt version.
- `is_active`: Boolean to indicate if the prompt version is in production. This is the default version that is returned when fetching the prompt by name.

## Create & update prompts

**Create**

<Tabs items={["Langfuse UI", "Python", "JS/TS"]}>
<Tab>

<CloudflareVideo
  videoId="3c9bf36417e79dd2d68c3bba2a8f0a98"
  aspectRatio={1.24}
  gifStyle
  className="max-w-xl"
/>

</Tab>
<Tab>

```python
langfuse.create_prompt(
  name = "prompt name",
  prompt = "This is a prompt with a {{variable}}",
  is_active = True # directly promote to production?
  config = {
    "temperature": 0.5,
    "top_p": 0.9
  } # optionally, add configs (e.g. model parameters or model tools)
)
```

</Tab>

<Tab>

```typescript
await langfuse.createPrompt({
  name: "prompt name",
  prompt: "This is a prompt with a {{variable}}",
  isActive: true, // directly promote to production?
  config: {
    temperature: 0.5,
    topP: 0.9,
  }, // optionally, add configs (e.g. model parameters or model tools)
});
```

</Tab>

</Tabs>

**Update**

Use the edit button in the Langfuse UI or create a new prompt version via the SDKs with the same name.

## Promote prompt to production

Set a prompt version to `active` when creating it via the SDKs. In the Langfuse UI, you can promote a prompt version to production:

<CloudflareVideo
  videoId="0a1cc222b766594947efd00af62e7a67"
  aspectRatio={16.2 / 9}
  gifStyle
  className="max-w-xl"
/>

## Retrieve prompt in your application

<Tabs items={["Python", "JS/TS", "Langchain (Python)",]}>
<Tab>

```python
from langfuse import Langfuse

# Initialize Langfuse client
langfuse = Langfuse()

# Get current production version
prompt = langfuse.get_prompt("prompt name")

# Get specific version
prompt = langfuse.get_prompt("prompt name", version=3)

# Get specific version and extend cache TTL from default 60 to 300 seconds
prompt = langfuse.get_prompt("prompt name", version=3, cache_ttl_seconds=300)

# Insert variables into prompt template
compiled_prompt = prompt.compile(input="test")
```

Attributes

```python
# Raw prompt including {{variables}}
prompt.prompt

# Config object
prompt.config
```

</Tab>

<Tab>

```typescript
import { Langfuse } from "langfuse";

// Iniitialize the Langfuse client
const langfuse = new Langfuse();

// Get current production version
const prompt = await langfuse.getPrompt("prompt name");

// Get specific version of a prompt (here version 3). Prompt is returned even if not active.
const prompt = await langfuse.getPrompt("prompt name", 3);

// Get specific version and extend cache TTL from default 1 to 5 minutes
const prompt = await langfuse.getPrompt("prompt name", 3, {
  cacheTtlSeconds: 300,
});

// Insert variables into prompt template
const compiledPrompt = prompt.compile({ input: "test" });
```

Attributes

```typescript
// Raw prompt including {{variables}}
prompt.prompt;

// Config object
prompt.config;
```

</Tab>

<Tab>

As Langfuse and Langchain process input variables of prompt templates differently (`{}` instead of `{{}}`), we provide the `prompt.get_langchain_prompt()` method to transform the Langfuse prompt into a string that can be used with Langchain's PromptTemplate.

```python
from langfuse import Langfuse
from langchain_core.prompts import ChatPromptTemplate

# Initialize Langfuse client
langfuse = Langfuse()

# Get current production version of a Langfuse prompt
langfuse_prompt = langfuse.get_prompt("prompt_name")

# Example using ChatPromptTemplate
langchain_prompt = ChatPromptTemplate.from_template(langfuse_prompt.get_langchain_prompt())
```

</Tab>

</Tabs>

### Caching in client SDKs

While [Langfuse Tracing](/docs/tracing/overview) is fully asynchronous and non-blocking, managing prompts in Langfuse adds latency to your application when retrieving the prompt. To minimize the impact on your application, prompts are cached for 60 seconds in the client SDKs once fetched. The cache TTL can be configured directly in the SDKs.

<Callout type="info">

When refetching a prompt fails but an expired version is in the cache, the SDKs will return the expired version, preventing application blockage due to network issues.

</Callout>

<Tabs items={["Python SDK", "JS/TS SDK"]}>
<Tab>

```python
# Get current production prompt version and cache for 5 minutes
prompt = langfuse.get_prompt("prompt name", cache_ttl_seconds=300)

# Get a specific prompt version and cache for 5 minutes
prompt = langfuse.get_prompt("prompt name", version=3, cache_ttl_seconds=300)

# Disable caching for a prompt
prompt = langfuse.get_prompt("prompt name", cache_ttl_seconds=0)
```

</Tab>

<Tab>

```typescript
// Get current production version and cache prompt for 5 minutes
const prompt = await langfuse.getPrompt("prompt name", undefined, {
  cacheTtlSeconds: 300,
});

// Get prompt version 3 and cache for 5 minutes
const prompt = await langfuse.getPrompt("prompt name", 3, {
  cacheTtlSeconds: 300,
});

// Disable caching for a prompt
const prompt = await langfuse.getPrompt("prompt name", undefined, {
  cacheTtlSeconds: 0,
});
```

</Tab>

</Tabs>

### Performance on first fetch (excluding cache hits)

We measured the execution time of the following snippet (prompt retrieval and compilation). Note that this excludes cache hits where the prompt is available immediately.

```python
prompt = langfuse.get_prompt("perf-test")
prompt.compile(input="test")
```

Results from 1000 sequential executions in a local jupyter notebook using Langfuse Cloud (includes network latency):

<div className="sm:grid sm:grid-cols-2 gap-4">

<Frame className="max-w-md">
  ![Performance Chart](/images/docs/prompt-performance-chart.png)
</Frame>

```
count  1000.000000
mean      0.178465 sec
std       0.058125 sec
min       0.137314 sec
25%       0.161333 sec
50%       0.165919 sec
75%       0.171736 sec
max       0.687994 sec
```

</div>

## Link generations in Langfuse Tracing to prompt versions

Add the prompt object to the `generation` call in the SDKs to link the generation in [Langfuse Tracing](/docs/tracing/overview) to the prompt version. This allows you to track metrics by prompt name and version in the Langfuse UI.

<Tabs items={["Python SDK", "JS/TS SDK"]}>
<Tab>

```diff
langfuse.generation(
    ...
+   prompt=prompt
    ...
)
```

</Tab>

<Tab>

```diff
langfuse.generation({
    ...
+   prompt: prompt
    ...
})
```

</Tab>

</Tabs>

## End-to-end examples

import { Terminal, FileCode } from "lucide-react";

<Cards num={2}>
  <Card
    title="Example OpenAI Functions"
    href="/docs/prompts/example-openai-functions"
    icon={<FileCode />}
  />
  <Card
    title="Example Langchain (Python)"
    href="/docs/prompts/example-langchain"
    icon={<FileCode />}
  />
</Cards>

## Demo

We used this feature for our Docs Q&A Chatbot and traced it with Langfuse. You can get view-only access to the project by signing up to the [public demo](/docs/demo).

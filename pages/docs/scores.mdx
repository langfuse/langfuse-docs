---
description: Scores serve as a metric for evaluating individual LLM executions or traces. Commonly used to measure quality, tonality, factual accuracy, completeness, and relevance, among others.
---

import { Frame } from "@/components/Frame";

# Scores & Evaluation

Scores serve as a metric for evaluating individual executions or traces.

A variety of scores may be utilized, with the most common metrics assessing aspects such as quality, tonality, factual accuracy, completeness, and relevance, among others.

In instances where the score pertains to a specific phase of a trace, for example, a singular LLM call, a message in a chat conversation, or a step in an agent, it is possible to attach the score directly to the observation. This enables targeted evaluation of that particular component.

The `Score` object in Langfuse:

| Attribute       | Type   | Description                                                                                  |
| --------------- | ------ | -------------------------------------------------------------------------------------------- |
| `name`          | string | Name of the score, e.g. user_feedback, hallucination_eval                                    |
| `value`         | number | Value of the score                                                                           |
| `traceId`       | string | Id of the trace the score relates to                                                         |
| `observationId` | string | Optional: Observation (e.g. LLM call) the score relates to                                   |
| `comment`       | string | Optional: Evaluation comment, commonly used for user feedback, eval output or internal notes |

## Kinds of scores

Scores in Langfuse are adaptable and designed to cater to the unique requirements of specific LLM applications. They typically serve to measure the following aspects:

- Quality
  - Factual accuracy
  - Completeness of the information provided
  - Verification against hallucinations
- Style
  - Sentiment portrayed
  - Tonality of the content
  - Potential toxicity
- Security
  - Similarity to prevalent prompt injections
  - Instances of model refusals (e.g., as a language model, ...)

This flexible scoring system allows for a comprehensive evaluation of various elements integral to the function and performance of the LLM application.

## Using scores across Langfuse

Scores can be used in multiple ways across Langfuse:

1. Displayed on trace to provide a quick overview
2. Segment all execution traces by scores to e.g. find all traces with a low quality score
3. [Analytics](/docs/analytics): Detailed score reporting with drill downs into use cases and user segments

## Ingesting scores

Most users of Langfuse ingest scores programmatically. These are common sources of scores:

| Source                            | examples                                                                               |
| --------------------------------- | -------------------------------------------------------------------------------------- |
| Explicit user feedback            | thumbs up/down, 1-5 star rating, free text                                             |
| Implicit user feedback            | time spent on a page, click-through rate, accepting/rejecting a model-generated output |
| Model-based evaluation frameworks | OpenAI Evals, Whylabs Langkit, custom model outputs                                    |

_These are the most common ways to use scores, but not an exhaustive list._

Read the detailed documentation on how to programmatically ingest your scores into Langfuse:

- Server-side via [SDKs](/docs/integrations/sdk) or [API](/docs/integrations/api)
- Client-side via [Web SDK](/docs/integrations/sdk/typescript-web) to e.g. record explicit or implicit user feedback

## Add manual score using Langfuse UI

Scores can also be added manually in the Langfuse UI.

Common use cases:

- **Collaboration**: Enable team collaboration by inviting other internal members to review a subset of traces. This human-in-the-loop evaluation can enhance the overall accuracy and reliability of your results by incorporating diverse perspectives and expertise.
- **Evaluating new product features**: This feature can be useful for new use cases where no other scores have been allocated yet.
- **Benchmarking of other scores**: Establish a human baseline score that can be used as a benchmark to compare and evaluate other scores. This can provide a clear standard of reference and enhance the objectivity of your performance evaluations.

<Frame>![Add manual score in UI](/images/docs/score-manual.gif)</Frame>

## Get in touch

Looking for a specific way to score your executions in Langfuse? Join the [Discord](https://discord.gg/7NXusRtqYU) and discuss your use case!

---
description: Annotate traces and observations with scores in the Langfuse UI to record human-in-the-loop evaluations.
---

# Annotation in Langfuse UI

Collaborate with your team and add [`scores`](/docs/scores) via the Langfuse UI.

<Frame>![Annotate in UI](/images/docs/annotation.gif)</Frame>

## Common use cases:

- **Collaboration**: Enable team collaboration by inviting other internal members to annotate a subset of traces and observations. This human-in-the-loop evaluation can enhance the overall accuracy and reliability of your results by incorporating diverse perspectives and expertise. 
- **Annotation data consistency**: Create score configurations for annotation workflows to ensure that all team members are using standardized scoring criteria. Hereby configure categorical, numerical or binary score types to capture different aspects of your data.
- **Evaluation of new product features**: This feature can be useful for new use cases where no other scores have been allocated yet.
- **Benchmarking of other scores**: Establish a human baseline score that can be used as a benchmark to compare and evaluate other scores. This can provide a clear standard of reference and enhance the objectivity of your performance evaluations.

## Get in touch

Looking for a specific way to annotate your executions in Langfuse? Join the [Discord](/discord) and discuss your use case!

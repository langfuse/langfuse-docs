---
title: Fully managed LLM-as-a-judge evaluation
description: Langfuse (open source) helps run model-based evaluations (llm-as-a-judge) on production data to monitor and improve LLMs applications.
---

# LLM-as-a-Judge

<AvailabilityBanner
  availability={{
    hobby: "full",
    pro: "full",
    team: "full",
    selfHosted: "ee",
  }}
/>

LLM-as-a-judge is a technique to evaluate the quality of LLM applications by using an LLM as a judge. The LLM is given a trace or a dataset entry and asked to score and reason about the output. The scores and reasoning are stored as [scores](/docs/scores/data-model) in Langfuse.

<details>
<summary>What are common evaluation tasks?</summary>

LLM-as-a-judge evaluation tasks can be very use-case-specific. Common tasks for which Langfuse provides prebuilt prompts are:

- Hallucination
- Helpfulness
- Relevance
- Toxicity
- Correctness
- Contextrelevance
- Contextcorrectness
- Conciseness

</details>

LLM-as-a-judge evaluators in Langfuse help to evaluate:

1. Production/development [traces](/docs/tracing)
2. Experiments that you run on [datasets](/docs/datasets)

<Callout type="info">

Alternatively, you can run any custom evaluation functions or packages on Langfuse data [via the API/SDKs](/docs/scores/custom).

Custom end-to-end example: [External evaluation pipeline](/docs/scores/external-evaluation-pipelines).

</Callout>

## Video Walkthrough

<Tabs
  items={[
    "LLM-as-a-Judge for Traces",
    "LLM-as-a-Judge for Dataset Experiments",
  ]}
>
  <Tab>
    <CloudflareVideo
      videoId="c2debc8ad9e9df71d56f813510ffdf80"
      aspectRatio={16 / 9}
      title="Model-based Evaluations in Langfuse"
      posterStartTime={137}
    />
  </Tab>
  <Tab>
    <iframe
      width="100%"
      className="aspect-[16/9] rounded-lg border mt-10 w-full"
      src="https://www.youtube.com/embed/JOGMn5nqCSM?si=9-Et0tKtOYffyvru"
      title="YouTube video player"
      frameborder="0"
      allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
      referrerpolicy="strict-origin-when-cross-origin"
      allowFullScreen
    ></iframe>
  </Tab>
</Tabs>

## Get Started

<Steps>

### Configure LLM provider

Langfuse supports a variety of LLM providers including OpenAI, Anthropic, Azure OpenAI, and AWS Bedrock.

To use LLM-as-a-judge, you have to configure your LLM provider in the Langfuse project settings.

**Note:** tool/function calling needs to be supported by the model for LLM-as-a-judge to work.

### Create an LLM-as-a-judge template

LLM-as-a-judge uses a prompt template and model configuration to evaluate traces. In Langfuse this configuration is stored in an `Evaluator Template` as it can be reused across multiple evaluators.

To help get you started, Langfuse includes a set of **predefined prompts** for common evaluation tasks, but you can also write your own or customize the Langfuse-provided prompts.

Prompt templates contain **`{{variables}}`** that are substituted with actual data when an evaluator is run. You can create an arbitrary number of custom variables that can later be referenced when creating the evaluator. Common variables are `input`, `output`, `context`, `ground_truth`, etc.

Langfuse uses function/tool calling to extract the evaluation output. At the bottom of the form, you can configure `score` and `reasoning` variables which will be used to instruct the LLM on how to score and reason about the evaluation.

<Frame border>![Langfuse](/images/docs/eval-hallucination-template.png)</Frame>

### Set up an evaluator

Now that you have created an evaluator template, you can configure on what data it should be applied by Langfuse.

Here we need to configure the following aspects:

- Which **Evaluator Template** to use
- **Trigger**: On what incoming data should the evaluator be executed?
  - [**Traces**](/docs/tracing): On new traces that are ingested into Langfuse. You can configure filters to select a subset of traces.
  - [**Datasets**](/docs/datasets): On all experiments run on a specific dataset in offline development.
- **Name of the `scores`** which will be created as a result of the evaluation.
- Specify how Langfuse should fill the **`{{variables}}`** in the template.
  - Langfuse traces can be deeply nested (see [conceptual overview](/docs/tracing)). You can query from the trace directly, or from any nested observation via its name.
  - Select whether to use the `Input`, `Output`, or `metadata` value.
- Optional: Add **sampling** to reduce costs when running evaluations on a large volume of production data.
- Optional: Configure **custom delay**. This is how you can ensure all data arrived at Langfuse servers before evaluation is executed. The time starts when the trace is first added to Langfuse while it might be still in progress. This is especially important for long-running agent executions.

<Frame border>![Langfuse](/images/docs/eval-config.png)</Frame>

</Steps>

<Callout type="info">

âœ¨ Done! You have created an evaluator which will now automatically be executed on all data that matches the selected trigger.

<Frame border className="max-w-lg">
  ![Langfuse](/images/docs/eval-score.png)
</Frame>

</Callout>

## Monitoring of Evaluators

Each evaluator has its own log page where you can view the progress and logs to potentially debug any issues.

<Frame border className="max-w-lg">
  ![Langfuse](/images/docs/evals-log.png)
</Frame>

## GitHub Discussions

import { GhDiscussionsPreview } from "@/components/gh-discussions/GhDiscussionsPreview";

<GhDiscussionsPreview labels={["feat-evals"]} />

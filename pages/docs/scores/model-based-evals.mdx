---
title: Fully managed LLM-as-a-judge evaluation
description: Langfuse (open source) helps run model-based evaluations (llm-as-a-judge) on production data to monitor and improve LLMs applications.
---

# LLM-as-a-Judge

<AvailabilityBanner
  availability={{
    hobby: "full",
    core: "full",
    pro: "full",
    enterprise: "full",
    selfHosted: "pro",
  }}
/>

LLM-as-a-judge is a technique to evaluate the quality of LLM applications by using an LLM as a judge. The LLM is given a trace or a dataset entry and asked to score and reason about the output. The scores and reasoning are stored as [scores](/docs/scores/data-model) in Langfuse.

<details>
<summary>What are common evaluation tasks?</summary>

LLM-as-a-judge evaluation tasks can be very use-case-specific. Common tasks for which Langfuse provides prebuilt prompts are:

- Hallucination
- Helpfulness
- Relevance
- Toxicity
- Correctness
- Contextrelevance
- Contextcorrectness
- Conciseness

</details>

LLM-as-a-judge evaluators in Langfuse help to evaluate:

1. Production/development [traces](/docs/tracing)
2. Experiments that you run on [datasets](/docs/datasets)

<Callout type="info">

Alternatively, you can run any custom evaluation functions or packages on Langfuse data [via the API/SDKs](/docs/scores/custom).

Custom end-to-end example: [External evaluation pipeline](/docs/scores/external-evaluation-pipelines).

</Callout>

## Video Walkthrough

<Tabs
  items={[
    "LLM-as-a-Judge for Traces",
    "LLM-as-a-Judge for Dataset Experiments",
  ]}
>
  <Tab>
    <CloudflareVideo
      videoId="c2debc8ad9e9df71d56f813510ffdf80"
      aspectRatio={16 / 9}
      title="Model-based Evaluations in Langfuse"
      posterStartTime={137}
    />
  </Tab>
  <Tab>
    <iframe
      width="100%"
      className="aspect-[16/9] rounded-lg border mt-10 w-full"
      src="https://www.youtube-nocookie.com/embed/JOGMn5nqCSM?si=9-Et0tKtOYffyvru"
      title="YouTube video player"
      frameborder="0"
      allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
      referrerpolicy="strict-origin-when-cross-origin"
      allowFullScreen
    ></iframe>
  </Tab>
</Tabs>

## Get Started

<Steps>

### Configure LLM provider

Langfuse supports a variety of LLM providers including OpenAI, Anthropic, Azure OpenAI, and AWS Bedrock. It also integrates with Atla, which specializes in training LLMs as AI judges for more reliable and nuanced evaluations.

To use LLM-as-a-judge, you have to configure your LLM provider in the Langfuse project settings.

**Note:** tool/function calling needs to be supported by the model for LLM-as-a-judge to work.

### Create an LLM-as-a-judge template

LLM-as-a-judge uses a prompt template and model configuration to evaluate traces. In Langfuse this configuration is stored in an `Evaluator Template` as it can be reused across multiple evaluators.

To help get you started, Langfuse includes a set of **predefined prompts** for common evaluation tasks, but you can also write your own or customize the Langfuse-provided prompts.

Prompt templates contain **`{{variables}}`** that are substituted with actual data when an evaluator is run. You can create an arbitrary number of custom variables that can later be referenced when creating the evaluator. Common variables are `input`, `output`, `context`, `ground_truth`, etc.

Langfuse uses function/tool calling to extract the evaluation output. At the bottom of the form, you can configure `score` and `reasoning` variables which will be used to instruct the LLM on how to score and reason about the evaluation.

<Callout type="info">

Currently, LLM-as-a-judge templates only support `numeric` scores. Support for `categorical` and `boolean` scores is on our roadmap. ([GitHub Issue](https://github.com/orgs/langfuse/discussions/4965))

</Callout>

<Frame border>![Langfuse](/images/docs/eval-hallucination-template.png)</Frame>

### Set up an evaluator

Now that you have created an evaluator template, you can configure on what data it should be applied by Langfuse.

Here we need to configure the following aspects:

- Which **Evaluator Template** to use
- **Trigger**: Decide on which data the evaluator should execute.
  - [**Traces**](/docs/tracing): On new traces that are ingested into Langfuse. You can configure filters to select a subset of traces.
  - [**Datasets**](/docs/datasets): On all experiments run on a specific dataset in offline development.
  - New/Existing data: Decide whether the evaluator should be run on new, existing, or all data.
- **Name of the `scores`** which will be created as a result of the evaluation.
- Specify how Langfuse should fill the **`{{variables}}`** in the template.
  - Langfuse traces can be deeply nested (see [conceptual overview](/docs/tracing)). You can query from the trace directly, or from any nested observation via its name.
  - Select whether to use the `Input`, `Output`, or `Metadata` value.
  - Optional: Provide a [JsonPath expression](https://github.com/JSONPath-Plus/JSONPath?tab=readme-ov-file#syntax-through-examples) to extract a specific value from the `Input`, `Output`, or `Metadata` value. If this is not provided, the entire value will be used.
- Optional: Add **sampling** to reduce costs when running evaluations on a large volume of production data.
- Optional: Configure **custom delay**. This is how you can ensure all data arrived at Langfuse servers before evaluation is executed. The time starts when the trace is first added to Langfuse while it might be still in progress. This is especially important for long-running agent executions.

<Frame border>![Langfuse](/images/docs/eval-config.png)</Frame>

</Steps>

<Callout type="info">

âœ¨ Done! You have created an evaluator which will now automatically be executed on all data that matches the selected trigger.

<Frame border className="max-w-lg">
  ![Langfuse](/images/docs/eval-score.png)
</Frame>

</Callout>

## Monitoring of Evaluators

Each evaluator has its own log page where you can view the progress and logs to potentially debug any issues.

<Frame border className="max-w-lg">
  ![Langfuse](/images/docs/evals-log.png)
</Frame>

## Troubleshooting

### LLM proxies

You can use an LLM proxy to power LLM-as-a-judge in Langfuse. Please create an LLM API Key in the project settings and set the base URL to resolve to your proxy's host. The proxy must accept the API format of one of our adapters and support tool calling.

For OpenAI compatible proxies, here is an example tool calling request that must be handled by the proxy in OpenAI format to support LLM-as-a-judge in Langfuse:

```bash
curl -X POST 'https://<host set in project settings>/chat/completions' \
-H 'accept: application/json' \
-H 'content-type: application/json' \
-H 'authorization: Bearer <api key entered in project settings>' \
-H 'x-test-header-1: <custom header set in project settings>' \
-H 'x-test-header-2: <custom header set in project settings>' \
-d '{
  "model": "<model set in project settings>",
  "temperature": 0,
  "top_p": 1,
  "frequency_penalty": 0,
  "presence_penalty": 0,
  "max_tokens": 256,
  "n": 1,
  "stream": false,
  "tools": [
    {
      "type": "function",
      "function": {
        "name": "extract",
        "parameters": {
          "type": "object",
          "properties": {
            "score": {
              "type": "string"
            },
            "reasoning": {
              "type": "string"
            }
          },
          "required": [
            "score",
            "reasoning"
          ],
          "additionalProperties": false,
          "$schema": "http://json-schema.org/draft-07/schema#"
        }
      }
    }
  ],
  "tool_choice": {
    "type": "function",
    "function": {
      "name": "extract"
    }
  },
  "messages": [
    {
      "role": "user",
      "content": "Evaluate the correctness of the generation on a continuous scale from 0 to 1. A generation can be considered correct (Score: 1) if it includes all the key facts from the ground truth and if every fact presented in the generation is factually supported by the ground truth or common sense.\n\nExample:\nQuery: Can eating carrots improve your vision?\nGeneration: Yes, eating carrots significantly improves your vision, especially at night. This is why people who eat lots of carrots never need glasses. Anyone who tells you otherwise is probably trying to sell you expensive eyewear or does not want you to benefit from this simple, natural remedy. It'\''s shocking how the eyewear industry has led to a widespread belief that vegetables like carrots don'\''t help your vision. People are so gullible to fall for these money-making schemes.\nGround truth: Well, yes and no. Carrots won'\''t improve your visual acuity if you have less than perfect vision. A diet of carrots won'\''t give a blind person 20/20 vision. But, the vitamins found in the vegetable can help promote overall eye health. Carrots contain beta-carotene, a substance that the body converts to vitamin A, an important nutrient for eye health.  An extreme lack of vitamin A can cause blindness. Vitamin A can prevent the formation of cataracts and macular degeneration, the world'\''s leading cause of blindness. However, if your vision problems aren'\''t related to vitamin A, your vision won'\''t change no matter how many carrots you eat.\nScore: 0.1\nReasoning: While the generation mentions that carrots can improve vision, it fails to outline the reason for this phenomenon and the circumstances under which this is the case. The rest of the response contains misinformation and exaggerations regarding the benefits of eating carrots for vision improvement. It deviates significantly from the more accurate and nuanced explanation provided in the ground truth.\n\n\n\nInput:\nQuery: {{query}}\nGeneration: {{generation}}\nGround truth: {{ground_truth}}\n\n\nThink step by step."
    }
  ]
}'
```

## GitHub Discussions

import { GhDiscussionsPreview } from "@/components/gh-discussions/GhDiscussionsPreview";

<GhDiscussionsPreview labels={["feat-evals"]} />

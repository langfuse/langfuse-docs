---
description: Langfuse (open source) helps run model-based evaluations on production data to monitor and improve LLMs applications.
---

# Model-based Evaluations in Langfuse

Model-based evaluations are a powerful tool to automate the evaluation of LLM applications integrated with Langfuse. With model-based evalutions, LLMs are used to score a specific session/trace/LLM-call in Langfuse on criteria such as correctness, toxicity, or hallucinations.

There are two ways to run model-based evaluations in Langfuse:

1. [Via the Python SDK or API](#sdk)
2. [Via the Langfuse UI (beta)](#ui)

## Via Python SDK [#sdk]

You can run your own model-based evals on data in Langfuse via the Python SDK. This gives you full flexibility to run various eval libraries on your production data and discover which work well for your use case.

Popular libraries:

- OpenAI Evals
- Langchain Evaluators ([Cookbook](/guides/cookbook/evaluation_with_langchain))
- RAGAS for RAG applications ([Cookbook](/guides/cookbook/evaluation_of_rag_with_ragas))
- UpTrain evals ([Cookbook](/guides/cookbook/evaluation_with_uptrain))
- Whylabs Langkit

## Via Langfuse UI (beta) [#ui]

<AvailabilityBanner
  availability={{
    hobby: "private-beta",
    pro: "private-beta",
    team: "private-beta",
    selfHosted: "not-available",
  }}
/>

<Callout type="info">
  Ping us via the chat widget if you are interested to join the private beta.
</Callout>

<CloudflareVideo
  videoId="..."
  aspectRatio={16 / 9.58}
  title="Model-based evaluations"
/>

<Steps>

### Store an API key

To use evals, you have to bring your won API keys. We store them encrypted on our servers. To do so, navigate to the settings page and store your API key.

### Create an eval template

First, we need to specify how the evaluation template:

- Select the model and its parameters.
- Customise one of the Langfuse managed prompt templates or write your own.
- We use function calling to extract the evaluation output. Specify the descriptions for the function parameters `score` and `reasoning`.

<Frame border>![Langfuse](/images/docs/eval-configuration.gif)</Frame>

### Create an eval config

Second, we need to specify on which `traces` Langfuse should run the template we created above.

- Select the evaluation template to run.
- Specify the name of the `scores` which will be created.
- Filter which newly ingested traces should be evaluated. (Coming soon: select existing traces)
- Specify how Langfuse should fill the variables in the template. Langfuse can extract data from `trace`, `generations`, `spans`, or `event` objects. This is how you get the context form the `trace` into the template.
- Modify sampling to save LLM API cost.
- Add a delay to the evaluation execution once a trace is ingested to ensure the trace is fully processed before the evaluation is executed.

<Frame border>![Langfuse](/images/docs/eval-config.png)</Frame>

### See scores

Upon receiving new traces, navigate to the trace detail view to see the associated scores.

<Frame border className="max-w-lg">
  ![Langfuse](/images/docs/eval-score.png)
</Frame>

</Steps>

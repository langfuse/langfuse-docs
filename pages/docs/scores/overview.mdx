---
title: Evaluation of LLM Applications
description: With Langfuse you can capture all your LLM evaluations in one place. You can combine a variety of different evaluation metrics like model-based evaluations, manual annotations or user feedback. This allows you to measure quality, tonality, factual accuracy, completeness, and other dimensions of your LLM application.
---

# LLM Evaluation & Scoring

## What is LLM Evaluation?

Evaluation is a critical aspect of developing and deploying LLM applications. Usually, teams use a multitude of different evaluation methods to score the performance of their AI application depending on the use case and the stage of the development process.

Langfuse provides a flexible [scoring system](/docs/scores/getting-started) to capture all your evaluations in one place and make them actionable.

## Why is Evaluation Important?

LLM evaluation is crucial for improving the accuracy and robustness of language models, ultimately enhancing the user experience and trust in your AI application. It helps detect hallucinations and measure performance across diverse tasks. A structured evaluation in production is vital for continuously improving your application.

## What are common Evaluation Methods?

Common evaluation methods for LLMs involve assessing both **quantitative** metrics and **qualitative** aspects to measure performance, coherence, and relevance. These evaluations help pinpoint the LLM Application's strengths and areas for improvement, ensuring it produces accurate and contextually appropriate outputs.

Langfuse supports all forms of evaluation methods due to its open architecture and API (learn more about the Langfuse `score` object [here](/docs/scores/getting-started)). Depending on your needs in the development process, you can use one or multiple of the following evaluation methods:

### 1. Model-based Evaluation (_LLM-as-a-Judge_)

[Model-based evaluations](/docs/scores/model-based-evals) (LLM-as-a-judge) are a powerful tool to _automatically_ assess LLM applications integrated with Langfuse. With this approach, an LLMs scores a particular session, trace, or LLM call in Langfuse based on factors such as accuracy, toxicity, or hallucinations.

There are two ways to run model-based evaluations in Langfuse:

- [Via the Langfuse UI (beta)](/docs/scores/model-based-evals#ui)
- [Via external evaluation pipelines](/docs/scores/model-based-evals#evaluation-pipeline)

### 2. Manual Annotation (in UI)

With [manual annotations](/docs/scores/annotation), you can annotate a subset of traces and observations by hand. This allows you to collaborate with your team and add scores via the Langfuse UI. Annotations can be used to establish a baseline for your evaluation metrics and to compare them with automated evaluations.

### 3. User Feedback

Capturing [user feedback](/docs/scores/user-feedback) in your AI application can be a valuable evaluation metric. You can add explicit (e.g., thumbs up/down, 1-5 star rating) or implicit (e.g., time spent on a page, click-through rate, accepting/rejecting a model-generated output, human-in-the-loop) user feedback to your LLM traces in Langfuse.

### 4. Custom via SDKs/API

Langfuse gives you full flexibility to ingest [custom scores via the Langfuse SDKs or API](/docs/scores/custom). The scoring workflow allows you to run custom quality checks (e.g. valid structured output format) on the output of your workflows at runtime, or to run custom external evaluation workflows.

## Getting Started

Learn how to configure and utilize `scores` in Langfuse to assess quality, accuracy, style, and security metrics in your LLM applications.

import { FileCode, BookOpen } from "lucide-react";

<Cards num={3}>
  <Card
    title="Getting Started with Langfuse Scoring"
    href="/docs/scores/getting-started"
    icon={<FileCode />}
  />
</Cards>

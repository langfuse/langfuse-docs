---
description: Capture feedback from users of your LLM application to measure overall quality and establish a baseline for your automated evaluations.
---

import FeedbackPreview from "@/components/feedbackPreview";

# User Feedback

User feedback is a great source to evaluate the quality of an LLM app's output. In Langfuse, feedback is collected as a [`score`](/docs/scores) and attached to an execution trace or an individual LLM generation.

## Types of Feedback

Depending on the type of the application, there are different types of feedback that can be collected that vary in quality, detail, and quantity.

- **Explicit Feedback**: Directly prompt the user to give feedback, this can be a rating, a like, a dislike, a scale or a comment. While it is simple to implement, quality and quantity of the feedback is often low.
- **Implicit Feedback**: Measure the user's behavior, e.g., time spent on a page, click-through rate, accepting/rejecting a model-generated output. This type of feedback is more difficult to implement but is often more frequent and reliable.

## Demo

We implemented collection of user feedback into the Q&A chatbot for the Langfuse docs.

<Frame>
  ![User feedback collection in
  Langfuse](/images/blog/docs-qa-chatbot/feedback.gif)
</Frame>

In this example you see the following steps:

1. Collection of feedback using the Langfuse Web SDK
   > _Negative, Langchain not included in response_
2. Browsing of feedback
3. Identification of the root cause of the low-quality response
   > Docs on Langchain integration are not included in embedding similarity search

‚Üí [Try the demo yourself](/docs/demo) and browse the collected feedback in Langfuse

## Example using LangfuseWeb

The easiest way to collect user feedback is via the **Langfuse Web SDK**. Thereby you can ingest scores directly from the browser. See the [Web SDK documentation](/docs/sdk/typescript/guide-web) for more details.

<div className="mt-5 px-3 py-2 ring-1 rounded ring-gray-400">

<h3 className="text-xl font-bold">User feedback on individual responses</h3>
<span className="text-sm">Chat application</span>

<div className="mt-5 grid grid-cols-1 lg:grid-cols-2 gap-4">
<div className="flex-1">

**Integration**

<div className="text-xs mt-2">

```typescript {1, 9, 17-18} filename="UserFeedbackComponent.tsx"
import { LangfuseWeb } from "langfuse";

export function UserFeedbackComponent(props: { traceId: string }) {
  const langfuseWeb = new LangfuseWeb({
    publicKey: env.NEXT_PUBLIC_LANGFUSE_PUBLIC_KEY,
  });

  const handleUserFeedback = async (value: number) =>
    await langfuseWeb.score({
      traceId: props.traceId,
      name: "user_feedback",
      value,
    });

  return (
    <div>
      <button onClick={() => handleUserFeedback(1)}>üëç</button>
      <button onClick={() => handleUserFeedback(0)}>üëé</button>
    </div>
  );
}
```

</div>

</div>
<div className="flex-1">

**Preview**

<FeedbackPreview />

</div>
</div>
</div>

Alternatively, you can ingest feedback as scores server-side via the SDKs for [Python](/docs/sdk/python) and [JS/TS](/docs/sdk/typescript/guide), or via the [HTTP API](/docs/api).

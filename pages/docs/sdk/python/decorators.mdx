---
description: A decorator-based integration to give you powerful tracing, evals, and analytics for your LLM application
---

# Decorator-based Python Integration

Integrate [Langfuse Tracing](/docs/tracing) into your LLM applications with the Langfuse Python SDK using the `@observe()` decorator.

The SDK supports both synchronous and asynchronous functions, automatically handling traces, spans, and generations, along with key execution details like inputs and outputs and timings. This setup allows you to concentrate on developing high-quality applications while benefitting from observability insights with minimal code. The decorator is fully itneroperable with our main integrations (more on this below): [OpenAI](/docs/integrations/openai), [Langchain](/docs/integrations/langchain), [LlamaIndex](/docs/integrations/llama-index)

See the [reference](https://feat-add-tracing-decorators.langfuse-python.pages.dev/langfuse/decorators) for a comprehensive list of all available parameters and methods.

Want more control over the traces logged to Langfuse? Check out the [low-level Python SDK](/docs/sdk/python/low-level-sdk).

## Example

_Simple example (decorator + openai integration)_

```python /@observe()/ filename="main.py"
from langfuse.decorators import observe
from langfuse.openai import openai # OpenAI integration

@observe()
def story():
    return openai.chat.completions.create(
        model="gpt-3.5-turbo",
        max_tokens=100,
        messages=[
          {"role": "system", "content": "You are a great storyteller."},
          {"role": "user", "content": "Once upon a time in a galaxy far, far away..."}
        ],
    ).choices[0].message.content

@observe()
def main():
    return story()

main()
```

_Trace in Langfuse ([public link](https://cloud.langfuse.com/project/cloramnkj0002jz088vzn1ja4/traces/fac231bc-90ee-490a-aa32-78c4269474e3?observation=36544d09-dec7-48ff-88c3-6c2ae3fe2baf))_

<Frame fullWidth>
  ![Simple OpenAI decorator
  trace](/images/docs/python-decorator-simple-trace.png)
</Frame>

## Installation & setup

<Steps>

### Install the Langfuse Python SDK

[![PyPI](https://img.shields.io/pypi/v/langfuse?style=flat-square)](https://pypi.org/project/langfuse/)

```bash
pip install langfuse
```

### Add Langfuse API keys

If you haven't done so yet, [sign up to Langfuse](https://cloud.langfuse.com/auth/sign-up) and obtain your API keys from the project settings. Alternatively, you can also run Langfuse locally or self-host.

import PythonEnv from "@/components-mdx/env-python.mdx";
import PythonEnvOs from "@/components-mdx/env-python-os.mdx";

<Tabs items={["os.environ", "python-dotenv"]}>
  <Tab>
    <PythonEnvOs />
  </Tab>
  <Tab>
    Use [`python-dotenv`](https://pypi.org/project/python-dotenv/) to load the
    environment variables from a `.env` file at the root of your application.
    <PythonEnv />
  </Tab>
</Tabs>

### Add the Langfuse decorator

Import the `@observe()` decorator and apply it to the functions you want to trace. By default it captures:

- nesting via context vars
- timings/durations
- function name
- args and kwargs as input dict
- returned values as output

The decorator will automatically create a trace for the top-level function and spans for any nested functions. Learn more about the tracing data model [here](/docs/tracing/overview).

```python /@observe()/
from langfuse.decorators import observe

@observe()
def fn():
    pass

@observe()
def main():
    fn()

main()
```

import { Callout } from "nextra/components";

<Callout type="info">
  Done! âœ¨ Read on to learn how to capture additional information, LLM calls,
  and more with Langfuse Python decorators.
</Callout>

<Callout type="warning">

In a short-lived environment like AWS Lambda, make sure to call `flush()` before the
function terminates to avoid losing events. [Learn more](#flush).

```python /langfuse_context.flush()/ /langfuse_context/
from langfuse.decorators import observe, langfuse_context

@observe()
def main():
    print("Hello, from the main function!")

main()

langfuse_context.flush()
```

</Callout>

</Steps>

## Decorator arguments

> See [SDK reference](https://feat-add-tracing-decorators.langfuse-python.pages.dev/langfuse/decorators#observe) for full details.

### Specify `as_type` of created observation

Observations in Langfuse can be of type `span`, `event`, and `generation` (learn more about the [tracing data model](/docs/tracing/overview)). By default, the `@observe()` decorator creates spans for nested functions and traces for the top-level function.

You can override this behavior by specifying the `as_type` parameter.

```python /@observe(as_type="generation")/
from langfuse.decorators import observe

@observe(as_type="generation")
def fn():
    pass

@observe()
def main():
    fn()

main()
```

### Capturing of input/output

By default, the `@observe()` decorator captures the input arguments and output results of the function.

**You can disable this** behavior by setting the `capture_input` and `capture_output` parameters to `False`.

```python /capture_input=False/ /capture_output=False/
from langfuse.decorators import observe

@observe(capture_input=False, capture_output=False)
def fn(secret_arg):
    return "super secret output"

fn("my secret arg")
```

You can **manually set the input and output** of the observation using `langfuse_context.update_current_observation` (details below).

```python /langfuse_context.update_current_observation/ /capture_input=False/ /capture_output=False/
from langfuse.decorators import langfuse_context, observe

@observe(capture_input=False, capture_output=False)
def fn(secret_arg):
    langfuse_context.update_current_observation(
        input="sanitized input", # any serializable object
        output="sanitized output", # any serializable object
    )
    return "super secret output"

fn("my secret arg")
```

This will result in a trace with only sanitized input and output, and no actual function arguments or return values.

## Decorator context

Use the `langfuse_context` object to interact with the decorator context. This object is a thread-local singleton and can be accessed from anywhere within the function context.

### Add additional attributes to the trace and observations

In addition to the attributes automatically captured by the decorator, you can add others to use the full features of Langfuse.

<Callout type="info">

Please read the reference for more details on available parameters:

- `langfuse_context.update_current_observation` ([reference](https://feat-add-tracing-decorators.langfuse-python.pages.dev/langfuse/decorators#LangfuseDecorator.update_current_observation)): Update the trace/span of the current function scope
- `langfuse_context.update_current_trace` ([reference](https://feat-add-tracing-decorators.langfuse-python.pages.dev/langfuse/decorators#LangfuseDecorator.update_current_trace)): Update the trace itself, can also be called within any deeply nested span within the trace

</Callout>

Below is an example demonstrating how to enrich traces and observations with custom parameters:

```python /langfuse_context.update_current_observation/ /langfuse_context.update_current_trace/
from langfuse.decorators import langfuse_context, observe

@observe()
def deeply_nested_fn():
    # Enrich the current observation with a custom name, input, and output
    # All of these parameters override the default values captured by the decorator
    langfuse_context.update_current_observation(
        name="Deeply nested LLM call",
        input="Ping?",
        output="Pong!"
    )
    # Updates the trace, overriding the default trace name `main` (function name)
    langfuse_context.update_current_trace(
        name="Trace name set from deeply_nested_llm_call",
        session_id="1234",
        user_id="5678",
        tags=["tag1", "tag2"],
        public=True
    )
    return "output" # This output will not be captured as we have overridden it

@observe()
def nested_fn():
    # Update the current span with a custom name and level
    # Overrides the default span name
    langfuse_context.update_current_observation(
        name="Nested Span",
        level="WARNING"
    )
    deeply_nested_fn()

@observe()
def main():
    # This will be the trace as it is the highest level function
    nested_fn()

# Execute the main function to generate the enriched trace
main()
```

### Get trace URL

You can get the URL of the current trace using `langfuse_context.get_current_trace_url()`. Works anywhere within the function context, also in deeply nested functions.

```python /langfuse_context.get_current_trace_url()/
from langfuse.decorators import langfuse_context, observe

@observe()
def main():
    print(langfuse_context.get_current_trace_url())

main()
```

### Trace/observation IDs

By default, Langfuse assigns random IDs to all logged events.

#### Get trace and observation IDs

You can access the current trace and observation IDs from the `langfuse_context` object.

```python /langfuse_context.get_current_trace_id()/ /langfuse_context.get_current_observation_id/
from langfuse.decorators import langfuse_context, observe

@observe()
def fn():
    print(langfuse_context.get_current_trace_id())
    print(langfuse_context.get_current_observation_id())

fn()
```

#### Set custom IDs

If you have your own unique ID (e.g. messageId, traceId, correlationId), you can easily set those as trace or observation IDs for effective lookups in Langfuse. Just pass the `langfuse_observation_id` keyword argument to the decorated function.

```python /langfuse_observation_id="my-custom-request-id"/ /langfuse_observation_id="my-custom-request-id"/
from langfuse.decorators import langfuse_context, observe

@observe()
def process_user_request(user_id, request_data, **kwargs):
    # Function logic here
    pass

@observe(**kwargs)
def main():
    process_user_request(
        "user_id",
        "request",
        langfuse_observation_id="my-custom-request-id",
    )


main(langfuse_observation_id="my-custom-request-id")
```

## Interoperability with integrations

The decorator is fully interoperable with our main integrations: [OpenAI](/docs/integrations/openai), [Langchain](/docs/integrations/langchain), [LlamaIndex](/docs/integrations/llama-index). Thereby you can easily trace and evaluate functions that use (a combination of) these integrations.

### OpenAI

The [drop-in `openai` integration](/docs/integrations/openai) is fully compatible with the `@observe()` decorator. It automatically adds a generation observation to the trace within the current context.

```python /openai/
from langfuse.decorators import observe
from langfuse.openai import openai

@observe()
def story():
    return openai.chat.completions.create(
        model="gpt-3.5-turbo",
        max_tokens=100,
        messages=[
          {"role": "system", "content": "You are a great storyteller."},
          {"role": "user", "content": "Once upon a time in a galaxy far, far away..."}
        ],
    ).choices[0].message.content

@observe()
def main():
    return story()

main()
```

## Adding scores

[Scores](https://langfuse.com/docs/scores/overview) are used to evaluate single observations or entire traces. They can created manually via the Langfuse UI or via the SDKs.

| Parameter | Type   | Optional | Description                                                           |
| --------- | ------ | -------- | --------------------------------------------------------------------- |
| name      | string | no       | Identifier of the score.                                              |
| value     | number | no       | The value of the score. Can be any number, often standardized to 0..1 |
| comment   | string | yes      | Additional context/explanation of the score.                          |

<Tabs items={["Within the decorator context","Outside the decorated function"]}>
<Tab>

You can attach a score to the current observation context by calling `langfuse_context.score_current_observation`. You can also score the entire trace from anywhere inside the nesting hierarchy by calling `langfuse_context.score_current_trace`:

```python
from langfuse.decorators import langfuse_context, observe

# This will create a new span under the trace
@observe()
def nested_span():
    langfuse_context.score_current_observation(
        name="feedback-on-span",
        value=1,
        comment="I like how personalized the response is",
    )

    langfuse_context.score_current_trace(
        name="feedback-on-trace",
        value=1,
        comment="I like how personalized the response is",
    )


# This will create a new trace
@observe()
def main():
    nested_span()

main()
```

</Tab>
<Tab>

The decorators expose the trace_id and observation_id which are necessary to add scores outside of the decorated functions. This is useful whenever you want to add scores asynchronously, e.g. based on user feedback.

```python
from langfuse import Langfuse
from langfuse.decorators import langfuse_context, observe

# Create a new trace
@observe()
def main():
    trace_id = langfuse_context.get_current_trace_id()
    return "function_result", trace_id

# Execute the main function to generate a trace
_, trace_id = main()
```

```python
# Score the trace from outside the trace context using the low-level SDK
langfuse_client = Langfuse()
langfuse_client.score(
    trace_id=trace_id,
    name="user-explicit-feedback",
    value=1,
    comment="I like how personalized the response is"
)
```

</Tab>
</Tabs>

## Additional configuration

### Flush observations [#flush]

The Langfuse SDK executes network requests in the background on a separate thread for better performance of your application. This can lead to lost events in short lived environments such as AWS Lambda functions when the Python process is terminated before the SDK sent all events to our backend.

To avoid this, ensure that the `langfuse_context.flush()` method is called before termination. This method is waiting for all tasks to have completed, hence it is blocking.

### Debug mode

Enable debug mode to get verbose logs. Set the debug mode via the environment variable `LANGFUSE_DEBUG=True`.

### Authentication check

Use `langfuse_context.auth_check()` to verify that your host and API credentials are valid. This operation is blocking and is not recommended for production use.

## API reference

See the [Python SDK API reference](https://feat-add-tracing-decorators.langfuse-python.pages.dev/langfuse/decorators) for more details.

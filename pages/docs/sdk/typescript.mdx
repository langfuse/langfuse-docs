import { Tab, Tabs } from "nextra-theme-docs";

# Typescript SDK

import { Callout } from "nextra/components";

<Callout type="info" emoji="ℹ️">
  As an example, we integrated the Typescript SDK into the Vercel AI Chatbot
  project which includes streamed responses from OpenAI, conversations, and user
  feedback on individual messages
  <ul>
    <li className="list-disc ml-8">
      Read [**blog post**](/blog/showcase-llm-chatbot) for screenshots and
      detailed explanations of the inner workings of the integration
    </li>
    <li className="list-disc ml-8">
      Source code ([GitHub](https://github.com/langfuse/ai-chatbot)):
      [route.ts](https://github.com/langfuse/ai-chatbot/blob/main/app/api/chat/route.ts)
      (tracing of LLM calls in API route),
      [chat-message-actions.tsx](https://github.com/langfuse/ai-chatbot/blob/main/components/chat-message-actions.tsx#L50-L55)
      (feedback collection in frontend)
    </li>
  </ul>
</Callout>

## 1. Initializing the client

```sh npm2yarn
npm i langfuse
```

For Deno (ESM CDN): `https://esm.sh/langfuse`

### Server: Langfuse

The server client can modify all entities in the Langfuse API and requires the secret key. The secret key must not be exposed to users.

```typescript filename="api.ts"
import { Langfuse } from "langfuse";

const langfuse = new Langfuse({
  secretKey: process.env.LANGFUSE_SECRET_KEY, // sk-lf-...
  publicKey: process.env.LANGFUSE_PUBLIC_KEY, // pk-lf-...
  // baseUrl: defaults to "https://cloud.langfuse.com"
});
```

### Frontend: LangfuseWeb

The frontend client can be used to send scores directly to the Langfuse API. This is useful when scores are based on user feedback that is explicitly or implicitly given in the frontend. It is initialised with the public key which can be exposed to users.

```typescript filename="frontend.ts"
import { LangfuseWeb } from "langfuse";

const langfuseWeb = new LangfuseWeb({
  publicKey: process.env.LANGFUSE_PUBLIC_KEY, // pk-lf-...
  // baseUrl: defaults to "https://cloud.langfuse.com"
});
```

Hint for Next.js users: prefix the public key with `NEXT_PUBLIC_` to make it available in the frontend.

## 2. Trace execution of backend

- Each backend execution is logged with a single `trace`.
- Each trace can contain multiple `observations` to log the individual steps of the execution.
  - Observations can be of different types
    - [Events](#event) are the basic building block. They are used to track discrete events in a trace.
    - [Spans](#spans) represent durations of units of work in a trace.
    - [Generations](#generation) are spans which are used to log generations of AI model. They contain additional attributes about the model and the prompt/completion and are specifically rendered in the Langfuse UI.
  - Observations can be [nested](#nesting-of-observations).

### Trace

Traces are the top-level entity in the Langfuse API. They represent an execution flow in a LLM application usually triggered by an external event.

Traces can be created and updated.

> `langfuse.trace()` takes the following parameters:
>
> - `name` (optional): identifier of the trace. Useful for sorting/filtering in the UI.
> - `metadata` (optional): additional metadata of the trace. Can be any JSON object.
> - `externalId` (optional): the id of the execution in the external system. Useful for linking traces to external systems. Frequently used to create scores without having access to the langfuse `traceId` (see association via external Id).
> - `userId` (optional): the id of the user that triggered the execution.

```typescript filename="api.ts"
const trace = langfuse.trace({
  name: "chat-app-session",
  userId: "user__935d7d1d-8625-4ef4-8651-544613e7bd22",
  metadata: { env: "production", user: "user@langfuse.com" },
});
```

### Flushing

By deafult, the Langfuse Typescript SDK is asynchronous to not add latency. All objects returned by the SDK have a `data` property that is a promise.

In short lived execution environments (e.g. serverless functions), it is recommended to flush the Langfuse client to make sure all events are successfully sent to the Langfuse server before the execution ends. The Langfuse client keeps track of all pending requests and `langfuse.flush()` returns a promise that resolves when all requests are completed.

```typescript filename="api.ts"
await langfuse.flush();
```

LangfuseWeb also supports flushing. However, synchronous use is recommended to adjust the frontend's loading/success/error state.

```typescript filename="frontend.ts"
handleUserFeedback = async (value: number, comment: string) => {
  setLoading(true);
  await langfuseWeb.score({
    traceId: message.traceId,
    observationId: message.id,
    name: "user-feedback",
    value,
    comment,
  });
  setLoading(false);
};
```

### Observation types

#### Event

Events are used to track discrete events in a trace.

> - `startTime`: the time at which the event started.
> - `name` (optional): identifier of the event. Useful for sorting/filtering in the UI.
> - `metadata` (optional): additional metadata of the event. JSON object.
> - `level` (optional): the level of the event. Can be `DEBUG`, `DEFAULT`, `WARNING` or `ERROR`. Used for sorting/filtering of traces with elevated error levels and for highlighting in the UI.
> - `statusMessage` (optional): the status message of the event. Additional field for context of the event. E.g. the error message of an error event.
> - `parentObservationId` (optional): the id of the span or event to which the event should be attached
> - `input` (optional): the input to the event. Can be any JSON object.
> - `output` (optional): the output to the event. Can be any JSON object.

```typescript filename="api.ts"
trace.event({
  traceId: trace.id,
  startTime: new Date(),
  name: "get-user-profile",
  metadata: {
    attempt: 2,
    httpRoute: "/api/retrieve-doc",
  },
  parentObservationId: llmCall.id,
  input: {
    userId: "user__935d7d1d-8625-4ef4-8651-544613e7bd22",
  },
  output: {
    firstName: "Max",
    lastName: "Mustermann",
    email: "max.mustermann@gmail.com",
  },
});
```

#### Span

Spans represent durations of units of work in a trace. We generated convenient SDK functions for generic spans as well as LLM spans.

> `*.span()` take the following parameters:
>
> - `startTime` (optional): the time at which the span started. If no startTime is provided, the current time will be used.
> - `endTime` (optional): the time at which the span ended. Can also be set using `span.update()`.
> - `name` (optional): identifier of the span. Useful for sorting/filtering in the UI.
> - `metadata` (optional): additional metadata of the span. Can be any JSON object. Can also be set or updated using `span.update()`.
> - `level` (optional): the level of the span. Can be `DEBUG`, `DEFAULT`, `WARNING` or `ERROR`. Used for sorting/filtering of traces with elevated error levels and for highlighting in the UI.
> - `statusMessage` (optional): the status message of the span. Additional field for context of the event. E.g. the error message of an error event.
> - `parentObservationId` (optional): the id of the observation to which the span should be attached
> - `input` (optional): the input to the span. Can be any JSON object.
> - `output` (optional): the output to the span. Can be any JSON object.

```typescript filename="api.ts"
const retrievalStartTime = new Date();

// const retrievedDoc = await retrieveDoc();

trace.span({
  traceId: trace.id,
  startTime: retrievalStartTime,
  endTime: new Date(),
  name: "embedding-retrieval",
  metadata: {
    httpRoute: "/api/retrieve-doc",
  },
  input: {
    userId: "user__935d7d1d-8625-4ef4-8651-544613e7bd22",
  },
  output: {
    firstName: "Max",
    lastName: "Mustermann",
    email: "max.mustermann@gmail.com",
  },
});
```

#### Generation

Generations are used to log generations of AI model. They contain additional attributes about the model and the prompt/completion and are specifically rendered in the Langfuse UI.

> `generation.log()` take the following parameters:
>
> - `startTime` (optional): the time at which the generation started.
> - `endTime` (optional): the time at which the generation ended.
> - `name` (optional): identifier of the generation. Useful for sorting/filtering in the UI.
> - `model` (optional): the name of the model used for the generation
> - `modelParameters` (optional): the parameters of the model used for the generation; can be any key-value pairs
> - `prompt` (optional): the prompt used for the generation; can be any string or JSON object (recommended for chat models or other models that use structured input)
> - `completion` (optional): the completion generated by the model
> - `usage` (optional): the usage of the model during the generation; takes three optional key-value pairs: `promptTokens`, `completionTokens`, and `totalTokens`. If `totalTokens` is not provided, we calculate it as `promptTokens + completionTokens`. Null values are ignored and hence `totalTokens` can be `null` if `promptTokens` and `completionTokens` are not provided.
> - `metadata` (optional): additional metadata of the generation. Can be any JSON object.
> - `level` (optional): the level of the generation. Can be `DEBUG`, `DEFAULT`, `WARNING` or `ERROR`. Used for sorting/filtering of traces with elevated error levels and for highlighting in the UI.
> - `statusMessage` (optional): the status message of the generation. Additional field for context of the event. E.g. the error message of an error event.
> - `parentObservationId` (optional): the id of the observation to which the generation should be attached as a child.

```typescript filename="api.ts"
const generationStartTime = new Date();

// const chatCompletion = await llm.respond(prompt);

trace.generation({
  startTime: generationStartTime,
  endTime: new Date(),
  name: "chat-completion",
  model: "gpt-3.5-turbo",
  modelParameters: {
    temperature: 0.9,
    maxTokens: 2000,
    topP: undefined,
  },
  prompt: messagesToSend,
  completion: chatCompletion.data.choices[0].message?.content,
  usage: {
    promptTokens: chatCompletion.data.usage?.prompt_tokens,
    completionTokens: chatCompletion.data.usage?.completion_tokens,
  },
  metadata: {
    userId: "user__935d7d1d-8625-4ef4-8651-544613e7bd22",
  },
});
```

### Nesting of observations

Nesting of observations is helpful to structure the trace in a hierarchical way. This is especially helpful for complex chains and agents.

```raw
Simple example

- trace: chat-app-session
  - span: chat-interaction
    - event: get-user-profile
    - generation: chat-completion
```

There are three options to nest observations:

<Tabs items={["Recommended (async)", "Trace externalId (async)", "parentObservationId (sync)"]}>
<Tab>
```typescript filename="api.ts"
const trace = langfuse.trace({ name: "chat-app-session" });
const span = trace.span({ name: "chat-interaction" });
const event = span.event({ name: "get-user-profile" });
const generation = span.generation({ name: "chat-completion" });
```

</Tab>
<Tab>

```typescript filename="api.ts"
// Trace creation is optional:
// if excternalId is used in observations,
// trace is automatically created if it does not exist
const trace = langfuse.trace({
  name: "chat-app-session".
  externalId: "my-apps-session-1234"
});

const span = langfuse.span({
  traceId: "my-apps-session-1234",
  traceIdType: "EXTERNAL",
  name: "chat-interaction",
});
```

```typescript filename="frontend.ts"
const score = await langfuseWeb.score({
  traceId: "my-apps-session-1234",
  traceIdType: "EXTERNAL",
  name: "user-feedback",
  value: 1,
});
```

</Tab>

<Tab>

```typescript filename="api.ts"
const trace = langfuse.trace({ name: "chat-app-session" });
const traceId = (await trace.data).id;
const span = langfuse.span({
  traceId,
  name: "chat-interaction",
});
const spanId = (await span.data).id;
const event = langfuse.event({
  traceId,
  parentObservationId: spanId,
  name: "get-user-profile",
});
const generation = langfuse.generation({
  traceId,
  parentObservationId: spanId,
  name: "chat-completion",
});
```

</Tab>

</Tabs>

## 3. Collect scores

Scores are used to evaluate executions/traces. They are attached to a single [trace](#trace). If the score relates to a specific step of the trace, the score can optionally also be atatched to the observation to enable evaluating it specifically.

> - `traceId`: the id of the trace to which the score should be attached
>   - Passed from backend: `traceId = (await trace.data).id`
> - `name`: identifier of the score, string
> - `value`: the value of the score; float; optional: scale it to e.g. 0..1 to make it comparable to other scores
> - `observationId` (optional): the id of the span, event or generation to which the score should be attached
>   - Passed from backend, e.g.: `observationId = (await generation.data).id`
> - `traceIdType` (optional): the type of the traceId. Can be `LANGFUSE` (default) or `EXTERNAL`. If `EXTERNAL` is used, the score will be attached to the trace with the given externalId.
> - `comment` (optional): additional context/explanation of the score

Scores can be modified by both Langfuse and LangfuseWeb clients.

langfuseWeb returns a promise that resolves when the score is created. Commonly the promise is awaited in the frontend to ensure that the score is created before the UI response is sent to the user.

```typescript filename="frontend.ts"
// with Langfuse trace id
await langfuseWeb.score({
  traceId: message.traceId,
  name: "user-feedback",
  value: 1,
  observationId: generation.id,
  comment: "I like how personalized the response is",
});
```

## Troubleshooting

If you encounter any issue, we are happy to help on [Discord](https://discord.gg/7NXusRtqYU) or shoot us an email: help@langfuse.com

---
description: Survey of common security problems facing LLM-based applications and how to use Langfuse to trace, prevent, and evaluate security risks.
---

# Security with Langfuse

There are a host of potential security risks involved with LLM-based applications. These include prompt injection, leakage of personally identifiable information (PII), or harmful prompts. Langfuse can be used to monitor and protect against these security risks.

Langfuse's features, especially [traces](https://langfuse.com/docs/tracing) and [model-based evaluations](https://langfuse.com/docs/scores/overview), can be used in tandem with security tools. Langfuse can be used to:

- Integrate with Security Libraries (e.g. Prompt Guard, Nemo Guardrails, Prompt Armour, Lakera Guard)
- Trace these security tools to observe and debug whether they work as expected
- Track and analyze risk scores
- Do all of this fully async, meaning it does not interrupt program execution

In the example below we use the open source library [LLM Guard](https://llm-guard.com/), but there are other open-source and/or paid security tools available, such as Prompt Armor, Nemo Guardrails, Microsoft Azure Responsible AI, and Lakera Guard.

## Overview

Short video here. Here's the rough outline of what I was going to say:
- Overview of what this doc is: a guide to how to think about security measures and how to integrate Langfuse
- What it is not: a comprehensive security tooling kit, exhaustive, nor a new feature
- Walk through an example: anonymize and deanonymize
  - then walk through langfuse UI to see how you can observe latency, accuracy, inputs/outputs, risk scores
  - then can show how to do model-based evaluation to provide another layer of security

## How Can Langfuse Help Mitigate Security Risks?

Security tools like LLM Guard can help with security measures in the following ways:

1. Catching and blocking a potentially harmful or inappropriate prompt before sending to the model
2. Redacting sensitive PII before being sending into the model and then un-redacting in the response
3. Evaluating prompts and completions in Langfuse based on qualities like toxicity, relevance, or sensitive material.

Langfuse can be used to trace each step of the security screening, recording metrics such as latency, accuracy, risk scores, and more. Traces can be monitored to ensure security tools are working as expected, and can also be used to benchmark different libraries against each other.

## Example: Personally Identifiable Information

Personally Identifiable Information (PII) includes:

- Credit card number
- Full name
- Phone number
- Email address
- Social Security number
- IP Address

Exposing PII to LLMs can pose serious security and privacy risks, such as violating contractual obligations or regulatory compliance requirements, or mitigating the risks of data leakage or a data breach.

The example below shows a simple application that summarizes a given court transcript. For privacy reasons, the application wants to anonymize PII before the information is fed into the model, and then un-redact the response to produce a coherent summary.

To read more about other security risks, including prompt injection, banned topics, or malicious URLs, please visit the LLM Guard [documentation](https://llm-guard.com/) or check out or Security [cookbook](/cookbook/security.ipynb).

### Installation & Setup

```sh
pip install llm-guard langfuse openai
```

### Use Anonymize and Deanonymize

We break up each step of the process into its own function so we can track each step separately in Langfuse.

```python
from llm_guard.input_scanners import Anonymize
from llm_guard.vault import Vault
from llm_guard.input_scanners.anonymize_helpers import BERT_LARGE_NER_CONF
from langfuse.decorators import observe, langfuse_context
from langfuse.openai import openai # OpenAI integration
from llm_guard.output_scanners import Deanonymize

vault = Vault()

prompt = """
Plaintiff, Jane Doe, by and through her attorneys, files this complaint against Defendant, Big Corporation, and alleges upon information and belief, except for those allegations pertaining to personal knowledge, that on or about July 15, 2023, at the Defendant's manufacturing facility located at 123 Industrial Way, Springfield, Illinois, Defendant negligently failed to maintain safe working conditions, leading to Plaintiff suffering severe and permanent injuries. As a direct and proximate result of Defendant's negligence, Plaintiff has endured significant physical pain, emotional distress, and financial hardship due to medical expenses and loss of income. Plaintiff seeks compensatory damages, punitive damages, and any other relief the Court deems just and proper."""

@observe()
def anonymize(input: str):
  scanner = Anonymize(vault, preamble="Insert before prompt", allowed_names=[], hidden_names=["Test LLC"],
                    recognizer_conf=BERT_LARGE_NER_CONF, language="en")
  sanitized_prompt, is_valid, risk_score = scanner.scan(prompt)
  return sanitized_prompt

@observe()
def deanonymize(sanitized_prompt: str, answer: str):
  scanner = Deanonymize(vault)
  sanitized_model_output, is_valid, risk_score = scanner.scan(sanitized_prompt, answer)

  return sanitized_model_output

@observe()
def summarize_transcript(prompt: str):
  sanitized_prompt = anonymize(prompt)

  answer = openai.chat.completions.create(
        model="gpt-3.5-turbo",
        max_tokens=100,
        messages=[
          {"role": "system", "content": "Summarize the given court transcript."},
          {"role": "user", "content": sanitized_prompt}
        ],
    ).choices[0].message.content

  sanitized_model_output = deanonymize(sanitized_prompt, answer)

  return sanitized_model_output

@observe()
def main():
    return summarize_transcript(prompt)

main()
```

<Callout type="info">
  See the trace generated from this example. You can track each step (Anonymize, Summarize, and Deanonymize) separately: https://cloud.langfuse.com/project/clvh71wlt0000wpuqhferayy5/traces/d31a6a8a-03b4-4487-8d87-40c3a68464a5?observation=aa28105d-c33e-4333-828a-d916de48e387
</Callout>

![Trace](../../public/images/docs/security-docs-trace.png)

## Advanced Usage

### How to Use Langfuse with Security Tools?

#### Model-Based Evaluation

<Callout type="info">
  Read more about model-based evaluations and how to get started with them here: https://langfuse.com/docs/scores/model-based-evals
</Callout>

Tools like LLM Guard help to scan prompts before they get sent to the model. You can then use Langfuse to provide another layer of security after the fact by evaluating the completed responses. Langfuse's [model-based evaluations](https://langfuse.com/docs/scores/model-based-evals) will run asynchronously and scan traces for things such as toxicity or sensitivity to flag potential risks and identify any gaps in your LLM security setup.

Integrating Langfuse into your team's workflow can help teams identify which security risks are most prevalent and build more robust tools around those specific issues.

#### Track Latency and Accuracy

Another benefit of Langfuse is monitoring the latency and accuracy of security tools. Langfuse allows users to see how much time (and also cost) each step the security process takes. Another way Langfuse can be used is allowing users to monitor the risk scores returned by certain scanners and using that information to calibrate the sensitivity of their models and fine-tune their models to protect against known security risks.
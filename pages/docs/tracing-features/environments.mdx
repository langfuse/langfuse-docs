---
description: Configure environments to organize your traces, observations, and scores.
---

# Environments

Environments allow you to organize your traces, observations, and scores from different contexts such as production, staging, or development.

You can configure the environment by setting the `LANGFUSE_TRACING_ENVIRONMENT` environment variable or by using the `environment` parameter in the client initialization. If both are specified, the initialization parameter takes precedence. If nothing is specified, the default environment is `default`.

The environment must be a string that follows this regex pattern: `^(?!langfuse)[a-z0-9-_]+$`

This means:

- Cannot start with "langfuse"
- Can only contain lowercase letters, numbers, hyphens, and underscores

<Tabs items={["Python", "JS/TS", "OpenAI (Python)", "OpenAI (JS/TS)", "Langchain (Python)", "Langchain (JS/TS)", "Vercel AI SDK (JS/TS)", "LlamaIndex (instrumentor)"]}>
<Tab>

When using the [`@observe()` decorator](/docs/sdk/python/decorators):

```python
from langfuse.decorators import langfuse_context, observe

# Either set the environment variable or configure the decorator. The latter takes precedence.
os.environ["LANGFUSE_TRACING_ENVIRONMENT"] = "production"

# Configure the decorator
langfuse_context.configure(environment="production")

@observe()
def fn():
    pass

fn()
```

When using the [low-level SDK](/docs/sdk/python/low-level-sdk):

```python
from langfuse import Langfuse

# Either set the environment variable or the constructor parameter. The latter takes precedence.
os.environ["LANGFUSE_TRACING_ENVIRONMENT"] = "production"

# Configure the client
langfuse = Langfuse(environment="production")

trace = langfuse.trace(
  name="Rap Battle",
)
```

</Tab>
<Tab>

```ts
import { Langfuse } from "langfuse";

const langfuse = new Langfuse({
  environment: "production",
});
```

See [JS/TS SDK docs](/docs/sdk/typescript/guide#environments) for more details.

</Tab>
<Tab>

When using the [OpenAI SDK Integration](/docs/integrations/openai)

```python
from langfuse.openai import openai

# Either set the environment variable or configure the openai import. The latter takes precedence.
os.environ["LANGFUSE_TRACING_ENVIRONMENT"] = "production"

# Configure the openai import
openai.langfuse_environment = "production"

completion = openai.chat.completions.create(
  name="test-chat",
  model="gpt-3.5-turbo",
  messages=[
    {"role": "system", "content": "You are a calculator."},
    {"role": "user", "content": "1 + 1 = "}],
)
```

</Tab>
<Tab>

```ts
import OpenAI from "openai";
import { observeOpenAI } from "langfuse";

const openai = observeOpenAI(new OpenAI(), {
  clientInitParams: {
    environment: "production",
  },
});
```

See [OpenAI Integration (JS/TS)](/docs/integrations/openai/js/get-started) for more details.

</Tab>

<Tab>

When using the [CallbackHandler](/docs/integrations/langchain/tracing)

```python
from langfuse.callback import CallbackHandler

# Either set the environment variable or the constructor parameter. The latter takes precedence.
os.environ["LANGFUSE_TRACING_ENVIRONMENT"] = "production"
handler = CallbackHandler(
  environment="production"
)
```

</Tab>

<Tab>

```ts
import { CallbackHandler } from "langfuse-langchain";

const handler = new CallbackHandler({
  environment: "production",
});
```

See [Langchain Integration (JS/TS)](/docs/integrations/langchain/tracing) for more details.

</Tab>

<Tab>

When using the [Vercel AI SDK Integration](/docs/integrations/vercel-ai-sdk)

```ts filename="instrumentation.ts" {/environment: "production"/}
import { registerOTel } from "@vercel/otel";
import { LangfuseExporter } from "langfuse-vercel";

export function register() {
  registerOTel({
    serviceName: "langfuse-vercel-ai-nextjs-example",
    traceExporter: new LangfuseExporter({ environment: "production" }),
  });
}
```

</Tab>

<Tab>

When using the [LlamaIndex Integration](/docs/integrations/llama-index/get-started)

```python
import os
from langfuse.llama_index import LlamaIndexInstrumentor

# Either set the environment variable or the constructor parameter. The latter takes precedence.
os.environ["LANGFUSE_TRACING_ENVIRONMENT"] = "production"
instrumentor = LlamaIndexInstrumentor(environment="production")
```

</Tab>

</Tabs>

## GitHub Discussions

import { GhDiscussionsPreview } from "@/components/gh-discussions/GhDiscussionsPreview";

<GhDiscussionsPreview labels={["feat-tracing-environments"]} />

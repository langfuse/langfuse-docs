---
description: Use Log Levels to control the verbosity of your logs and highlight errors and warnings.
---

# Log Levels

Traces can have a lot of observations ([data model](/docs/tracing#introduction-to-traces-in-langfuse)). You can differentiate the importance of observations with the `level` attribute to control the verbosity of your traces and highlight errors and warnings. Available `levels`: `DEBUG`, `DEFAULT`, `WARNING`, `ERROR`.

In addition to the level, you can also include a `statusMessage` to provide additional context.

<Frame border>
  ![Trace log level and statusMessage](/images/docs/trace-log-level.png)
</Frame>

<Tabs items={["Python SDK (v3)", "Python SDK (v2)", "JS/TS", "OpenAI SDK", "Langchain", "LlamaIndex"]}>
<Tab>
When using the [`@observe()` decorator](/docs/sdk/python/decorators):

```python
from langfuse import observe, get_client

@observe()
def my_function():
    langfuse = get_client()
    
    # ... processing logic ...
    # Update the current span with a warning level
    langfuse.update_current_span(
        level="WARNING",
        status_message="This is a warning"
    )
```

When creating spans or generations directly:

```python
from langfuse import get_client

langfuse = get_client()

# Using context managers (recommended)
with langfuse.start_as_current_span(name="my-operation") as span:
    # Set level and status message on creation
    with span.start_as_current_span(
        name="potentially-risky-operation",
        level="WARNING",
        status_message="Operation may fail"
    ) as risky_span:
        # ... do work ...

        # Or update level and status message later
        risky_span.update(
            level="ERROR",
            status_message="Operation failed with unexpected input"
        )

# You can also update the currently active span without a direct reference
with langfuse.start_as_current_span(name="another-operation"):
    # ... some processing ...
    langfuse.update_current_span(
        level="DEBUG",
        status_message="Processing intermediate results"
    )
```

Levels can also be set when creating generations:

```python
langfuse = get_client()

with langfuse.start_as_current_generation(
    name="llm-call",
    model="gpt-4o",
    level="DEFAULT"  # Default level
) as generation:
    # ... make LLM call ...

    if error_detected:
        generation.update(
            level="ERROR",
            status_message="Model returned malformed output"
        )
```

</Tab>
<Tab>

When using the [`@observe()` decorator](/docs/sdk/python/decorators):

```python
from langfuse.decorators import langfuse_context, observe

@observe()
def fn():
    langfuse_context.update_current_observation(
        level="WARNING",
        status_message="This is a warning"
    )

# outermost function becomes the trace, level and status message are only available on observations
@observe()
def main():
    fn()

main()
```

When using the [low-level SDK](/docs/sdk/python/low-level-sdk):

```python
from langfuse import Langfuse
langfuse = Langfuse()

trace = langfuse.trace()

# Add an observation (span) with a level and status message
trace.span(
    level="WARNING",
    status_message="This is a warning"
)
```

</Tab>
<Tab>

```ts
import { Langfuse } from "langfuse";
const langfuse = new Langfuse();

const trace = langfuse.trace();

// Add an observation (span) with a level and status message
trace.span({
  level: "WARNING",
  statusMessage: "This is a warning",
});
```

See [JS/TS SDK docs](/docs/sdk/typescript/guide) for more details.

</Tab>
<Tab>

When using the [OpenAI SDK Integration](/docs/integrations/openai), `level` and `statusMessage` are automatically set based on the OpenAI API response. See [example](/docs/integrations/openai/python/track-errors).

</Tab>
<Tab>

When using the [LangChain Integration](/docs/integrations/langchain), `level` and `statusMessage` are automatically set for each step in the LangChain pipeline.

</Tab>
<Tab>

When using the [LlamaIndex Integration](/docs/integrations/llama-index), `level` and `statusMessage` are automatically set for each step in the LlamaIndex pipeline.

</Tab>

</Tabs>

## Filter Trace by Log Level

When viewing a single trace, you can filter the observations by log level.

<CloudflareVideo
  videoId="70f03a9919ed0b3f4807aecfccaffc2c"
  aspectRatio={16 / 9}
  gifStyle
/>

## GitHub Discussions

import { GhDiscussionsPreview } from "@/components/gh-discussions/GhDiscussionsPreview";

<GhDiscussionsPreview labels={["feat-log-levels"]} />

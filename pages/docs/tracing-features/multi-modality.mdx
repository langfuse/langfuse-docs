---
title: Multi-Modality & Attachments
description: Langfuse fully supports multi-modal LLM traces, including text, images, audio, and attachments.
---

# Multi-Modality and Attachments

<Callout type="info">

Support for media attachments to traces is currently in beta. Please report any [issues](/issues) and add feature requests to this ongoing [discussion thread](https://github.com/orgs/langfuse/discussions/3004).

</Callout>

Langfuse supports multi-modal traces including **text, images, audio, and other attachments**.

By default, **[base64 encoded data URIs](https://developer.mozilla.org/en-US/docs/Web/URI/Schemes/data#syntax) are handled automatically by the Langfuse SDKs**. They are extracted from the payloads commonly used in multi-modal LLMs, uploaded to Langfuse's object storage, and linked to the trace.

This also works if you:

1. Reference media files via external URLs.
2. Customize the handling of media files in the SDKs via the `LangfuseMedia` class.
3. Integrate via the Langfuse API directly.

Learn more on how to get started and how this works under the hood below.

_Examples_

import MultiModalImageGallery from "@/components-mdx/multi-modal-image-gallery.mdx";

<MultiModalImageGallery />

## Availability

### Langfuse Cloud

Multi-modal attachments on Langfuse Cloud are free while in beta. We will be rolling out a new pricing metric to account for the additional storage and compute costs associated with large multi-modal traces in the coming weeks.

### Self-hosting

Multi-modal attachments are available today. You need to configure your own object storage bucket via the Langfuse environment variables (`LANGFUSE_S3_MEDIA_UPLOAD_*`). See self-hosting documentation for details on these environment variables. S3-compatible APIs are supported across all major cloud providers and can be self-hosted via minio. Note that the configured storage bucket must have a publicly resolvable hostname to support direct uploads via our SDKs and media asset fetching directly from the browser.

## Supported media formats

Langfuse supports:

- **Images**: .png, .jpg, .webp
- **Audio files**: .mpeg, .mp3, .wav
- **Other attachments**: .pdf, plain text

If you require support for additional file types, please let us know in our [GitHub Discussion](https://github.com/orgs/langfuse/discussions/3004) where we're actively gathering feedback on multi-modal support.

## Get Started

### Base64 data URI encoded media

If you use base64 encoded images, audio, or other files in your LLM applications, upgrade to the latest version of the Langfuse SDKs. The Langfuse SDKs automatically detect and handle base64 encoded media by extracting it, uploading it separately as a Langfuse Media file, and including a reference in the trace.

This works with standard Data URI ([MDN](https://developer.mozilla.org/en-US/docs/Web/URI/Schemes/data#syntax)) formatted media (like those used by OpenAI and other LLMs).

This [notebook](/guides/cookbook/example_multi_modal_traces) includes a couple of examples using the OpenAI SDK and LangChain.

### External media (URLs)

Langfuse supports in-line rendering of media files via URLs if they follow common formats. In this case, the media file is not uploaded to Langfuse's object storage but simply rendered in the UI directly from the source.

Supported formats:

<Tabs items={["Markdown images", "OpenAI content parts"]}>
<Tab>

```md
![Alt text](https://example.com/image.jpg)
```

</Tab>
<Tab>

```json
{
  "content": [
    {
      "role": "system",
      "content": "You are an AI trained to describe and interpret images. Describe the main objects and actions in the image."
    },
    {
      "role": "user",
      "content": [
        {
          "type": "text",
          "text": "What's happening in this image?"
        },
        {
          "type": "image_url",
          "image_url": {
            "url": "https://example.com/image.jpg"
          }
        }
      ]
    }
  ]
}
```

</Tab>
</Tabs>

### Custom attachments

If you want to have more control or your media is not base64 encoded, you can upload arbitrary media attachments to Langfuse via the SDKs using the new `LangfuseMedia` class. Wrap media with LangfuseMedia before including it in trace inputs, outputs, or metadata. See the multi-modal documentation for examples.

<Tabs items={["Python SDK (v3)", "Python SDK (v2)", "JS/TS"]}>
<Tab>
```python
from langfuse import get_client, observe
from langfuse.media import LangfuseMedia

# Create a LangfuseMedia object from a file
with open("static/bitcoin.pdf", "rb") as pdf_file:
    pdf_bytes = pdf_file.read()

# Wrap media in LangfuseMedia class
pdf_media = LangfuseMedia(content_bytes=pdf_bytes, content_type="application/pdf")

# Using with the decorator
@observe()
def process_document():
    langfuse = get_client()

    # Update the current trace with the media file
    langfuse.update_current_trace(
        metadata={"document": pdf_media}
    )

    # Or update the current span
    langfuse.update_current_span(
        input={"document": pdf_media}
    )

# Using with context managers
langfuse = get_client()

with langfuse.start_as_current_span(name="analyze-document") as span:
    # Include media in the span input, output, or metadata
    span.update(
        input={"document": pdf_media},
        metadata={"file_size": len(pdf_bytes)}
    )

    # Process document...

    # Add results with media to the output
    span.update(output={
        "summary": "This document explains Bitcoin...",
        "original": pdf_media
    })
```

</Tab>
<Tab>

```python
from langfuse.decorators import observe, langfuse_context
from langfuse.media import LangfuseMedia

with open("static/bitcoin.pdf", "rb") as pdf_file:
        pdf_bytes = pdf_file.read()

# Wrap media in LangfuseMedia class
wrapped_obj = LangfuseMedia(
    obj=pdf_bytes, content_bytes=pdf_bytes, content_type="application/pdf"
)

# Optionally, access media via wrapped_obj.obj
wrapped_obj.obj

@observe()
def main():
    langfuse_context.update_current_trace(
      input=wrapped_obj,
      metadata={
          "context": wrapped_obj
      },
    )

    return # Limitation: LangfuseMedia object does not work in decorated function IO, needs to be set via update_current_trace or update_current_observation

main()
```

</Tab>
<Tab>

```typescript
import { Langfuse, LangfuseMedia } from "langfuse";
import fs from "fs";

// Initialize Langfuse client
const langfuse = new Langfuse();

// Wrap media in LangfuseMedia class
const wrappedMedia = new LangfuseMedia({
  contentBytes: fs.readFileSync("./static/bitcoin.pdf"),
  contentType: "application/pdf",
});

// Optionally, access media via wrappedMedia.obj
console.log(wrappedMedia.obj);

// Include media in any trace or observation
const trace = langfuse.trace({
  name: "test-trace-10",
  metadata: {
    context: wrappedMedia,
  },
});
```

</Tab>
<Tab>

When using [OpenTelemetry](/docs/opentelemetry/get-started), multi-modal content should be handled as span attributes. Since OpenTelemetry doesn't have native multi-modal support, you have several options:

**Option 1: Base64 Data URIs as Attributes**

```python
from opentelemetry import trace
import base64

tracer = trace.get_tracer(__name__)

with tracer.start_as_current_span("process-image") as span:
    # Include base64 encoded media as span attributes
    with open("image.jpg", "rb") as image_file:
        image_data = base64.b64encode(image_file.read()).decode()
        data_uri = f"data:image/jpeg;base64,{image_data}"
    
    # Set as span attribute - will be processed by Langfuse
    span.set_attribute("input.image", data_uri)
    span.set_attribute("gen_ai.prompt.0.content", f"Analyze this image: {data_uri}")
```

**Option 2: External URLs**

```python
from opentelemetry import trace

tracer = trace.get_tracer(__name__)

with tracer.start_as_current_span("analyze-image") as span:
    # Reference external images via URLs
    image_url = "https://example.com/image.jpg"
    
    span.set_attribute("input.image_url", image_url)
    span.set_attribute("gen_ai.prompt.0.content", f"![Image]({image_url})")
```

**Option 3: Media References in OpenAI Format**

```python
from opentelemetry import trace
import json

tracer = trace.get_tracer(__name__)

with tracer.start_as_current_span("multimodal-llm-call") as span:
    # Structure multi-modal content in OpenAI format
    messages = [
        {
            "role": "user",
            "content": [
                {"type": "text", "text": "What's in this image?"},
                {
                    "type": "image_url",
                    "image_url": {"url": "data:image/jpeg;base64,/9j/4AAQ..."}
                }
            ]
        }
    ]
    
    span.set_attribute("gen_ai.prompt_json", json.dumps(messages))
    span.set_attribute("gen_ai.request.model", "gpt-4o")
```

**Note:** OpenTelemetry itself doesn't have specialized multi-modal handling. When using OpenTelemetry with Langfuse, the base64 data URIs in span attributes will be automatically detected and processed by Langfuse according to its multi-modal media handling.

Learn more about OpenTelemetry integration in the [OpenTelemetry documentation](/docs/opentelemetry/get-started).

</Tab>
</Tabs>

### API

If you use the API directly to log traces to Langfuse, you need to follow these steps:

<Steps>

### Upload media to Langfuse

1. If you use base64 encoded media: you need to extract it from the trace payloads similar to how the Langfuse SDKs do it.
2. Initialize the upload and get a `mediaId` and `presignedURL`: [`POST /api/public/media`](https://api.reference.langfuse.com/#post-/api/public/media).
3. Upload media file: `PUT [presignedURL]`.

See this [end-to-end example](/guides/cookbook/example_multi_modal_traces#custom-via-api) (Python) on how to use the API directly to upload media files.

### Add reference to mediaId in trace/observation

Use the [Langfuse Media Token](#media-token) to reference the `mediaId` in the trace or observation `input`, `output`, or `metadata`.

</Steps>

## How does it work?

When using media files (that are not referenced via external URLs), Langfuse handles them in the following way:

### 1. Media Upload Process

#### Detection and Extraction

- Langfuse supports media files in traces and observations on `input`, `output`, and `metadata` fields
- SDKs separate media from tracing data client-side for performance optimization
- Media files are uploaded directly to object storage (AWS S3 or compatible)
- Original media content is replaced with a reference string

#### Security and Optimization

- Uploads use presigned URLs with content validation (content length, content type, content SHA256 hash)
- Deduplication: Files are simply replaced by their `mediaId` reference string if already uploaded
- File uniqueness determined by project, content type, and content SHA256 hash

#### Implementation Details

- Python SDK: Background thread handling for non-blocking execution
- JS/TS SDKs: Asynchronous, non-blocking implementation
- API support for direct uploads (see [guide](/guides/cookbook/example_multi_modal_traces#custom-via-api))

### 2. Media Reference System [#media-reference]

The base64 data URIs and the wrapped `LangfuseMedia` objects in Langfuse traces are replaced by references to the `mediaId` in the following standardized token format, which helps reconstruct the original payload if needed:

```
@@@langfuseMedia:type={MIME_TYPE}|id={LANGFUSE_MEDIA_ID}|source={SOURCE_TYPE}@@@
```

- `MIME_TYPE`: MIME type of the media file, e.g., `image/jpeg`
- `LANGFUSE_MEDIA_ID`: ID of the media file in Langfuse's object storage
- `SOURCE_TYPE`: Source type of the media file, can be `base64_data_uri`, `bytes`, or `file`

Based on this token, the Langfuse UI can automatically detect the `mediaId` and render the media file inline. The `LangfuseMedia` class provides utility functions to extract the `mediaId` from the reference string.

### 3. Resolving Media References

When dealing with traces, observations, or dataset items that include media references, you can convert them back to their base64 data URI format using the `resolve_media_references` utility method provided by the Langfuse client. This is particularly useful for reinserting the original content during fine-tuning, dataset runs, or replaying a generation. The utility method traverses the parsed object and returns a deep copy with all media reference strings replaced by the corresponding base64 data URI representations.

<Tabs items={["Python SDK (v3)", "Python SDK (v2)", "JS/TS", "OpenTelemetry"]}>
<Tab>

```python
from langfuse import get_client

# Initialize Langfuse client
langfuse = get_client()

# Example object with media references
obj = {
    "image": "@@@langfuseMedia:type=image/jpeg|id=some-uuid|source=bytes@@@",
    "nested": {
        "pdf": "@@@langfuseMedia:type=application/pdf|id=some-other-uuid|source=bytes@@@"
    }
}

# Resolve media references to base64 data URIs
resolved_obj = langfuse.resolve_media_references(
    obj=obj,
    resolve_with="base64_data_uri"
)

# Result:
# {
#     "image": "data:image/jpeg;base64,/9j/4AAQSkZJRg...",
#     "nested": {
#         "pdf": "data:application/pdf;base64,JVBERi0xLjcK..."
#     }
# }
```

</Tab>
<Tab>

```python
from langfuse import Langfuse

# Initialize Langfuse client
langfuse = Langfuse()

# Example object with media references
obj = {
    "image": "@@@langfuseMedia:type=image/jpeg|id=some-uuid|source=bytes@@@",
    "nested": {
        "pdf": "@@@langfuseMedia:type=application/pdf|id=some-other-uuid|source=bytes@@@"
    }
}

# Resolve media references to base64 data URIs
resolved_trace = langfuse.resolve_media_references(
    obj=obj,
    resolve_with="base64_data_uri"
)

# Result:
# {
#     "image": "data:image/jpeg;base64,/9j/4AAQSkZJRg...",
#     "nested": {
#         "pdf": "data:application/pdf;base64,JVBERi0xLjcK..."
#     }
# }
```

</Tab>
<Tab>

```typescript
import { Langfuse } from "langfuse";

// Initialize Langfuse client
const langfuse = new Langfuse();

// Example object with media references
const obj = {
  image: "@@@langfuseMedia:type=image/jpeg|id=some-uuid|source=bytes@@@",
  nested: {
    pdf: "@@@langfuseMedia:type=application/pdf|id=some-other-uuid|source=bytes@@@",
  },
};

// Resolve media references to base64 data URIs
const resolvedTrace = await langfuse.resolveMediaReferences({
  obj: obj,
  resolveWith: "base64DataUri",
});

// Result:
// {
//     image: "data:image/jpeg;base64,/9j/4AAQSkZJRg...",
//     nested: {
//         pdf: "data:application/pdf;base64,JVBERi0xLjcK..."
//     }
// }
```

</Tab>
<Tab>

When using [OpenTelemetry](/docs/opentelemetry/get-started), media references in traces are handled automatically by the Langfuse backend when processing OpenTelemetry spans. However, if you need to resolve media references manually, you would need to use the Langfuse SDK alongside OpenTelemetry:

```python
from opentelemetry import trace
from langfuse import get_client
import json

# Get the Langfuse client for media resolution
langfuse = get_client()

tracer = trace.get_tracer(__name__)

with tracer.start_as_current_span("process-media-references") as span:
    # Example span attribute with media references
    span_data = {
        "image": "@@@langfuseMedia:type=image/jpeg|id=some-uuid|source=bytes@@@",
        "nested": {
            "pdf": "@@@langfuseMedia:type=application/pdf|id=some-other-uuid|source=bytes@@@"
        }
    }
    
    # Set as span attribute (Langfuse will handle automatically)
    span.set_attribute("input.data", json.dumps(span_data))
    
    # If you need to resolve manually for processing
    resolved_data = langfuse.resolve_media_references(
        obj=span_data,
        resolve_with="base64_data_uri"
    )
    
    # Use resolved_data for further processing
    span.set_attribute("processed.resolved_data", json.dumps(resolved_data))
```

**Note:** When using OpenTelemetry with Langfuse, media references in span attributes are automatically resolved in the Langfuse UI. Manual resolution is typically only needed if you want to process the media content in your application code.

Learn more about OpenTelemetry integration in the [OpenTelemetry documentation](/docs/opentelemetry/get-started).

</Tab>
</Tabs>

## GitHub Discussions

import { GhDiscussionsPreview } from "@/components/gh-discussions/GhDiscussionsPreview";

<GhDiscussionsPreview labels={["feat-multimodal"]} />

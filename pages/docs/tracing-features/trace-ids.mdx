---
title: Trace IDs & Distributed Tracing
description: Bring your own trace IDs for distributed tracing and linking traces across services.
---

# Trace IDs & Distributed Tracing

Langfuse allows you to bring your own trace IDs (e.g., messageId, traceId, correlationId) for

- distributed tracing
- and linking traces across services for lookups between services.

<Callout>

By default, Langfuse assigns random IDs (uuid, cuid) to all logged events.

</Callout>

<Callout type="info">

It is recommended to use your own domain specific IDs (e.g., messageId, traceId, correlationId) as it helps with downstream use cases like:

- [deeplinking](/docs/tracing-features/url) to the trace from your own ui or logs
- [evaluating](/docs/scores) and adding custom metrics to the trace
- [fetching](/docs/api) the trace from the API

</Callout>

## Data Model

Trace IDs in Langfuse:

- Must be unique within a project
- Are used to identify and group related observations
- Can be used for distributed tracing across services
- Support upsert operations (creating or updating based on ID)

## Usage

<Tabs items={["Python", "JS/TS", "OpenTelemetry", "OpenAI (Python)", "OpenAI (JS/TS)", "Langchain (Python)", "Langchain (JS/TS)", "LiteLLM"]}>
<Tab>

When using the [`@observe()` decorator](/docs/sdk/python/decorators):

```python
from langfuse.decorators import langfuse_context, observe
import uuid

@observe()
def process_user_request(user_id, request_data, **kwargs):
    # Function logic here
    pass

def main():
    # Custom trace ID
    custom_trace_id = "custom-" + str(uuid.uuid4())

    # Pass id as kwarg
    # Can be passed to multiple functions
    process_user_request(
        user_id=user_id,
        request_data=request_data,
        langfuse_parent_trace_id=custom_trace_id,
    )
```

When using the [low-level SDK](/docs/sdk/python/low-level-sdk):

```python
from langfuse import Langfuse

# Set custom trace ID during creation
trace = langfuse.trace(
    id="my-custom-trace-id",
    name="Rap Battle",
)
```

</Tab>
<Tab>

```ts
import { Langfuse } from "langfuse";
import { v4 as uuidv4 } from "uuid";

const langfuse = new Langfuse();

// Create trace with custom ID
const traceId = uuidv4();
const trace = langfuse.trace({
  id: traceId,
  name: "chat-app-session",
  userId: "user_123456789",
  metadata: { user: "user@langfuse.com" },
  tags: ["production"],
});

// Create observations with custom IDs
const span = trace.span({
  id: "custom-span-id",
  name: "chat-interaction",
});

const generation = trace.generation({
  id: "custom-generation-id",
  name: "chat-completion",
});
```

</Tab>
<Tab>

When using [OpenTelemetry](/docs/opentelemetry/get-started), trace IDs are handled automatically by the OpenTelemetry SDK. You can access and set trace IDs using the OpenTelemetry context:

```python
from opentelemetry import trace
from opentelemetry.trace import Status, StatusCode

tracer = trace.get_tracer(__name__)

with tracer.start_as_current_span("my-operation") as span:
    # Get the trace ID
    trace_id = format(span.get_span_context().trace_id, "032x")

    # Set custom attributes
    span.set_attribute("custom.trace_id", trace_id)
```

</Tab>
<Tab>

When using the [OpenAI SDK Integration](/docs/integrations/openai), you have two options for working with trace IDs:

1. Directly set the trace_id in the completion call:

```python
from langfuse.openai import openai

# Set trace_id directly in the completion call
completion = openai.chat.completions.create(
    name="test-chat",
    model="gpt-3.5-turbo",
    messages=[
        {"role": "system", "content": "You are a calculator."},
        {"role": "user", "content": "1 + 1 = "}
    ],
    trace_id="my-custom-trace-id"  # Set your custom trace ID
)
```

2. Use the [`@observe()` decorator](/docs/sdk/python/decorators) for automatic trace management:

```python
from langfuse.decorators import langfuse_context, observe
from langfuse.openai import openai

@observe()
def calculate_sum(a: int, b: int, **kwargs):
    # Get the current trace ID if needed
    trace_id = langfuse_context.get_current_trace_id()

    completion = openai.chat.completions.create(
        name="calculator",
        model="gpt-3.5-turbo",
        messages=[
            {"role": "system", "content": "You are a calculator. Only output the numeric result."},
            {"role": "user", "content": f"{a} + {b} = "}
        ]
    )

    # The completion will automatically be added to the current trace
    return completion.choices[0].message.content, trace_id

# Use with custom trace ID
result, trace_id = calculate_sum(5, 3, langfuse_observation_id="my-custom-trace-id")
```

The decorator approach is recommended when you want to:

- Group multiple OpenAI calls into a single trace
- Add additional context or metadata to the trace
- Track the entire function execution, not just the OpenAI call

</Tab>
<Tab>

```ts
import OpenAI from "openai";
import { observeOpenAI } from "langfuse";

// Create a trace with custom ID
const trace = langfuse.trace({
  id: "custom-trace-id",
  name: "openai-chat",
});

const openai = observeOpenAI(new OpenAI(), {
  parent: trace, // Link OpenAI calls to the trace
});

const completion = await openai.chat.completions.create({
  model: "gpt-3.5-turbo",
  messages: [{ role: "user", content: "Hello!" }],
});
```

</Tab>
<Tab>

When using Langchain with Langfuse, you have two options for working with trace IDs:

1. Using the [CallbackHandler](/docs/integrations/langchain/tracing) and langchain run config:

```python
from langfuse.callback import CallbackHandler
import uuid

predefined_run_id = str(uuid.uuid4())

langfuse_handler = CallbackHandler()

# Pass run_id to the chain invocation
chain.invoke(
    {"input": "test"},
    config={
        "callbacks": [langfuse_handler],
        "run_id": predefined_run_id,  # This becomes the trace ID
    },
)
```

2. Using the [`@observe()` decorator](/docs/sdk/python/decorators) for automatic trace management:

```python
from langfuse.decorators import langfuse_context, observe
from langchain_openai import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from langchain.schema import StrOutputParser

prompt = ChatPromptTemplate.from_template("what is the city {person} is from?")
model = ChatOpenAI()
chain = prompt | model | StrOutputParser()

@observe()
def get_person_city(person: str, **kwargs):
    # Get the Langchain handler from the current context
    langfuse_handler = langfuse_context.get_current_langchain_handler()

    # The handler is automatically scoped to the current trace
    response = chain.invoke(
        {"person": person},
        config={"callbacks": [langfuse_handler]}
    )

    # Get trace ID if needed
    trace_id = langfuse_context.get_current_trace_id()
    return response, trace_id

# Use with custom trace ID
result, trace_id = get_person_city(
    "Einstein",
    langfuse_observation_id="my-custom-trace-id"
)
```

The decorator approach is recommended when you want to:

- Group multiple Langchain operations into a single trace
- Combine Langchain with other integrations (OpenAI, LlamaIndex, etc.)
- Add additional context or metadata to the trace
- Track the entire function execution, not just the Langchain operations

</Tab>
<Tab>

```ts
import { CallbackHandler } from "langfuse-langchain";

// Create a trace with custom ID
const trace = langfuse.trace({ id: "special-id" });

// CallbackHandler will use the trace with the specified ID
const langfuseHandler = new CallbackHandler({ root: trace });

// Use the handler in your chain
const chain = new LLMChain({
  llm: model,
  prompt,
  callbacks: [langfuseHandler],
});
```

</Tab>
<Tab>

When using [LiteLLM](/docs/integrations/litellm/tracing):

```python
from litellm import completion

# Set custom trace ID and other parameters
response = completion(
  model="gpt-3.5-turbo",
  messages=[
    {"role": "user", "content": "Hi ðŸ‘‹"}
  ],
  metadata={
      "generation_name": "test-generation",
      "generation_id": "gen-id",
      "trace_id": "trace-id",
      "trace_user_id": "user-id",
      "session_id": "session-id",
      "tags": ["tag1", "tag2"]
  },
)
```

</Tab>
</Tabs>

## Best Practices

1. **Consistent ID Format**: Use a consistent format for your trace IDs across your application.
2. **Unique IDs**: Ensure trace IDs are unique within your project to avoid conflicts.
3. **Distributed Tracing**: Use the same trace ID across different services to link related operations.
4. **Error Handling**: Implement proper error handling when working with custom trace IDs.

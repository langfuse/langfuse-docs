---
description: Fully async and typed SDK, runs in Node.js, edge, Deno
---

import { Callout } from "nextra/components";

# JS/TS Langchain SDK

<div className="flex flex-row flex-wrap gap-2 mt-2">
  <a href="https://github.com/langfuse/langfuse-langchain">
    <img
      alt="Github repository langfuse/langfuse-js"
      src="https://img.shields.io/badge/repo-langfuse--js-blue?style=flat-square&logo=Github"
    />
  </a>
  <a href="https://github.com/langfuse/langfuse-js/actions/workflows/ci.yml?query=branch%3Amain">
    <img
      src="https://img.shields.io/github/actions/workflow/status/langfuse/langfuse-js/ci.yml?style=flat-square&logo=Github&label=tests"
      alt="CI test status"
    />
  </a>
  <a href="https://www.npmjs.com/package/langfuse-langchain">
    <img
      src="https://img.shields.io/npm/v/langfuse-langchain?style=flat-square&label=npm+langfuse-langchain"
      alt="npm langfuse-langchain"
    />
  </a>
</div>

If you are working with Node.js, Deno, or Edge functions, the `langfuse` library is the simplest way to integrate Langfuse into your Langchain application. The library queues calls to make them non-blocking.

Supported runtimes:

- [x] Node.js
- [x] Edge: Vercel, Cloudflare, ...
- [x] Deno

Want to work without Langchain? Use [Langfuse](/docs/sdk/typescript)
for tracing and [LangfuseWeb](/docs/sdk/typescript-web) to capture feedback from the browser.

## Installation

```sh
npm i langfuse-langchain
# or
yarn add langfuse-langchain

# Deno
import CallbackHandler from 'https://esm.sh/langfuse-langchain'
```

In your application, set the **api keys** to create a client.

```typescript
import { CallbackHandler } from "langfuse-langchain";

const handler = new CallbackHandler({
  secretKey: "sk-lf-...",
  publicKey: "pk-lf-...",
  // options
});
```

### Options

| Variable | Description                                                                         | Default value                                                                                                                                           |
| -------- | ----------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------- |
| baseUrl  | BaseUrl of the Langfuse API                                                         | `"https://cloud.langfuse.com"`                                                                                                                          |
| release  | The release number/hash of the application to provide analytics grouped by release. | `process.env.LANGFUSE_RELEASE` or [common system environment names](https://github.com/langfuse/langfuse-js/blob/main/langfuse-core/src/release-env.ts) |
| userId   | For user-level analytics ([docs](/docs/user-explorer))                              | `undefined`                                                                                                                                             |

<Callout type="info" emoji="ℹ️">
  In short-lived environments (e.g. serverless functions), make sure to always
  call `handler.shutdownAsync()` at the end to flush the queue and await all
  pending requests. ([Learn more](/docs/sdk/typescript#lambda))
</Callout>

#### Create a simple LLM call using Langchain

```typescript /callbacks: [langfuseHandler]/
import { LLMChain } from "langchain/chains";
import { OpenAI } from "langchain/llms/openai";
import { PromptTemplate } from "langchain/prompts";

import { CallbackHandler } from "langfuse-langchain";

// create a handler
const handler = new CallbackHandler({
  publicKey: LF_PUBLIC_KEY,
  secretKey: LF_SECRET_KEY,
});

// create a model
const model = new OpenAI({
  temperature: 0,
  openAIApiKey: "sk-...",
});
// create a prompt
const prompt = PromptTemplate.fromTemplate(
  "What is a good name for a company that makes {product}?"
);
// create a chain
const chain = new LLMChain({
  llm: model,
  prompt,
  callbacks: [langfuseHandler],
});

// execute the chain
const resA = await chain.call(
  { product: "colorful hockey sticks" },
  { callbacks: [langfuseHandler] }
);
```

There are two ways to integrate callbacks into Langchain:

- _Constructor Callbacks_: Set when initializing an object, like `new LLMChain({ ..., callbacks: [langfuseHandler] })`. This approach will use the callback for every call made on that specific object. However, it won't apply to its child objects, making it limited in scope.
- _Request Callbacks_: Defined when issuing a request, like `chain.run(..., { callbacks: [langfuseHandler] })`. This not only uses the callback for that specific request but also for any subsequent sub-requests it triggers.

For comprehensive data capture especially for complex chains or agents, it's advised to use the both approaches, as demonstrated above [docs](https://js.langchain.com/docs/modules/callbacks/#when-do-you-want-to-use-each-of-these).

#### Stateful Langchain callbacks

The Langchain client can also be constructed with `Traces` or `Spans`. This allows to nest Langchain executions anywhere in the `Trace` an hence to add any metadata and ids that are important.

```typescript /callbacks: [handler]/ /callbacks: [spanHandler]/
import { OpenAI } from "langchain/llms/openai";
import { CallbackHandler, Langfuse } from "langfuse-langchain";

// configure the normal Langchain sdk
const langfuse = new Langfuse({
  publicKey: LF_PUBLIC_KEY,
  secretKey: LF_SECRET_KEY,
  baseUrl: LF_HOST,
});

// create a trace and a handler nested into the trace.
const trace = langfuse.trace({ name: "joke-trace" });
const handler = new CallbackHandler({ root: trace });

// call the llm with the handler
const llm = new OpenAI({ callbacks: [handler] });
await llm.call("Tell me a joke", { callbacks: [handler] });

// create a span within the trace
const span = trace.span({ name: "joke-span" });
const spanHandler = new CallbackHandler({ root: span });

// call the llm with the handler
const llmSpan = new OpenAI({ callbacks: [spanHandler] });
await llmSpan.call("Tell me a better joke", { callbacks: [spanHandler] });

await handler.flushAsync();
```

## Shutdown [#shutdown]

The Langfuse SDKs buffer events and flush them asynchronously to the Langfuse server. You should call shutdown to exit cleanly before your application exits.

```typescript
handler.shutdown();
// or
await handler.shutdownAsync();
```

## Execution identifier

The SDK provides a function to return the `traceId` of the trace which is currently used. Similarly, there is another function to expose the langchain `runId`. This `runId` is the latest top-level id of a Langchain run which is also used to create `Spans` or `Generations` in Langfuse.

Both of these ids can be used to create scores for the correct `Span` in Langfuse.

```typescript /handler.getTraceId();/ /handler.getLangchainRunId();/ /callbacks: [handler]/
import { OpenAI } from "langchain/llms/openai";
import { CallbackHandler, Langfuse } from "langfuse-langchain";

// configure the normal Langchain sdk
const langfuse = new Langfuse({
  publicKey: LF_PUBLIC_KEY,
  secretKey: LF_SECRET_KEY,
  baseUrl: LF_HOST,
});

// create a trace and a handler nested into the trace.
const trace = langfuse.trace({ id: "special-id" });
const handler = new CallbackHandler({ root: trace });

handler.getTraceId(); // returns "special-id"

// call the llm with the handler
const llm = new OpenAI({ callbacks: [handler] });
await llm.call("Tell me a joke", { callbacks: [handler] });

handler.getLangchainRunId(); // returns the latest run id

await llm.call("Tell me a better joke", { callbacks: [handler] });

handler.getLangchainRunId(); // returns the latest run id, different from the first one
```

## Upgrading from v1.x.x to v2.x.x [#upgrade1to2]

The `CallbackHandler` can be used in multiple invocations of a Langchain chain as shown below.

```typescript
import { CallbackHandler } from "langfuse-langchain";

// create a handler
const handler = new CallbackHandler({
  publicKey: LF_PUBLIC_KEY,
  secretKey: LF_SECRET_KEY,
});

import { LLMChain } from "langchain/chains";

// create a chain
const chain = new LLMChain({
  llm: model,
  prompt,
  callbacks: [langfuseHandler],
});

// execute the chain
await chain.call(
  { product: "<user_input_one>" },
  { callbacks: [langfuseHandler] }
);
await chain.call(
  { product: "<user_input_two>" },
  { callbacks: [langfuseHandler] }
);
```

So far, invoking the chain multiple times would group the observations in one trace.

```bash
TRACE
|
|-- SPAN: Retrieval
|   |
|   |-- SPAN: LLM Chain
|   |   |
|   |   |-- GENERATION: ChatOpenAi
|-- SPAN: Retrieval
|   |
|   |-- SPAN: LLM Chain
|   |   |
|   |   |-- GENERATION: ChatOpenAi
```

We changed this, so that each invocation will end up on its own trace. This is a more sensible default setting for most users.

```bash
TRACE_1
|
|-- SPAN: Retrieval
|   |
|   |-- SPAN: LLM Chain
|   |   |
|   |   |-- GENERATION: ChatOpenAi

TRACE_2
|
|-- SPAN: Retrieval
|   |
|   |-- SPAN: LLM Chain
|   |   |
|   |   |-- GENERATION: ChatOpenAi
```

If you still want to group multiple invocations on one trace, you can scope the CallbackHandler to a single trace using the following approach. See [docs above](#execution-identifier) for more information.

```typescript
const trace = langfuse.trace({ id: "special-id" });
const handler = new CallbackHandler({ root: trace });
```

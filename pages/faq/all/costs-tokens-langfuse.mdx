---
title: How do I track LLM cost and tokens in Langfuse?
tags: [features]
---

# Tracking LLM Cost and Tokens in Langfuse

You can track the cost and token usage for any large language model (LLM) and endpoint in Langfuse. There are two main ways to do this:

1. Automatic calculation by Langfuse: For [supported models](/docs/model-usage-and-cost#infer), Langfuse can infer the cost. This requires you to use the correct model names. We currently support OpenAI and Anthropic models out of the box. Refer to the _Models_ tab in Langfuse for the exact model names.

2. Explicit cost and token ingestion: You can [ingest](/docs/model-usage-and-cost#infer) the cost and token counts for your LLM calls that you already track with Langfuse. Some model providers return the cost and tokens as part of the response payload. You can pass these back to Langfuse.

Bonus: Cost tracking is natively supported in our [OpenAI integration](/docs/model-usage-and-cost#compatibility-with-openai).
Bonus2: A great blog article on how to [track AWS Bedrock costs in Langfuse](https://www.metadocs.co/2024/07/03/monitor-your-langchain-app-cost-using-bedrock-with-langfuse/).

For more information and examples, please see the [cost and tokens in Langfuse](/docs/model-usage-and-cost) documentation.

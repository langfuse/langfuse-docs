---
source: ⚠️ Jupyter Notebook
title: Integrate TrueFoundry AI Gateway with Langfuse
sidebarTitle: TrueFoundry
logo: /images/integrations/truefoundry-logo.png
description: Guide on using TrueFoundry's AI gateway to access multiple LLM models with Langfuse via the OpenAI SDK.
category: Integrations
---

# Observability for TrueFoundry AI Gateway with Langfuse

This guide shows you how to integrate TrueFoundry's AI gateway with Langfuse. TrueFoundry's API endpoints are fully compatible with the OpenAI SDK, allowing you to trace and monitor your AI applications seamlessly.

> **What is TrueFoundry AI Gateway?** [TrueFoundry AI Gateway](https://www.truefoundry.com/ai-gateway) is a unified interface that provides access to multiple AI models with advanced features for control, visibility, security, and cost optimization in your Generative AI applications.

> **What is Langfuse?** [Langfuse](https://langfuse.com) is an open source LLM engineering platform that helps teams trace LLM calls, monitor performance, and debug issues in their AI applications.

<Steps>
## Step 1: Install Dependencies

```python
%pip install openai langfuse
```

## Step 2: Set Up Environment Variables

Next, set up your Langfuse API keys. You can get these keys by signing up for a free [Langfuse Cloud](https://cloud.langfuse.com/) account or by [self-hosting Langfuse](https://langfuse.com/self-hosting). These environment variables are essential for the Langfuse client to authenticate and send data to your Langfuse project.

```python
import os

# Get keys for your project from the project settings page: https://cloud.langfuse.com
os.environ["LANGFUSE_PUBLIC_KEY"] = "pk-lf-..." 
os.environ["LANGFUSE_SECRET_KEY"] = "sk-lf-..." 
os.environ["LANGFUSE_HOST"] = "https://cloud.langfuse.com" # 🇪🇺 EU region
# os.environ["LANGFUSE_HOST"] = "https://us.cloud.langfuse.com" # 🇺🇸 US region

# TrueFoundry Configuration - Create a Truefoundry account and generate a Personal Access Token
# Following instructions in: https://docs.truefoundry.com/gateway/authentication
os.environ["TRUEFOUNDRY_API_KEY"] = "your-truefoundry-token"
os.environ["TRUEFOUNDRY_BASE_URL"] = "https://your-control-plane.truefoundry.cloud/api/llm"
```

```python
from langfuse import get_client
 
get_client().auth_check()
```

    True

## Step 3: Get TrueFoundry Gateway Configuration

Before using the gateway, you need to get the base URL and model name from your TrueFoundry AI Gateway:

1. **Navigate to AI Gateway Playground**: Go to your TrueFoundry AI Gateway playground
2. **Access Unified Code Snippet**: Use the Langchain library code snippet  
3. **Copy Base URL**: You will get the base path from the unified code snippet
4. **Copy model name**: You will get the model name from the same code snippet (ensure you use the same model name as written)

![Get Base URL from Unified Code Snippet](/images/unified-code-tfy.png)

## Step 4: Use Langfuse OpenAI Drop-in Replacement

Use Langfuse's OpenAI-compatible client (from langfuse.openai import OpenAI) to trace all requests sent through the TrueFoundry gateway. For detailed setup instructions on the AI gateway and authentication, refer to the [TrueFoundry documentation](https://docs.truefoundry.com/gateway/intro-to-llm-gateway).

```python
from langfuse.openai import OpenAI
import os

# Initialize OpenAI client with TrueFoundry Gateway
client = OpenAI(
    api_key=os.environ["TRUEFOUNDRY_API_KEY"],
    base_url=os.environ["TRUEFOUNDRY_BASE_URL"]  # Base URL from unified code snippet
)
```

## Step 5: Run an Example

```python
# Make a request through TrueFoundry Gateway with Langfuse tracing
response = client.chat.completions.create(
    model="openai-main/gpt-4o",  # Paste the model ID you copied from TrueFoundry Gateway
    messages=[
        {"role": "system", "content": "You are a helpful AI assistant specialized in explaining AI concepts."},
        {"role": "user", "content": "Why does an AI gateway help enterprises?"},
    ],
    max_tokens=500,
    temperature=0.7
)

print(response.choices[0].message.content)

# Flush via global client
langfuse = get_client()
langfuse.flush()
```

## Step 6: See Traces in Langfuse

After running the example, log in to Langfuse to view the detailed traces, including:

- Request parameters
- Response content  
- Token usage and latency metrics
- LLM model information through TrueFoundry gateway

![Langfuse Trace Example](/images/langfuse-trace-tfy.png)

_View your traces in your Langfuse dashboard to monitor and analyze your AI application performance._

> **Note**: All other features of Langfuse will work as expected, including prompt management, evaluations, custom dashboards, and advanced observability features. The TrueFoundry integration seamlessly supports the full Langfuse feature set.
</Steps>

import LearnMore from "@/components-mdx/integration-learn-more.mdx";

<LearnMore />

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ki7E44X5ViQB"
   },
   "source": [
    "---\n",
    "description: Drop-in replacement of OpenAI SDK to get full observability in Langfuse by changing only the import\n",
    "category: Integrations\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mfMAzJYcirtK"
   },
   "source": [
    "# Cookbook: OpenAI Integration (Python)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B0A389k2irtK"
   },
   "source": [
    "This is a cookbook with examples of the Langfuse Integration for OpenAI (Python).\n",
    "\n",
    "Follow the [integration guide](https://langfuse.com/integrations/model-providers/openai-py) to add this integration to your OpenAI project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uq04G_FSWjF-"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XYoil3FcOIQt"
   },
   "source": [
    "The integration is compatible with OpenAI SDK versions `>=0.27.8`. It supports async functions and streaming for OpenAI SDK versions `>=1.0.0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hVOOiBtUPtOO"
   },
   "outputs": [],
   "source": [
    "%pip install langfuse openai --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7-s-hY3PPupC"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Get keys for your project from the project settings page: https://cloud.langfuse.com\n",
    "os.environ[\"LANGFUSE_PUBLIC_KEY\"] = \"pk-lf-...\" \n",
    "os.environ[\"LANGFUSE_SECRET_KEY\"] = \"sk-lf-...\" \n",
    "os.environ[\"LANGFUSE_BASE_URL\"] = \"https://cloud.langfuse.com\" # ðŸ‡ªðŸ‡º EU region\n",
    "# os.environ[\"LANGFUSE_BASE_URL\"] = \"https://us.cloud.langfuse.com\" # ðŸ‡ºðŸ‡¸ US region\n",
    "\n",
    "# Your openai key\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-proj-...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "ldSEJ0bAP4sj"
   },
   "outputs": [],
   "source": [
    "# instead of: import openai\n",
    "from langfuse.openai import openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2ovnAAdbaLmD"
   },
   "source": [
    "## Examples\n",
    "\n",
    "### Chat completion (text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "c8RhokKUP9I0"
   },
   "outputs": [],
   "source": [
    "completion = openai.chat.completions.create(\n",
    "  name=\"test-chat\",\n",
    "  model=\"gpt-4o\",\n",
    "  messages=[\n",
    "      {\"role\": \"system\", \"content\": \"You are a very accurate calculator. You output only the result of the calculation.\"},\n",
    "      {\"role\": \"user\", \"content\": \"1 + 1 = \"}],\n",
    "  temperature=0,\n",
    "  metadata={\"someMetadataKey\": \"someValue\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Link to trace in Langfuse](https://cloud.langfuse.com/project/cloramnkj0002jz088vzn1ja4/traces/3e9d8ae6670747cb60b1434b97c3bfdb?timestamp=2025-11-13T14%3A28%3A51.831Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SAqxBgOqKTzO"
   },
   "source": [
    "### Chat completion (image)\n",
    "\n",
    "Simple example using the OpenAI vision's functionality. Images may be passed in the `user` messages. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = openai.chat.completions.create(\n",
    "  name=\"test-url-image\",\n",
    "  model=\"gpt-4o-mini\", # GPT-4o, GPT-4o mini, and GPT-4 Turbo have vision capabilities\n",
    "  messages=[\n",
    "      {\"role\": \"system\", \"content\": \"You are an AI trained to describe and interpret images. Describe the main objects and actions in the image.\"},\n",
    "      {\"role\": \"user\", \"content\": [\n",
    "        {\"type\": \"text\", \"text\": \"Whatâ€™s depicted in this image?\"},\n",
    "        {\n",
    "          \"type\": \"image_url\",\n",
    "          \"image_url\": {\n",
    "            \"url\": \"https://static.langfuse.com/langfuse-dev/langfuse-example-image.jpeg\",\n",
    "          },\n",
    "        },\n",
    "      ],\n",
    "    }\n",
    "  ],\n",
    "  temperature=0,\n",
    "  metadata={\"someMetadataKey\": \"someValue\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M4iJpqYQirtM"
   },
   "source": [
    "Go to https://cloud.langfuse.com or your own instance to see your generation.\n",
    "\n",
    "![Chat completion](https://langfuse.com/images/docs/multi-modal-trace.png)\n",
    "\n",
    "[Link to trace in Langfuse](https://cloud.langfuse.com/project/cloramnkj0002jz088vzn1ja4/traces/b1dc51c45bc87df7d4b292717da072af?timestamp=2025-11-13T14%3A28%3A58.703Z&observation=e6af2e5e6f815256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chat completion (streaming)\n",
    "\n",
    "Simple example using the OpenAI streaming functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "b9gRlb2rKTaA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why don't scientists trust atoms?\n",
      "\n",
      "Because they make up everything!None"
     ]
    }
   ],
   "source": [
    "completion = openai.chat.completions.create(\n",
    "  name=\"test-chat\",\n",
    "  model=\"gpt-4o\",\n",
    "  messages=[\n",
    "      {\"role\": \"system\", \"content\": \"You are a professional comedian.\"},\n",
    "      {\"role\": \"user\", \"content\": \"Tell me a joke.\"}],\n",
    "  temperature=0,\n",
    "  metadata={\"someMetadataKey\": \"someValue\"},\n",
    "  stream=True\n",
    ")\n",
    "\n",
    "for chunk in completion:\n",
    "  print(chunk.choices[0].delta.content, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F2pvm0qLKg7Q"
   },
   "source": [
    "### Chat completion (async)\n",
    "\n",
    "Simple example using the OpenAI async client. It takes the Langfuse configurations either from the environment variables or from the attributes on the `openai` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Hggwggv_MKpV"
   },
   "outputs": [],
   "source": [
    "from langfuse.openai import AsyncOpenAI\n",
    "\n",
    "async_client = AsyncOpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "ZIUKD8Z3KmvQ"
   },
   "outputs": [],
   "source": [
    "completion = await async_client.chat.completions.create(\n",
    "  name=\"test-chat\",\n",
    "  model=\"gpt-4o\",\n",
    "  messages=[\n",
    "      {\"role\": \"system\", \"content\": \"You are a very accurate calculator. You output only the result of the calculation.\"},\n",
    "      {\"role\": \"user\", \"content\": \"1 + 100 = \"}],\n",
    "  temperature=0,\n",
    "  metadata={\"someMetadataKey\": \"someValue\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M4iJpqYQirtM"
   },
   "source": [
    "Go to https://cloud.langfuse.com or your own instance to see your generation.\n",
    "\n",
    "![Chat completion](https://langfuse.com/images/docs/openai-chat.png)\n",
    "\n",
    "[Link to trace in Langfuse](https://cloud.langfuse.com/project/cloramnkj0002jz088vzn1ja4/traces/00d7d6cd4278b6003fc79337cd6d4394?timestamp=2025-11-13T14%3A33%3A14.490Z&observation=79d778c36b344d7e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ky7CtCNzaSrn"
   },
   "source": [
    "### Functions\n",
    "\n",
    "Simple example using Pydantic to generate the function schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jJfBdHowaRgs"
   },
   "outputs": [],
   "source": [
    "%pip install pydantic --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "2gA-zGk7VYYp"
   },
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from pydantic import BaseModel\n",
    "\n",
    "class StepByStepAIResponse(BaseModel):\n",
    "    title: str\n",
    "    steps: List[str]\n",
    "schema = StepByStepAIResponse.model_json_schema() # returns a dict like JSON schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "ORtNcN4-afDC"
   },
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import json\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-5\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Explain how to assemble a PC\"}\n",
    "    ],\n",
    "    tools=[\n",
    "        {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": \"get_answer_for_user_query\",\n",
    "                \"description\": \"Get user answer in a series of steps\",\n",
    "                \"parameters\": StepByStepAIResponse.model_json_schema()\n",
    "            }\n",
    "        }\n",
    "    ],\n",
    "    tool_choice={\"type\": \"function\", \"function\": {\"name\": \"get_answer_for_user_query\"}}\n",
    ")\n",
    "\n",
    "tool_call = response.choices[0].message.tool_calls[0]\n",
    "args = json.loads(tool_call.function.arguments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qurrm-Ntp24O"
   },
   "source": [
    "Go to https://cloud.langfuse.com or your own instance to see your generation.\n",
    "\n",
    "![Function](https://langfuse.com/images/docs/openai-function.png)\n",
    "\n",
    "[Link to trace in Langfuse](https://cloud.langfuse.com/project/cloramnkj0002jz088vzn1ja4/traces/cbbd69af7e00a939da55cbba80b2313e?timestamp=2025-11-13T14%3A38%3A08.304Z)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Langfuse Features (User, Tags, Metadata, Session)\n",
    "\n",
    "You can access additional Langfuse features by adding the relevant attributes to the OpenAI request. The Langfuse integration will parse these attributes. See [docs](https://langfuse.com/integrations/model-providers/openai-py#custom-trace-properties) for details on all available features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langfuse.openai import openai\n",
    " \n",
    "completion = openai.chat.completions.create(\n",
    "  name=\"test-chat\",\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a calculator.\"},\n",
    "    {\"role\": \"user\", \"content\": \"1 + 1 = \"}],\n",
    "  temperature=0,\n",
    "  metadata={\n",
    "    \"langfuse_tags\": [\"tag-1\", \"tag-2\"],\n",
    "    \"langfuse_user_id\": \"user-123\",\n",
    "    \"langfuse_session_id\": \"session-123\",\n",
    "    \"langfuse_metadata\": {\"key\": \"value\"}\n",
    "  }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example trace: https://cloud.langfuse.com/project/cloramnkj0002jz088vzn1ja4/traces/286c5c70-b077-4826-a493-36c510362a5a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AzureOpenAI\n",
    "\n",
    "The integration also works with the `AzureOpenAI` and `AsyncAzureOpenAI` classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AZURE_OPENAI_KEY=\"\"\n",
    "AZURE_ENDPOINT=\"\"\n",
    "AZURE_DEPLOYMENT_NAME=\"cookbook-gpt-4o-mini\" # example deployment name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instead of: from openai import AzureOpenAI\n",
    "from langfuse.openai import AzureOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = AzureOpenAI(\n",
    "    api_key=AZURE_OPENAI_KEY,  \n",
    "    api_version=\"2023-03-15-preview\",\n",
    "    azure_endpoint=AZURE_ENDPOINT\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.chat.completions.create(\n",
    "  name=\"test-chat-azure-openai\",\n",
    "  model=AZURE_DEPLOYMENT_NAME, # deployment name\n",
    "  messages=[\n",
    "      {\"role\": \"system\", \"content\": \"You are a very accurate calculator. You output only the result of the calculation.\"},\n",
    "      {\"role\": \"user\", \"content\": \"1 + 1 = \"}],\n",
    "  temperature=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example trace: https://cloud.langfuse.com/project/cloramnkj0002jz088vzn1ja4/traces/7ceb3ee3-0f2a-4f36-ad11-87ff636efd1e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Su1OaQq3rPPh"
   },
   "source": [
    "## Group multiple generations into a single trace\n",
    "\n",
    "Many applications require more than one OpenAI call. The `@observe()` decorator allows you to nest all LLM calls of a single API invocation into the same `trace` in Langfuse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zMDVxzS1ltWU"
   },
   "outputs": [],
   "source": [
    "from langfuse.openai import openai\n",
    "from langfuse import observe\n",
    "\n",
    "@observe() # decorator to automatically create trace and nest generations\n",
    "def main(country: str, user_id: str, **kwargs) -> str:\n",
    "    # nested generation 1: use openai to get capital of country\n",
    "    capital = openai.chat.completions.create(\n",
    "      name=\"geography-teacher\",\n",
    "      model=\"gpt-4o\",\n",
    "      messages=[\n",
    "          {\"role\": \"system\", \"content\": \"You are a Geography teacher helping students learn the capitals of countries. Output only the capital when being asked.\"},\n",
    "          {\"role\": \"user\", \"content\": country}],\n",
    "      temperature=0,\n",
    "    ).choices[0].message.content\n",
    "\n",
    "    # nested generation 2: use openai to write poem on capital\n",
    "    poem = openai.chat.completions.create(\n",
    "      name=\"poet\",\n",
    "      model=\"gpt-4o\",\n",
    "      messages=[\n",
    "          {\"role\": \"system\", \"content\": \"You are a poet. Create a poem about a city.\"},\n",
    "          {\"role\": \"user\", \"content\": capital}],\n",
    "      temperature=1,\n",
    "      max_tokens=200,\n",
    "    ).choices[0].message.content\n",
    "\n",
    "    return poem\n",
    "\n",
    "# run main function and let Langfuse decorator do the rest\n",
    "print(main(\"Bulgaria\", \"admin\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ehx2NZuIrPPh"
   },
   "source": [
    "![Trace with multiple OpenAI calls](https://langfuse.com/images/docs/openai-trace-grouped.png)\n",
    "\n",
    "[Link to trace in Langfuse](https://cloud.langfuse.com/project/cloramnkj0002jz088vzn1ja4/traces/db22d0a442216b485abd83cc9df6d9ee?timestamp=2025-11-13T15:05:05.559Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-HeMqTWgK4xL"
   },
   "source": [
    "## Fully featured: Interoperability with Langfuse SDK\n",
    "\n",
    "The `trace` is a core object in Langfuse and you can add rich metadata to it. See [Python SDK docs](https://langfuse.com/docs/sdk/python#traces-1) for full documentation on this.\n",
    "\n",
    "Some of the functionality enabled by custom traces:\n",
    "- custom name to identify a specific trace-type\n",
    "- user-level tracking\n",
    "- experiment tracking via versions and releases\n",
    "- custom metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "28to65wpK4xL"
   },
   "outputs": [],
   "source": [
    "from langfuse.openai import openai\n",
    "from langfuse import observe, get_client, propagate_attributes\n",
    "langfuse = get_client()\n",
    "\n",
    "@observe() # decorator to automatically create trace and nest generations\n",
    "def main(country: str, user_id: str, **kwargs) -> str:\n",
    "\n",
    "    # Propagate attributes to all child observations\n",
    "    with propagate_attributes(\n",
    "        session_id=\"1234\",\n",
    "        user_id=user_id,\n",
    "        tags=[\"tag1\", \"tag2\"],\n",
    "        metadata = {\"env\": \"development\"}\n",
    "    ):\n",
    "\n",
    "      # nested generation 1: use openai to get capital of country\n",
    "      capital = openai.chat.completions.create(\n",
    "        name=\"geography-teacher\",\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a Geography teacher helping students learn the capitals of countries. Output only the capital when being asked.\"},\n",
    "            {\"role\": \"user\", \"content\": country}],\n",
    "        temperature=0,\n",
    "      ).choices[0].message.content\n",
    "\n",
    "      # nested generation 2: use openai to write poem on capital\n",
    "      poem = openai.chat.completions.create(\n",
    "        name=\"poet\",\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a poet. Create a poem about a city.\"},\n",
    "            {\"role\": \"user\", \"content\": capital}],\n",
    "        temperature=1,\n",
    "        max_tokens=200,\n",
    "      ).choices[0].message.content\n",
    "\n",
    "    return poem\n",
    "\n",
    "# create random trace_id, could also use existing id from your application, e.g. conversation id\n",
    "trace_id = langfuse.create_trace_id()\n",
    "\n",
    "# run main function, set your own id, and let Langfuse decorator do the rest\n",
    "print(main(\"Bulgaria\", \"admin\", langfuse_observation_id=trace_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Link to trace in Langfuse](https://cloud.langfuse.com/project/cloramnkj0002jz088vzn1ja4/traces/0d73de5c936a1fce6c63bd5b20a7c0c2?timestamp=2025-11-13T15:13:27.879Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O3jxed-VrPPi"
   },
   "source": [
    "## Programmatically add scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uMO6tn53rPPi"
   },
   "source": [
    "You can add [scores](https://langfuse.com/docs/scores) to the trace, to e.g. record user feedback or some programmatic evaluation. Scores are used throughout Langfuse to filter traces and on the dashboard. See the docs on scores for more details.\n",
    "\n",
    "The score is associated to the trace using the `trace_id`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "J0argbJhrPPi"
   },
   "outputs": [],
   "source": [
    "from langfuse import observe, get_client\n",
    "langfuse = get_client()\n",
    "\n",
    "@observe() # decorator to automatically create trace and nest generations\n",
    "def main():\n",
    "    # get trace_id of current trace\n",
    "    trace_id = langfuse.get_current_trace_id()\n",
    "\n",
    "    # rest of your application ...\n",
    "\n",
    "    return \"res\", trace_id\n",
    "\n",
    "# execute the main function to generate a trace\n",
    "_, trace_id = main()\n",
    "\n",
    "# Score the trace from outside the trace context\n",
    "langfuse.create_score(\n",
    "    trace_id=trace_id,\n",
    "    name=\"my-score-name\",\n",
    "    value=1\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

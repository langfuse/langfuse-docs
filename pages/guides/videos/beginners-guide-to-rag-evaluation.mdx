---
title: "Beginner's Guide to RAG Evaluation with Langfuse and Ragas"
description: "Webinar by Prof. Tom Yeh on how to evaluate RAG applications with Langfuse and Ragas."
ogImage: /images/videos/beginners-guide-to-rag-evaluation.jpg
---

# Beginner's Guide to RAG Evaluation with Langfuse and Ragas

<Callout type="info">

We recommend this webinar by Prof. Tom Yeh on evaluating Retrieval
Augmented Generation (RAG) applications. It provides an excellent introduction
to RAG and explains how Langfuse can help debug and evaluate RAG systems,
particularly when combined with [Ragas](https://docs.ragas.io) metrics.

- Presenter: Tom Yeh, Associate Professor at University of Colorado Boulder
- Resources: [Webinar slides](https://aibyhand.substack.com/p/beginners-guide-to-rag-evaluation) on Tom's blog

</Callout>

<iframe
  width="100%"
  className="aspect-video rounded mt-10"
  src="https://www.youtube-nocookie.com/embed/2fqs8Nlh5UI?si=A6CYkwA0iGs-7uak&amp;start=537"
  title="YouTube video player"
  frameborder="0"
  allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
  referrerpolicy="strict-origin-when-cross-origin"
  allowFullScreen
></iframe>

## Our Notes

### 1. RAG Overview

- User inputs a question.
- Instead of directly querying a large language model (LLM), the input is augmented with context retrieved from a database.
- The augmented query is then sent to the generator to produce a response.

### 2. Components of RAG

- Retriever: Fetches relevant context from a database.
- Augmentation: Combines user query with retrieved context.
- Generator: Produces an answer based on the augmented query.

### 3. Evaluation of RAG Systems

- Trace Analysis:
  - Tracing the steps from user input to final output to understand system performance.
  - Involves logging each step such as retrieval, augmentation, and generation.
- Metrics:
  - Conciseness: Measures how succinct an answer is.
  - Helpfulness: Evaluates the usefulness of an answer.
- Tools:
  - [LangFuse](/): Used for tracing and logging RAG operations.
  - [RAGAS](https://docs.ragas.io) (RAG Assessment): Provides detailed metrics for evaluating RAG systems.

### 4. LangFuse

- [Features](/docs):
  - Logs each step in the RAG process.
  - Provides a timeline view of operations.
  - Allows for comparison of different interactions.
  - Many additional LLM Ops features such as prompt management, cost analysis, benchmarking, and more.
- Demo:
  - Demonstrated a chatbot application using LangFuse for tracing and logging interactions. Public link: [langfuse.com/demo](https://langfuse.com/demo)
  - Showed how to analyze the performance of the RAG system using LangFuse metrics.

### 5. RAGAS

- [Metrics](https://docs.ragas.io/en/stable/concepts/metrics/index.html):
  - Faithfulness: Accuracy of the generated answer based on retrieved context.
  - Hallucination: Incorrect information generated by the model.
  - Answer Relevancy: Relevance of the generated answer to the original question.
  - Context Recall: Ability to retrieve all relevant information.
  - Context Precision: Accuracy of the retrieved context.
- Implementation:
  - Uses prompts to evaluate faithfulness and relevancy.
  - Ground truth data is used to evaluate retrieval metrics.

### 6. Cost Considerations

- Evaluating the cost associated with using large language models for generation and evaluation.
- Emphasized the need to balance between expensive and cheaper models for different tasks.

### 7. Additional Metrics

- Mentioned other metrics like context utilization, context entity recall, and noise sensitivity.
- Highlighted the importance of choosing the right metrics based on specific needs and explaining them to stakeholders.

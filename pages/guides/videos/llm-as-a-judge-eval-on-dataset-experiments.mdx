---
title: "LLM-as-a-Judge Evaluators for Dataset Experiments"
description: "10 min walkthrough on how to reliably evaluate your LLM application changes using Langfuse's new managed LLM-as-a-judge evaluators."
ogImage: /images/videos/llm-as-a-judge-eval-on-dataset-experiments.jpg
---

# LLM-as-a-Judge Evaluators for Dataset Experiments

10 min walkthrough on how to reliably evaluate your LLM application changes using Langfuse's new managed LLM-as-a-judge evaluators.

This feature helps teams:

- Automatically evaluate experiment runs against test datasets
- Compare metrics across different versions
- Identify regressions before they hit production
- Score outputs based on criteria like hallucination, helpfulness, relevance, and more

Works with popular LLM providers including OpenAI, Anthropic, Azure OpenAI, and AWS Bedrock through function calling.

<iframe
  width="100%"
  className="aspect-[16/9] rounded-lg border mt-6 w-full"
  src="https://www.youtube.com/embed/JOGMn5nqCSM?si=9-Et0tKtOYffyvru"
  title="YouTube video player"
  frameborder="0"
  allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
  referrerpolicy="strict-origin-when-cross-origin"
  allowFullScreen
></iframe>

More details:

- [Changelog](/changelog/2024-11-19-llm-as-a-judge-for-datasets)
- [Docs: LLM-as-a-Judge Evaluators](/docs/scores/model-based-evals)
- [Docs: Dataset Experiments](/docs/datasets)

---
title: Why do customers choose Langfuse?
description: Learn why we built Langfuse and what makes it different from other LLM engineering platforms.
---

# Why do customers choose Langfuse?

Customers choose Langfuse because we are...

- The most used open-source LLM Engineering platform [(blog post)](/blog/2024-11-most-used-oss-llmops)
- Model and framework agnostic with [50+ integrations](/integrations)
- Built for production & scale
- Designed for complex agents and multi-step workflows
- Offering a complete toolbox for AI Engineering
- Incrementally adoptable, start with one feature and expand to the full platform over time
- API-first, all features are available via API for custom integrations
- Based on [OpenTelemetry](/integrations/native/opentelemetry) for interoperability
- Easy to [self-host](/self-hosting)

import { CredibilitySentence } from "@/components/credibilitySentence";
import { EnterpriseLogos } from "@/components/EnterpriseLogos";

<CredibilitySentence style="list" />

Select customers include:

<EnterpriseLogos />

## Open Source

- Langfuse is [open source](/handbook/chapters/open-source).
- You can [self-host](/self-hosting) it, including the scalable observability backend that powers Langfuse Cloud without scalability limitations.
- All product capabilities—tracing, evaluations, prompt management, experiments, annotation, the playground, and more—are MIT licensed without any usage limits.
- We are transparent about [what's on the roadmap](/docs/roadmap) and [how we operate](/handbook).
- We iterate with the [community on GitHub](https://github.com/orgs/langfuse/discussions) because best practices for building great LLM applications are rapidly evolving. Please share your feedback and ideas with us.
- Self-hosting keeps data within your infrastructure; Cloud offloads operational overhead. You can switch between OSS, Enterprise self-host, and Langfuse Cloud at any time—no feature flags to untangle, no vendor lock-in.

## Extensive Integrations built on OpenTelemetry

- Native SDKs for [Python](/docs/observability/sdk/python/overview) and [JavaScript/TypeScript](/docs/observability/sdk/typescript/overview)
- [50+ integrations](/integrations) with popular frameworks, model providers, and tools
- Framework support: [LangChain](/integrations/frameworks/langchain), [LlamaIndex](/integrations/frameworks/llamaindex), [OpenAI Agents](/integrations/frameworks/openai-agents), [Vercel AI SDK](/integrations/frameworks/vercel-ai-sdk), [CrewAI](/integrations/frameworks/crewai), and many more
- Model providers: [OpenAI](/integrations/model-providers/openai-py), [Anthropic](/integrations/model-providers/anthropic), [Google Gemini](/integrations/model-providers/google-gemini), [Amazon Bedrock](/integrations/model-providers/amazon-bedrock), and others
- LLM Gateways: [LiteLLM](/integrations/gateways/litellm), [OpenRouter](/integrations/gateways/openrouter), [Portkey](/integrations/gateways/portkey)
- Native [OpenTelemetry](/integrations/native/opentelemetry) support for maximum interoperability

## Developer First

- Langfuse is built for developers.
- We are creating a technical product with great developer experience.
- Langfuse is powerful yet simple, allowing you to build custom logic on top of it.
- All data is accessible via [public APIs](/docs/api-and-data-platform/features/public-api) and [SDKs](/docs/api-and-data-platform/features/query-via-sdk).
- [MCP Server](/docs/api-and-data-platform/features/mcp-server) for AI-native workflows and integrations.

## Reliable Partner

- All changes to the Langfuse API and integrations are covered by [semantic versioning](https://semver.org); we test public interfaces to ensure backward compatibility and run end-to-end integration tests for the most common use cases in CI.
- Raised a [$4M seed round](/blog/announcing-our-seed-round) from Lightspeed Ventures, General Catalyst, Y Combinator, and angel investors.
- [Langfuse has been included twice in the Thoughtworks Tech Radar](https://www.thoughtworks.com/en-us/radar/platforms/langfuse) as a recommended platform.
- Strong adoption and community growth ([see metrics](#public-metrics)).
- [Learn more](/about) about us as a team.

## Built for Complex Use Cases

- We designed Langfuse with complex, nested LLM calls in mind—especially for agents and multi-step workflows.
- [Agent graphs](/docs/observability/features/agent-graphs) provide visual representations of complex agent workflows, helping you understand and debug multi-step reasoning processes.
- Langfuse enables hierarchical representations of your application in [traces](/docs/observability/overview). Why are traces the core abstraction for LLM Engineering? Learn more in this [webinar](/guides/videos/webinar-observability-llm-systems).
- [Multi-modal support](/docs/observability/features/multi-modality) for tracing text, images, audio, and other modalities.
- [MCP tracing](/docs/observability/features/mcp-tracing) for Model Context Protocol server interactions.
- We go beyond Input/Output to include all the context of your app via [metadata](/docs/observability/features/metadata), [tags](/docs/observability/features/tags), and [sessions](/docs/observability/features/sessions).

## Comprehensive Platform

We are building the core development platform you need to build robust LLM applications. Langfuse offers four integrated pillars:

- **[Observability](/docs/observability/overview)**: Comprehensive tracing for LLM applications, including [agent graphs](/docs/observability/features/agent-graphs), [sessions](/docs/observability/features/sessions), [token & cost tracking](/docs/observability/features/token-and-cost-tracking), and [multi-modality](/docs/observability/features/multi-modality).
- **[Prompt Management](/docs/prompt-management/overview)**: Version control, [LLM Playground](/docs/prompt-management/features/playground), [A/B testing](/docs/prompt-management/features/a-b-testing), [GitHub integration](/docs/prompt-management/features/github-integration), and collaborative workflows.
- **[Evaluation](/docs/evaluation/overview)**: [LLM-as-a-Judge](/docs/evaluation/evaluation-methods/llm-as-a-judge), [human annotations](/docs/evaluation/evaluation-methods/annotation), [experiments](/docs/evaluation/experiments/overview), and [datasets](/docs/evaluation/experiments/datasets) for systematic testing.
- **[Metrics & Data Platform](/docs/metrics/overview)**: [Custom dashboards](/docs/metrics/features/custom-dashboards), [metrics API](/docs/metrics/features/metrics-api), and extensive [data export options](/docs/api-and-data-platform/overview).

## Built for Scale

import EnterpriseCloudScale from "@/components-mdx/enterprise-cloud-scale.mdx";

<EnterpriseCloudScale />

## Security and Compliance

- We take security and compliance seriously.
- **Certifications**: Langfuse Cloud is [SOC 2 Type II](/security/soc2) and [ISO 27001](/security/iso27001) certified.
- **Privacy**: [GDPR](/security/gdpr) compliant with [DPA](/security/dpa) available. [HIPAA](/security/hipaa) aligned with BAA available.
- **Data Regions**: Choose between US, EU, or HIPAA-ready data regions on Langfuse Cloud—or [self-host](/self-hosting) anywhere.
- **Data Control**: [Data masking](/docs/observability/features/masking), [data retention](/docs/administration/data-retention), and [data deletion](/docs/administration/data-deletion) capabilities.
- More details in our [Security & Privacy Center](/security).

## Public Metrics

import PublicMetricsDashboard from "@/components-mdx/public-metrics-dashboard.mdx";

<PublicMetricsDashboard />

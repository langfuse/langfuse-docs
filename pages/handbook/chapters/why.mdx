---
title: Why do customers choose Langfuse?
description: Learn why we built Langfuse and what makes it different from other LLM observability platforms.
---

# Why do customers choose Langfuse?

Customers choose Langfuse because we are...

- The most used open-source LLMOps platform [(blog post)](/blog/2024-11-most-used-oss-llmops)
- Model and framework agnostic
- Built for production & scale
- Offering a complete toolbox for AI Engineering
- Incrementally adoptable, start with one feature and expand to the full platform over time
- API-first, all features are available via API for custom integrations
- Easy to self-host

import { CredibilitySentence } from "@/components/credibilitySentence";
import { EnterpriseLogos } from "@/components/EnterpriseLogos";

<CredibilitySentence style="list" />

Select customers include:

<EnterpriseLogos />

## Open Source

- Langfuse is [open source](/open-source).
- You can [self-host](/self-hosting) it, including the scalable observability backend that powers Langfuse Cloud without scalability limitations.
- Langfuse's core tracing is MIT licensed and will always be freely available.
- We are transparent about [what we are building](/roadmap) and how.
- We [iterate with our users](https://github.com/orgs/langfuse/discussions) and celebrate their feedback.
- Integrates via [OpenTelemetry](/docs/opentelemetry/get-started) to increase interoperability with other observability tools.

## Developer First

- Langfuse is built for developers.
- We are creating a technical product with great developer experience.
- Langfuse is powerful yet simple, allowing you to build custom logic on top of it.

## Reliable Partner

- All changes to the Langfuse API and integrations are covered by [semantic versioning](https://semver.org); we test public interfaces to ensure backward compatibility and run end-to-end integration tests for the most common use cases in CI.
- Raised a [$4M seed round](/blog/announcing-our-seed-round) from Lightspeed Ventures, General Catalyst, Y Combinator, and angel investors.
- [Langfuse has twice been included in the Thoughtworks Tech Radar](https://www.thoughtworks.com/en-us/radar/platforms/langfuse) as a recommended platform.
- Strong adoption and community growth ([see metrics](#public-metrics)).
- [Learn more](/about) about us as a team.

## Built for Complex Use Cases

- We designed Langfuse with complex, nested LLM calls in mind.
- Langfuse enables hierarchical representations of your application in [traces](/docs/tracing). Why are traces the core abstraction for LLMOps? Learn more in this [webinar](/guides/videos/webinar-observability-llm-systems).
- We go beyond Input/Output to include all the context of your app.

## Built for Scale

import EnterpriseCloudScale from "@/components-mdx/enterprise-cloud-scale.mdx";

<EnterpriseCloudScale />

## No Lock-In

- We are building Langfuse to be a core part of your stack.
- Langfuse has open [APIs](/docs/api) to integrate with your infrastructure.
- Langfuse is model and framework agnostic, aiming for interoperability of our integrations.

## Comprehensive Feature Set

- We are building the core development tool you need to build robust LLM applications â€“ more on this [here](/docs).
- [Tracing](/docs/tracing) lies at our core. If a feature integrates well with traces, we will build it.

## Security and Compliance

- We take security and compliance seriously.
- Privacy: You can choose to host Langfuse youself or use Langfuse Cloud (EU or US).
- Security: Langfuse Cloud is ISO 27001 and SOC 2 Type 2 certified, GDPR compliant and HIPAA aligned.
- More details in our [Security and Privacy Center](/security).

## Public Metrics

import PublicMetricsDashboard from "@/components-mdx/public-metrics-dashboard.mdx";

<PublicMetricsDashboard />

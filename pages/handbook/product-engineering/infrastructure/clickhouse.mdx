# ClickHouse Cloud

We are using [ClickHouse Cloud](https://clickhouse.com/cloud) as our main data warehouse for storing and querying observability data such as traces, observations, and scores.

We have two organizations in place: `Langfuse GmbH` and `Langfuse HIPAA`.
The former contains separate warehouses for our staging, EU, and US accounts.
The latter is a dedicated HIPAA-compliant organization for the HIPAA cloud region.

## Terminology

- **Organization**: The top-level entity in ClickHouse Cloud that contains multiple warehouses. Support tickets and billing are managed at the organization level.
- **Warehouse**: A logical group of services that share a common data-storage and ClickHouse-Keeper configuration. I.e. warehouses allow Compute-Compute separation.
- **Service**: A set of replicas that share a common endpoint. Services can be scaled horizontally and vertically.
- **Node/Replica**: An individual instance within a Service. Matches to a Kubernetes Pod.

## Common Concepts

In this section, we cover some common concepts that we use in Langfuse and their implications.

### ReplacingMergeTrees

All tracing tables use the [ReplacingMergeTree](https://clickhouse.com/docs/engines/table-engines/mergetree-family/replacingmergetree) engine.
This allows us to perform updates by writing a new row with a higher version number and the same sorting key and have them automatically be deduplicated in the background.
As we allow updates on traces, observations, and scores, this is a central feature.

The deduplication in ReplacingMergeTrees happens eventually and there is no guarantee for it to ever happen.
Therefore, we perform additional read-side deduplication for most queries to return consistent results.

There are three main ways to perform deduplication on read that we use:
- **FINAL modifier**: This automatically performs the full deduplication that ClickHouse runs on merges.
   Especially for larger tables, this can be very expensive and slow with a high memory consumption.
   In addition, this blocks the use of data skipping indexes on older ClickHouse versions.
- **ORDER BY event_ts LIMIT 1 BY ...**: If we ensure that the sorting order avoids duplicates on updates (i.e. only sorts by time and event_ts), we can
   use the LIMIT 1 BY clause to return only one row per identifier. This is often the cheapest to compute and makes use of data skipping indexes,
   but requires careful ordering of the tables. We also observed gotchas where the timestamp shifted between updates which may cause seemingly random results.
- **argMax/argMin functions**: These functions allow us to select the row with the highest/lowest version for each sorting key.
   This relies on a group-by call on the common identifier, e.g. `GROUP BY project_id, trace_id`.
   We rarely use this as it has a high memory consumption as it needs to keep the full result set in memory before returning.

### Inserts

ClickHouse benefits heavily from large inserts as each insert creates an initial data part that needs to be merged later in the background.
Therefore, we have multiple batch mechanisms in place to accumulate data before initiating an insert.

The worker container stores all ClickHouse writes into the `ClickHouseWriter` in an in-memory buffer.
From there, we regularly flush the data to ClickHouse in configurable batch sizes and intervals.
In addition, we use `async_inserts` on ClickHouse that collect data server-side before acknowledging the insert and writing it to disk.

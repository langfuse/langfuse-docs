import { SiGithub, SiTypescript, SiPython, SiDiscord } from "react-icons/si";
import { AiOutlineCloud } from "react-icons/ai";
import { BsCode } from "react-icons/bs";
import { Button } from "@/components/ui/button";
import Link from "next/link";
import { Buttons } from "@/components/demoVideo";
import Image from "next/image";
import ycImg from "@/src/yc.svg";
import { ProductUpdateSignup } from "@/components/productUpdateSignup";
import { Tabs, TabsContent, TabsList, TabsTrigger } from "@/components/ui/tabs";
import {
  Card,
  CardContent,
  CardDescription,
  CardFooter,
  CardHeader,
  CardTitle,
} from "@/components/ui/card";
import ObservabilityExamples from "@/components/observabilityExamples/index.mdx";

<div className="lg:container">
<div className="flex flex-col gap-9 mt-20 mb-10 md:mt-24 md:mb-20 lg:my-40 items-center">
  <h1 className="text-center text-4xl">`ðŸª¢ langfuse`</h1>
  <div className="max-w-3xl">
    <div className="text-2xl text-center font-semibold mb-3">
      Open-source observability for LLM applications
    </div>
    <div className="text-lg text-center">
      Investigate how LLM `agent`, `chains`, `chats`, `embeddings` behave in production. Collect user feedback, and identify root causes that lead to quality problems.
    </div>
  </div>
  <div className="flex gap-3 justify-center flex-wrap">
    <Buttons />
  </div>
  <div className="justify-center">
    <ProductUpdateSignup />
  </div>
  <div>
    <div className="text-center text-gray-500 pb-3 pt-10">Backed by</div>
    <Image src={ycImg} height={30} />
  </div>
</div>

## 1. Observe

Monitoring LLM applications requires including the context. This can be the full user session of a chat application, retrieval results of a QA-chain, or the full execution trace of an agent. `langfuse` was designed to capture the full context, be flexibly extendible while being incrementally adoptable.

_Available data sources:_

<div className="flex gap-3 my-2 flex-wrap mb-2">
  <Button variant="outline" asChild>
    <Link href="/docs/sdk/typescript">
      <SiTypescript className="mr-2" />
      <span>Typescript SDK</span>
    </Link>
  </Button>
  <Button variant="outline" asChild>
    <Link href="/docs/sdk/python">
      <SiPython className="mr-2" />
      <span>Python SDK</span>
    </Link>
  </Button>
  <Button variant="outline" asChild>
    <Link href="/docs/reference">
      <BsCode className="mr-2 h-5 w-5" />
      <span>API (Swagger)</span>
    </Link>
  </Button>
</div>

_View examples by application type_

<ObservabilityExamples />

## 2. Collect feedback

- right from the frontend
- see the full context when they want to examine, for example, executions that have received negative customer feedback.

## 3. Dashboards

- usage & costs
- overall sentiment of users

## 4. Benefits

- open source
- model agnostic
- fully async, does not add latency
- managed or self-hosted

## get started

- link to quickstart

import { Feature } from "@/components/feature";
import screenDashboard from "@/src/screenshots_dashboard.png";
import screenSegmentTraces from "@/src/screenshots_segment_traces.png";
import screenSideBySide from "@/src/screenshots_side_by_side.png";

<Feature src={screenDashboard} imageAlt="test" imagePosition="left">

## Monitor

Stay on top of trends in output quality and adoption of your LLM-based features

Track token usage by application, user, and model (coming soon)

</Feature>

<Feature src={screenSegmentTraces} imageAlt="test" imagePosition="right">

## Understand and segment use cases

LLM-based features can be used in all sorts of ways. With langfuse, you can easily segment executions by user attributes, scores, and other dimensions.

Model-based clustering of prompts/completions to understand popular use cases (coming soon)

</Feature>

<Feature src={screenSideBySide} imageAlt="test" imagePosition="left">

## Identify quality bottlenecks

Compare low-quality executions side-by-side to gain a deep understanding on why chains did not perform well

Model-based comparison of executions with automated suggestions (coming soon)

</Feature>

import { TwoCards, StartCard, Divider } from "@/components/cards";

<TwoCards className="my-24">
<StartCard isOutline>

## Open source

- Powered by NextJs and postgres
- Fully typed SDKs for Typescript and Python
- Simple self-hosting on your own infrastructure

<div className="flex gap-3 mt-3 flex-wrap">
  <Button asChild>
    <Link href="https://github.com/langfuse/langfuse/">
      <SiGithub className="mr-2" />
      <span>GitHub</span>
    </Link>
  </Button>
  <Button variant="secondary" asChild>
    <Link href="https://discord.gg/7NXusRtqYU">
      <SiDiscord className="mr-2" />
      <span>Discord</span>
    </Link>
  </Button>
</div>

</StartCard>
<StartCard isOutline>

## Build your data moat

- Retrieval API (coming soon) to query prompts/completions based on similarity to current prompt
- Exports of high-quality prompt/completions for fine tuning; e.g. use GPT-4 completions to train own Llama models

</StartCard>
</TwoCards>

## Langfuse is easy to get started with

Follow the full [quickstart instructions](/docs/get-started) to get started with langfuse.

<div className="lg:grid lg:grid-cols-2 lg:gap-7">
<div>

### 1. Capture execution data

<div className="flex gap-3 my-2 flex-wrap">
  <Button variant="outline" asChild>
    <Link href="/docs/reference">
      <BsCode className="mr-2 h-5 w-5" />
      <span>API</span>
    </Link>
  </Button>
  <Button variant="outline" asChild>
    <Link href="/docs/sdk/python">
      <SiPython className="mr-2" />
      <span>Python SDK</span>
    </Link>
  </Button>
  <Button variant="outline" asChild>
    <Link href="/docs/sdk/typescript">
      <SiTypescript className="mr-2" />
      <span>Typescript SDK</span>
    </Link>
  </Button>
</div>

```typescript filename="server.ts"
const trace = langfuse.trace({
  name: "chat-completion",
  metadata: {
    userId: "user__935d7d1d",
  },
});

const event = trace.event({
  name: "context-retrieved",
  output: "...",
});

const generationStartTime = new Date();

// const chatCompletion = await llm.respond(prompt);

const generation = trace.generation({
  startTime: generationStartTime,
  endTime: new Date(),
  name: "chat-completion",
  model: "gpt-3.5-turbo",
  modelParameters: {
    temperature: 0.9,
    maxTokens: 2000,
  },
  prompt: "Hello, how are you?",
  completion: "I'm fine, thanks. How are you?",
  usage: {
    promptTokens: 22,
    completionTokens: 28,
  },
});
```

</div>
<div>

### 2. Capture scores from user feedback

<div className="flex gap-3 my-2 flex-wrap">
  <Button variant="outline" asChild>
    <Link href="/docs/reference#restricted-access-with-public-key-client-side">
      <BsCode className="mr-2 h-5 w-5" />
      <span>API</span>
    </Link>
  </Button>
  <Button variant="outline" asChild>
    <Link href="/docs/sdk/typescript#frontend">
      <SiTypescript className="mr-2" />
      <span>Typescript SDK</span>
    </Link>
  </Button>
</div>

```typescript filename="UserFeedbackComponent.tsx"
// feedback button handler
const handleUserFeedback = async (value: number) =>
  await langfuseWeb.score({
    traceId: props.traceId,
    name: "user_feedback",
    value,
  });
```

</div>
</div>

### 3. Run langfuse server

<TwoCards className="mt-6">
<StartCard isDark>
  
### Langfuse cloud

- Fully managed
- Generous free tier to get started ([pricing](/docs/cloud#pricing))

<div className="flex gap-2 pt-3 flex-wrap">
  <Button asChild>
    <Link href="https://cloud.langfuse.com/auth/sign-up">
      <AiOutlineCloud className="mr-2 h-4 w-4" />
      <span>Sign up</span>
    </Link>
  </Button>
  <Button variant="ghost">Docs</Button>
</div>

</StartCard>
<Divider text="OR" />
<StartCard isOutline>
  
### Open source

- Full data ownership
- Within your own VPS

<div className="flex gap-2 pt-3 items-end flex-wrap">
  <Button asChild>
    <Link href="https://github.com/langfuse/langfuse/">
      <SiGithub className="mr-2" />
      <span>GitHub</span>
    </Link>
  </Button>
  <Button variant="secondary" asChild>
    <Link href="/docs/local">Local</Link>
  </Button>
  <Button variant="secondary">
    <Link href="/docs/self-host">Self-host</Link>
  </Button>
</div>

</StartCard>
</TwoCards>

<div className="h-24" />

## Get in touch

`langfuse` is being actively developed in open source together with the community. Join our [Discord](https://discord.gg/7NXusRtqYU)! Provide feedback, report bugs, or request features via GitHub issues. If you want to chat about your use case, reach out to us via email: contact@langfuse.com

## Roadmap

- [x] API server for collecting execution records, metadata, scores, and basic UI to explore data, runs locally or self-hosted
- [x] [SDKs](/docs/sdk) for tracing and scoring, typescript and python
- [x] [Docs](/docs) and walkthrough (v1) ([video](#), [NextJS example](https://github.com/langfuse/langfuse-demo), [Jupyter Notebook example](https://github.com/langfuse/langfuse-demo-python/blob/main/notebook.ipynb))
- [x] [Dashboard](/docs/interface#dashboard) to explore data
- [x] Authentication and authorization on project level in UI, API, SDKs
- [x] [langfuse cloud](/docs/cloud) to simplify setup
- [x] New typescript SDK with support for edge-runtimes, advanced nesting, and custom ids
- [ ] Integration with frameworks, e.g. langchain
- [ ] Wrapper of SDKs to simplify data integration, e.g. OpenAI, Anthropic
- [ ] Model-based evaluation of execution logs and LLM calls
- [ ] Pre-built SDK functions for best-practice scoring
- [ ] API to retrieve few-shot examples based on embedding similarity to current prompt
- [ ] LLM playground to interactively adapt prompts based on production examples

Want to see something else? Share your thoughts by creating a [GitHub issue](https://github.com/langfuse/langfuse/issues/new/choose) or on [Discord](https://discord.gg/7NXusRtqYU)

</div>

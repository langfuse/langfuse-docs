import { Card, Cards } from "nextra-theme-docs";
import { SiGithub, SiTypescript, SiPython, SiDiscord } from "react-icons/si";
import { AiOutlineCloud } from "react-icons/ai";
import { BsCode } from "react-icons/bs";
import { Button } from "/components/ui/button";
import Link from "next/link";
import { Buttons } from "/components/demoVideo";
import Image from "next/image";
import ycImg from "../src/yc.svg";

<div className="lg:container">
<div className="flex flex-col gap-9 mt-20 mb-10 md:mt-24 md:mb-20 lg:my-40 items-center">
  <div className="text-center text-4xl">
   `ðŸª¢ langfuse`
  </div>
  <div className="max-w-3xl">
    <div className="text-2xl text-center font-semibold mb-3">
      Open-source experimentation platform for LLM-based applications
    </div>
    <div className="text-lg text-center">
      Improve LLM-based applications by logging how they are used in production, capturing user feedback and identifying quality bottlenecks
    </div>
  </div>

  <div className="flex gap-3 justify-center flex-wrap">
    <Buttons />
  </div>
  <div>
    <div className="text-center text-gray-500 pb-3 pt-10">Backed by</div>
    <Image src={ycImg} height={30} />
  </div>
</div>

import { Feature } from "/components/feature";
import screenDashboard from "../src/screenshots_dashboard.png";
import screenSegmentTraces from "../src/screenshots_segment_traces.png";
import screenSideBySide from "../src/screenshots_side_by_side.png";

<Feature src={screenDashboard} imageAlt="test" imagePosition="left">

## Monitor

Stay on top of trends in output quality and adoption of your LLM-based features

Track token usage by application, user, and model (coming soon)

</Feature>

<Feature src={screenSegmentTraces} imageAlt="test" imagePosition="right">

## Understand and segment use cases

LLM-based features can be used in all sorts of ways. With langfuse, you can easily segment executions by user attributes, scores, and other dimensions.

Model-based clustering of prompts/completions to understand popular use cases (coming soon)

</Feature>

<Feature src={screenSideBySide} imageAlt="test" imagePosition="left">

## Identify quality bottlenecks

Compare low-quality executions side-by-side to gain a deep understanding on why chains did not perform well

Model-based comparison of executions with automated suggestions (coming soon)

</Feature>

import { TwoCards, StartCard, Divider } from "/components/cards";

<TwoCards className="my-24">
<StartCard isOutline>
  
## Open source

- Powered by NextJs and postgres
- Fully typed SDKs for Typescript and Python
- Simple self-hosting on your own infrastructure

<div className="flex gap-3 mt-3 flex-wrap">
  <Button asChild>
    <Link href="https://github.com/langfuse/langfuse/">
      <SiGithub className="mr-2" />
      GitHub
    </Link>
  </Button>
  <Button variant="secondary" asChild>
    <Link href="https://discord.gg/7NXusRtqYU">
      <SiDiscord className="mr-2" />
      Discord
    </Link>
  </Button>
</div>

</StartCard>
<StartCard isOutline>
  
## Build your data moat

- Retrieval API (coming soon) to query prompts/completions based on similarity to current prompt
- Exports of high-quality prompt/completions for fine tuning; e.g. use GPT-4 completions to train own Llama models

</StartCard>
</TwoCards>

## Langfuse is easy to get started with

Follow the full [quickstart instructions](/docs/get-started) to get started with langfuse.

<div className="lg:grid lg:grid-cols-2 lg:gap-7">
<div>

### 1. Capture execution data

<div className="flex gap-3 my-2 flex-wrap">
  <Button variant="outline" asChild>
    <Link href="/docs/reference">
      <BsCode className="mr-2 h-5 w-5" />
      API
    </Link>
  </Button>
  <Button variant="outline" asChild>
    <Link href="/docs/sdk/python">
      <SiPython className="mr-2" />
      Python SDK
    </Link>
  </Button>
  <Button variant="outline" asChild>
    <Link href="/docs/sdk/typescript">
      <SiTypescript className="mr-2" />
      Typescript SDK
    </Link>
  </Button>
</div>

```typescript filename="server.ts"
const startTime = new Date();

// const chatCompletion = await llm.respond(prompt);

const generation = await client.generations.log({
  traceId: trace.id,
  startTime: generationStart,
  endTime: new Date(),
  name: "chat-completion",
  model: "gpt-3.5-turbo",
  modelParameters: {
    temperature: 0.9,
    maxTokens: 2000,
  },
  prompt: "Hello, how are you?",
  completion: "I'm fine, thanks. How are you?",
  usage: {
    promptTokens: 22,
    completionTokens: 28,
  },
  metadata: {
    userId: "user__935d7d1d-8625-4ef4-8651-544613e7bd22",
  },
});
```

</div>
<div>

### 2. Capture scores from user feedback

<div className="flex gap-3 my-2 flex-wrap">
  <Button variant="outline" asChild>
    <Link href="/docs/reference#restricted-access-with-public-key-client-side">
      <BsCode className="mr-2 h-5 w-5" />
      API
    </Link>
  </Button>
  <Button variant="outline" asChild>
    <Link href="/docs/sdk/typescript#frontend">
      <SiTypescript className="mr-2" />
      Typescript SDK
    </Link>
  </Button>
</div>

```typescript filename="UserFeedbackComponent.tsx"
// feedback button handler
const handleUserFeedback = (value: number) =>
  langfuseClient.score.create({
    traceId: props.traceId,
    name: "user_feedback",
    value,
  });
```

</div>
</div>

### 3. Run langfuse server

<TwoCards className="mt-6">
<StartCard isDark>
  
### Langfuse cloud

- Fully managed
- Generous free tier to get started ([pricing](/docs/cloud#pricing))

<div className="flex gap-2 pt-3 flex-wrap">
  <Button asChild>
    <Link href="https://cal.com/marc-kl/langfuse-cloud">
      <AiOutlineCloud className="mr-2 h-4 w-4" />
      Early access
    </Link>
  </Button>
  <Button variant="ghost">Docs</Button>
</div>

</StartCard>
<Divider text="OR" />
<StartCard isOutline>
  
### Open source

- Full data ownership
- Within your own VPS

<div className="flex gap-2 pt-3 items-end flex-wrap">
  <Button asChild>
    <Link href="https://github.com/langfuse/langfuse/">
      <SiGithub className="mr-2" />
      GitHub
    </Link>
  </Button>
  <Button variant="secondary" asChild>
    <Link href="/docs/local">Local</Link>
  </Button>
  <Button variant="secondary">
    <Link href="/docs/self-host">Self-host</Link>
  </Button>
</div>

</StartCard>
</TwoCards>

<div className="h-24" />

## Why are we building `langfuse`?

1. **Situation:** LLMs introduce a _'black box'_ character to Software Engineering: outputs of LLM-based applications are unpredictable
2. **Problem:** Quality of application in production cannot be assured with testing before launching the feature
3. **Consequence:** Engineers need to embrace active learning from users in production to improve their LLM-based applications. Many teams build UX around their applications to make the imperfect outputs usable and to collect feedback from users.
4. **Need:** Holistic monitoring of what happens in production, linked to user feedback and other signals to understand how to improve the application.

## Get in touch

`langfuse` is being actively developed in open source together with the community. Join our [Discord](https://discord.gg/7NXusRtqYU)! Provide feedback, report bugs, or request features via GitHub issues. If you want to chat about your use case, reach out to us via email: contact@langfuse.com

## Roadmap

- [x] API server for collecting execution records, metadata, scores, and basic UI to explore data, runs locally or self-hosted
- [x] [SDKs](/docs/sdk) for tracing and scoring, typescript and python
- [x] [Docs](/docs) and walkthrough (v1) ([video](#), [NextJS example](https://github.com/langfuse/langfuse-demo), [Jupyter Notebook example](https://github.com/langfuse/langfuse-demo-python/blob/main/notebook.ipynb))
- [x] [Dashboard](/docs/interface#dashboard) to explore data
- [x] Authentication and authorization on project level in UI, API, SDKs
- [x] [langfuse cloud](/docs/cloud) to simplify setup
- [ ] Model-based evaluation of execution logs and LLM calls
- [ ] Pre-built SDK functions for best-practice scoring
- [ ] Tracing integration with frameworks, e.g. langchain
- [ ] API to retrieve few-shot examples based on embedding similarity to current prompt
- [ ] LLM playground to interactively adapt prompts based on production examples

Want to see something else? Share your thoughts by creating a [GitHub issue](https://github.com/langfuse/langfuse/issues/new/choose) or on [Discord](https://discord.gg/7NXusRtqYU)

</div>

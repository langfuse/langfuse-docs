---
title: Observability and Tracing for the Vercel AI SDK
sidebarTitle: Vercel AI SDK
logo: /images/integrations/vercel_ai_sdk_icon.png
description: Open source observability for Vercel AI SDK applications with Langfuse - metrics, evaluations, prompt management, playground, datasets.
---

import EnvJS from "@/components-mdx/env-js.mdx";

# Vercel AI SDK - Observability & Analytics

<Callout type="info">
  Telemetry is an experimental feature of the AI SDK and might change in the
  future.
</Callout>

The [Vercel AI SDK](https://sdk.vercel.ai/docs/introduction) is the TypeScript toolkit designed to help developers build AI-powered applications with React, Next.js, Vue, Svelte, Node.js, and more.

The SDK supports tracing via OpenTelemetry. With the `LangfuseExporter` you can collect these traces in Langfuse.

_Full Demo_

<CloudflareVideo
  videoId="b96948a6c378dbd66b0d44292d3524d2"
  title="Langfuse integration with Vercel AI SDK"
  aspectRatio={16 / 9}
  className="mt-6"
  posterStartTime={112}
/>

## Get Started

<Callout type="info">
  You need to be on `"ai": "^3.3.0"` to use the telemetry feature as it was
  recently added. In case of any issues, please update to the latest version as
  this feature is under active development.
</Callout>

<Steps>

### Enable Telemetry

While telemetry is experimental ([docs](https://sdk.vercel.ai/docs/ai-sdk-core/telemetry#enabling-telemetry)), you can enable it by setting `experimental_telemetry` on each request that you want to trace.

```ts {4}
const result = await generateText({
  model: openai("gpt-4-turbo"),
  prompt: "Write a short story about a cat.",
  experimental_telemetry: { isEnabled: true },
});
```

### Collect Traces With `LangfuseExporter`

First, install the required package:

```bash
npm install langfuse-vercel
```

To collect the traces in Langfuse, you need to add the `LangfuseExporter` to your application.

You can set the Langfuse credentials via environment variables or directly to the `LangfuseExporter` constructor. Create a project in the Langfuse dashboard to get your `secretKey` and `publicKey`.

<Tabs items={["Environment Variables", "Constructor"]}>

<Tab>

<EnvJS />

</Tab>

<Tab>

```ts
import { LangfuseExporter } from "langfuse-vercel";

new LangfuseExporter({
  secretKey: "sk-lf-...",
  publicKey: "pk-lf-...",
  baseUrl: "https://cloud.langfuse.com", // ðŸ‡ªðŸ‡º EU region
  // baseUrl: "https://us.cloud.langfuse.com", // ðŸ‡ºðŸ‡¸ US region
});
```

</Tab>
</Tabs>

Now you need to register this exporter via the OpenTelemetry SDK.

<Tabs items={["NextJs","Node.js"]}>
<Tab>

NextJS has support for OpenTelemetry instrumentation on the framework level. Learn more about it in the [Next.js OpenTelemetry guide](https://nextjs.org/docs/app/building-your-application/optimizing/open-telemetry).

Install dependencies:

```bash
npm install @vercel/otel langfuse-vercel @opentelemetry/api-logs@0.57.2 @opentelemetry/instrumentation@0.57.2 @opentelemetry/sdk-logs@0.57.2
```

<details>
<summary>Next.js &lt;15: enable OpenTelemetry instrumentation hook</summary>

Enable the `instrumentationHook` in your `next.config.js`:

```ts filename="next.config.js" {4}
/** @type {import('next').NextConfig} */
const nextConfig = {
  experimental: {
    instrumentationHook: true,
  },
};

module.exports = nextConfig;
```

</details>

Add `LangfuseExporter` to your instrumentation:

```ts filename="instrumentation.ts" {7}
import { registerOTel } from "@vercel/otel";
import { LangfuseExporter } from "langfuse-vercel";

export function register() {
  registerOTel({
    serviceName: "langfuse-vercel-ai-nextjs-example",
    traceExporter: new LangfuseExporter(),
  });
}
```

</Tab>
<Tab>

```bash
npm install langfuse-vercel @opentelemetry/sdk-node @opentelemetry/auto-instrumentations-node
```

```ts {5, 8, 31}
import { openai } from "@ai-sdk/openai";
import { generateText } from "ai";
import { NodeSDK } from "@opentelemetry/sdk-node";
import { getNodeAutoInstrumentations } from "@opentelemetry/auto-instrumentations-node";
import { LangfuseExporter } from "langfuse-vercel";

const sdk = new NodeSDK({
  traceExporter: new LangfuseExporter(),
  instrumentations: [getNodeAutoInstrumentations()],
});

sdk.start();

async function main() {
  const result = await generateText({
    model: openai("gpt-3.5-turbo"),
    maxTokens: 50,
    prompt: "Invent a new holiday and describe its traditions.",
    experimental_telemetry: {
      isEnabled: true,
      functionId: "my-awesome-function",
      metadata: {
        something: "custom",
        someOtherThing: "other-value",
      },
    },
  });

  console.log(result.text);

  await sdk.shutdown(); // Flushes the trace to Langfuse
}

main().catch(console.error);
```

</Tab>
</Tabs>

<Callout type="info" emoji="âœ¨">
  Done! All traces that contain AI SDK spans are automatically captured in
  Langfuse.
</Callout>

</Steps>

## Example Application

We created a sample repository ([langfuse/langfuse-vercel-ai-nextjs-example](https://github.com/langfuse/langfuse-vercel-ai-nextjs-example)) based on the [next-openai](https://github.com/vercel/ai/tree/main/examples/next-openai) template to showcase the integration of Langfuse with Next.js and Vercel AI SDK.

## Customization

### Group multiple executions in one trace

You can open a Langfuse trace and pass the trace ID to AI SDK calls to group multiple execution spans under one trace. The passed name in functionId will be the root span name of the respective execution.

```typescript
import { randomUUID } from "crypto";
import { Langfuse } from "langfuse";

const langfuse = new Langfuse();
const parentTraceId = randomUUID();

langfuse.trace({
  id: parentTraceId,
  name: "holiday-traditions",
});

for (let i = 0; i < 3; i++) {
  const result = await generateText({
    model: openai("gpt-3.5-turbo"),
    maxTokens: 50,
    prompt: "Invent a new holiday and describe its traditions.",
    experimental_telemetry: {
      isEnabled: true,
      functionId: `holiday-tradition-${i}`,
      metadata: {
        langfuseTraceId: parentTraceId,
        langfuseUpdateParent: false, // Do not update the parent trace with execution results
      },
    },
  });

  console.log(result.text);
}

await langfuse.flushAsync();
await sdk.shutdown();
```

The resulting trace hierarchy will be:

![Vercel nested trace in Langfuse UI](/images/docs/vercel-nested-trace.png)

### Disable Tracking of Input/Output

By default, the exporter captures the input and output of each request. You can disable this behavior by setting the `recordInputs` and `recordOutputs` options to `false`.

### Link Langfuse prompts to traces

You can link Langfuse prompts to Vercel AI SDK generations by setting the `langfusePrompt` property in the `metadata` field:

```typescript
import { generateText } from "ai";
import { Langfuse } from "langfuse";

const langfuse = new Langfuse();

const fetchedPrompt = await langfuse.getPrompt("my-prompt");

const result = await generateText({
  model: openai("gpt-4o"),
  prompt: fetchedPrompt.prompt,
  experimental_telemetry: {
    isEnabled: true,
    metadata: {
      langfusePrompt: fetchedPrompt.toJSON(),
    },
  },
});
```

The resulting generation will have the prompt linked to the trace in Langfuse. Learn more about prompts in Langfuse [here](/docs/prompt-management/get-started).

### Pass Custom Attributes

All of the `metadata` fields are automatically captured by the exporter. You can also pass custom trace attributes to e.g. track users or sessions.

```ts showLineNumbers {6-12}
const result = await generateText({
  model: openai("gpt-4-turbo"),
  prompt: "Write a short story about a cat.",
  experimental_telemetry: {
    isEnabled: true,
    functionId: "my-awesome-function", // Trace name
    metadata: {
      langfuseTraceId: "trace-123", // Langfuse trace
      tags: ["story", "cat"], // Custom tags
      userId: "user-123", // Langfuse user
      sessionId: "session-456", // Langfuse session
      foo: "bar", // Any custom attribute recorded in metadata
    },
  },
});
```

## Debugging

Enable the `debug` option to see the logs of the exporter.

```ts
new LangfuseExporter({ debug: true });
```

## Troubleshooting

- If you deploy on Vercel, Vercel's OpenTelemetry Collector is only available on Pro and Enterprise Plans ([docs](https://vercel.com/docs/otel)).
- You need to be on `"ai": "^3.3.0"` to use the telemetry feature as it was recently added. In case of any issues, please update to the latest version as this feature is under active development.
- On NextJS, make sure that you only have a single instrumentation file.
- If you use Sentry, make sure to either:
  - set `skipOpenTelemetrySetup: true` in Sentry.init
  - follow Sentry's docs on how to manually set up Sentry with OTEL

### Short-lived environments

In short-lived environments such as Vercel Cloud Functions, AWS Lambdas etc. you may force an export and flushing of spans after function execution and prior to environment freeze or shutdown by awaiting a call to the `forceFlush` method on the LangfuseExporter instance.

## Learn more

See the [telemetry documentation](https://sdk.vercel.ai/docs/ai-sdk-core/telemetry) of the Vercel AI SDK for more information.

## GitHub Discussions

import { GhDiscussionsPreview } from "@/components/gh-discussions/GhDiscussionsPreview";

<GhDiscussionsPreview labels={["integration-vercel-ai-sdk"]} />

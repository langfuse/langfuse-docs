---
title: Open Source LLM Observability via OpenTelemetry
description: How to connect Langfuse with OpenTelemetry to observe LLM applications.
sidebarTitle: OpenTelemetry
logo: /images/integrations/opentelemetry_icon.svg
---

# LLM Observability via OpenTelemetry

[OpenTelemetry](https://opentelemetry.io/) is a [CNCF](https://www.cncf.io/) project that provides a set of specifications, APIs, libraries that define a standard way to collect distributed traces and metrics from your application.

Langfuse can operate as an OpenTelemetry Backend to receive traces on the `/api/public/otel` (OTLP) endpoint. In addition to the [Langfuse SDKs](/docs/sdk/overview) and [native integrations](/integrations), this OpenTelemetry endpoint is designed to increase compatibility with frameworks, libraries, and languages beyond the SDKs and native integrations. Popular OpenTelemetry libraries include OpenLLMetry and OpenLIT which extend Language support of Langfuse tracing to Java and Go and cover frameworks such as AutoGen, Semantic Kernel, and more.

As the [Semantic Conventions](https://opentelemetry.io/docs/specs/semconv/attributes-registry/gen-ai/) for GenAI attributes on traces are still evolving, Langfuse maps the received OTel traces to the [Langfuse data model](/docs/tracing-data-model) and supports additional attributes that are popular in the OTel GenAI ecosystem ([property mapping](#property-mapping)). Please contribute to the discussion on [GitHub](https://github.com/orgs/langfuse/discussions/2509) if an integration does not work as expected or does not parse the correct attributes.

> **Using other OTEL-based tools?** If you're using Langfuse alongside other OpenTelemetry-based tools, you may run into conflicts. See [Using Langfuse with an Existing OpenTelemetry Setup](/faq/all/existing-otel-setup) for configuration guidance.

```mermaid
flowchart LR
  subgraph App["Application"]
    Logic["Operations, LLM calls, events"]
    OTel["OpenTelemetry instrumentation (SDKs, libraries)"]
    Logic --> OTel
  end

  OTel -->|"OTLP traces"| Endpoint["Langfuse OTLP endpoint /api/public/otel"]

  subgraph Langfuse["Langfuse"]
    Endpoint --> Map["OTel to Langfuse attribute mapping"]
    Map --> UI["Langfuse data model"]
  end

  classDef subgraphBox fill:none,stroke:#888,stroke-width:2px;
  class App,Langfuse subgraphBox;
```

## Ingestion Options

### OpenTelemetry native Langfuse SDK v3

The quickest path to start tracing with Langfuse is the new **OTEL-native Langfuse SDK v3**. The SDK is a thin layer on top of the official OpenTelemetry client that automatically converts emitted spans into rich Langfuse observations (spans, generations, events, and [other observation types](/docs/observability/features/observation-types)) and adds first-class helpers for LLM-specific features such as token usage, cost tracking, prompt linking, and scoring.

Because it lives in the shared OpenTelemetry context, spans from other OTEL-instrumented libraries can be exported to Langfuse too. By default, Langfuse focuses on LLM-relevant spans (Langfuse SDK spans, spans with `gen_ai.*` attributes, and known LLM instrumentors). To export everything, use a permissive custom filter as described in the [advanced SDK docs](/docs/observability/sdk/advanced-features#filtering-by-instrumentation-scope).

Get started by following the dedicated guide for the Python implementation here: [/docs/observability/sdk/overview](/docs/observability/sdk/overview).

### OpenTelemetry endpoint

Langfuse can receive traces on the `/api/public/otel` (OTLP) endpoint.

If you use a Collector that uses the OpenTelemetry SDK to export traces, you can use the following configuration:

```bash
OTEL_EXPORTER_OTLP_ENDPOINT="https://cloud.langfuse.com/api/public/otel" # ðŸ‡ªðŸ‡º EU data region
# OTEL_EXPORTER_OTLP_ENDPOINT="https://us.cloud.langfuse.com/api/public/otel" # ðŸ‡ºðŸ‡¸ US data region
# OTEL_EXPORTER_OTLP_ENDPOINT="http://localhost:3000/api/public/otel" # ðŸ  Local deployment (>= v3.22.0)

OTEL_EXPORTER_OTLP_HEADERS="Authorization=Basic ${AUTH_STRING}"
```

<Callout type="info">

Langfuse uses [Basic Auth](https://en.wikipedia.org/wiki/Basic_access_authentication) to authenticate requests.

You can use the following command to get the base64 encoded API keys (referred to as `AUTH_STRING`): `echo -n "pk-lf-1234567890:sk-lf-1234567890" | base64`.
For long API Keys on GNU systems, you may have to add `-w 0` at the end since `base64` auto-wraps columns.

</Callout>

<Callout type="info">

If your collector requires signal-specific environment variables, the trace endpoint is `/api/public/otel/v1/traces`.

```bash
OTEL_EXPORTER_OTLP_TRACES_ENDPOINT="https://cloud.langfuse.com/api/public/otel/v1/traces" # EU data region
# OTEL_EXPORTER_OTLP_TRACES_ENDPOINT="https://us.cloud.langfuse.com/api/public/otel/v1/traces" # US data region
```

</Callout>

Please note that Langfuse does not support `gRPC` for the OpenTelemetry endpoint. Please use `HTTP/protobuf` instead.

### Custom via OpenTelemetry SDKs

You can use the OpenTelemetry SDKs to directly export traces to Langfuse with the configuration mentioned above. Thereby, Language support of Langfuse is extended to other languages than the ones supported by the [Langfuse SDKs](/docs/sdk/overview) (Python and JS/TS).

As a reference, see [this example notebook](/docs/opentelemetry/example-python-sdk) on how to use the OpenTelemetry Python SDK to export traces to Langfuse.

### Use OpenTelemetry GenAI Instrumentation Libraries

Any OpenTelemetry compatible instrumentation can be used to export traces to Langfuse. Check out the following end-to-end examples of popular instrumentation SDKs to get started:

**Libraries**

- [OpenLIT](/docs/opentelemetry/example-openlit)
- [OpenLLMetry](/docs/opentelemetry/example-openllmetry)
- [Arize](/docs/opentelemetry/example-arize)
- [MLflow](/docs/opentelemetry/example-mlflow)

<details>

<summary>Comparison of OpenTelemetry Instrumentation Libraries</summary>

| Category   | Item                          | OpenLLMetry | openlit | Arize |
| ---------- | ----------------------------- | ----------- | ------- | ----- |
| LLMs       | AI21                          |             | âœ…      |       |
|            | Aleph Alpha                   | âœ…          |         |       |
|            | Amazon Bedrock                | âœ…          | âœ…      | âœ…    |
|            | Anthropic                     | âœ…          | âœ…      | âœ…    |
|            | Assembly AI                   |             | âœ…      |       |
|            | Azure AI Inference            |             | âœ…      |       |
|            | Azure OpenAI                  | âœ…          | âœ…      |       |
|            | Cohere                        | âœ…          | âœ…      |       |
|            | DeepSeek                      |             | âœ…      |       |
|            | ElevenLabs                    |             | âœ…      |       |
|            | GitHub Models                 |             | âœ…      |       |
|            | Google AI Studio              |             | âœ…      |       |
|            | Google Generative AI (Gemini) | âœ…          |         |       |
|            | Groq                          | âœ…          | âœ…      | âœ…    |
|            | HuggingFace                   | âœ…          | âœ…      | âœ…    |
|            | IBM Watsonx AI                | âœ…          |         |       |
|            | Mistral AI                    | âœ…          | âœ…      | âœ…    |
|            | NVIDIA NIM                    |             | âœ…      |       |
|            | Ollama                        | âœ…          | âœ…      |       |
|            | OpenAI                        | âœ…          | âœ…      | âœ…    |
|            | OLA Krutrim                   |             | âœ…      |       |
|            | Prem AI                       |             | âœ…      |       |
|            | Replicate                     | âœ…          |         |       |
|            | SageMaker (AWS)               | âœ…          |         |       |
|            | Titan ML                      |             | âœ…      |       |
|            | Together AI                   | âœ…          | âœ…      |       |
|            | vLLM                          |             | âœ…      |       |
|            | Vertex AI                     | âœ…          | âœ…      | âœ…    |
|            | xAI                           |             | âœ…      |       |
| Vector DBs | AstraDB                       |             | âœ…      |       |
|            | Chroma                        | âœ…          |         |       |
|            | ChromaDB                      |             | âœ…      |       |
|            | LanceDB                       | âœ…          |         |       |
|            | Marqo                         | âœ…          |         |       |
|            | Milvus                        | âœ…          | âœ…      |       |
|            | Pinecone                      | âœ…          | âœ…      |       |
|            | Qdrant                        | âœ…          | âœ…      |       |
|            | Weaviate                      | âœ…          |         |       |
| Frameworks | AutoGen / AG2                 |             | âœ…      | âœ…    |
|            | ControlFlow                   |             | âœ…      |       |
|            | CrewAI                        | âœ…          | âœ…      | âœ…    |
|            | Crawl4AI                      |             | âœ…      |       |
|            | Dynamiq                       |             | âœ…      |       |
|            | EmbedChain                    |             | âœ…      |       |
|            | FireCrawl                     |             | âœ…      |       |
|            | Guardrails AI                 |             | âœ…      | âœ…    |
|            | Haystack                      | âœ…          | âœ…      | âœ…    |
|            | Julep AI                      |             | âœ…      |       |
|            | LangChain                     | âœ…          | âœ…      | âœ…    |
|            | LlamaIndex                    | âœ…          | âœ…      | âœ…    |
|            | Letta                         |             | âœ…      |       |
|            | LiteLLM                       | âœ…          | âœ…      | âœ…    |
|            | mem0                          |             | âœ…      |       |
|            | MultiOn                       |             | âœ…      |       |
|            | Phidata                       |             | âœ…      |       |
|            | SwarmZero                     |             | âœ…      |       |
|            | LlamaIndex Workflows          |             |         | âœ…    |
|            | LangGraph                     |             |         | âœ…    |
|            | DSPy                          |             |         | âœ…    |
|            | Prompt flow                   |             |         | âœ…    |
|            | Instructor                    |             |         | âœ…    |
| GPUs       | AMD Radeon                    |             | âœ…      |       |
|            | NVIDIA                        |             | âœ…      |       |
| JavaScript | OpenAI Node SDK               |             |         | âœ…    |
|            | LangChain.js                  |             |         | âœ…    |
|            | Vercel AI SDK                 |             |         | âœ…    |

</details>

**Framework integrations powered by OpenTelemetry**

- [Hugging Face smolagents](/integrations/frameworks/smolagents)
- [CrewAI](/integrations/frameworks/crewai)
- [AutoGen](/integrations/frameworks/autogen)
- [Semantic Kernel](/integrations/frameworks/semantic-kernel)
- [Pydantic AI](/integrations/frameworks/pydantic-ai)
- [Spring AI](/integrations/frameworks/spring-ai)
- [LlamaIndex](/integrations/frameworks/llamaindex)
- [LlamaIndex Workflows](/integrations/frameworks/llamaindex-workflows)

### Export from OpenTelemetry Collector

```mermaid
graph LR
    A[Application with OTel Instrumentation] --> B[OpenTelemetry Collector]
    B --> C[Langfuse]
```

If you run an [OpenTelemetry Collector](https://opentelemetry.io/docs/collector), you can use the following configuration to export traces to Langfuse:

```yml
receivers:
  otlp:
    protocols:
    grpc:
      endpoint: 0.0.0.0:4317
    http:
      endpoint: 0.0.0.0:4318

processors:
  batch:
  memory_limiter:
    # 80% of maximum memory up to 2G
    limit_mib: 1500
    # 25% of limit up to 2G
    spike_limit_mib: 512
    check_interval: 5s

exporters:
  otlphttp/langfuse:
    endpoint: "https://cloud.langfuse.com/api/public/otel" # EU data region
    # endpoint: "https://us.cloud.langfuse.com/api/public/otel" # US data region
    headers:
      Authorization: "Basic ${AUTH_STRING}" # Previously encoded API keys

service:
  pipelines:
    traces:
      receivers: [otlp]
      processors: [memory_limiter, batch]
      exporters: [otlphttp/langfuse]
```

#### Filtering Spans sent to Langfuse

In case you want to selectively send OTel Spans to Langfuse, you can use the OTel Collector [filterprocessor](https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/processor/filterprocessor/README.md).
It enables you to filter spans based on attributes, span names, and more.
As this applies on a Span level, you may risk incomplete traces and should be careful when applying complex filter rules.
Langfuse also requires that a root span is sent to our backend to ensure that a trace is created correctly.

With the configuration below, you would only forward Spans which have a `gen_ai.system` attribute set to `openai`:

```yml
receivers:
  otlp:
    protocols:
    grpc:
      endpoint: 0.0.0.0:4317
    http:
      endpoint: 0.0.0.0:4318

processors:
  filter/openaisystem:
    error_mode: ignore
    traces:
      span:
        - 'attributes["gen_ai.system"] != "openai"'

exporters:
  otlphttp/langfuse:
    endpoint: "https://cloud.langfuse.com/api/public/otel" # EU data region
    # endpoint: "https://us.cloud.langfuse.com/api/public/otel" # US data region
    headers:
      Authorization: "Basic ${AUTH_STRING}" # Previously encoded API keys

service:
  pipelines:
    traces:
      receivers: [otlp]
      processors: [filter/openaisystem]
      exporters: [otlphttp/langfuse]
```

## Attribute Mapping [#property-mapping]

Langfuse aims to be compliant with the [OpenTelemetry GenAI semantic conventions](https://opentelemetry.io/docs/specs/semconv/gen-ai/gen-ai-agent-spans/) and support major LLM instrumentation frameworks.

Furthermore, Langfuse uses attributes within the `langfuse.*` namespace to map OpenTelemetry span attributes directly to the Langfuse data model. These specific attributes always take precedence over the generic OpenTelemetry conventions and are recommended for all users that are manually instrumenting their applications.

<Callout type="info">
  Please [raise an issue on GitHub](/issues) if any mapping or integration does
  not work as expected or does not parse the correct attributes.
</Callout>

Langfuse distinguishes between trace-level attributes and observation-level attributes. 
- [Trace-level attributes](#trace-level-attributes) represent shared context for an entire interaction. If Langfuse detects these attributes on a specific span, it will treat them as properties of the whole trace.
- [Observation-level attributes](#observation-level-attributes) describe individual steps within a trace. Langfuse keeps them on the observation level. 

### How Metadata Mapping Works [#metadata-mapping]

OpenTelemetry spans can carry arbitrary attributes. Langfuse handles these attributes differently depending on how they are named:

| Attribute Type | Where it Appears in Langfuse | Example |
|----------------|------------------------------|---------|
| **Explicit metadata mapping** | First-level key in `metadata` (filterable) | `langfuse.trace.metadata.customer_tier` â†’ `metadata.customer_tier` |
| **Unmapped OTel attributes** | Nested under `metadata.attributes` (catch-all) | `http.method` â†’ `metadata.attributes.http.method` |
| **Resource attributes** | Nested under `metadata.resourceAttributes` | `service.name` â†’ `metadata.resourceAttributes.service.name` |

<Callout type="info">
**Langfuse SDKs vs. standard OpenTelemetry SDKs**

- **Langfuse SDKs** provide utility functions (like `update()` with a `metadata` parameter) that automatically set the `langfuse.*.metadata.*` prefixed attributes. This means custom metadata appears at the first level and is filterable.
- **Standard OpenTelemetry SDKs** set attributes directly on spans. Unless you explicitly use the `langfuse.trace.metadata.*` or `langfuse.observation.metadata.*` prefix, these attributes end up in the `metadata.attributes` catch-all and are not directly filterable in Langfuse.
</Callout>

### Propagating Trace Attributes to All Spans [#propagating-attributes]

When using OpenTelemetry instrumentation to send traces to Langfuse, certain trace-level attributes should be propagated to **all spans** within a trace to enable accurate aggregations and filtering in Langfuse. These attributes include:

- `userId` (via `langfuse.user.id` or `user.id`)
- `sessionId` (via `langfuse.session.id` or `session.id`)
- `metadata` (via `langfuse.trace.metadata.*` for top-level metadata keys)
- `version` (via `langfuse.version`)
- `release` (via `langfuse.release`)
- `tags` (via `langfuse.trace.tags`)

Starting in a future release, Langfuse aggregation queries and filters will operate across individual observations (spans) rather than just at the trace level. This means that if you want to filter or aggregate by these attributes, they must be present on each span in the trace. We strongly recommend implementing this propagation now to ensure compatibility with future versions of Langfuse.

#### Using OpenTelemetry Baggage for Propagation

The recommended approach for propagating these attributes across all spans is to use [OpenTelemetry Baggage](https://opentelemetry.io/docs/concepts/signals/baggage/) with a `BaggageSpanProcessor`. Baggage is a built-in OpenTelemetry mechanism for context propagation that automatically copies specified key-value pairs to all spans within a trace context.

To implement this pattern:

1. Set the desired attributes as baggage entries at the beginning of your trace
2, Set the attributes on the currently active span
3. Configure a `BaggageSpanProcessor` in your OpenTelemetry setup to automatically copy baggage entries to span attributes
4. The processor will ensure all downstream spans in the trace context receive these attributes

For implementation details and code examples, refer to the OpenTelemetry documentation for [Python](https://pypi.org/project/opentelemetry-processor-baggage/) and [JavaScript](https://www.npmjs.com/package/@opentelemetry/baggage-span-processor).

<Callout type="warning">
  **Security Consideration**: OpenTelemetry baggage is propagated across service boundaries and to third-party APIs. **Do not include sensitive information** (passwords, API keys, personal data, etc.) in baggage when using this approach, as it will be transmitted to all downstream services.
</Callout>

#### Alternative: Using Langfuse SDKs

If you're using the [Langfuse SDKs](/docs/observability/sdk/overview) with OpenTelemetry integration, you can use the convenience methods `propagate_attributes()` (Python) or `propagateAttributes()` (TypeScript) which handle attribute propagation automatically. These methods provide a simpler interface and are the recommended approach when using Langfuse SDKs. 

### Trace-Level Attributes

These attributes are applied to the trace record in Langfuse. They may be set on any span in the trace.

| Langfuse Field | Description | Mapped from OTel Attribute |
|-------|-------------|------------------------|
| <a id="name"></a>`name` | The name of the trace. | â€¢ `langfuse.trace.name`: `string`<br/>â€¢ Span name of the root span |
| <a id="userId"></a>`userId` | The unique identifier for the end-user. | â€¢ `langfuse.user.id`: `string`<br/>â€¢ `user.id`: `string` |
| <a id="sessionId"></a>`sessionId` | The unique identifier for the user session. | â€¢ `langfuse.session.id`: `string`<br/>â€¢ `session.id`: `string` |
| <a id="release"></a>`release` | The release version of your application. | â€¢ `langfuse.release`: `string` |
| <a id="public"></a>`public` | A boolean flag to mark a trace as public, allowing it to be shared via a URL. | â€¢ `langfuse.trace.public`: `boolean` |
| <a id="tags"></a>`tags` | An array of strings to categorize or label the trace. | â€¢ `langfuse.trace.tags`: `string[]` |
| <a id="metadata"></a>`metadata` | A flexible object for storing any additional, unstructured data on the trace. See note below. | â€¢ `langfuse.trace.metadata.*`: `string`<br/>â€¢ Root span's observation metadata |
| <a id="input"></a>`input` | The initial input for the entire trace. | â€¢ `langfuse.trace.input`: `string`<br/>â€¢ Root span's observation input |
| <a id="output"></a>`output` | The final output for the entire trace. | â€¢ `langfuse.trace.output`: `string`<br/>â€¢ Root span's observation output |
| <a id="version"></a>`version` | The [version](/docs/observability/features/releases-and-versioning) of the trace, useful for tracking changes to your application logic. | â€¢ Root span's attributes mapped to `version` |
| <a id="environment"></a>`environment` | The deployment [environment](/docs/observability/features/environments) where the trace was generated. | â€¢ Root span's attributes mapped to `environment` |

<Callout type="info">
**Filtering by metadata key in Langfuse**

Langfuse only supports filtering on top-level keys within the `metadata` of an event.

By default, all OpenTelemetry attributes and resource attributes are mapped into an `attributes` and `resourceAttributes` key within `metadata` and are thus not queryable.

If you want to query on specific attributes, you can use the `langfuse.trace.metadata` prefix to map them to the top-level `metadata` object of the trace.
The following snippet will produce a filterable `user_name` property in the `metadata` object of the trace:

```python
with tracer.start_as_current_span("Langfuse Attributes") as span:
    span.set_attribute("langfuse.trace.metadata.user_name", "user-123")
```

</Callout>

### Observation-Level Attributes

These attributes are applied to individual observations (spans) within a trace ([data model](/docs/observability/data-model)).

| Langfuse Field | Description | Mapped from OTel Attribute |
|-------|-------------|------------------------|
| <a id="type"></a>`type` | The [type of observation](/docs/observability/features/observation-types). Any span with a `model` attribute is tracked as a `generation`. | â€¢ `langfuse.observation.type`: `"span" \| "generation" \| "event"`, default: `"span"` |
| <a id="level"></a>`level` | The [severity level](/docs/observability/features/log-levels) of the observation. | â€¢ `langfuse.observation.level`: `"DEBUG" \| "DEFAULT" \| "WARNING" \| "ERROR"`, default: `"DEFAULT"`<br/>â€¢ Inferred from `span.status.code` |
| <a id="statusMessage"></a>`statusMessage` | A message describing the status of the observation, often used for errors. | â€¢ `langfuse.observation.status_message`: `string`<br/>â€¢ Inferred from `span.status.message` |
| <a id="obs-metadata"></a>`metadata` | A flexible object for storing additional unstructured data. See note below. | â€¢ `langfuse.observation.metadata.*`: `string` |
| <a id="obs-input"></a>`input` | The input data for this specific observation. | â€¢ `langfuse.observation.input`: `(JSON) string`<br/>â€¢ `gen_ai.prompt`<br/>â€¢ `input.value` (OpenInference)<br/>â€¢ `mlflow.spanInputs` (MLFlow) |
| <a id="obs-output"></a>`output` | The output data from this specific observation. | â€¢ `langfuse.observation.output`: `(JSON) string`<br/>â€¢ `gen_ai.completion`<br/>â€¢ `output.value` (OpenInference)<br/>â€¢ `mlflow.spanOutputs` (MLFlow) |
| <a id="model"></a>`model` | The name of the generative model used. *Generation only.* | â€¢ `langfuse.observation.model.name`<br/>â€¢ `gen_ai.request.model`<br/>â€¢ `gen_ai.response.model`<br/>â€¢ `llm.model_name`<br/>â€¢ `model` |
| <a id="modelParameters"></a>`modelParameters` | Key-value pairs for model invocation settings. *Generation only.* | â€¢ `langfuse.observation.model.parameters`: `JSON string`<br/>â€¢ `gen_ai.request.*`<br/>â€¢ `llm.invocation_parameters.*` |
| <a id="usage"></a>`usage` | Token counts for the generation. *Generation only.* | â€¢ `langfuse.observation.usage_details`: `JSON string`<br/>â€¢ `gen_ai.usage.*`<br/>â€¢ `llm.token_count.*` |
| <a id="cost"></a>`cost` | The calculated cost in USD. *Generation only.* | â€¢ `langfuse.observation.cost_details`: `JSON string`<br/>â€¢ `gen_ai.usage.cost` (set as `total` key) |
| <a id="prompt"></a>`prompt` | The name of a versioned prompt managed in Langfuse. *Generation only.* | â€¢ `langfuse.observation.prompt.name`: `string`<br/>â€¢ `langfuse.observation.prompt.version`: `integer` |
| <a id="completionStartTime"></a>`completionStartTime` | Timestamp for when the model began generating. *Generation only.* | â€¢ `langfuse.observation.completion_start_time`: `ISO 8601 date string` |
| <a id="obs-version"></a>`version` | The [version](/docs/observability/features/releases-and-versioning) of the observation. | â€¢ `langfuse.version`: `string` |
| <a id="obs-environment"></a>`environment` | The deployment [environment](/docs/observability/features/environments) where the observation was generated. | â€¢ `langfuse.environment`<br/>â€¢ `deployment.environment`<br/>â€¢ `deployment.environment.name` |

<Callout type="info">
**Filtering by metadata key in Langfuse**

Langfuse only supports filtering on top-level keys within the `metadata` of an event.

By default, all OpenTelemetry attributes and resource attributes are mapped into an `attributes` and `resourceAttributes` key within `metadata` and are thus not queryable.

If you want to query on specific attributes, you can use the `langfuse.observation.metadata` prefix to map them to the top-level `metadata` object of the observation.
The following snippet will produce a filterable `user_name` property in the `metadata` object:

```python
with tracer.start_as_current_span("Langfuse Attributes") as span:
    span.set_attribute("langfuse.observation.metadata.user_name", "user-123")
```

</Callout>

## Troubleshooting

- If you encounter `4xx` errors while self-hosting Langfuse, please upgrade your deployment to the latest version. The OpenTelemetry endpoint was first introduced in Langfuse [v3.22.0](https://github.com/langfuse/langfuse/releases/tag/v3.22.0) and has seen significant improvements since then.
- Langfuse does not support `gRPC` for the OpenTelemetry endpoint. Please use `HTTP/protobuf` instead.

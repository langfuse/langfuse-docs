---
title: "Promptfoo Integration with Langfuse Prompt Management"
sidebarTitle: Promptfoo
logo: /images/integrations/promptfoo_icon.svg
description: "Integrate Promptfoo with Langfuse for LLM testing, red teaming, and prompt management. Use Promptfoo evaluations with Langfuse-managed prompts."
---

# Promptfoo Integration with Langfuse Prompt Management

## What is Promptfoo?

[Promptfoo](https://www.promptfoo.dev/) is an open-source LLM testing and evaluation platform that helps teams ensure the quality, security, and reliability of their LLM applications. It provides a comprehensive toolkit for testing prompts across different models and configurations.

Key features of Promptfoo:
- **LLM evaluation** — Run prompts against multiple models and compare outputs side by side with customizable metrics
- **Red teaming** — Automatically generate adversarial test cases to probe for vulnerabilities like prompt injection, jailbreaks, and harmful outputs
- **Vulnerability scanning** — Detect security issues in your LLM application before they reach production
- **CI/CD integration** — Run evaluations as part of your continuous integration pipeline
- **Model comparison** — Test the same prompts across GPT-4, Claude, Gemini, and other models to find the best fit
- **Custom assertions** — Define pass/fail criteria using JavaScript, Python, or LLM-as-a-Judge evaluators

## Langfuse + Promptfoo Integration

Integrate Promptfoo with Langfuse to take advantage of Langfuse's [prompt management](/docs/prompt-management/get-started) features during your Promptfoo evaluations. By using this integration, you can easily reference and manage prompts directly within Langfuse, making your LLM testing process more efficient and organized.

With Langfuse Prompt Management, you can:

- Update prompts without redeploying your application.
- Track and revert to previous prompt versions.
- Monitor and optimize prompt performance.
- Integrate prompts seamlessly with your tools and applications.
- Manage prompts via UI, SDKs, or API with minimal latency.

For more details, visit the [Langfuse Prompt Management documentation](/docs/prompt-management/get-started).

Thanks to the team at Promptfoo for developing this integration ([docs](https://www.promptfoo.dev/docs/integrations/langfuse)).

## Quick Start Guide

<Steps>

### Step 1: Set up Langfuse

1. Install the langfuse SDK: `npm install langfuse`
2. Visit [Langfuse](https://cloud.langfuse.com) to create an account.
3. Create a new project and copy your Langfuse API keys.
4. Set the `LANGFUSE_PUBLIC_KEY`, `LANGFUSE_SECRET_KEY`, and `LANGFUSE_BASE_URL` environment variables as desired.

### Step 2: Create a Langfuse Prompt

You can create, edit and manage your prompts via the Langfuse UI, SDKs, or API.

<LangTabs items={["Langfuse UI", "Python SDK", "JS/TS SDK"]}>
<Tab>

<Video
  src="https://static.langfuse.com/docs-videos/langfuse-ui-prompt-create.mp4"
  aspectRatio={1.24}
  gifStyle
  className="max-w-xl"
/>

</Tab>
<Tab>

```python
# Create a text prompt
langfuse.create_prompt(
    name="movie-critic",
    type="text",
    prompt="As a {{criticlevel}} movie critic, do you like {{movie}}?",
    labels=["production"],  # directly promote to production
    config={
        "model": "gpt-3.5-turbo",
        "temperature": 0.7,
        "supported_languages": ["en", "fr"],
    },  # optionally, add configs (e.g. model parameters or model tools) or tags
)

# Create a chat prompt
langfuse.create_prompt(
    name="movie-critic-chat",
    type="chat",
    prompt=[
      { role: "system", content: "You are an {{criticlevel}} movie critic" },
      { role: "user", content: "Do you like {{movie}}?" },
    ],
    labels=["production"],  # directly promote to production
    config={
        "model": "gpt-3.5-turbo",
        "temperature": 0.7,
        "supported_languages": ["en", "fr"],
    },  # optionally, add configs (e.g. model parameters or model tools) or tags
)
```

If you already have a prompt with the same name, the prompt will be added as a new version.

</Tab>

<Tab>

```typescript
// Create a text prompt
await langfuse.createPrompt({
  name: "movie-critic",
  type: "text",
  prompt: "As a {{criticlevel}} critic, do you like {{movie}}?",
  labels: ["production"], // directly promote to production
  config: {
    model: "gpt-3.5-turbo",
    temperature: 0.7,
    supported_languages: ["en", "fr"],
  }, // optionally, add configs (e.g. model parameters or model tools) or tags
});

// Create a chat prompt
await langfuse.createPrompt({
  name: "movie-critic-chat",
  type: "chat",
  prompt: [
    { role: "system", content: "You are an {{criticlevel}} movie critic" },
    { role: "user", content: "Do you like {{movie}}?" },
  ],
  labels: ["production"], // directly promote to production
  config: {
    model: "gpt-3.5-turbo",
    temperature: 0.7,
    supported_languages: ["en", "fr"],
  }, // optionally, add configs (e.g. model parameters or model tools) or tags
});
```

If you already have a prompt with the same name, the prompt will be added as a new version.

</Tab>

</LangTabs>

### Step 3: Reference Prompts in Promptfoo

Now you can use the prompts you created in Langfuse with Promptfoo. Use the `langfuse://` prefix for your prompts in your Promptfoo configuration file, followed by the Langfuse prompt ID and version. For example:

```yaml
prompts:
  - "langfuse://foo-bar-prompt:3"
providers:
  - openai:gpt-4o-mini
tests:
  - vars:
      # ...
```

Variables from your Promptfoo test cases will be automatically plugged into the Langfuse prompt as variables.

</Steps>

## GitHub Discussions

import { GhDiscussionsPreview } from "@/components/gh-discussions/GhDiscussionsPreview";

<GhDiscussionsPreview labels={["integration-promptfoo"]} />

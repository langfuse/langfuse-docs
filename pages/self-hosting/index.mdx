---
description: Step-by-step guide to run Langfuse on your local machine using docker compose.
---

import { Callout } from "nextra/components";

# Self-host Langfuse

<Callout type="info">
  Looking for a managed solution? Consider [Langfuse
  Cloud](https://cloud.langfuse.com) maintained by the Langfuse team.
</Callout>

Langfuse is open source and can be self-hosted using Docker. This section contains guides for different deployment scenarios.

## Deployment Options

The following options are available:

- Langfuse Cloud: A fully managed version of Langfuse that is hosted and maintained by the Langfuse team.
- Self-host Langfuse: Run Langfuse on your own infrastructure.
  - [Local](/self-hosting/local): Run Langfuse on your own machine in 5 minutes using Docker Compose.
  - [VM](/self-hosting/docker-compose): Run Langfuse on a single VM using Docker Compose.
  - [Docker](/self-hosting/docker)
  - [Kubernetes (Helm)](/self-hosting/kubernetes-helm): Run Langfuse on a Kubernetes cluster using Helm.

## Architecture

Langfuse only depends on open source components and can be deployed locally, on cloud infrastructure, or on-premises.

Please find below an overview of the architecture and the services that Langfuse consists of.

```mermaid
flowchart TB
    User["User/API/SDKs"]
    subgraph vpc["VPC"]
        Web["Web Server (langfuse/langfuse)"]
        Worker["Worker (langfuse/worker)"]
        DB[Postgres Database]
        Redis[Redis Cache/Queue]
        Clickhouse["Clickhouse Database (observability data)"]
        S3[S3/Blob Store]
    end
    LLM["LLM API / Gateway"]

    User --> Web
    Web --> S3
    Web --> DB
    Web --> Redis
    Web --> Clickhouse
    Web -.->|"optional for playground"| LLM

    Redis --> Worker
    Worker --> Clickhouse
    Worker --> DB
    Worker --> S3
    Worker -.->|"optional for evals"| LLM
```

Langfuse consists of multiple storage components and two Docker containers:

- **Langfuse Web**: The main web application serving the Langfuse UI and APIs.
- **Langfuse Worker**: A worker that asynchronously processes events.
- **Postgres**: The main database for transactional workloads.
- **Redis**: A fast in-memory data structure store. Used for queue and cache operations.
- **S3/Blob Store**: Object storage to persist all incoming events, multi-modal inputs, and large exports.
- **Clickhouse**: High-performance OLAP database which stores traces, observations, and scores.
- **LLM API / Gateway**: Some features depend on an external LLM API or gateway.

### Postgres Database

Langfuse requires a persistent Postgres database to store its state.
You can use a managed service on AWS, Azure, or GCP, or host it yourself.
At least version 12 is required.

### Redis

Langfuse uses Redis for caching and queue operations.
You can use a managed service on AWS, Azure, or GCP, or host it yourself.
At least version 7 is required and the instance must have `maxmemory-policy=noeviction` configured.
You may use Valkey instead of Redis, but there is no active support from the Langfuse team as of now.
See [Redis](/docs/deployment/v3/components/redis) for more details on how to connect Redis to Langfuse.

### S3/Blob Store

Langfuse requires an S3-compatible blob store to persist all incoming events, multi-modal inputs, and large exports.
You can use a managed service on AWS, or GCP, or host it yourself using MinIO.
Langfuse also has experimental support for Azure Blob Storage.
See [S3/Blob Store](/docs/deployment/v3/components/blobstorage) for more details on how to connect a blob store to Langfuse
and more details on Azure Blob Storage.

### Clickhouse

Langfuse uses Clickhouse as an OLAP database to store traces, observations, and scores.
You can use the managed service by Clickhouse Cloud, or host it yourself.
See [ClickHouse](/docs/deployment/v3/components/clickhouse) for more details on how to connect ClickHouse to Langfuse.

### LLM API / Gateway

Optionally, you can configure Langfuse to use an external LLM API or gateway for add-on features. Langfuse tracing does not need access to the LLM API as traces are captured client-side. Langfuse supports: OpenAI, Azure OpenAI, Anthropic, Google Vertex, and Amazon Bedrock. Via the OpenAI API, many other LLM services and proxies can be used.

## Support

If you experience any issues, please join us on [Discord](/discord) or contact the maintainers at support@langfuse.com.

For support with production deployments, the Langfuse team provides dedicated enterprise support. To learn more, reach out to enterprise@langfuse.com or [schedule a demo](/schedule-demo).

Alternatively, you may consider using [Langfuse Cloud](/docs/deployment/cloud), which is a fully managed version of Langfuse. You can find information about its security and privacy [here](/docs/data-security-privacy).

## FAQ

import { FaqPreview } from "@/components/faq/FaqPreview";

<FaqPreview tags={["self-hosting"]} />

## GitHub Discussions

import { GhDiscussionsPreview } from "@/components/gh-discussions/GhDiscussionsPreview";

<GhDiscussionsPreview labels={["self-hosting"]} />

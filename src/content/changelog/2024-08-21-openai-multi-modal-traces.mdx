---
date: 2024-08-21
title: Support for Multi-Modal Traces in Langfuse UI
description: Add multiple modalities to a single trace, including text and images. 
author: Marlies
ogCloudflareVideo: 59d62219e3b4f9e107bd3a685e2ebff0
---

import { ChangelogHeader } from "@/components/changelog/ChangelogHeader";
import { FileCode, BookOpen } from "lucide-react";

<ChangelogHeader />

Langfuse now supports rendering multi-modal traces. In addition to text, you may now include images (passed via URL). We follow OpenAI's content format convention. For more details, please refer to [Multi-Modal Tracing](/docs/tracing-features/multi-modality). 

## How to trace vision input in Langfuse?

If you use the [OpenAI Python SDK](/integrations/model-providers/openai-py), you can follow our [OpenAI cookbook](/guides/cookbook/integration_openai_sdk) which includes an example for tracing images in Langfuse.

<Cards num={3}>
  <Card
    title="Tracing: Multi-Modal Content"
    href="/docs/tracing-features/multi-modality"
    icon={<FileCode />}
  />
  <Card
    title="Cookbook: OpenAI Integration (Python)"
    href="/guides/cookbook/integration_openai_sdk"
    icon={<FileCode />}
  />
</Cards>
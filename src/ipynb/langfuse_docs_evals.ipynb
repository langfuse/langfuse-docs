{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Model based evals with Langfuse\n",
        "\n",
        "- [View as notebook on GitHub](https://github.com/langfuse/langfuse-docs/blob/main/src/ipynb/langfuse_docs_evals.ipynb)\n",
        "- [Open as notebook in Google Colab](http://colab.research.google.com/github/langfuse/langfuse-docs/blob/main/src/ipynb/langfuse_docs_evals.ipynb)\n",
        "\n",
        "\n",
        "Evaluating the quality of LLM features is very time consuming and error prone as it is very tiering and difficult to analyse large bodies of texts. This cookbook shows, how evals can be used to automate this. For this, we use the data we captured in [Langfuse](http://langfuse.com/) already.\n",
        "\n",
        "While this cookbook contains a Langchain example, it can easily be adjusted to use any other eval library.\n",
        "\n",
        "This cookbook follows three steps:\n",
        "1. Fetch `Generations` stored in Langfuse\n",
        "2. Evaluate these `Generations` using Langchain\n",
        "3. Submit results back to Langfuse as `Scores`\n",
        "\n",
        "\n",
        "----\n",
        "Not using Langfuse yet? Get started by capturing LLM events: [Python](https://langfuse.com/docs/integrations/sdk/python), [TS/JS](https://langfuse.com/docs/integrations/sdk/typescript)"
      ],
      "metadata": {
        "id": "SWL354n0DECo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup\n",
        "\n",
        "First you need to install `langfuse` and `langchain` via pip and then set the environment variables. The following table explains each of these:\n",
        "\n",
        "\n",
        "| Variable | Description |\n",
        "| --- | --- |\n",
        "| LF_PK | Public API Key found in the Langfuse UI\n",
        "| LF_SK | Secret API Key found in the Langfuse UI\n",
        "| LF_HOST | Secret API Key found in the Langfuse UI\n",
        "| EVAL_MODEL | OpenAI model used to evaluate each prompt/completion pair\n",
        "| OPENAI_API_KEY | OpenAI API Key found in the OpenAI UI. Beware that executing evals results in API calls and costs.\n",
        "| EVAL_TYPES | Dict of Langchain evals to be executed per `Generation` if set to `Teue`.\n",
        "\n",
        "\n",
        "\n",
        "Afterwards, we initialise the SDK, more information can be found [here](https://langfuse.com/docs/integrations/sdk/python#1-installation)."
      ],
      "metadata": {
        "id": "WbfTYaTkEu3G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install langfuse langchain openai"
      ],
      "metadata": {
        "id": "Qclwxd9LRPAL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['LF_PK'] = \"pk-lf-...\"\n",
        "os.environ['LF_SK'] = \"sk-lf-...\"\n",
        "os.environ['LF_HOST'] = \"https://cloud.langfuse.com\"\n",
        "\n",
        "os.environ['EVAL_MODEL'] = \"text-davinci-003\"\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"]='sk-...'\n",
        "\n",
        "EVAL_TYPES={\n",
        "    \"conciseness\": True,\n",
        "    \"relevance\": True,\n",
        "    \"coherence\": True,\n",
        "    \"harmfulness\": True,\n",
        "    \"maliciousness\": True,\n",
        "    \"helpfulness\": True,\n",
        "    \"controversiality\": True,\n",
        "    \"misogyny\": True,\n",
        "    \"criminality\": True,\n",
        "    \"insensitivity\": True\n",
        "}\n"
      ],
      "metadata": {
        "id": "CQhmQQpLRa1K"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "8viV4KT5RMjA"
      },
      "outputs": [],
      "source": [
        "from langfuse import Langfuse\n",
        "\n",
        "langfuse = Langfuse(os.environ.get(\"LF_PK\"), os.environ.get(\"LF_SK\"), os.environ.get(\"LF_HOST\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fetching data\n",
        "\n",
        "Below, we load all `Generations` from Langfuse filtered by name. The name can be submitted via our SDKs when capturing LLM calls. See [docs](https://langfuse.com/docs/integrations/sdk/python#generation) on how to do that."
      ],
      "metadata": {
        "id": "bjMZ1VLhF2Vv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fetch_all_pages(name, limit=50):\n",
        "    page = 1\n",
        "    all_data = []\n",
        "\n",
        "    while True:\n",
        "        response = langfuse.get_generations(name=name, limit=limit, page=page)\n",
        "        if not response.data:\n",
        "            break\n",
        "\n",
        "        all_data.extend(response.data)\n",
        "        page += 1\n",
        "\n",
        "    return all_data"
      ],
      "metadata": {
        "id": "3r3jOEX0RvXi"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generations = fetch_all_pages(name=\"OpenAI\")\n",
        "print(len(generations))"
      ],
      "metadata": {
        "id": "cAnLShvjBDBU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation\n",
        "\n",
        "In this section, we define a function to set up the Langchain eval based on the entries in `EVAL_TYPES`. More on the Langchain evals can be found [here](https://python.langchain.com/docs/guides/evaluation/)."
      ],
      "metadata": {
        "id": "hYM6UG_dGbb6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.evaluation import load_evaluator, EvaluatorType\n",
        "from langchain import PromptTemplate, OpenAI, LLMChain\n",
        "from langchain.evaluation.criteria import LabeledCriteriaEvalChain\n",
        "\n",
        "def get_evaluator_for_key(key: str):\n",
        "  llm = OpenAI(temperature=0, model=os.environ.get('EVAL_MODEL'))\n",
        "  if key == 'hallucination':\n",
        "    criteria = {\n",
        "        \"hallucination\": (\n",
        "            \"Does this submission contain information\"\n",
        "            \" not present in the input or reference?\"\n",
        "        ),\n",
        "    }\n",
        "    return LabeledCriteriaEvalChain.from_llm(\n",
        "        llm=llm,\n",
        "        criteria=criteria,\n",
        "    )\n",
        "  elif key == \"correctness\":\n",
        "    evaluator = LabeledCriteriaEvalChain.from_llm(\n",
        "      llm=llm,\n",
        "      criteria='correctness',\n",
        "   )\n",
        "  else:\n",
        "      return load_evaluator(\"criteria\", criteria=key, llm=llm)\n"
      ],
      "metadata": {
        "id": "7NijTmslvyK8"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Scoring\n",
        "\n",
        "In this section, we execute the evaluation for each `Generation` loaded above. Each score is provided to Langchain via the [scoring API](https://langfuse.com/docs/scores). In the Langfuse UI, you can filter Traces by `Scores` and look into the details for each.\n",
        "\n",
        "![Image of Trace](https://langfuse.com/images/docs/trace.jpg)\n"
      ],
      "metadata": {
        "id": "tzZZfztGdrIQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langfuse.model import InitialScore\n",
        "\n",
        "\n",
        "def execute_eval_and_score():\n",
        "\n",
        "  for generation in generations:\n",
        "    criteria = [key for key, value in EVAL_TYPES.items() if value]\n",
        "\n",
        "    for criterion in criteria:\n",
        "      print(criterion)\n",
        "      eval_result = get_evaluator_for_key(criterion).evaluate_strings(\n",
        "          prediction=generation.completion,\n",
        "          input=generation.prompt,\n",
        "      )\n",
        "      print(eval_result)\n",
        "\n",
        "      langfuse.score(InitialScore(name='conciseness', traceId=generation.trace_id, observationId=generation.id, value=eval_result[\"score\"], comment=eval_result['reasoning']))\n",
        "\n",
        "execute_eval_and_score()\n",
        "langfuse.flush()\n"
      ],
      "metadata": {
        "id": "qMa2OEtqvyGg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Get in touch\n",
        "\n",
        "Looking for a specific way to score your executions in Langfuse? Join the [Discord](https://langfuse.com/discord) and discuss your use case!"
      ],
      "metadata": {
        "id": "CkeLD_ciVD2w"
      }
    }
  ]
}
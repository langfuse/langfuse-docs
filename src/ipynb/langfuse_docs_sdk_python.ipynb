{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nJXInxopjbLA"
      },
      "source": [
        "---\n",
        "description: Fully async and typed Python SDK. Uses Pydantic objects for verification.\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mqBspBzuRk9C"
      },
      "source": [
        "# Python SDK\n",
        "\n",
        "[![PyPI](https://img.shields.io/pypi/v/langfuse?style=flat-square)](https://pypi.org/project/langfuse/)\n",
        "\n",
        "- [View as notebook on GitHub](https://github.com/langfuse/langfuse-docs/blob/main/src/ipynb/langfuse_docs_sdk_python.ipynb)\n",
        "- [Open as notebook in Google Colab](http://colab.research.google.com/github/langfuse/langfuse-docs/blob/main/src/ipynb/langfuse_docs_sdk_python.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qL7HhNyIYNwn"
      },
      "source": [
        "\n",
        "This section explains, how you can report LLM data to Langfuse while owning the APIs to your infrastructure such as LLM providers or databases.\n",
        "\n",
        "Using langchain? Use the [langchain integration](https://langfuse.com/docs/langchain)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lc6Uxbl3R5El"
      },
      "source": [
        "## 1. Initializing the client\n",
        "\n",
        "The Langfuse SDKs are hosted on the pypi index."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F21wZSUyKLzb"
      },
      "outputs": [],
      "source": [
        "%pip install langfuse"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fAupsw1pR_6q"
      },
      "source": [
        "Initialize the client with api keys and optionally your environment. In the example we are using the cloud environment which is also the default. The Python client can modify all entities in the Langfuse API and therefore requires the secret key."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "iDfYwZf4KUnY"
      },
      "outputs": [],
      "source": [
        "ENV_HOST = \"https://cloud.langfuse.com\"\n",
        "ENV_SECRET_KEY = \"sk-lf-1234567890\"\n",
        "ENV_PUBLIC_KEY = \"pk-lf-1234567890\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tObpj55ElZRp"
      },
      "source": [
        "#### Async\n",
        "Some of our users use async environments in e.g. FastAPI, others want to work in a synchronous environment. We are able to cater both by providing two types of clients. All the functions available via `langfuse_async` return a Coroutine and need to be awaited. Apart from that are the APIs identical."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "PuPgkTU476y4"
      },
      "outputs": [],
      "source": [
        "from langfuse import Langfuse, LangfuseAsync\n",
        "\n",
        "langfuse_sync = Langfuse(ENV_PUBLIC_KEY, ENV_SECRET_KEY, ENV_HOST)\n",
        "langfuse = LangfuseAsync(ENV_PUBLIC_KEY, ENV_SECRET_KEY, ENV_HOST)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CT4uaBm4SLvw"
      },
      "source": [
        "## 2. Trace execution of backend\n",
        "\n",
        "- Each backend execution is logged with a single `trace`.\n",
        "- Each trace can contain multiple `observations` to log the individual steps of the execution.\n",
        "  - Observations can be nested.\n",
        "  - Observations can be of different types\n",
        "    - `Events` are the basic building block. They are used to track discrete events in a trace.\n",
        "    - `Spans` represent durations of units of work in a trace.\n",
        "    - `Generations` are spans which are used to log generations of AI model. They contain additional metadata about the model and the prompt/completion and are specifically rendered in the langfuse UI."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3GjVFk7N9jZr"
      },
      "source": [
        "### Traces\n",
        "\n",
        "Traces are the top-level entity in the Langfuse API. They represent an execution flow in a LLM application usually triggered by an external event.\n",
        "\n",
        "Traces can be created and updated.\n",
        "\n",
        "`trace.create()` takes the following parameters:\n",
        "\n",
        "- `name` (optional): identifier of the trace. Useful for sorting/filtering in the UI.\n",
        "- `metadata` (optional): additional metadata of the trace. Can be any JSON object.\n",
        "- `externalId` (optional): the id of the execution in the external system. Useful for linking traces to external systems. Frequently used to create scores without having access to the Langfuse `traceId`.\n",
        "- `userId` (optional): the id of the user who triggered the execution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Z9Kxxjp004WD"
      },
      "outputs": [],
      "source": [
        "from langfuse.model import CreateTrace\n",
        "\n",
        "trace = await langfuse.trace(CreateTrace(\n",
        "    name = \"docs-retrieval\",\n",
        "    userId = \"user__935d7d1d-8625-4ef4-8651-544613e7bd22\",\n",
        "    metadata = {\n",
        "        \"env\": \"production\",\n",
        "        \"email\": \"user@langfuse.com\",\n",
        "    }\n",
        "))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UtWxwt3H90qF"
      },
      "source": [
        "### Span\n",
        "\n",
        "Spans represent durations of units of work in a trace. We generated convenient SDK functions for generic spans as well as LLM spans.\n",
        "\n",
        "`span.create()` take the following parameters:\n",
        "\n",
        "- `startTime` (optional): the time at which the span started. If no startTime is provided, the current time will be used.\n",
        "- `endTime` (optional): the time at which the span ended. Can also be set using `span.update()`.\n",
        "- `name` (optional): identifier of the span. Useful for sorting/filtering in the UI.\n",
        "- `metadata` (optional): additional metadata of the span. Can be any JSON object. Can also be set or updated using `span.update()`.\n",
        "- `level` (optional): the level of the event. Can be `DEBUG`, `DEFAULT`, `WARNING` or `ERROR`. Used for sorting/filtering of traces with elevated error levels and for highlighting in the UI.\n",
        "- `statusMessage` (optional): the status message of the event. Additional field for context of the event. E.g. the error message of an error event.\n",
        "- `input` (optional): the input to the span. Can be any JSON object.\n",
        "- `output` (optional): the output to the span. Can be any JSON object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "otJQPNC198Ti"
      },
      "outputs": [],
      "source": [
        "import datetime\n",
        "from langfuse.model import CreateSpan\n",
        "\n",
        "retrievalStartTime = datetime.datetime.now()\n",
        "\n",
        "# retrieveDocs = retrieveDoc()\n",
        "# ...\n",
        "\n",
        "span = await trace.span(CreateSpan(\n",
        "        name=\"embedding-search\",\n",
        "        startTime=retrievalStartTime,\n",
        "        endTime=datetime.datetime.now(),\n",
        "        metadata={\"database\": \"pinecone\"},\n",
        "        input = {'query': 'This document entails the OKR goals for ACME'},\n",
        "        output = {\"response\": \"[{'name': 'OKR Engineering', 'content': 'The engineering department defined the following OKR goals...'},{'name': 'OKR Marketing', 'content': 'The marketing department defined the following OKR goals...'}]\"}\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uNPQH8Nz-duo"
      },
      "source": [
        "### Generation\n",
        "\n",
        "Generations are used to log generations of AI model. They contain additional metadata about the model and the prompt/completion and are specifically rendered in the langfuse UI.\n",
        "\n",
        "`generation.log()` take the following parameters:\n",
        "\n",
        "- `startTime` (optional): the time at which the generation started.\n",
        "- `endTime` (optional): the time at which the generation ended.\n",
        "- `name` (optional): identifier of the generation. Useful for sorting/filtering in the UI.\n",
        "- `model` (optional): the name of the model used for the generation\n",
        "- `modelParameters` (optional): the parameters of the model used for the generation; can be any key-value pairs\n",
        "- `prompt` (optional): the prompt used for the generation; can be any string or JSON object (recommended for chat models or other models that use structured input)\n",
        "- `completion` (optional): the completion generated by the model\n",
        "- `usage` (optional): the usage of the model during the generation; takes two optional key-value pairs: `promptTokens` and `completionTokens`\n",
        "- `metadata` (optional): additional metadata of the generation. Can be any JSON object.\n",
        "- `level` (optional): the level of the event. Can be `DEBUG`, `DEFAULT`, `WARNING` or `ERROR`. Used for sorting/filtering of traces with elevated error levels and for highlighting in the UI.\n",
        "- `statusMessage` (optional): the status message of the event. Additional field for context of the event. E.g. the error message of an error event."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nJfTbXvNQ6iD",
        "outputId": "9329a2e8-a8cd-44c6-99bf-e5f3fe99792e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<langfuse.client.StatefulGenerationClientAsync at 0x7d811f537d30>"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langfuse.model import CreateGeneration, Usage\n",
        "\n",
        "import datetime\n",
        "\n",
        "generationStartTime = datetime.datetime.now()\n",
        "\n",
        "# chat_completion = openai.ChatCompletion.create(model=\"gpt-3.5-turbo\", messages=[{\"role\": \"user\", \"content\": \"Hello world\"}])\n",
        "# ...\n",
        "\n",
        "await trace.generation(CreateGeneration(\n",
        "    name=\"summary-generation\",\n",
        "    startTime=generationStartTime,\n",
        "    endTime=datetime.datetime.now(),\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    modelParameters={\"maxTokens\": \"1000\", \"temperature\": \"0.9\"},\n",
        "    prompt=[{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": \"Please generate a summary of the following documents \\nThe engineering department defined the following OKR goals...\\nThe marketing department defined the following OKR goals...\"}],\n",
        "    completion=\"The Q3 OKRs contain goals for multiple teams...\",\n",
        "    usage=Usage(promptTokens=50, completionTokens = 49),\n",
        "    metadata={\"interface\": \"whatsapp\"}\n",
        "))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wfzAYslz9Aks"
      },
      "source": [
        "### Events\n",
        "\n",
        "Events are used to track discrete events in a trace.\n",
        "\n",
        "- `startTime`: the time at which the event started.\n",
        "- `name` (optional): identifier of the event. Useful for sorting/filtering in the UI.\n",
        "- `metadata` (optional): additional metadata of the event. JSON object.\n",
        "- `level` (optional): the level of the event. Can be `DEBUG`, `DEFAULT`, `WARNING` or `ERROR`. Used for sorting/filtering of traces with elevated error levels and for highlighting in the UI.\n",
        "- `statusMessage` (optional): the status message of the event. Additional field for context of the event. E.g. the error message of an error event.\n",
        "- `input` (optional): the input to the event. Can be any JSON object.\n",
        "- `output` (optional): the output to the event. Can be any JSON object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "tuSjykFW9Iw1"
      },
      "outputs": [],
      "source": [
        "from langfuse.model import CreateEvent\n",
        "import datetime\n",
        "\n",
        "event = await span.event(CreateEvent(\n",
        "        name=\"chat-docs-retrieval\",\n",
        "        startTime=datetime.datetime.now(),\n",
        "        metadata={\"key\": \"value\"},\n",
        "        input = {\"key\": \"value\"},\n",
        "        output = {\"key\": \"value\"}\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u4h-gogK-YLh"
      },
      "source": [
        "`span.update()` take the following parameters:\n",
        "\n",
        "- `spanId`: the id of the span to update\n",
        "- `endTime` (optional): the time at which the span ended\n",
        "- `metadata` (optional): merges with existing metadata of the span. Can be any JSON object."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WDoIw_Wj7r3b"
      },
      "source": [
        "### Nesting of observations\n",
        "\n",
        "Nesting of observations is helpful to structure the trace in a hierarchical way. This is especially helpful for complex chains and agents.\n",
        "\n",
        "```\n",
        "Simple example\n",
        "- trace: chat-app-session\n",
        "  - span: chat-interaction\n",
        "    - event: get-user-profile\n",
        "    - generation: chat-completion\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "R0qJYfnw8BDb"
      },
      "outputs": [],
      "source": [
        "trace = await langfuse.trace(CreateTrace(name = \"chat-app-session\"))\n",
        "span = await trace.span(CreateSpan(name = \"chat-interaction\"))\n",
        "event = await span.event(CreateEvent(name = \"get-user-profile\"))\n",
        "generation = await span.generation(CreateGeneration(name = \"chat-completion\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EABnZymiSej8"
      },
      "source": [
        "## 3. Collect scores\n",
        "\n",
        "Scores are used to evaluate executions/traces. They are always attached to a single trace. If the score relates to a specific step of the trace, the score can optionally also be atatched to the observation to enable evaluating it specifically.\n",
        "\n",
        "- `traceId`: the id of the trace to which the score should be attached\n",
        "- `name`: identifier of the score, string\n",
        "- `value`: the value of the score; float; optional: scale it to e.g. 0..1 to make it comparable to other scores\n",
        "- `traceIdType` (optional): the type of the traceId. Can be `LANGFUSE` (default) or `EXTERNAL`. If `EXTERNAL` is used, the score will be attached to the trace with the given externalId.\n",
        "- `comment` (optional): additional context/explanation of the score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mj19Zby3SfT9",
        "outputId": "e0b54931-3417-4a2a-ff07-a6da2d0f11f3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<langfuse.client.StatefulClientAsync at 0x7d813b40e380>"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langfuse.model import CreateScore\n",
        "\n",
        "\n",
        "await trace.score(CreateScore(\n",
        "    name=\"user-explicit-feedback\",\n",
        "    value=1,\n",
        "    comment=\"I like how personalized the response is\"\n",
        "))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-q5aljyIoU42"
      },
      "source": [
        "### Flushing\n",
        "\n",
        "The Langfuse client executes network requests in the background so that it is not blocking your API handler in any way. When you close your application, we shut down gracefully and ensure everything is sent to our backend.\n",
        "\n",
        "Sometimes, for example in short-lived cloud functions, you want to ensure that the SDK sent everything to our backend. For this, you can use the `flush()` function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5jpWEosnINa4",
        "outputId": "f8dcb7a8-248d-4089-8378-6b194d6c3c04"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "None\n"
          ]
        }
      ],
      "source": [
        "await langfuse.flush() # returns a coroutine"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mqBspBzuRk9C"
      },
      "source": [
        "# Python SDK\n",
        "\n",
        "[![PyPI](https://img.shields.io/pypi/v/langfuse?style=flat-square)](https://pypi.org/project/langfuse/)\n",
        "\n",
        "- [View as notebook on GitHub](https://github.com/langfuse/langfuse-docs/blob/main/src/ipynb/langfuse_docs_sdk_python.ipynb)\n",
        "- [Open as notebook in Google Colab](http://colab.research.google.com/github/langfuse/langfuse-docs/blob/main/src/ipynb/langfuse_docs_sdk_python.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qL7HhNyIYNwn"
      },
      "source": [
        "\n",
        "This section explains, how you can report LLM data to Langfuse while owning the APIs to your infrastructure such as LLM providers or databases.\n",
        "\n",
        "Using langchain? Use the [langchain integration](https://langfuse.com/docs/langchain)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lc6Uxbl3R5El"
      },
      "source": [
        "## 1. Initializing the client\n",
        "\n",
        "The Langfuse SDKs are hosted on the pypi index."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F21wZSUyKLzb",
        "outputId": "60bc2595-1015-4897-e5c4-7f51ebbf7363"
      },
      "outputs": [],
      "source": [
        "%pip install langfuse"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fAupsw1pR_6q"
      },
      "source": [
        "Initialize the client with api keys and optionally your environment. In the example we are using the cloud environment which is also the default. The Python client can modify all entities in the Langfuse API and therefore requires the secret key."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "iDfYwZf4KUnY"
      },
      "outputs": [],
      "source": [
        "ENV_HOST = \"https://cloud.langfuse.com\"\n",
        "ENV_SECRET_KEY = \"sk-lf-...\"\n",
        "ENV_PUBLIC_KEY = \"pk-lf-...\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "PuPgkTU476y4"
      },
      "outputs": [],
      "source": [
        "from langfuse import Langfuse\n",
        "\n",
        "langfuse = Langfuse(ENV_PUBLIC_KEY, ENV_SECRET_KEY, ENV_HOST)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CT4uaBm4SLvw"
      },
      "source": [
        "## 2. Trace execution of backend\n",
        "\n",
        "- Each backend execution is logged with a single `trace`.\n",
        "- Each trace can contain multiple `observations` to log the individual steps of the execution.\n",
        "  - Observations can be nested.\n",
        "  - Observations can be of different types\n",
        "    - `Events` are the basic building block. They are used to track discrete events in a trace.\n",
        "    - `Spans` represent durations of units of work in a trace.\n",
        "    - `Generations` are spans which are used to log generations of AI model. They contain additional metadata about the model and the prompt/completion and are specifically rendered in the langfuse UI."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3GjVFk7N9jZr"
      },
      "source": [
        "### Traces\n",
        "\n",
        "Traces are the top-level entity in the Langfuse API. They represent an execution flow in a LLM application usually triggered by an external event.\n",
        "\n",
        "Traces can be created and updated.\n",
        "\n",
        "`trace.create()` takes the following parameters:\n",
        "\n",
        "- `name` (optional): identifier of the trace. Useful for sorting/filtering in the UI.\n",
        "- `metadata` (optional): additional metadata of the trace. Can be any JSON object.\n",
        "- `externalId` (optional): the id of the execution in the external system. Useful for linking traces to external systems. Frequently used to create scores without having access to the Langfuse `traceId`.\n",
        "- `userId` (optional): the id of the user who triggered the execution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Z9Kxxjp004WD"
      },
      "outputs": [],
      "source": [
        "from langfuse.api.model import CreateTrace\n",
        "\n",
        "trace = langfuse.trace(CreateTrace(\n",
        "    name = \"docs-retrieval\",\n",
        "    userId = \"user__935d7d1d-8625-4ef4-8651-544613e7bd22\",\n",
        "    metadata = {\n",
        "        \"env\": \"production\",\n",
        "        \"email\": \"user@langfuse.com\",\n",
        "    }\n",
        "))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UtWxwt3H90qF"
      },
      "source": [
        "### Span\n",
        "\n",
        "Spans represent durations of units of work in a trace. We generated convenient SDK functions for generic spans as well as LLM spans.\n",
        "\n",
        "`span.create()` take the following parameters:\n",
        "\n",
        "- `startTime` (optional): the time at which the span started. If no startTime is provided, the current time will be used.\n",
        "- `endTime` (optional): the time at which the span ended. Can also be set using `span.update()`.\n",
        "- `name` (optional): identifier of the span. Useful for sorting/filtering in the UI.\n",
        "- `metadata` (optional): additional metadata of the span. Can be any JSON object. Can also be set or updated using `span.update()`.\n",
        "- `level` (optional): the level of the event. Can be `DEBUG`, `DEFAULT`, `WARNING` or `ERROR`. Used for sorting/filtering of traces with elevated error levels and for highlighting in the UI.\n",
        "- `statusMessage` (optional): the status message of the event. Additional field for context of the event. E.g. the error message of an error event.\n",
        "- `input` (optional): the input to the span. Can be any JSON object.\n",
        "- `output` (optional): the output to the span. Can be any JSON object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "otJQPNC198Ti"
      },
      "outputs": [],
      "source": [
        "import datetime\n",
        "from langfuse.api.model import CreateSpan\n",
        "\n",
        "retrievalStartTime = datetime.datetime.now()\n",
        "\n",
        "# retrieveDocs = retrieveDoc()\n",
        "# ...\n",
        "\n",
        "span = trace.span(CreateSpan(\n",
        "        name=\"embedding-search\",\n",
        "        startTime=retrievalStartTime,\n",
        "        endTime=datetime.datetime.now(),\n",
        "        metadata={\"database\": \"pinecone\"},\n",
        "        input = {'query': 'This document entails the OKR goals for ACME'},\n",
        "        output = {\"response\": \"[{'name': 'OKR Engineering', 'content': 'The engineering department defined the following OKR goals...'},{'name': 'OKR Marketing', 'content': 'The marketing department defined the following OKR goals...'}]\"}\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uNPQH8Nz-duo"
      },
      "source": [
        "### Generation\n",
        "\n",
        "Generations are used to log generations of AI model. They contain additional metadata about the model and the prompt/completion and are specifically rendered in the langfuse UI.\n",
        "\n",
        "`generation.log()` take the following parameters:\n",
        "\n",
        "- `startTime` (optional): the time at which the generation started.\n",
        "- `endTime` (optional): the time at which the generation ended.\n",
        "- `name` (optional): identifier of the generation. Useful for sorting/filtering in the UI.\n",
        "- `model` (optional): the name of the model used for the generation\n",
        "- `modelParameters` (optional): the parameters of the model used for the generation; can be any key-value pairs\n",
        "- `prompt` (optional): the prompt used for the generation; can be any string or JSON object (recommended for chat models or other models that use structured input)\n",
        "- `completion` (optional): the completion generated by the model\n",
        "- `usage` (optional): the usage of the model during the generation; takes two optional key-value pairs: `promptTokens` and `completionTokens`\n",
        "- `metadata` (optional): additional metadata of the generation. Can be any JSON object.\n",
        "- `level` (optional): the level of the event. Can be `DEBUG`, `DEFAULT`, `WARNING` or `ERROR`. Used for sorting/filtering of traces with elevated error levels and for highlighting in the UI.\n",
        "- `statusMessage` (optional): the status message of the event. Additional field for context of the event. E.g. the error message of an error event."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nJfTbXvNQ6iD",
        "outputId": "a8ed1055-238b-4967-acee-ce026a53aa5b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<langfuse.client.StatefulGenerationClient at 0x7f5b914d52a0>"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langfuse.api.model import CreateGeneration, Usage\n",
        "import datetime\n",
        "\n",
        "generationStartTime = datetime.datetime.now()\n",
        "\n",
        "# chat_completion = openai.ChatCompletion.create(model=\"gpt-3.5-turbo\", messages=[{\"role\": \"user\", \"content\": \"Hello world\"}])\n",
        "# ...\n",
        "\n",
        "trace.generation(CreateGeneration(\n",
        "    name=\"summary-generation\",\n",
        "    startTime=generationStartTime,\n",
        "    endTime=datetime.datetime.now(),\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    modelParameters={\"maxTokens\": \"1000\", \"temperature\": \"0.9\"},\n",
        "    prompt=[{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": \"Please generate a summary of the following documents \\nThe engineering department defined the following OKR goals...\\nThe marketing department defined the following OKR goals...\"}],\n",
        "    completion=\"The Q3 OKRs contain goals for multiple teams...\",\n",
        "    usage=Usage(promptTokens=50, completionTokens = 49),\n",
        "    metadata={\"interface\": \"whatsapp\"}\n",
        "))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wfzAYslz9Aks"
      },
      "source": [
        "### Events\n",
        "\n",
        "Events are used to track discrete events in a trace.\n",
        "\n",
        "- `startTime`: the time at which the event started.\n",
        "- `name` (optional): identifier of the event. Useful for sorting/filtering in the UI.\n",
        "- `metadata` (optional): additional metadata of the event. JSON object.\n",
        "- `level` (optional): the level of the event. Can be `DEBUG`, `DEFAULT`, `WARNING` or `ERROR`. Used for sorting/filtering of traces with elevated error levels and for highlighting in the UI.\n",
        "- `statusMessage` (optional): the status message of the event. Additional field for context of the event. E.g. the error message of an error event.\n",
        "- `input` (optional): the input to the event. Can be any JSON object.\n",
        "- `output` (optional): the output to the event. Can be any JSON object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "tuSjykFW9Iw1"
      },
      "outputs": [],
      "source": [
        "from langfuse.api.model import CreateEvent\n",
        "import datetime\n",
        "\n",
        "event = span.event(CreateEvent(\n",
        "        name=\"chat-docs-retrieval\",\n",
        "        startTime=datetime.datetime.now(),\n",
        "        metadata={\"key\": \"value\"},\n",
        "        input = {\"key\": \"value\"},\n",
        "        output = {\"key\": \"value\"}\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u4h-gogK-YLh"
      },
      "source": [
        "`span.update()` take the following parameters:\n",
        "\n",
        "- `spanId`: the id of the span to update\n",
        "- `endTime` (optional): the time at which the span ended\n",
        "- `metadata` (optional): merges with existing metadata of the span. Can be any JSON object."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WDoIw_Wj7r3b"
      },
      "source": [
        "### Nesting of observations\n",
        "\n",
        "Nesting of observations is helpful to structure the trace in a hierarchical way. This is especially helpful for complex chains and agents.\n",
        "\n",
        "```\n",
        "Simple example\n",
        "- trace: chat-app-session\n",
        "  - span: chat-interaction\n",
        "    - event: get-user-profile\n",
        "    - generation: chat-completion\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "R0qJYfnw8BDb"
      },
      "outputs": [],
      "source": [
        "trace = langfuse.trace(CreateTrace(name = \"chat-app-session\"))\n",
        "span = trace.span(CreateSpan(name = \"chat-interaction\"))\n",
        "event = span.event(CreateEvent(name = \"get-user-profile\"))\n",
        "generation = span.generation(CreateGeneration(name = \"chat-completion\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EABnZymiSej8"
      },
      "source": [
        "## 3. Collect scores\n",
        "\n",
        "Scores are used to evaluate executions/traces. They are always attached to a single trace. If the score relates to a specific step of the trace, the score can optionally also be atatched to the observation to enable evaluating it specifically.\n",
        "\n",
        "- `traceId`: the id of the trace to which the score should be attached\n",
        "- `name`: identifier of the score, string\n",
        "- `value`: the value of the score; float; optional: scale it to e.g. 0..1 to make it comparable to other scores\n",
        "- `traceIdType` (optional): the type of the traceId. Can be `LANGFUSE` (default) or `EXTERNAL`. If `EXTERNAL` is used, the score will be attached to the trace with the given externalId.\n",
        "- `comment` (optional): additional context/explanation of the score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mj19Zby3SfT9",
        "outputId": "3ee4ec75-c29b-4ecd-d763-ce6a53e73d5c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<langfuse.client.StatefulClient at 0x7f5b914d6200>"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langfuse.api.model import CreateScore\n",
        "\n",
        "\n",
        "trace.score(CreateScore(\n",
        "    name=\"user-explicit-feedback\",\n",
        "    value=1,\n",
        "    comment=\"I like how personalized the response is\"\n",
        "))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kEh4aFFfIK5I",
        "outputId": "1d491464-7b55-434e-b578-a12321e456aa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'status': 'success'}\n"
          ]
        }
      ],
      "source": [
        "result = await langfuse.async_flush()\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-q5aljyIoU42"
      },
      "source": [
        "### Flushing\n",
        "\n",
        "The Langfuse client is built asynchronous to not add latency. Only when calling the flush function, the network requests to the langfuse backend will be executed.\n",
        "\n",
        "Langfuse offers two different fush functions. `async_flush` returns a coroutine and hence can be used in async contexts such as this Notebook. `flush` is a synchronous function and takes care of asynchronous code in the background and is blocking."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5jpWEosnINa4",
        "outputId": "2df9f734-0ebc-43e5-fc2c-21ed1bccb6fa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'status': 'success'}\n"
          ]
        }
      ],
      "source": [
        "# result = await client.flush()\n",
        "# returns a result and executes a coroutine in the background\n",
        "\n",
        "result = await langfuse.async_flush() # returns a coroutine\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DgtTX-Vd_Ac-"
      },
      "source": [
        "## Troubleshooting\n",
        "\n",
        "If you encounter any issue, we are happy to help on [Discord](https://discord.gg/7NXusRtqYU) or shoot us an email: help@langfuse.com"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
